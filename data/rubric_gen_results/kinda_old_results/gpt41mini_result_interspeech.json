[
  {
    "id": "adigwe24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7118,
      "completion_tokens": 183,
      "total_tokens": 7301
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 and Section 5.2",
          "Reasoning": "The authors used a parallel corpus of natural speech consisting of utterances taken from spontaneous conversation and their read-aloud counterparts from the same British English female speaker. Since these are natural speech recordings, they are human generated audio data afforded by recorded human voices."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 6.1 and Section 6.2",
          "Reasoning": "The authors trained a modified FastSpeech2 text-to-speech model and used it to synthesise speech with targeted prosodic modifications on eight predefined sets of feature values. The resulting synthetic speech samples were generated by the model and are thus model generated audio data."
        }
      ]
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7970,
      "completion_tokens": 238,
      "total_tokens": 8208
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4, Section 5.2, Section 6.2",
            "reasoning": "The annotators were 16 native English speakers recruited from University of Edinburgh, who performed perceptual judgements and interviews; they are general native speakers, not domain experts, and multiple participants were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 5.2, Section 6.2",
            "reasoning": "Explicit instructions were provided in Task 1 and Task 2 for participants explaining their tasks, e.g., distinguishing conversational vs read speech in pairs and rating conversationality for individual utterances, respectively."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper describes forced-choice tasks and rating with explanations, but does not detail any scoring rubrics or formalized rating criteria for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "No indication or description of annotation guideline examples or annotated sample examples provided to annotators was found in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9100,
      "completion_tokens": 312,
      "total_tokens": 9412
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4, Main Experimental Setup; Section 5, Task 1; Section 6, Task 2",
          "reasoning": "Quality assurance in this dataset involves 16 native English speakers from the University of Edinburgh, each individually listening to and evaluating audio samples in one-on-one sessions with an interviewer. The participants are members of the target demographic (native English speakers), and the interviewing procedure is conducted by presumably expert researchers. This process constitutes a rigorous human quality assurance by experts overseeing the collection and verification of listener judgments."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4, Main Experimental Setup; Section 5, Task 1; Section 6, Task 2",
          "reasoning": "The 16 human annotators (listeners) performing quality assurance on the data were native English speakers but there is no indication they possess subject matter expertise relevant to conversational speech or speech synthesis evaluation. They are described as naive listeners, thus they count as multiple human non-expert annotators providing quality assurance through their listening judgements."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8718,
      "completion_tokens": 492,
      "total_tokens": 9210
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The paper uses utterance pairs selected randomly from a parallel corpus of spontaneous conversation and their read counterparts, taken from the same British English Female speaker. Furthermore, 16 native English speakers were recruited for interviews and evaluations specifically for this study, indicating original human data collection and novel experimental data created by human subjects."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 6.1",
          "reasoning": "The study trained a modified FastSpeech2 TTS model to generate synthetic speech with controlled prosodic variations. They generated 64 synthetic samples with predefined prosodic variations to test listener perception. These synthetic speech samples generated by the trained TTS model represent new data created entirely by a machine learning model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The natural speech stimuli are sourced from an existing parallel corpus (read and spontaneous utterances by the same speaker) that was collected previously (as cited from previous work [16]). The paper compiles this data for use in their Task 1 experiments without major modifications."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though some modifications are done via prosodic manipulation in synthesis, these do not constitute adapted versions of existing natural speech data. Instead, synthetic speech is created anew from text using the TTS model; thus, the data is not merely modified from existing natural recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods are clearly specified and documented."
        }
      }
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9236,
      "completion_tokens": 346,
      "total_tokens": 9582
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 5 and 6",
          "reasoning": "The authors use the natural speech data from a parallel corpus of spontaneous and read utterances (Section 5.1) and synthetically generated prosodic variations (Section 6.1) exclusively for evaluation purposes, assessing listener perception of conversationality and measuring responses to natural and synthetic speech stimuli."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.4",
          "reasoning": "The dataset from the natural speech pairs is used to analyze perceptual dimensions and listener explanations on conversational speaking style through thematic analysis of qualitative interview data, providing insights into factors influencing conversationality judgments."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9959,
      "completion_tokens": 514,
      "total_tokens": 10473
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 5.1, 6.1",
          "reasoning": "The dataset introduced consists of utterances from a British English female speaker, including both spontaneous conversational speech and read-aloud speech in English, as explicitly stated in Section 5.1. Additionally, the synthetic speech used in Task 2 is trained and generated using the LJ Speech dataset, which contains English content (Section 6.1). Therefore, the datasets are monolingual with English as the only language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the FastSpeech2 TTS model and WaveGlow vocoder are referenced (Section 6.1), the dataset itself consists of speech utterances, not programming codes or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves speech utterances; there is no indication that it contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural English speech, with no mention of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English; therefore, unknown does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data (English), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7177,
      "completion_tokens": 215,
      "total_tokens": 7392
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in paper",
          "reasoning": "The paper does not provide any link or mention of publicly available code repositories related to the dataset creation, data collection, preprocessing, or generation. The datasets used in Task 1 and Task 2 rely on existing datasets or synthetic speech generated using published models, but no code for constructing or processing the datasets introduced in this paper is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 5.1 and 6.1",
          "reasoning": "The paper documents the datasets used: Task 1 uses a parallel corpus of conversational vs read utterances from a British English female speaker, described in Section 5.1, including selection criteria and control measures. Task 2 uses a modified FastSpeech2 model trained on LJ Speech with specified prosodic controls, with details provided in Section 6.1 including description of prosodic variation parameters. This documentation is sufficient to understand dataset construction and experiment design for reproducibility purposes."
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8369,
      "completion_tokens": 177,
      "total_tokens": 8546
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The new dataset introduced and used for evaluation is AfriSpeech-200, a 200-hour Pan-African accented English speech corpus containing clinical and general domain speech with audio recordings collected from over 2,300 unique speakers with 120 accents from over 10 African countries, indicating the audio data was human recorded speech."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The AfriSpeech-200 dataset includes ground truth transcripts associated with the audio clips. These transcripts are human created or human recorded text corresponding to the spoken audio, representing the human generated textual modality accompanying the audio."
        }
      ]
    }
  },
  {
    "id": "afonja24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9221,
      "completion_tokens": 213,
      "total_tokens": 9434
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.3.1",
            "reasoning": "The ground truth medical named entities were automatically extracted from the transcripts using Amazon Comprehend Medical, a commercially available medical NER model, so the annotation process was performed by an AI model rather than humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no evidence in the paper that specific annotation instructions were provided to any human annotators since the extraction was done automatically via a commercial model."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation guidelines related to manual annotation for the medical named entity extraction; as annotations were automated, no rubrics were provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples or sample annotated data are presented or described for medical named entity annotation since it was performed by an automated system."
          }
        }
      ]
    }
  },
  {
    "id": "afonja24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10351,
      "completion_tokens": 392,
      "total_tokens": 10743
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3.1 Extracting Ground Truth Entities",
          "reasoning": "The dataset was not manually annotated with medical named entities. Instead, a commercially available medical NER model (Amazon Comprehend Medical) was used to automatically extract medical named entities from the ground truth transcripts, serving as silver annotations. This indicates that an AI model was used as the primary quality assurance method for dataset annotation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3.3 MedTextAlign: Extracting Predicted Named Entities",
          "reasoning": "The authors developed MedTextAlign, a fuzzy string matching algorithm, to align predicted medical named entities to ground truth entities. This algorithm uses automated verification techniques such as longest common character subsequence to evaluate ASR errors. This process constitutes an automated verification technique used as quality assurance specifically to validate and match entities in predicted transcripts."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the use of an AI model for generating silver annotations and an automated alignment algorithm, indicating that quality assurance processes are documented and applied."
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9969,
      "completion_tokens": 327,
      "total_tokens": 10296
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset AfriSpeech-200 is described as a 200-hour Pan-African accented English speech corpus collected from over 2,300 unique human speakers across more than 10 African countries, containing clinical and general domain speech. This indicates that the speech data was generated by human speakers and recorded, making it original content created by humans from scratch, not derived or translated from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper suggests the dataset was generated by AI or machine learning models. The speech data originates from human speakers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources without modification; it is a newly recorded corpus from human speech."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications or adaptations; rather, it is newly recorded speech."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the origin and method of data generation as human speech corpus collection, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10487,
      "completion_tokens": 292,
      "total_tokens": 10779
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2, Section 5.5",
          "reasoning": "The AfriSpeech-200 dataset is used to fine-tune pretrained ASR models in supervised learning settings to improve performance on accented medical speech, as detailed in the fine-tuning experiments and results sections."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 5, Table 4",
          "reasoning": "The dataset is employed for benchmarking and evaluating various open-source and commercial ASR models, assessing metrics such as WER, medical WER, medical CER, and medical named entity recall on accented clinical speech."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 to 5.5",
          "reasoning": "The dataset facilitates detailed analysis of model performance trends, error types, and impact of fine-tuning across entity categories and different ASR architectures."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11210,
      "completion_tokens": 542,
      "total_tokens": 11752
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset consists of English speech with African accents, but there is no mention of multiple human languages within the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset contains only English speech, despite various African accents and dialects, no indication of exactly two human languages present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The AfriSpeech-200 dataset is described as a Pan-African accented English speech corpus, indicating all data is in English spoken with various accents, i.e., monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "There is no data in any single non-English language; the dataset only includes accented English speech."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of speech data with transcripts, no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are present in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech only; there is no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or indication of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset explicitly states it is English speech with African accents; the language is known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains linguistic content (English speech)."
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8428,
      "completion_tokens": 175,
      "total_tokens": 8603
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any link, repository, or mention of the code related to the collection, creation, or preprocessing of the AfriSpeech-200 dataset or any other new dataset component. The dataset is acknowledged as contributed by Intron Health, but no code repository is referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper provides a detailed description of the AfriSpeech-200 dataset in Section 3.1, including number of hours, number of accents, speakers, dataset splits, and types of medical named entities considered. While the dataset creation pipeline or recording setup is not exhaustively described, sufficient dataset statistics and composition are provided to offer insight into its contents."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9447,
      "completion_tokens": 176,
      "total_tokens": 9623
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. VoxSim Dataset - Data source, Listening test procedure, Quality control",
          "Reasoning": "The VoxSim dataset consists of approximately 70k human perceptual speaker similarity ratings for 41k pairs of audio utterances sampled from the VoxCeleb1 dataset. The VoxCeleb1 utterances are speech segments extracted from human recordings in YouTube videos. The similarity scores are collected from twelve human evaluators via a listening test, thus the data modality is audio and the origin involves human-generated utterances recorded by humans as well as human involvement in the subjective rating process. The paper explicitly states the evaluation of voice similarity through listening tests by human evaluators, confirming human involvement. It is not model-generated or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10299,
      "completion_tokens": 232,
      "total_tokens": 10531
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2, Listening test procedure",
            "reasoning": "Twelve evaluators participated in the listening test to evaluate speaker similarity scores, indicating multiple human annotators with expertise or specific evaluation training were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2, Listening test procedure",
            "reasoning": "Evaluators were instructed to rate speaker similarity on a 6-point Likert scale and to rate evenly across scale points, focusing on voice characteristics regardless of utterance content, language, or acoustic environment, demonstrating clear annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2, Listening test procedure",
            "reasoning": "The annotation used a specified 6-point Likert scale from 'Definitely different speakers' to 'Definitely the same speaker', serving as a rubric for consistent scoring."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2, Figure 2",
            "reasoning": "An example annotation page for the listening test is shown in Figure 2, providing concrete examples to guide annotators during the task."
          }
        }
      ]
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11429,
      "completion_tokens": 429,
      "total_tokens": 11858
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human annotator with subject matter expertise conducting quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human experts performed quality assurance. The annotators are described as 'twelve evaluators' with no explicit claim of subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information supports that a single non-expert evaluator conducted QA."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2 VoxSim Dataset, Listening test procedure and Quality control paragraphs",
          "reasoning": "The quality assurance was carried out by twelve human evaluators performing perceptual similarity ratings. The paper does not identify them as experts or having specialized knowledge in speaker recognition, implying they are non-experts. Multiple non-expert human annotators participated in the listening tests and quality was assured by monitoring their evaluation consistency (EER) and excluding outlier ratings."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI models were used as judges in the quality assurance process of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While min-max normalization and EER computation were used to monitor evaluators, these are standard verification metrics rather than a full automatic QA process verifying labels. The primary QA was human-based."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a quality assurance process involving multiple human evaluators. Thus, QA was applied and documented."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11047,
      "completion_tokens": 492,
      "total_tokens": 11539
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (VoxSim Dataset), particularly sections 'Listening test procedure' and 'Quality control'",
          "reasoning": "The paper describes collecting nearly 70,000 speaker similarity scores through a listening test performed by twelve human evaluators who rated pairs of utterances sampled from the existing VoxCeleb1 dataset. This subjective evaluation by humans is original content created from scratch, not translated or adapted from prior material, as it involves new perceptual ratings obtained through controlled listening tests."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models; the voice similarity ratings are based on human listening tests, not model-generated content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any translation from another language was involved in producing the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine-based language translation is involved or mentioned in the data collection or generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (VoxSim Dataset) 'Data source' paragraph",
          "reasoning": "The utterances themselves are sourced from the existing VoxCeleb1 dataset, meaning the audio data is collected from an existing corpus without modification. The new dataset collates these utterances into pairs for evaluation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 (VoxSim Dataset) 'Listening test procedure', 'Quality control', and 'Data statistics'",
          "reasoning": "The dataset is derived by taking existing audio utterances from VoxCeleb1 and creating pairs, followed by collecting new speaker similarity ratings via human perception, which transforms and enriches the original data with new annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data source and method of generation, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11565,
      "completion_tokens": 471,
      "total_tokens": 12036
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 3.1",
          "reasoning": "The paper describes training the SVSNet model from scratch on the VoxSim dataset (Section 4.1). This shows that VoxSim is used to train at least one model from randomly initialized parameters, as SVSNet is not pre-trained and trained purely on VoxSim data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 3.1",
          "reasoning": "The paper clearly states that ECAPA-TDNN and WavLM-ECAPA models are fine-tuned from pre-trained speaker recognition models using the VoxSim dataset (Section 4.1). Thus, VoxSim is used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes no mention or evidence of using VoxSim for reinforcement learning based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Table 3",
          "reasoning": "The VoxSim dataset test set is used for evaluating the performance of speaker similarity prediction models, as shown in the results reported in Table 3 and discussed in Section 4.1. This demonstrates its use as an evaluation benchmark."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 and Figure 3",
          "reasoning": "The paper includes qualitative analysis such as t-SNE visualization of speaker embeddings extracted by the model trained using VoxSim to study characteristics of speaker similarity (Section 4.1). This evidences the use of VoxSim for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that VoxSim serves as a knowledge base or is used for retrieval-augmented generation or similar augmentation tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset VoxSim is actively used for training (both from scratch and fine-tuning), evaluation, and analysis, as explicitly detailed in the paper."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12288,
      "completion_tokens": 520,
      "total_tokens": 12808
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2: VoxSim Dataset - Data source",
          "reasoning": "The VoxSim dataset is derived from VoxCeleb1, which consists of utterances from speakers of a wide range of nationalities and languages. The paper explicitly states that the utterances contain various languages and diverse linguistic contexts, indicating that the dataset includes more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two languages, so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not restricted to only English; it contains multiple languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple non-English languages and English, so it is not monolingual in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech utterances and speaker similarity ratings, with no mention of programming or structured code-related content in the data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of mathematical or formal logical expressions or symbolic representations within the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only; biological or non-human communication is not involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language contents are documented and specified as diverse human languages from VoxCeleb1 speakers."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain human language utterances."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9506,
      "completion_tokens": 223,
      "total_tokens": 9729
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not mention the availability of any code repository or provide links related to code for dataset construction or processing. It only mentions the data source (VoxCeleb1) and describes the data collection and annotation process, but there is no explicit statement or URL indicating that code used for generating or processing the VoxSim dataset is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: VoxSim Dataset, notably 'Data source', 'Listening test procedure', 'Quality control', and 'Data statistics'",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including the data source (VoxCeleb1), the method for creating utterance pairs, the listening test procedure with 12 evaluators and the 6-point Likert scale used, quality control steps such as evaluating the EER and outlier removal, and dataset statistics including train/test splits. This comprehensive information documents the dataset creation sufficiently for reproducibility and understanding."
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10571,
      "completion_tokens": 237,
      "total_tokens": 10808
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Dataset",
          "Reasoning": "The DEFCOMM-DB dataset is composed of video recordings collected from YouTube showing real-world English-speaking interactions featuring defensive communication types. The videos are genuine, non-acted dialogues captured by human-operated cameras and publicly available, thus making the video modality human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Dataset",
          "Reasoning": "From each collected video, the audio stream is extracted representing the speech of participants in natural defensive communication settings. The audio is directly derived from the videos recorded by humans, meaning the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Dataset",
          "Reasoning": "Textual modality consists of speech transcriptions obtained from the video recordings. Though not explicitly detailed, these transcriptions are clearly based on human speech content, thus originating from human-generated data rather than model-generated."
        }
      ]
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11423,
      "completion_tokens": 219,
      "total_tokens": 11642
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2",
            "reasoning": "The dataset was annotated by 11 participants including AI researchers, a master's student in psychology, and undergraduates from social science, psychology, computer science, economics, and management. Given the variety of backgrounds and that most annotators are not declared as experts in defensive communication, they are considered multiple human non-experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Annotators were provided with verbal and written instructions explaining the study purpose, detailed descriptions of the four classes, and technical information about labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not mention any formal scoring rubrics or detailed rating scales used in the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Annotators received examples of videos featuring targeted reactions to help guide their labeling."
          }
        }
      ]
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12553,
      "completion_tokens": 311,
      "total_tokens": 12864
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotation and Section 2.2.1 Inter-Annotator Agreement",
          "reasoning": "The dataset annotations were performed by 11 human annotators, including four AI researchers and a master's student in psychology, indicating multiple human experts participated in the quality assurance. The paper describes training and instructions given to annotators and reports Krippendorf's alpha to assess agreement, showing a rigorous QA process with multiple experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not done by a single non-expert; multiple annotators with expertise were involved."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some annotators were undergraduates without explicit expert backgrounds, the group includes multiple experts and researchers, and thus the QA is not solely conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not performed by an AI model judging annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic verification was described for quality assurance of the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a clear QA process with multiple expert annotators."
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12171,
      "completion_tokens": 360,
      "total_tokens": 12531
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of existing real-world video recordings sourced from YouTube, not created from scratch by humans specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data consists of real video recordings from online sources and is not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the data being translated from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate or process the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The DefComm-DB dataset was collected from existing YouTube video recordings of genuine, non-acted dialogues from various real-world settings without significant modifications to the original data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though processed and annotated, the dataset is not described as transformed or adapted data derived from existing sources in a significant manner; it mainly consists of collected videos."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the source and method of data generation."
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12689,
      "completion_tokens": 313,
      "total_tokens": 13002
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Settings)",
          "reasoning": "The DefComm-DB dataset is used to train Support Vector Machine models with features extracted from audio and textual modalities. The models are trained in a supervised manner to classify different forms of defensive communication, indicating supervised fine-tuning on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Settings and Results)",
          "reasoning": "The dataset is partitioned into train, development, and test sets, and the models' performance is evaluated on the unseen test set using metrics like Unweighted Average Recall (UAR). Thus, DefComm-DB is used for evaluation and benchmarking of classification methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset) and Section 2.2.1 (Inter-Annotator Agreement)",
          "reasoning": "The paper provides detailed analysis of the dataset including annotation procedure and inter-annotator agreement, reflecting use for analyzing patterns and annotation reliability."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13412,
      "completion_tokens": 552,
      "total_tokens": 13964
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset DefComm-DB consists of videos featuring English-speaking individuals only, indicating no more than one language present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset entries are exclusively in English, so exactly two human languages are not present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "DefComm-DB contains genuine non-acted dialogues between English-speaking individuals (Section 2: 'It comprises genuine non-acted dialogues between English-speaking individuals'), indicating that all dataset entries are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset is specified to include English-speaking individuals, so it does not contain single language data in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset only contains video, audio, and textual transcripts of spoken language; no programming or structured code is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no indication that the dataset includes mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset consists entirely of human communication and does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no mention or indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 2 Dataset",
          "reasoning": "The language of the dataset is explicitly stated as English-speaking dialogues, so the language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken English utterances and their transcriptions; thus, they do contain language."
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10630,
      "completion_tokens": 195,
      "total_tokens": 10825
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper about code availability for dataset construction.",
          "reasoning": "The paper does not provide any link, URL, or statement indicating that the code used for collecting, preprocessing, or generating the dataset DefComm-DB is publicly available. The dataset is mentioned to be available for academic researchers, but there is no indication that the associated code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Dataset) and subsections 2.1 (Search Parameters) and 2.2 (Annotation)",
          "reasoning": "The paper describes the dataset creation process in detail including the data sources (YouTube videos), criteria for selecting videos based on defensive communication markers, search queries used to find relevant videos, and the annotation procedure with annotator background and inter-annotator agreement analysis. This provides reasonably complete documentation of the dataset construction process for reproducibility."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7195,
      "completion_tokens": 122,
      "total_tokens": 7317
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: EmoSet++",
          "Reasoning": "EmoSet++ is introduced as a comprehensive multi-lingual, multi-cultural speech emotion corpus comprising 37 unique emotion datasets with 150,907 speech recordings totaling 119.5 hours. The datasets encompass human speech recordings that were collected in various conditions. Thus, the data is in audio modality and is human generated from human speech recordings as explicitly stated in the description of EmoSet++ in Section 2."
        }
      ]
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8047,
      "completion_tokens": 289,
      "total_tokens": 8336
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2",
            "reasoning": "EmoSet++ integrates 37 existing emotion datasets collected and combined by the authors, who mapped 106 distinct emotion classes into 6 classes based on Russell's Circumplex of Affect. The original annotation of emotions in these datasets was presumably done by multiple human annotators of varying expertise, but the paper does not specify expert annotators or annotation procedures. The authors manually mapped labels but do not mention re-annotation or new annotations by experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not provide any description of detailed annotation instructions provided for annotation; EmoSet++ is a compilation of existing datasets with re-labeling done by the authors, not new annotation requiring instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No mention or description of detailed rubric or scoring criteria provided with the dataset compilation or labeling; mapping of emotion classes to 6 affective classes is based on a theoretical model but no rubric for annotation is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not include or cite any annotation examples or annotated samples illustrating the labeling process of EmoSet++; it only notes mapping of existing labels and dataset splits."
          }
        }
      ]
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9177,
      "completion_tokens": 316,
      "total_tokens": 9493
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information regarding quality assurance by a single human expert annotator for any new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple human expert annotators for any of its introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was done by a single non-expert human annotator for any dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert human annotators for the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance performed by an AI model as a judge is not mentioned in relation to dataset annotation or validation in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss automated verification or algorithmic/rule-based quality assurance processes for dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces EmoSet++, which is a collection/aggregation of 37 existing emotion datasets, but does not describe any new dataset annotation, nor any specific quality assurance process applied or documented for the combined or mapped dataset labels. Therefore, no quality assurance process is described or performed for these new dataset aggregates in the paper."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8795,
      "completion_tokens": 380,
      "total_tokens": 9175
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created entirely new speech emotion recordings or datasets that were recorded from scratch by human contributors. Instead, the data used is aggregated from existing speech emotion datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not claim the creation of any datasets that were generated by AI or machine learning models. The new data introduced is based on real speech recordings from existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any human translations applied to the datasets."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe data created via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (EmoSet++)",
          "reasoning": "The authors introduce EmoSet++, which is described as an integration of 37 unique emotion datasets comprising existing speech emotion corpora collected from various sources and languages. The data is aggregated (collated) from these existing datasets without indication of substantial modification, constituting a large multi-lingual corpus through collation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the data labels from the integrated datasets are mapped into six classes based on Russell's Circumplex of Affect (Section 2), this label mapping is a transformation of annotations rather than the data itself being modified or derived. The underlying audio data remains from existing sources. Therefore, the dataset is best characterized as collated rather than derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data for EmoSet++ is explicitly described: it is an aggregation of multiple existing speech emotion datasets, thus is not undocumented."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9313,
      "completion_tokens": 366,
      "total_tokens": 9679
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The EmoSet++ dataset is used for supervised fine-tuning rather than unsupervised or self-supervised pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using EmoSet++. The usage described is fine-tuning pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Section 3.1 explains that the authors fine-tune the HuBERT model on EmoSet++ in a supervised manner with a linear classification head added for emotion classification. This fine-tuning is conducted in a round-robin fashion over the 37 datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training techniques on EmoSet++ or other datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "EmoSet++ is explicitly used for training (fine-tuning) models rather than exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using EmoSet++ primarily for analysis of trends or characteristics; it is used mainly as a resource for fine-tuning."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that EmoSet++ is used as a knowledge base for retrieval or augmenting generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical use of the EmoSet++ dataset for supervised fine-tuning of speech emotion recognition models."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10036,
      "completion_tokens": 471,
      "total_tokens": 10507
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2 (EmoSet++)",
          "reasoning": "EmoSet++ is explicitly described as a comprehensive multi-lingual, multi-cultural speech emotion corpus integrating 37 unique emotion datasets spanning 15 languages, including commonly spoken languages such as English, German, Mandarin, and rarer languages like Persian and Urdu."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of speech emotion recordings, no indication of inclusion of programming or structured code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of speech recordings and emotion labels, not mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "EmoSet++ contains human speech data for emotion recognition, without any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificially constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the languages contained within EmoSet++, and thus the languages are not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of human language speech recordings with emotion annotations, so the dataset contains language."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7254,
      "completion_tokens": 190,
      "total_tokens": 7444
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and 5",
          "reasoning": "The paper describes EmoSet++ as an aggregation of 37 existing speech emotion datasets. It does not provide any direct link or mention availability of code for collecting or assembling this aggregated dataset. The only related shared resource is the ExHuBERT model on HuggingFace, but no repository or code for the dataset construction is mentioned."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "The paper provides a description of the dataset assembly process in Section 2, detailing that EmoSet++ extends EmoSet, integrates 37 datasets totaling 150,907 samples and 119.5 hours, the languages covered, details on label mapping to six classes using Russel's Circumplex of Affect, and dataset split methods. This information provides documentation on how the dataset was constructed and processed."
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8213,
      "completion_tokens": 122,
      "total_tokens": 8335
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Table 1",
          "Reasoning": "The authors introduce a new dataset of speech audio recordings consisting of approximately 2000 OOV words per language (Hindi and Tamil) recorded by volunteers who are not professional voice artists. These recordings were made in acoustic pods with professional microphones, capturing human speech from volunteers. This dataset is newly created by the authors specifically for augmenting TTS training data to improve OOV performance, therefore it is human generated audio data."
        }
      ]
    }
  },
  {
    "id": "anand24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9065,
      "completion_tokens": 527,
      "total_tokens": 9592
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 (INDI OOVB Benchmark Creation)",
            "reasoning": "A single human language expert manually classified individual words into seven categories from a curated list of over 1000 words. This expert also creatively generated words for categories lacking enough OOV words."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper specifies that a single human expert classified and created words but does not mention any detailed annotation instructions provided to the annotator."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "There is no indication of any scoring rubrics or formal guidelines for classification; the classification appears to be manual and based on the expert's judgment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No examples or example annotations are mentioned as part of the classification guidelines in the text."
          }
        },
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.2 (Gathering Volunteers)",
            "reasoning": "The OOV words were recorded by volunteers who were explicitly noted as non-professional voice artists. There were six speakers (two male and one female each for Hindi and Tamil), likely non-experts in voice artistry."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 (Recording Setup and Process)",
            "reasoning": "Volunteers were requested to practice with the recording script before sessions, and live supervision by a language expert ensured clarity and correctness of pronunciations, suggesting that procedural instructions and guidance were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No scoring rubrics or formal assessment criteria are described for volunteers; the process was supervised but no formal rubric was mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No annotation or recording examples were mentioned as part of the guidelines for volunteers."
          }
        }
      ]
    }
  },
  {
    "id": "anand24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10195,
      "completion_tokens": 469,
      "total_tokens": 10664
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 (creation of INDI OOVB benchmark), Section 3.3",
          "reasoning": "A single human language expert manually classifies the OOV words into categories during the benchmark creation (Section 2). During the recording process (Section 3.3), an expert proficient in the language listens live to the volunteers' recordings to identify and point out pronunciation mistakes and filters out inappropriate words. The annotator is explicitly described as an expert in the language, indicating expertise and singular responsibility in this quality assurance role."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only mentions a single language expert performing word classification and quality assurance during recording. There is no mention of multiple human expert annotators involved in quality control."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of quality assurance by a single non-expert annotator is provided. The individuals performing QA are described as experts."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI or machine models used as judges for quality assurance of datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 (creation of INDI OOVB benchmark)",
          "reasoning": "The initial process to identify OOV words from corpora uses a greedy algorithm to select words maximizing coverage of missing OOV character bigrams, which is an automated, algorithmic method for dataset construction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described in multiple sections using human expert verification and automated methods, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9813,
      "completion_tokens": 505,
      "total_tokens": 10318
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 - Recording OOV Words with Volunteers",
          "reasoning": "The paper details the collection of new speech data recorded by volunteers who are non-professional voice artists. This data consists of recordings of out-of-vocabulary words prepared specifically for this study, involving human volunteers reading curated scripts. The data was thus created entirely from scratch by human contributors and not derived, translated, or generated by models."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models. All new speech data is recorded from human volunteers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper of data produced by translating content from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine-translated data in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 - Creation of INDI OOVB Benchmark",
          "reasoning": "The INDI OOVB benchmark text corpus was created by collating and aggregating existing text corpora from multiple diverse sources such as Sangraha, Bharat Parallel Corpus, IndicVoices transcriptions, Wikipedia, government websites, and others without significant modification apart from classification and filtering."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 - Creation of INDI OOVB Benchmark",
          "reasoning": "After collating the text corpora, the authors used a greedy algorithm and human expert intervention to select and classify out-of-vocabulary words based on missing character bigrams. This process involved filtering, categorizing, and creating some words especially when sufficient words were not found in categories, showing some derived adaptations applied to the collated data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The sources and methods of data generation are clearly specified for both the collated text benchmark and the newly recorded speech data."
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10331,
      "completion_tokens": 314,
      "total_tokens": 10645
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3, 4.1, 5.2",
          "reasoning": "The newly recorded dataset of OOV words with volunteer speakers is used to fine-tune pre-trained TTS models. This supervised fine-tuning using the new dataset significantly improves the intelligibility error rates on OOV words without degrading voice quality or perceptual quality."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2, 4.3, 5.1",
          "reasoning": "The paper introduces the INDI-COOV benchmark dataset containing OOV words across various real-world categories for Hindi and Tamil. This dataset is used exclusively for evaluation and benchmarking of TTS model performance on OOV words."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.5",
          "reasoning": "The dataset and evaluation results are analyzed to identify common pronunciation errors and patterns in TTS outputs for both OOV and IV words, helping understand error trends but not directly used for training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11054,
      "completion_tokens": 499,
      "total_tokens": 11553
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2 (Creation of INDI-COOV benchmark) and throughout the paper",
          "reasoning": "The proposed new datasets introduced in the paper, specifically the INDI-COOV benchmark and the volunteers' OOV word recordings, cover more than two human languages. The languages explicitly mentioned are Hindi and Tamil, and the data includes code-mixed examples involving English-Hindi and English-Tamil within the Codemixed (CM) category. Therefore, the data entries comprise at least three human languages: Hindi, Tamil, and English, making the dataset multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset entries contain programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include any biological or non-human communication sequences in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural human language content in Hindi, Tamil, and English code-mixed forms."
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8272,
      "completion_tokens": 165,
      "total_tokens": 8437
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.2",
          "reasoning": "The paper mentions that the recording scripts used during data collection are released to facilitate reproducibility, but there is no explicit mention or link to publicly available code or code repositories for dataset construction, preprocessing, or generation in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including the steps for curating the OOV benchmark (Section 2) and the process of recording OOV words with volunteers (Section 3). It describes corpus compilation, word selection, categorization, recording script preparation, volunteer recruitment, ethical considerations, and recording setup, thus offering comprehensive transparency about the dataset creation."
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7666,
      "completion_tokens": 270,
      "total_tokens": 7936
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Corpus Creation, Section 1 Introduction",
          "Reasoning": "The MuA ViC corpus includes audio tracks from TED and TEDx talk recordings, which are human recorded speeches from over 8000 speakers in 9 languages. This audio data is human generated as it is captured from real-world human speech events."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Corpus Creation, Section 1 Introduction",
          "Reasoning": "MuA ViC includes video tracks from TED and TEDx talk recordings capturing speakers on stage, recorded via cameras operated in these events. This video data is human generated as it originates from human-captured recordings of live talks."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3 Corpus Creation",
          "Reasoning": "The corpus contains human transcriptions of the speech, which are human generated. Additionally, translations are present that come either from human translations sourced from TED2020 or pseudo-translation labels generated by M2M-100 418M machine translation models. Therefore, the text modality includes both human generated and model generated data."
        }
      ]
    }
  },
  {
    "id": "anwar23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8518,
      "completion_tokens": 275,
      "total_tokens": 8793
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3: Corpus Creation",
            "reasoning": "Section 3 describes the corpus creation process mentioning human transcriptions and alignments with TED and TEDx talks. It explicitly mentions human transcriptions were used. Since transcription quality requires expertise, and TED talks involve professional-level speech, it is reasonable they were done by experts or multiple human experts. The paper states human transcriptions without indicating crowd-sourcing or non-expert annotators, so 'Multiple Human Experts' is appropriate."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3: Corpus Creation",
            "reasoning": "Though the paper does not detail the instructions, the use of human transcriptions for TED and TEDx talks implies annotation guidelines and instructions were provided to annotators to ensure transcription quality and consistency."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3: Corpus Creation",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation instruments used during annotation or transcription; thus, there is no evidence that rubrics were provided to annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3: Corpus Creation",
            "reasoning": "The paper does not provide or mention annotation examples or sample transcriptions given to annotators as part of their guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "anwar23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9648,
      "completion_tokens": 314,
      "total_tokens": 9962
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that quality assurance was conducted by a single human annotator who was a subject matter expert or a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information in the paper suggests that quality assurance was conducted by a single human annotator without subject matter expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human non-expert annotators being involved in quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any AI model being used as a judge or for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 Corpus Creation",
          "reasoning": "The paper describes aligning audio and video tracks with human transcriptions and text translations by exact and fuzzy text matching techniques, including the use of machine translation models for pseudo-translation labels. This indicates an automated verification and alignment process used for dataset creation and quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes alignment and matching processes for annotation, indicating some form of quality assurance rather than none."
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9266,
      "completion_tokens": 463,
      "total_tokens": 9729
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is sourced from existing TED and TEDx talk recordings; no indication that data was originally created from scratch by humans specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus contains natural audio-visual recordings; no mention of data generated originally by AI or ML models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "Section 3, Corpus Creation",
          "reasoning": "The paper states that text translations were obtained from the TED2020 corpus, but it does not specify that translations were done by humans for this dataset; thus no clear evidence of newly generated human translations."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 3, Corpus Creation",
          "reasoning": "The paper states that for untranslated train set examples, pseudo-translation labels were acquired from a machine translation model M2M-100 418M, indicating some data translations were generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3, Corpus Creation",
          "reasoning": "The dataset is collected by sourcing existing TED and TEDx talk recordings, reusing and aligning audio and video tracks with existing human transcriptions and text translations from publicly available corpora (LRS3-TED and TED2020)."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3, Corpus Creation",
          "reasoning": "The authors developed a fuzzy text matching strategy to align the TED2020 translations with LRS3-TED transcriptions, and generated pseudo-translation labels from machine translation models, which involves modifying and deriving new data from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation methods are described explicitly in the paper."
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9784,
      "completion_tokens": 425,
      "total_tokens": 10209
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.2, 4.3",
          "reasoning": "The paper describes fine-tuning pre-trained models on the MuA ViC dataset for supervised audio-visual speech recognition and speech-to-text translation tasks. Specifically, Sections 4.2 and 4.3 present baseline results using fine-tuned models on this dataset, showing improvements in performance for both recognition and translation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning techniques or RL-based post-training methods applied on this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.2, 4.3",
          "reasoning": "MuA ViC serves as a benchmark dataset for evaluating and benchmarking audio-visual speech recognition and speech-to-text translation models. The paper reports extensive evaluation metrics such as WER and BLEU scores on its test sets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specifically use the dataset for analyzing trends or patterns beyond reporting standard evaluation results."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for augmenting models via retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10507,
      "completion_tokens": 358,
      "total_tokens": 10865
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 1 Introduction, Table 1",
          "reasoning": "The dataset MuA ViC introduced in this paper contains audio-visual speech data in 9 human languages: English, Arabic, German, Greek, Spanish, French, Italian, Portuguese, and Russian. This is clearly stated in Section 1 and shown in detail in Table 1, confirming it covers more than two human languages, justifying the multilingual label."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any presence of programming or structured code-related content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of human spoken language speech and visual data, with no biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any use of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7725,
      "completion_tokens": 123,
      "total_tokens": 7848
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Conclusion",
          "reasoning": "The paper explicitly states that the corpus is available at https://github.com/facebookresearch/muavic, indicating that the code and data for dataset construction are publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 'Corpus Creation'",
          "reasoning": "The dataset creation process is documented in the paper's Section 3, describing data sourcing from TED and TEDx talks, alignment with transcriptions and translations, matching strategies, and handling of untranslated data with pseudo-translation labels."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6048,
      "completion_tokens": 234,
      "total_tokens": 6282
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1",
          "Reasoning": "The audio signals (S0, S1, S2) were synthesized using XKL for the vowels and natural utterance segments for consonants, controlled and manipulated programmatically as described, indicating model generation without direct human recording for the whole stimuli."
        },
        {
          "Modality": "video",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "Visual stimuli were produced using physical models of the human vocal tract attached to a miniature robot hand unit controlled by servo motors and Arduino, generating synchronized jaw movements, i.e., robot-generated visual data rather than recorded human video."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "Tactile stimuli were generated using the same robot hand unit controlled by servo motors and Arduino to provide tactile cues to participants, representing signals generated by the robotic apparatus without direct human capture."
        }
      ]
    }
  },
  {
    "id": "arai22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6900,
      "completion_tokens": 203,
      "total_tokens": 7103
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that thirty-three normal Japanese listeners participated in the experiment, with no indication that they were experts, thus indicating multiple human non-expert annotators provided perception responses."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 describes the procedure in which participants were asked to indicate what they heard (/atta/ or /a/+ta/), implying instructions were given to participants for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not describe any formal scoring rubrics or detailed criteria for annotation beyond simple forced-choice response."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No examples of annotation samples or annotated stimuli were provided in the text or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "arai22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8030,
      "completion_tokens": 301,
      "total_tokens": 8331
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance or validation of the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by multiple non-expert human annotators is described or mentioned in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an automatic verification process or algorithmic rule-based quality check was applied to dataset annotations or content in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any specific quality assurance process applied to the creation, validation, or annotation of the datasets introduced (e.g., audio stimuli, physical model movement data, or tactile stimuli synchronization). The focus is on the experimental methodology and stimuli creation, but no mention of QA procedures is provided."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7648,
      "completion_tokens": 490,
      "total_tokens": 8138
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1, 2.2, and 2.3",
          "reasoning": "The audio stimuli S0, S1, and S2 were synthesized using XKL for vowels and natural recorded consonant sounds, prepared specifically for this study. The visual stimuli involved a physical model of the human vocal tract with a miniature robot hand unit controlled to produce articulatory jaw movements synchronously with the audio. The tactile stimuli used the same mechanical setup to provide tactile cues. These stimuli were created from scratch by the researchers for this study to investigate multimodal speech perception, thus constituting original content created by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of data generated entirely by AI or machine learning models independently creating the dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was produced by translating content from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was involved in generating the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not merely collected or aggregated from existing sources but was newly created for the experiment."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 4 Discussion",
          "reasoning": "The audio stimuli were synthesized by modifying formant transitions on a base vowel and natural consonant sounds, which constitutes a transformation of existing elements. Additionally, the stimuli are based on previously studied utterances (/atta/ and /a/+pause+/ta/) from prior work, indicating these data are derived through modifications and adaptations of pre-existing speech characteristics and recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper sufficiently documents the origin and method of generation of the datasets."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8166,
      "completion_tokens": 277,
      "total_tokens": 8443
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 Results and Section 4 Discussion",
          "reasoning": "The newly created stimuli (audio, audio+visual using vocal tract model, and audio+tactile) are used to measure listeners' perception responses. The data is collected to evaluate perception differences under different stimulus conditions, thus the dataset is used for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Discussion",
          "reasoning": "The paper analyzes patterns in perception responses (response rates) to determine how visual and tactile cues affect speech perception of geminate consonants in Japanese. The dataset is used primarily for analyzing perceptual trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the usage of the newly created stimuli dataset in evaluation experiments and perception analysis."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8889,
      "completion_tokens": 389,
      "total_tokens": 9278
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 3.1 Participants",
          "reasoning": "The dataset consists of speech stimuli and perception data focused on Japanese utterances such as /atta/ and /a/+pause+/ta/. The paper clearly states that the participants are native Japanese listeners and studies are conducted on Japanese geminate consonants. There is no mention of other languages in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken Japanese stimuli and human participants' perception responses, so it contains entries with language."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6107,
      "completion_tokens": 131,
      "total_tokens": 6238
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention any publicly available code or link to a repository containing the code used for generating the stimuli or conducting the experiments."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed descriptions in Section 2 about the preparation of audio, visual, and tactile stimuli, including how the physical vocal tract model and tactile devices were used and synchronized with audio signals. Section 3 details the experimental procedure and participant information. This documentation sufficiently describes the dataset creation process and experimental setup."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6675,
      "completion_tokens": 267,
      "total_tokens": 6942
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, 3.3, 3.5",
          "Reasoning": "The paper describes recordings of bilateral behind-the-ear (BTE) hearing-aid microphone audio from eight normal-hearing participants engaged in interactive conversational tasks. This audio data is captured from human participants during the experiments, making it audio modality data generated through human involvement."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, 3.5",
          "Reasoning": "Head movement was tracked using 9-DOF Razor inertial measurement units (IMUs) worn by participants to record yaw, pitch, and roll head movements. The IMU signals derived from human participants during the conversational tasks constitute sensor/signal data originated through human involvement."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.5",
          "Reasoning": "Four Logitech HD webcams recorded each participant during the recordings. This video data is captured through human-operated cameras during the study, thus it is human-generated video data."
        }
      ]
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7527,
      "completion_tokens": 156,
      "total_tokens": 7683
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.6",
            "reasoning": "Section 3.6 states that the first author manually annotated the audio recordings himself, listening and inspecting waveforms."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "No mention of detailed annotation instructions provided to annotators; only one annotator who manually performed annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "No mention of scoring rubrics or criteria for annotation evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "No examples of annotations or guideline examples are included or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8657,
      "completion_tokens": 404,
      "total_tokens": 9061
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.6 Annotation",
          "reasoning": "Quality assurance for the dataset annotations was conducted manually by the first author listening to the audio and visually inspecting the waveform. The paper specifies that annotations were made by this single individual, implying that a single human annotator was responsible. There is no explicit detail on the author's expertise, but given the context and involvement in the research, it is reasonable to consider them a subject matter expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only mentions that the first author conducted manual annotation and does not mention multiple annotators or experts participating in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotator is implied to be an expert (the first author involved in the research), so this label does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that multiple non-expert annotators were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No descriptions of automated verification processes for quality assurance of annotations are present."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A manual annotation and quality assurance process by the first author is described, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8275,
      "completion_tokens": 430,
      "total_tokens": 8705
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Methods), especially 3.1 Participants, 3.2 Experimental Set-up, 3.3 Acoustic conditions, 3.4 Tasks",
          "reasoning": "The paper describes novel data recorded from human participants during interactive conversational tasks designed and conducted by the authors. The speech and head movement data were collected from eight normal-hearing participants who performed two- and four-person conversational tasks under controlled acoustic conditions. This data is original content created entirely from scratch by human contributors and is not translated, adapted, or derived from any pre-existing dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any datasets were generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper mentions use of existing speech databases for babble noise and acoustic stimuli, the dataset introduced consists of newly recorded conversations and head movement data, not simply collected or aggregated existing data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data does not appear to be directly based on existing sources with modifications; rather, it is newly recorded experimental data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of data generation is explicitly documented and described."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8793,
      "completion_tokens": 376,
      "total_tokens": 9169
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses the dataset for analysis and testing of an existing DOA algorithm, but does not use it exclusively for evaluation or benchmarking of models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4 (Results), 5 (Discussion), and 6 (Conclusions)",
          "reasoning": "The new dataset of speech and head movements recorded during two- and four-person interactive conversational tasks in different acoustic conditions is used primarily to analyze trends and characteristics of head movements and conversational behavior. Detailed analyses of head positions, velocities, and conversational roles are presented. The dataset supports investigation of behavioral patterns and acoustic environment effects rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the dataset for behavioral analysis and algorithm testing, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9516,
      "completion_tokens": 470,
      "total_tokens": 9986
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Participants and throughout the paper",
          "reasoning": "The dataset consists of recordings of eight normal-hearing participants' conversations in English. The paper states that six participants were native English speakers and two were fluent non-native speakers, implying that the conversational content recorded in the dataset is exclusively English. The tasks (e.g., '20 questions' and NASA survival task) and recorded conversations involved only English language utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains conversational audio and head movement data, but no content with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human conversational speech and head movement data, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificial languages in the data."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language in the form of English conversational speech."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6734,
      "completion_tokens": 169,
      "total_tokens": 6903
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any links to code repositories or software related to the data collection, preprocessing, or dataset construction. There is no indication that such code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Methods), including 3.1 Participants, 3.2 Experimental Set-up, 3.3 Acoustic conditions, 3.4 Tasks, and 3.5 Software and Hardware.",
          "reasoning": "The paper describes in detail the dataset creation process including participant recruitment and demographics, experimental set-up and conditions, tasks performed, equipment used for data acquisition, and annotation procedures. This provides sufficient documentation of the dataset creation process, enabling transparency and potential reproducibility of the data collection process."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 14424,
      "completion_tokens": 209,
      "total_tokens": 14633
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Abstract; Introduction; Conclusion",
          "Reasoning": "The paper introduces a new reverberant dataset used for training the proposed TRUNet network, generated from measured room impulse responses (RIRs) of an actual microphone array, meaning the acoustic signals were recorded with human involvement (via microphones) but simulated reverberation effects are modeled. The dataset involves multi-channel audio signals containing speech mixtures with reverberation, created realistically by combining human speech with measured RIRs. Hence, the modality is audio. The dataset is both human generated (the speech signals and recorded RIRs) and model generated (the dataset is simulated by convolving speech with RIRs to create reverberant mixtures). This is explicitly stated in the Abstract and Introduction, and confirmed in the Conclusion section where they mention training the network on a large reverberant dataset generated from measured RIRs of an actual microphone array."
        }
      ]
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 15276,
      "completion_tokens": 282,
      "total_tokens": 15558
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 1 (Abstract and Introduction), Section 4 (Experiments)",
            "reasoning": "The dataset used is a large reverberant dataset generated from measured room impulse responses (RIRs) of an actual microphone array, as described in the Abstract and Introduction. The data generation process involves synthesizing reverberant mixtures of speech signals using measured RIRs, which is a deterministic, simulation-based process rather than human annotation. This indicates the dataset creation is an automatic process driven by simulation rather than manual annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "The paper does not describe any instructions provided for annotators since no manual annotation involving humans was performed; the dataset was automatically generated via simulation using measured RIRs and speech data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "Because the dataset is generated automatically and does not rely on human annotations or rating, no scoring rubrics for annotation are discussed or applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "No annotation examples are provided or needed, as the dataset generation is automatic and not based on human labeling or classification, thus no annotation guideline examples exist or were provided."
          }
        }
      ]
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 16406,
      "completion_tokens": 256,
      "total_tokens": 16662
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts being involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance by multiple non-expert annotators is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by an AI model acting as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention automated verification or algorithmic rule-based quality assurance for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a reverberant dataset generated from measured room impulse responses but does not describe any quality assurance procedures applied to validate the dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 16024,
      "completion_tokens": 424,
      "total_tokens": 16448
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the dataset used was created entirely from scratch by human contributors. Instead, the dataset was generated using measured room impulse responses and existing speech data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being translated from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being machine translated."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as simply collected or aggregated from existing sources without significant modification; rather it was generated in a specific manner for this research."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Abstract and Section 1 Introduction",
          "reasoning": "The dataset for training and testing is generated from measured room impulse responses (RIRs) of an actual microphone array and existing speech signals. This involves applying real measured RIRs to clean speech corpora to simulate reverberant speech mixtures. Thus the data is derived from existing sources (speech corpora) with transformations applied (convolution with measured RIRs) to create a realistic, reverberant, multi-channel dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides explicit descriptions on how the dataset was generated, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 16542,
      "completion_tokens": 351,
      "total_tokens": 16893
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Implicit) and Section 4.2",
          "reasoning": "The paper describes training the proposed TRUNet model end-to-end on a large reverberant dataset. The dataset is used directly to train the model from random initialization, without mention of prior pre-training. This is implied in the discussion of network training and evaluation in Section 4.2 and in the methods description."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) and Section 4.2",
          "reasoning": "Evaluation on a realistic and challenging reverberant dataset generated from measured room impulse responses is performed to benchmark and measure the performance of the proposed TRUNet and other methods, as described in Section 4.1 and 4.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and throughout the experimental results",
          "reasoning": "The dataset is used to analyze the effectiveness of the proposed TRUNet design and loss function variations on source separation performance, providing insight into spectral, temporal, and spatial processing components."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly shows practical usage of the proposed reverberant dataset for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 17265,
      "completion_tokens": 412,
      "total_tokens": 17677
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any dataset comprising exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 1 and Section 4",
          "reasoning": "The datasets used for training and evaluation consist of English speech signals only, as referenced by the usage of the CSTR VCTK corpus (English multi-speaker corpus) and other English speech data sources. The paper discusses speech signals with no mention of other languages, indicating monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of non-English language content in the datasets is given in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset itself does not contain programming or structured code content; rather the paper discusses deep learning models and architectures, but these are not datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain mathematical or logical notation as entries; such notation is used in the paper to describe methods and models only."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data like DNA sequences or animal signals are mentioned for the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially constructed languages such as Klingon or Esperanto in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved are clearly documented (English), so language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of spoken language data (English speech), so they do contain language entries."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 14483,
      "completion_tokens": 149,
      "total_tokens": 14632
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link provided",
          "reasoning": "The paper does not mention or provide a link to any code repository for the dataset construction or generation process. There is no information about making the code publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 and throughout the text",
          "reasoning": "The paper describes the dataset as a large reverberant dataset generated from measured room impulse responses of an actual microphone array. It discusses the acoustic scenario, the reverberant conditions, and the generation process with references to measured room impulse responses to simulate realistic reverberant sound source separation environments, indicating some level of documentation regarding the dataset creation process."
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10634,
      "completion_tokens": 117,
      "total_tokens": 10751
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and 4.1",
          "Reasoning": "SpeechGLUE is the new dataset introduced in the paper as a speech version of the GLUE benchmark. It is created by converting GLUE's text sentences to speech utterances using a single-speaker TTS system (VITS model), which is a model-generated process. Therefore, the data modality is audio and the source is model generated, not human recorded or manual."
        }
      ]
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11486,
      "completion_tokens": 208,
      "total_tokens": 11694
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2, Section 4.1",
            "reasoning": "The SpeechGLUE dataset is created by automatically converting existing GLUE text datasets to speech via a TTS system (VITS model trained on LJSpeech), not by human annotators but through automatic synthesis."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention any instructions provided for annotation, as no human annotation was performed for the new dataset creation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no mention of scoring rubrics associated with annotation, since the dataset labels come from the existing GLUE datasets and no new manual annotation was done."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No annotation examples or guidelines for human annotators are provided because the dataset was generated automatically via TTS conversion from text."
          }
        }
      ]
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12616,
      "completion_tokens": 357,
      "total_tokens": 12973
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the new dataset SpeechGLUE."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of the SpeechGLUE dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator for SpeechGLUE."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by multiple non-expert annotators in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention AI models being used for quality assurance purposes for the new dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 SpeechGLUE dataset",
          "reasoning": "The SpeechGLUE dataset is derived by converting existing text datasets (GLUE) into speech using a text-to-speech (TTS) system (VITS model) in an automated manner. The paper describes the use of TTS conversion and some automatic filtering (e.g., removal of samples with impractical content such as numerous digits or null text); there is no mention of human annotation or manual validation. Thus, the quality assurance process relies on the automatic synthesis and filtering, which is an automatic verification process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is implied through the automated TTS conversion and filtering steps described, hence QA is considered present as an automatic process."
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12234,
      "completion_tokens": 464,
      "total_tokens": 12698
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset created entirely from scratch by human contributors by recording humans reading new sentences or generating novel text data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The SpeechGLUE dataset is generated by synthesizing speech from existing GLUE text data using a text-to-speech (TTS) system (VITS model). This speech data is generated by a machine learning model without human recording, thus it is original data created by a model rather than collected from humans."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or mention of content translated by machine translation systems in the dataset generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4.1",
          "reasoning": "The SpeechGLUE dataset is based on the existing GLUE benchmark text data which is publicly available. The authors collected this existing dataset and converted it to speech via TTS without fundamentally altering the original textual data structure."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The original GLUE text dataset is derived into SpeechGLUE by applying a text-to-speech system to produce speech signals. This transformation from text to speech involves modifications and adaptations, thus the data is derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and generation process of the SpeechGLUE data is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12752,
      "completion_tokens": 238,
      "total_tokens": 12990
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4.1, Table 2",
          "reasoning": "SpeechGLUE is introduced as a speech version of the GLUE benchmark and is used exclusively for evaluating the linguistic knowledge captured by speech SSL models. The dataset is generated by converting GLUE text data into speech using TTS, and is used to extensively evaluate speech SSL models in various NLU tasks without updating the upstream model parameters, indicating its exclusive use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13475,
      "completion_tokens": 509,
      "total_tokens": 13984
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses datasets in English; no indication of multiple human languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are derived from English GLUE tasks, converted into speech via TTS. No mention of exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The SpeechGLUE dataset is created by converting English text sentences from the GLUE benchmark into corresponding speech utterances using text-to-speech (TTS) systems. All data entries are in English, as the original GLUE benchmark is English-only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly English-based; no non-English language data entries are described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-related content is described as part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic notations are part of the dataset; tasks are natural language-based."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that biological sequences or non-human communication data are included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses natural English language only, with no constructed or fictional language content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) used are clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language data, so language is applicable."
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10693,
      "completion_tokens": 170,
      "total_tokens": 10863
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3.2 and Introduction",
          "reasoning": "The paper explicitly states that they release SpeechGLUE for reproducibility at https://github.com/ashi-ta/speechGLUE, indicating that the code used for constructing their new dataset (SpeechGLUE) is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The paper provides a detailed description of the dataset creation process for SpeechGLUE, including how the original GLUE text datasets are converted to speech using a TTS system (VITS trained on LJSpeech), the resampling methods, data size details in Table 2, and text normalization steps. This level of detail clearly documents their dataset construction."
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5957,
      "completion_tokens": 98,
      "total_tokens": 6055
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Speech material",
          "Reasoning": "The authors introduced the PATAFreq recording database, collected recently during the 2020 COVID-19 lockdown, featuring audio recordings of read speech from 9 native French speakers over multiple sessions, recorded using professional microphones and soundcards, thus confirming human-generated audio data directly recorded for this new dataset."
        }
      ]
    }
  },
  {
    "id": "audibert22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6809,
      "completion_tokens": 263,
      "total_tokens": 7072
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 2.1",
            "reasoning": "The recordings were self-recorded by 9 native French speakers close to the speech research community, not described as experts performing manual annotation. The manual segmentation of texts into chunks was performed, but the annotating individuals are not described as experts; the segmentations appear to be manual but do not specify expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1",
            "reasoning": "Speakers were asked to record regularly with instructions regarding recording frequency, environment, and equipment; the text reading task involved manual segmentation into chunks as well, suggesting instructions existed for segmentation or recording procedures."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no mention of formal scoring rubrics or rating scales for annotation or segmentation; only a self-assessed 4-level scale is mentioned for fatigue, emotion, vocal fatigue, and voice use, but not for the manual segmentation or acoustic annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not provide or mention annotation examples or exemplar segmentations illustrating the annotation or segmentation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "audibert22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7939,
      "completion_tokens": 341,
      "total_tokens": 8280
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert for annotation or validation of the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report multiple experts performing quality assurance or annotation validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any quality assurance done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of AI models used explicitly for quality assurance of annotations or data content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 and 2.3",
          "reasoning": "The data annotation involved manual segmentation into chunks and delimitation of silent pauses, but the paper indicates use of custom scripts and automated extraction of acoustic features with Praat software. This suggests that quality checks for feature extraction and data consistency were likely done via automated processing rather than manual validation. No explicit human QA process is described, and the procedure for feature extraction appears fully automated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors describe methodology involving manual segmentation and use of automated feature extraction, indicating some form of annotation control and processing. Although no explicit human quality assurance detail is given, the process is not entirely absent of quality assurance, so 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7557,
      "completion_tokens": 379,
      "total_tokens": 7936
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset consists of speech recordings of 9 native French speakers who recorded themselves reading two texts over approximately two months, specifically collected by the authors during the COVID-19 lockdown. The data is original content created by human participants performing a reading task and is not translated, adapted, or derived from existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes data generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation or language adaptation produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation techniques to produce the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was newly collected specifically for this study; it was not aggregated or collected from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset is analyzed and acoustic features derived from the raw recordings, the raw speech recordings themselves are original and not derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8075,
      "completion_tokens": 226,
      "total_tokens": 8301
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Results) and Section 4 (Discussion)",
          "reasoning": "The paper introduces the PATAFreq recording database collected from 9 French speakers over multiple sessions and uses it to analyze intra-speaker and inter-speaker variability. The dataset is used primarily to study trends and characteristics of speech and voice features across sessions, focusing on variability and stability rather than training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8798,
      "completion_tokens": 503,
      "total_tokens": 9301
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes only native French speakers reading texts; no indication of multiple languages is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of French speech; no presence of exactly two languages is reported."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves English; the study explicitly mentions native French speakers."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1, Methods: Speech material",
          "reasoning": "The dataset consists of speech recordings from nine native French speakers reading French texts; hence the dataset is monolingual with French as the only language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although analysis uses code (e.g., Python application and Praat scripts), the dataset entries themselves are speech recordings in French, not programming code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset entries include mathematical or logical notation; features are numeric descriptors of speech but not datasets of formal symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech recordings; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages; all speech data is in natural French."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly documented as French."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (French speech), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6016,
      "completion_tokens": 170,
      "total_tokens": 6186
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.1 Methods - Speech material",
          "reasoning": "The paper mentions a dedicated application developed in Python used by most speakers for recordings, but does not provide any link, repository, or indication that the code is publicly available. No explicit mention or reference to publicly shared code for data collection or preprocessing is given."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 - Methods",
          "reasoning": "The paper provides a detailed description of the dataset creation process including participant demographics, recording procedure, equipment used, recording environment instructions, number of sessions per speaker, and the speech material recorded. The acoustic feature extraction process is also clearly documented with references to specific tools and scripts used. This documentation allows understanding and potential reproduction of the data collection and processing pipeline."
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7353,
      "completion_tokens": 102,
      "total_tokens": 7455
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Protocol and corpus",
          "Reasoning": "The paper introduces the Dialogs Re-enacted Across Languages (DRAL) corpus, collected by recording bilingual participants having spontaneous conversations and then reenacting utterances in another language. This data comprises matched English-Spanish utterance pairs recorded from real human speakers, thus the data are audio modality and were obtained through human generation."
        }
      ]
    }
  },
  {
    "id": "avila23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8205,
      "completion_tokens": 279,
      "total_tokens": 8484
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 Protocol and corpus",
            "reasoning": "The paper describes data collected via the DRAL protocol with bilingual participants who are nonprofessional speakers re-enacting conversations in their other language. These participants are implied to be non-expert bilingual speakers rather than experts, and there is no mention of expert annotators labeling or reviewing the data beyond the natural bilingual participants' recordings."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 Protocol and corpus",
            "reasoning": "The paper explains the DRAL protocol involves directions from a producer guiding bilingual participants to select utterances and re-enact them in the other language, sometimes requiring several attempts to get them right. This indicates that instructions were provided to participants during data collection."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 Protocol and corpus",
            "reasoning": "No mention is made of scoring rubrics or formal scoring criteria used during annotation or data collection, only guidance to get a close re-enactment and pragmatic diversity."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 Protocol and corpus",
            "reasoning": "There is no evidence in the paper that example annotations or example instructions were provided as annotation guidelines; the protocol relies on participant re-enactments rather than explicit annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "avila23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9335,
      "completion_tokens": 526,
      "total_tokens": 9861
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": true,
          "reference": "Section 2: Protocol and corpus",
          "reasoning": "The corpus was collected using the DRAL protocol involving pairs of nonprofessional, bilingual participants who converse and later re-enact utterances in their other language. These participants are bilingual speakers but described as nonprofessional, indicating they are not speech experts or necessarily expert annotators. There is no mention of expert annotators or detailed quality assurance by experts. The re-enactment process includes multiple attempts to get the re-enactment right, which suggests some level of self-validation by the non-expert bilingual speakers themselves."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2: Protocol and corpus",
          "reasoning": "Data collection involves pairs of bilingual speakers (42 speakers total) who first have conversations and then re-enact utterances. This inherently involves multiple non-expert humans contributing to the matched pairs. However, there is no evidence of a separate QA process performed by additional annotators. The multiple bilingual participants effectively serve as non-expert annotators in the production and verification of the data during re-enactment."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses using an AI model to predict prosody and a prosodic dissimilarity metric, there is no indication that these AI models were used for quality assurance or validation of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of automated verification procedures or code-based quality checks being applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although there is no explicit mention of a formal QA protocol, the data collection protocol involves bilingual speakers re-enacting utterances to produce matched pairs, which suggests some naturalistic validation by participants themselves. Therefore, some level of implicit QA exists, disqualifying the 'N/A' label."
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8953,
      "completion_tokens": 396,
      "total_tokens": 9349
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Protocol and corpus",
          "reasoning": "The authors developed a new data collection protocol named DRAL, where bilingual speakers have spontaneous conversations and then re-enact utterances in their other language, creating matched bilingual utterance pairs. This data is original, created entirely by human participants through natural conversations and their bilingual re-enactments, not translated, adapted, or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the dataset was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not described as being produced through human translation of content from one language to another, but rather through re-enactment by bilingual speakers."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No parts of the data are indicated to have been generated via machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a collection or aggregation of existing sources; it is newly collected data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on existing sources with modifications; it was newly recorded and created following the DRAL protocol."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of dataset creation are clearly described and documented."
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9471,
      "completion_tokens": 364,
      "total_tokens": 9835
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used exclusively for evaluation or benchmarking; rather, it supports other uses."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2, 4, 5, 6, 7",
          "reasoning": "The DRAL corpus is introduced as a cross-language dialog dataset and is primarily used for analyzing prosodic differences and patterns across English and Spanish (e.g. Sections 4 and 7 describe correlation analyses and qualitative analyses using the dataset). The paper extensively uses it to investigate prosodic features and transfer, serving as a basis for exploratory analysis rather than model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the DRAL corpus is used as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10194,
      "completion_tokens": 362,
      "total_tokens": 10556
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The authors introduce the Dialogs Re-enacted Across Languages (DRAL) corpus, which is explicitly described as comprising matched English-Spanish utterance pairs. The corpus currently has 1871 matched EN-ES utterance pairs spoken by bilingual participants. This means the dataset includes exactly two human languages, English and Spanish."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7412,
      "completion_tokens": 168,
      "total_tokens": 7580
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 and final paragraph of the paper",
          "reasoning": "The paper states that they share all data, code, and observations at a public repository: https://github.com/joneavila/DRAL, indicating that the code related to data collection and possibly processing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Protocol and corpus) and referenced technical report [26]",
          "reasoning": "The paper provides detailed description of the data collection protocol (the DRAL protocol) in Section 2, explaining the steps and participant setup, as well as information about the collected corpus. Additionally, they reference a technical report [26] for further details and have made the dataset available, indicating good documentation of the creation process."
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11016,
      "completion_tokens": 159,
      "total_tokens": 11175
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset, Real-world Recordings and Section 4.5 Recording from a Live Performance",
          "Reasoning": "The authors introduced a new real-world dataset comprising approximately 300 video recordings from 4 different live music shows collected from YouTube, which are user (human) generated audio recordings. These recordings were used for evaluation of their crowdsourced audio enhancement method. Additionally, the paper describes synthetic audio signals generated by mixing speech or music sources with various types of additive noise, which were algorithmically generated to simulate noisy crowdsourced recordings used for method evaluation. Therefore, the data includes both human generated real recordings and model generated synthetic audio signals."
        }
      ]
    }
  },
  {
    "id": "aziz24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11868,
      "completion_tokens": 262,
      "total_tokens": 12130
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 (Real-world Recordings) and Evaluation Sections 4.4, 4.5",
            "reasoning": "The paper introduces a synthetic dataset and a real-world dataset collected from YouTube recordings of live music shows. The synthetic data are generated from known sources with added noises (Section 4.1), and the real-world data are multiple user-uploaded recordings collected online. There is no mention of human annotators labeling or annotating these datasets; the data originate from automatic collection or synthetic generation, thus annotation is done by automatic processes (e.g., alignment, normalization)."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide nor mention any instructions or guidelines for annotators since no human annotation process is described for the new datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No scoring rubrics or evaluation guidelines for annotators are mentioned or provided related to the new datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not describe or provide any annotation examples related to labeling or human annotation of the new dataset(s)."
          }
        }
      ]
    }
  },
  {
    "id": "aziz24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12998,
      "completion_tokens": 351,
      "total_tokens": 13349
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper that multiple human experts were involved to conduct quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human annotator conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.3 and 4.4",
          "reasoning": "The paper describes that a subjective test (MUSHRA protocol) was conducted by human participants who rated the quality of recordings on a scale of 0 to 100, indicating multiple human non-expert annotators participated in evaluating the dataset quality. There is no indication these participants were experts, so they are considered non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process does not involve AI models as judges; AI models are used in the methods but not for QA of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "Objective evaluation metrics such as SI-SNR, PESQ, and STOI are used to assess quality in a standardized automated manner, constituting automated verification of the data quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents both subjective human evaluation and objective automated evaluation of the dataset quality; therefore, QA process is described and performed."
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12616,
      "completion_tokens": 496,
      "total_tokens": 13112
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Real-world Recordings",
          "reasoning": "The paper states that the authors have collected real world user recordings of live music shows from YouTube, which are independent user-generated videos capturing live performances. The collection involves new data created by humans capturing events using their devices, which were then gathered by the authors for their experiments."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.1 Synthetic Data",
          "reasoning": "The paper describes synthetic audio signals used for evaluation by mixing speech or music signals with various types of noise from existing corpora. This synthetic data is generated by artificially combining and modifying existing recordings to simulate noisy audio inputs, thus these data are generated by models (processing steps) rather than raw human recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Real-world Recordings",
          "reasoning": "The real-world dataset comprises a collection of multiple different user recordings of live music shows obtained from YouTube. The authors aggregated these independent user-generated recordings without mentioning significant modification besides alignment and normalization, thus this is collated data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Synthetic Data",
          "reasoning": "The synthetic dataset is derived from existing speech and noise corpora (e.g., LibriSpeech, DEMAND, AudioSet) by combining and mixing signals with added noise to synthetically generate noisy inputs. This involves transformation and adaptation of existing data to create new evaluation material."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation process are described adequately in the paper."
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13134,
      "completion_tokens": 224,
      "total_tokens": 13358
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.4, Section 4.5",
          "reasoning": "The paper describes collecting approximately 300 real-world user recordings from 4 different music shows, and conducting evaluations using synthetic and real-world recordings to benchmark the proposed method against baselines. This is used explicitly for measuring performance, using metrics such as SI-SNR, PESQ, STOI, and subjective MUSHRA tests."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13857,
      "completion_tokens": 476,
      "total_tokens": 14333
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of multiple human languages in the dataset. The focus is on audio signals from live music shows and synthetic noisy signals, without specifying any linguistic content in more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as containing content in exactly two human languages. There is no evidence in the paper suggesting bilingual language data in the introduced datasets."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Real-world Recordings and Synthetic Data Descriptions",
          "reasoning": "The speech data used for synthetic experiments is from the LibriSpeech corpus, which contains English speech recordings. The real-world music show recordings collected from YouTube are likely to primarily contain English content or music without specific mention of other languages. There is no indication of multiple languages being present in the datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the datasets include monolingual non-English language data. Speech data is from English corpora, and music shows are not specified to be in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio signals, not code or programming language related entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical notation in the methodology descriptions, the datasets themselves do not contain mathematical or logical notations as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human-generated audio recordings; there is no mention of biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is specified (English speech and music recordings), so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio recordings with spoken content and music, which involve human language audio, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11075,
      "completion_tokens": 214,
      "total_tokens": 11289
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.4",
          "reasoning": "The paper explicitly states in the Abstract: 'Code, dataset, and models are available.' Furthermore, in Section 4.4 it provides a link (https://shiranaziz.github.io/crowdsourced_audio_enhancement/) where code, datasets, models and audio examples are provided. This indicates that the code related to dataset preprocessing and generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 and 4.4",
          "reasoning": "Section 4.1 describes the collection process of real-world recordings from YouTube, including alignment and normalization steps. Section 4.4 discusses evaluation using synthetic datasets created from known sources (e.g., LibriSpeech, DEMAND, AudioSet) and provides details on the synthetic data creation (using certain numbers of sources, SNR levels, and noises). This indicates that the dataset creation process is documented adequately in the paper."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7533,
      "completion_tokens": 116,
      "total_tokens": 7649
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 - DISPLACE Corpus, especially 3.1 Recording Setup and 3.2 Data Collection",
          "Reasoning": "The DISPLACE corpus is a new dataset introduced by the authors, consisting of single-channel far-field audio recordings of multi-speaker, multi-lingual conversational speech with natural overlap, noise and reverberation. The audio was recorded in real academic environments using far-field microphones capturing natural conversational speech among human participants."
        }
      ]
    }
  },
  {
    "id": "baghel23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8385,
      "completion_tokens": 250,
      "total_tokens": 8635
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 states the data annotations were generated with professional annotators who listened to the lapel microphone recordings. The process was tedious and involved multiple levels of quality checks before final annotations, indicating multiple expert annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 mentions participants were informed of guidelines prior to recording, and Section 3.3 describes the annotation tasks clearly, implying that detailed instructions were provided to annotators to ensure consistent speech, speaker, and language labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation criteria provided to annotators during annotation. Rubrics typically refer to explicit scoring standards, which are not described in the annotation section."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention providing annotation examples to the annotators to guide the annotation process. There are no references to example annotations or samples being shared as part of the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "baghel23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9515,
      "completion_tokens": 420,
      "total_tokens": 9935
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Annotation",
          "reasoning": "Annotations were generated with professional annotators who listened to lapel microphone recordings. The paper describes multiple annotation tasks involving speech, speaker, and language labeling. It also mentions the use of multiple levels of quality checks before finalizing the annotations. Although the paper does not specify the exact number of annotators, the use of 'professional annotators' in plural form and multiple quality check levels implies multiple human experts were involved in the quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that non-expert annotators conducted quality assurance, nor that multiple non-experts were involved."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model used as a judge or as part of the quality assurance process for the annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic verification is described for quality assurance on the dataset annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a quality assurance process involving professional annotators and multiple levels of quality checks, so QA was applied."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9133,
      "completion_tokens": 417,
      "total_tokens": 9550
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Collection and Section 3.3 Annotation",
          "reasoning": "The DISPLACE dataset was created by recording actual multilingual and multi-speaker conversational speech with human participants in physical rooms. The participants spoke spontaneously on selected topics, and professional human annotators generated speech, speaker, and language annotations from close-talking microphone recordings. This process indicates that the data is original content created entirely from scratch by human contributors and is not translated, adapted, or derived from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new data was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a collection or aggregation of existing sources without modification; rather, it was specifically recorded and annotated for the challenge."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on existing sources with modifications; it was newly recorded and annotated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and generation method of the data is clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9651,
      "completion_tokens": 525,
      "total_tokens": 10176
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the DISPLACE dataset for pre-training large models, and no unsupervised or self-supervised pre-training is discussed."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The DISPLACE challenge explicitly states that no training data is provided; participants can use proprietary or public resources for training, but the dataset itself is not used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning pre-trained models using the DISPLACE dataset; rather, it is used for evaluation purposes."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training methods such as RLHF using the DISPLACE dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.4 - Development and Evaluation set; Section 4 - Challenge Tasks; Sections 6 and 7 - Challenge Results and Summary",
          "reasoning": "The DISPLACE dataset is provided for development and evaluation sets only, specifically for benchmarking speaker and language diarization systems in challenge tracks. The paper describes the evaluation metrics and results with DER, indicating exclusive use for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 - Figure 1 and 2 (language, speaker, overlap distributions); Section 7 - Summary",
          "reasoning": "The paper analyzes the dataset characteristics such as language distribution, speaker demographics, overlap durations, and segment statistics to highlight the challenges and traits of the multi-lingual multi-speaker conversational speech in the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base to augment models or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset utility is clearly described and demonstrated through evaluation and analysis."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10374,
      "completion_tokens": 602,
      "total_tokens": 10976
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.4 and 3.2",
          "reasoning": "The DISPLACE corpus contains conversations involving multiple Indian languages including Hindi, Kannada, Bengali, Malayalam, Telugu, Tamil, and Indian English. Section 3.4 explicitly states the corpus contains these multiple languages, and Section 3.2 describes participants fluent in multiple Indian languages and Indian accented English engaging in code-mixed conversations. Therefore, the dataset contains entries with more than two human languages, qualifying it as multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves more than two languages as indicated by references to at least seven languages (Hindi, Kannada, Bengali, Malayalam, Telugu, Tamil, Indian English). Hence, it is not restricted to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple Indian languages along with Indian English as a code-mixed multilingual dataset. It is not English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly multilingual with multiple Indian languages and some English usage. It is not restricted to a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of programming, structured code-related content or datasets involving programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is conversational speech and linguistic in nature with no mention of mathematical or logical formal expressions in the dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech conversations only; there is no indication of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural Indian languages and Indian English; no constructed or fictional languages are indicated."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset are explicitly documented and annotated, including multiple Indian languages and Indian English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of human conversational speech and contains multiple human languages; thus, the dataset definitely includes language content."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7592,
      "completion_tokens": 166,
      "total_tokens": 7758
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link referencing code availability",
          "reasoning": "The paper does not mention or provide any link to code related to data collection, preprocessing, or dataset generation. It only discusses dataset collection procedures and annotations but does not indicate public release of the associated code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: DISPLACE Corpus (subsections 3.1 to 3.4)",
          "reasoning": "The paper provides detailed descriptions of the dataset collection setup, data collection process, annotation methodology, and dataset composition across multiple subsections in Section 3. This includes recording environment, participant selection, annotation details, language and speaker distributions, and evaluation partitions. Hence, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7121,
      "completion_tokens": 92,
      "total_tokens": 7213
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, The Hume Vocal Burst Dataset",
          "Reasoning": "The Hume-VB dataset is a newly introduced large-scale dataset consisting of over 35 hours of nonverbal vocalization audio recorded from 1,702 human speakers using their own microphones at home, hence the data is human-generated audio recordings."
        }
      ]
    }
  },
  {
    "id": "baird22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7973,
      "completion_tokens": 200,
      "total_tokens": 8173
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2",
            "reasoning": "The Hume Vocal Burst dataset consists of nonverbal vocalizations collected from 1,702 speakers, where the labeling involves a broad range of subtle emotional states and demographic information. The paper does not specify expert annotators but implies large-scale annotation likely involving multiple non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention detailed annotation instructions provided to annotators for the HUME-VB dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "There is no indication or description of scoring rubrics or formal criteria used within the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not provide or reference examples included in the annotation guidelines for the dataset."
          }
        }
      ]
    }
  },
  {
    "id": "baird22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9103,
      "completion_tokens": 306,
      "total_tokens": 9409
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the dataset annotations was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention in the paper regarding multiple human experts performing quality assurance for the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process executed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are given in the paper about quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that an AI model was used for quality assurance or as a judge of dataset annotation quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic rule-based quality assurance methods are described in the paper with respect to dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process applied to the HUME-VB dataset annotations. There is no information about annotators, expert involvement, adjudication, or automated QA procedures, implying that either no quality assurance process was applied or it was not documented in this work."
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8721,
      "completion_tokens": 388,
      "total_tokens": 9109
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The paper explicitly states that the Hume Vocal Bursts (HUME-VB) dataset was collected from 1,702 speakers who recorded 36:47:04 of audio data of nonverbal vocalizations in their own homes via their own microphones. This indicates original data created entirely from scratch by human contributors, not translated, adapted, or derived from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of human translation for the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation for the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources; rather, the data was freshly collected from human subjects."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the dataset is based on existing sources with modifications or adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are well specified in the paper."
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9239,
      "completion_tokens": 333,
      "total_tokens": 9572
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3, Section 4",
          "reasoning": "The paper describes training models (single-task and multi-task deep learning architectures) from scratch using the Hume Vocal Bursts dataset, without mention of starting from pre-trained models. Section 3.3 details model training from scratch with Adam optimizer, and Section 4 presents experimental results from these trainings."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The dataset is used to evaluate model performance for emotional expression, age, and country of origin prediction tasks. The paper reports metrics like Concordance Correlation Coefficient, Unweighted Average Recall, and Mean Absolute Error on validation and test splits, indicating use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and 5",
          "reasoning": "The paper analyzes model performance trends across single-task and multi-task learning, and discusses acoustic feature relevance and the relationship between emotional expression and demographic traits. Section 5 suggests further analysis of acoustic features, indicating the dataset is used for pattern analysis beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9962,
      "completion_tokens": 527,
      "total_tokens": 10489
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2 - The Hume Vocal Burst Dataset",
          "reasoning": "The HUME-VB dataset includes vocal burst samples from speakers in four countries: USA, China, South Africa, and Venezuela. This indicates that the dataset contains data in multiple languages corresponding to these countries, thus making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains data from more than two countries and presumably more than two languages, so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to English only; it includes samples from speakers in multiple countries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to a single non-English language; multiple languages from several countries are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only human nonverbal vocalizations and does not include programming or structured code-related contents."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that mathematical or formal logical expressions are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human nonverbal vocalizations and does not include biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset are identifiable based on the countries of origin of the speakers and are documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are human vocalizations and thus contain language elements, so language is present."
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7180,
      "completion_tokens": 222,
      "total_tokens": 7402
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and throughout the paper",
          "reasoning": "The paper mentions using the Hume Vocal Burst (HUME-VB) dataset, which was first presented as part of a 2022 ICML Expressive Vocalizations Workshop & Competition [18], but does not provide any link or information regarding code availability for the dataset collection, preprocessing, or generation. There is no explicit mention of publicly available code associated with the dataset in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: The Hume Vocal Burst Dataset",
          "reasoning": "Section 2 provides detailed description of the HUME-VB dataset, including its size (36:47:04 hours of audio), number of speakers (1,702), demographic details (age range, gender count, countries of origin), data collection conditions (collected at subjects' homes via their own microphones), and distribution of samples across train, validation, and test splits. This constitutes a clear and transparent documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7955,
      "completion_tokens": 288,
      "total_tokens": 8243
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1, 2.3",
          "Reasoning": "The dataset includes studio-recorded speech sentences spoken by human speakers (human generated) and domestic noise recordings (human generated). Additionally, the speech-in-noise scenes were simulated by convolving these signals with Binaural Room Impulse Responses and applying hearing aid processing algorithms (model generated) to create the processed signals. The audio was presented to human hearing impaired listeners for responses, confirming human involvement."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The listener responses were recorded as voice and then transcribed using Google Cloud Speech-to-Text API (model generated). These transcripts were further validated and corrected by human transcribers (human generated). The ground-truth sentences are known and were used for alignment to compute intelligibility scores."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The dataset includes quantitative listener characteristics such as bilateral pure-tone audiograms measured at standard frequencies, hearing loss classification, and other standard hearing test results. These measurements originate from clinical human testing and are thus human generated tabular data."
        }
      ]
    }
  },
  {
    "id": "barker22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8807,
      "completion_tokens": 235,
      "total_tokens": 9042
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3",
            "reasoning": "The intelligibility scores were obtained from 27 hearing-impaired listeners who repeated sentences heard through hearing aids. Their voice recordings were transcribed automatically and then a team of transcribers validated and corrected errors, indicating multiple human non-expert annotators conducted the text validation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to the listeners or the transcribers regarding how to annotate the responses; only the procedure is explained."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "No evidence is provided that formal scoring rubrics were given to annotators or listeners; the intelligibility score was computed using a direct count of words correct from aligned transcriptions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "The paper does not mention providing annotation examples or sample annotations for the transcription validation process or listener responses."
          }
        }
      ]
    }
  },
  {
    "id": "barker22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9937,
      "completion_tokens": 394,
      "total_tokens": 10331
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 (The Listeners and Listening Tests)",
          "reasoning": "The paper describes a panel of 27 hearing-impaired listeners who participated in listening tests. The listener responses were transcribed using Google Cloud Speech-to-Text API, and then a team of transcribers validated the outputs and corrected errors. The use of a team suggests multiple human annotators were involved in quality assurance. Given that the transcribers are involved in validation and correction, and the listeners themselves are the target demographic, this indicates that multiple human experts or members of the target demographic performed QA on the annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that QA was conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that QA was conducted by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description suggests an AI model was used to perform QA or judgment of annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 (The Listeners and Listening Tests)",
          "reasoning": "Initial transcripts of listener responses were generated automatically using the Google Cloud Speech-to-Text API, indicating an automated process was used as a first step of annotation. However, these automatic transcripts were then validated and corrected by human transcribers. Therefore, an automated process was part of the quality assurance pipeline."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes involving both automated transcripts and human validation are described in the paper; therefore, QA is present and documented."
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9555,
      "completion_tokens": 501,
      "total_tokens": 10056
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The panel of hearing-impaired listeners were recruited and characterized by audiograms and other hearing tests. Listening tests were conducted using software that presented processed speech-in-noise signals to these listeners in their homes. The listeners responded by repeating what they heard, and their verbal responses were recorded, transcribed, and manually corrected by human transcribers. This process resulted in original, human-generated data comprising 7233 listener intelligibility responses. Thus, this dataset is original content created by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described does not include any data generated entirely by AI or machine learning models; rather, AI/ML systems were submitted by participants for prediction tasks."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data involved translating content from one language to another via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated via automatic machine translation processes according to the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not merely aggregated or collected from existing sources; it was produced through new experimental recordings and tests."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The speech-in-noise signals used were based on prior resources: 1500 samples were selected from the 10,000 speech-in-noise scenes generated for the prior Clarity Enhancement Challenge (CEC1). These scenes were generated by simulating acoustic environments and processing with various hearing aid algorithms. Thus, the audio stimuli represent derived data based on existing generated data, with additional signal processing and hearing aid algorithm applications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10073,
      "completion_tokens": 312,
      "total_tokens": 10385
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The paper describes that participants use the provided dataset to train models predicting speech intelligibility scores. Multiple submitted systems employ supervised learning to map inputs (processed signals and listener data) to intelligibility scores, indicating usage for supervised fine-tuning of models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3, Section 4, and Section 5",
          "reasoning": "The dataset is explicitly used as a benchmark to evaluate and compare intelligibility prediction models submitted to the Clarity Prediction Challenge (CPC1). Results, correlations, and ranking of models are presented using this dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Results) and Sections 4.1 to 4.3",
          "reasoning": "The dataset is used for analyzing model performances, intelligibility prediction challenges, hearing loss modeling strategies, and binaural processing approaches, providing insights into the state of the art and model behaviors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10796,
      "completion_tokens": 545,
      "total_tokens": 11341
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses speech stimuli that are studio-recorded sentences in English, and there is no indication of inclusion of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates the use of exactly two human languages in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 The Signals, Paragraph 1; Reference [11]",
          "reasoning": "The dataset uses 1,500 speech-in-noise scenes derived from a corpus of British English speech recordings (see Section 2.1 and reference [11]). The target sentences are studio-recorded English sentences. There is no mention of other spoken languages used."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not use a non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although machine learning methods and software are used for prediction models, the dataset itself does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical or logical expressions are used in the modeling and evaluation, but the dataset itself does not contain such notation as part of its entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech stimuli and listener responses, with no biological sequences or non-human communication systems involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses natural human language (English) sentences, not constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language is clearly specified as English, so language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data, specifically English speech, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8014,
      "completion_tokens": 171,
      "total_tokens": 8185
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section provides code links",
          "reasoning": "The paper does not include any explicit references, links, or mentions of publicly available code repositories related to the dataset construction, preprocessing, or generation. While it references prior work and datasets, there is no indication that the code used to create or process the new dataset for this challenge is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Materials",
          "reasoning": "Section 2 ('Materials') of the paper provides a detailed description of the dataset construction, including the speech-in-noise signals, hearing aid systems, listener characteristics, and intelligibility measurements. It describes the signal synthesis, HA processing, listener recruitment, and listening tests with transcription methodology, thus providing comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6663,
      "completion_tokens": 206,
      "total_tokens": 6869
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Corpus Description",
          "Reasoning": "The new dataset consists of audio recordings of patient-therapist dyadic conversations collected during eight-week Cognitive Behavioral Therapy sessions via voice and video call. These recordings were captured from real therapy sessions conducted by humans."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Corpus Description",
          "Reasoning": "The dataset includes video call recordings of the therapy sessions, as explicitly stated that the patients attended via voice and video call, thus involving recorded video data from human interactions."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Corpus Description",
          "Reasoning": "The recorded conversations were manually transcribed by humans into text, resulting in textual conversation transcripts that form part of the dataset."
        }
      ]
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7515,
      "completion_tokens": 195,
      "total_tokens": 7710
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 states that two CBT experts performed the annotation of these therapeutic conversations by listening to the session and completing the WAI-O-S questionnaire."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "Annotators filled out the 12-item WAI-O-S questionnaire, implying instructions associated with the established WAI-O-S inventory were provided or followed."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The WAI-O-S questionnaire is a standardized instrument using a Likert scale from 1 to 7, which serves as a scoring rubric for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention providing annotation examples to annotators or in guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8645,
      "completion_tokens": 300,
      "total_tokens": 8945
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies that two CBT experts performed the annotation of therapeutic conversations, indicating multiple experts were involved rather than a single expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Annotation task",
          "reasoning": "The annotation of the therapeutic conversations was performed by two CBT experts, meaning quality assurance was conducted by multiple human experts with subject matter expertise. This is supported by the description: 'Two CBT experts performed the annotation' and the calculation of inter-annotator agreement (IAA) to assess annotation reliability."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are explicitly described as CBT experts, thus they are not non-experts."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotators were experts, not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models being used as judges for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification of the annotations or the transcription quality is described; quality assurance was conducted via human expert annotation and IAA measurement."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance was clearly conducted and documented via multiple human expert annotations and inter-annotator agreement evaluation."
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8263,
      "completion_tokens": 438,
      "total_tokens": 8701
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus - Description, Section 3.2 Annotation task",
          "reasoning": "The dataset comprises original, manually collected and transcribed patient-therapist conversations from eight therapy sessions of three patients undergoing Cognitive Behavioral Therapy (CBT), recorded via voice and video calls. These are real interactions, not translated or derived from existing datasets. Additionally, the Working Alliance Inventory (WAI) ratings were manually annotated by two CBT experts based on listening to the sessions, further supporting that the data was created by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated by AI or machine learning models as original data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being translated from another language by human translators in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being applied to the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not merely collected from existing sources or aggregated but was newly recorded and transcribed in the context of the study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is original and not derived from existing datasets or sources with modifications; however, features such as lexical entrainment and turn-taking are derived analysis on the data, but the dataset itself is original."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and collection method of the data are specified clearly."
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8781,
      "completion_tokens": 226,
      "total_tokens": 9007
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and Section 5",
          "reasoning": "The corpus of patient-therapist spoken conversations collected during an eight-week CBT intervention is used primarily for analyzing correlations between speech/language features and working alliance ratings (WAI). The paper details the analysis of speech and language features extracted from the dataset and their association with working alliance metrics, providing insights rather than using the dataset for model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9504,
      "completion_tokens": 374,
      "total_tokens": 9878
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Corpus Description)",
          "reasoning": "The corpus consists of 23 Italian dyadic conversations during Cognitive Behavioral Therapy sessions, and the Italian form of the WAI Observer-rated Shortened inventory was administered (Section 2.2). This indicates that the dataset entries are in a single, non-English language\u2014Italian."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of transcribed spoken conversations in Italian, so it contains language."
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6722,
      "completion_tokens": 164,
      "total_tokens": 6886
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part of the paper provides a link or reference to code repositories or code artifacts.",
          "reasoning": "The paper does not mention any publicly available code or provide links to code repositories for dataset construction, preprocessing, or any related analysis tools."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Corpus and subsections 3.1 Description and 3.2 Annotation task",
          "reasoning": "The paper documents the dataset creation process, including details of data collection (patient-therapist conversations over eight weeks), manual transcription, manual annotation with Working Alliance Inventory (WAI) by experts, inter-annotator agreement, and feature extraction methods. This documentation provides a clear description of the dataset collection and annotation workflow."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7097,
      "completion_tokens": 234,
      "total_tokens": 7331
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Abstract",
          "Reasoning": "The mClothoAQA dataset includes 1991 audio files. These audio files are the same as those from the ClothoAQA dataset, which is derived from the Clotho dataset known to contain real environmental audio recordings. The paper states it created the mClothoAQA dataset by machine-translating questions and answers but did not generate new audio files, so the audio is human recorded data."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3",
          "Reasoning": "The question-answer pairs for the mClothoAQA dataset were generated by machine-translating the original English questions and answers from ClothoAQA into seven other languages using Google's machine translation API, and then they were verified and adjusted by humans. Since the majority of the data creation involved machine translation (i.e., model generated), the text data is considered model generated, with minor human involvement only in verification and adjustment."
        }
      ]
    }
  },
  {
    "id": "behera23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7949,
      "completion_tokens": 234,
      "total_tokens": 8183
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3: mClothoAQA Dataset",
            "reasoning": "The paper states that after machine translation of the original English ClothoAQA dataset, the question-answer pairs were verified and adjusted by humans. It does not mention expert annotators or specialists, implying the annotation and verification were performed by multiple human non-experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3: mClothoAQA Dataset",
            "reasoning": "There is no mention or description of any detailed annotation instructions provided to the humans conducting verification or adjustments of translated Q-A pairs."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3: mClothoAQA Dataset",
            "reasoning": "The paper does not describe the presence or use of scoring rubrics for the verification or adjustment of annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3: mClothoAQA Dataset",
            "reasoning": "No examples of annotation or guidelines including examples are provided or referenced in the paper for the annotation or verification process."
          }
        }
      ]
    }
  },
  {
    "id": "behera23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9079,
      "completion_tokens": 359,
      "total_tokens": 9438
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper that multiple human experts conducted quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3 (mClothoAQA Dataset)",
          "reasoning": "The dataset was generated by machine-translating question-answer pairs from the ClothoAQA dataset into seven other languages using Google's machine translation API, followed by verification and adjustment by humans. Specifically, the original ClothoAQA dataset questions and answers were crowdsourced, involving multiple annotators without explicit mention of subject matter expertise, and the translations were verified and adjusted by humans. The paper does not detail annotator expertise but implies multiple non-expert humans performed quality assurance by checking and adjusting the translations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of using AI models as judges for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although translations were generated automatically, the paper does not describe automated verification or rule-based quality assurance processes for validating dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is documented in the form of human verification and adjustment of machine-translated questions and answers."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8697,
      "completion_tokens": 449,
      "total_tokens": 9146
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces the mClothoAQA dataset derived from the ClothoAQA dataset, which is crowdsourced English data, but the new multilingual data is created by translation, not original human creation from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not generated by AI or machine learning models generating original content but rather is translated from an existing dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "Though the paper mentions that after machine translation, question-answer pairs were 'verified and adjusted by humans', there is no indication that the dataset was originally translated by humans. The initial translation is machine-based, with some human correction afterwards, which does not constitute full human translation."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The mClothoAQA dataset is built by translating the questions and answers from the original English ClothoAQA dataset into seven other languages using Google's machine translation API, as explicitly stated in Section 3."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a mere aggregation or collection of existing data without modification; it is generated by applying machine translation to an existing dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "mClothoAQA is derived from the ClothoAQA dataset by applying machine translation and subsequent human verification and adjustment, which constitutes modification and adaptation of the original dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9215,
      "completion_tokens": 480,
      "total_tokens": 9695
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the mClothoAQA dataset for pre-training large models on general patterns or in an unsupervised/self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that models are trained from randomly initialized parameters using the mClothoAQA dataset. Instead, they use pre-trained embeddings for audio and text as input features."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5 Experimental Results and Analysis",
          "reasoning": "The paper describes training and evaluating a multi-lingual AQA model using the mClothoAQA dataset in a supervised fashion on the question-answer pairs, fine-tuning the model's parameters for classification tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no discussion or mention of reinforcement learning or RL-based post-training methods applied with the mClothoAQA dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Experimental Results and Analysis",
          "reasoning": "The dataset is explicitly used for evaluating model performance across different languages using accuracy metrics on test splits."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 mClothoAQA Dataset and Section 5 Experimental Results and Analysis",
          "reasoning": "The authors analyze characteristics of the dataset such as question word distributions and answer word clouds, demonstrating analysis beyond purely modeling."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the dataset in supervised training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9938,
      "completion_tokens": 552,
      "total_tokens": 10490
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract and Section 3",
          "reasoning": "The paper introduces the mClothoAQA dataset, which contains audio question-answer pairs in eight human languages: English, French, Hindi, German, Spanish, Italian, Dutch, and Portuguese. This clearly indicates that the dataset includes more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of eight languages, not exactly two."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the original ClothoAQA dataset is English-only, the new dataset introduced, mClothoAQA, is explicitly multilingual, so it is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset has multiple languages including English and seven other languages, so it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio files and question-answer pairs in natural human languages; there is no mention of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of natural language questions and answers about audio; the paper does not indicate inclusion of mathematical or formal logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the audio files include environmental sounds, the dataset entries are natural language question-answer pairs, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies real human languages only; no constructed or fictional languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved in the dataset are explicitly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language entries in multiple human languages; thus, language content is present."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7156,
      "completion_tokens": 160,
      "total_tokens": 7316
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Footnote in introduction",
          "reasoning": "The paper explicitly states that the dataset and code can be accessed at https://github.com/swarupbehera/mAQA, indicating the code related to the dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (mClothoAQA Dataset)",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 3, including the use of Google's machine translation API for translating existing English QA pairs into seven other languages, human verification and adjustment, and examples of the data. This documentation is sufficient to understand how the multi-lingual dataset was constructed from the original ClothoAQA dataset."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7687,
      "completion_tokens": 142,
      "total_tokens": 7829
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Sections 1.1, 2, 5, and 6",
          "Reasoning": "The PERCEPT-R corpus introduced by the authors consists of audio recordings of child speakers of American English, specifically capturing speech containing rhotics (/\u0279/) and clinical speech sound disorders. The audio data were collected directly by research staff using participant-worn headsets (e.g., Sennheiser MKE 2) in lab-based studies. This confirms the modality as 'audio' and the origin as human-generated recordings, not model generated or of unknown provenance."
        }
      ]
    }
  },
  {
    "id": "benway22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8539,
      "completion_tokens": 221,
      "total_tokens": 8760
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 1, Introduction and Section 3.2, Leave-One-Participant-Out Validation",
            "reasoning": "The corpus contains perceptual judgments of rhotic versus derhotic /\u0279/ made by multiple listeners, implied to be experts such as language or speech pathologists, as mentioned in the description of perceptual ratings and references to listener ratings and unanimous labels."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not explicitly describe detailed annotation instructions or guidelines provided to the annotators for perceptual rating of /\u0279/."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "There is no mention of formal scoring rubrics or criteria used during annotation provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "No examples of annotation or labeling provided to annotators are mentioned in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "benway22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9669,
      "completion_tokens": 455,
      "total_tokens": 10124
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 1.1, Introduction, and throughout the description of data collection",
          "reasoning": "The dataset was collected directly by members of originating study teams, most often a research assistant and a speech language pathologist. Speech language pathologists are subject matter experts in clinical speech disorders, indicating that QA was conducted by human experts. There is no indication of single or multiple experts specifically, but the mention of speech language pathologists implies expert involvement. The paper does not mention multiple experts working together on the same annotation, so the most conservative applicable label is Single Human Expert QA."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention multiple expert annotators performing QA collectively or that annotations were cross-checked by multiple experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper indicates QA involves speech language pathologists, who are experts, therefore this label does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert annotators were involved in QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models (support vector classifiers) were trained using the data, these models were not used for quality assurance of the annotations or labels in the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based algorithms used to verify annotation quality during the collection or QA process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described and involves expert human annotators (speech language pathologists), so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9287,
      "completion_tokens": 418,
      "total_tokens": 9705
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 1.1 Motivation and Contribution, 5 Discussion",
          "reasoning": "The PERCEPT-R corpus consists of audio recordings of child speakers with typical speech and residual speech sound disorders, collected directly by the authors' research teams including research assistants and speech language pathologists. The data was generated from original recordings made specifically for this study and previous originating studies, not translated, adapted, or derived from any pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any part of the dataset was generated by AI or machine learning models; rather, the dataset consists of human-recorded child speech."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved translation from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation involved in data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the corpus compiles data from several originating studies, these consist of newly collected recordings. The data is not merely aggregated from existing datasets without modification, but instead compiled from original human-collected recordings."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present the corpus as a transformation or adaptation of existing datasets; rather it is presented as original collected data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of dataset generation is documented in the paper."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9805,
      "completion_tokens": 460,
      "total_tokens": 10265
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4",
          "reasoning": "The PERCEPT-R corpus is used to train support vector classifiers to predict perceptual judgments of rhotic and derhotic /\u0279/. The paper demonstrates training classifiers with leave-one-participant-out cross-validation, which corresponds to supervised learning fine-tuning of models on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using RL-based post-training methods such as reinforcement learning with the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4",
          "reasoning": "The dataset is explicitly used for evaluating classifier performance with metrics such as f-metric reported for classifying /\u0279/. Leave-one-participant-out cross-validation is used as an evaluation approach."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Discussion)",
          "reasoning": "The paper discusses analyzing trends and characteristics such as distribution of participant-specific f-metrics and formant features, indicating the dataset is used for analysis beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the PERCEPT-R corpus is used as a knowledge base to augment models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the PERCEPT-R dataset for training classifiers, evaluating model performance, and analyzing speech characteristics."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10528,
      "completion_tokens": 524,
      "total_tokens": 11052
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The PERCEPT-R corpus contains child speech data in American English only, with no mention of additional human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly describes the dataset as American English child speech only, with no indication of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 (Introduction); Section 5 (Discussion)",
          "reasoning": "The corpus is described as child speakers of American English, focusing solely on American English speech, with no other languages included. This is explicitly stated throughout the paper, confirming the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not of a non-English language; it is specifically American English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio recordings and linguistic labels related to speech; there is no indication of code or programming language content within the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses features and classifiers, the dataset itself contains speech audio and labels, not mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is focused on child human speech in English, with no mention of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as American English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language entries (American English child speech), so it does contain language."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7746,
      "completion_tokens": 170,
      "total_tokens": 7916
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link",
          "reasoning": "The paper does not mention or provide any link, repository, or appendix containing the code related to data collection, preprocessing, or dataset construction for PERCEPT-R. There is no evidence of publicly available code for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 1 and 5 (Introduction and Discussion)",
          "reasoning": "The paper extensively describes the data collection process, participant demographics, recording methodology (e.g., participant-worn headset), and data composition. It details the number of participants, tokens, word shapes, and audio recording details. The documentation also discusses data labeling, integration with analysis software (Phon), and demonstrated utility for classification tasks, providing sufficient transparency and completeness of dataset creation."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7408,
      "completion_tokens": 82,
      "total_tokens": 7490
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The paper introduces a new dataset comprising 1012 recordings of isolated American English nouns spoken by 6 English native speakers. These recordings are audio data captured from human speakers, as explicitly stated in Section 3.1."
        }
      ]
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8260,
      "completion_tokens": 223,
      "total_tokens": 8483
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that syllable boundaries were annotated manually for the training data collected, implying a human expert performed the annotation of syllable boundaries."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any specific annotation instructions provided to human annotators for marking syllable boundaries or stress patterns. The stress labels were assumed from dictionary entries, and syllable boundaries were manually annotated without mention of guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no discussion in the paper about scoring rubrics or grading schemes related to annotation quality or stress labels, indicating rubrics were not provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide or reference any annotation examples or exemplar annotations in guidelines for the manual annotation of syllable boundaries or stress labels."
          }
        }
      ]
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9390,
      "completion_tokens": 401,
      "total_tokens": 9791
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the quality assurance was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although syllable boundaries were manually annotated, the paper does not specify the number or expertise of annotators; therefore, we cannot conclude that multiple human experts performed QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only states 'manually annotated' syllable boundaries but does not specify the number or expertise of annotators; thus, we cannot conclude QA was performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe that quality assurance was performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset pipeline uses automatic phonetic and syllable segmentation tools (BAS Web Services), these are part of data preprocessing, not described as a quality assurance step to verify annotation correctness."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The only quality assurance mentioned is assuming native speakers produced correct stress patterns and manual annotation of syllable boundaries, with no explicit or documented quality assurance process described to validate the annotations' accuracy or consistency."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9008,
      "completion_tokens": 359,
      "total_tokens": 9367
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset consists of 1012 recordings of 92 different isolated American English nouns spoken by 6 English native speakers. This data was collected by Schwab and colleagues as original speech recordings, implying that it is original content created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that data was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the data consists of translated content by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data in the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not described as collected or aggregated from existing sources without significant modification; it was newly recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset was based on existing sources with modifications or adaptations; it was newly recorded audio data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear documentation about the origin of the data."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9526,
      "completion_tokens": 381,
      "total_tokens": 9907
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 (Training a Classifier) and Section 4 (Results and Discussion)",
          "reasoning": "The paper describes the use of the newly collected data of 1012 recordings of isolated English words spoken by natives to train machine learning classifiers from scratch. Multiple classifiers including Support Vector Machine and Neural Network were trained on extracted acoustic features from the dataset to detect lexical stress."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset to fine-tune pre-trained models; it discusses training classifiers from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training techniques are mentioned in relation to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Results and Discussion)",
          "reasoning": "The dataset is used in 10-fold cross-validation for evaluating the performance of the classifiers and the final Voting Classifier, measuring F1 scores and accuracy."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results and Discussion)",
          "reasoning": "The paper analyzes the importance of different acoustic feature groups (duration, pitch, loudness, spectral) using the dataset, studying their relative impact on classification performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented models or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively and clearly used for training classifiers, evaluating them, and analyzing feature importance, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10249,
      "completion_tokens": 511,
      "total_tokens": 10760
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The new dataset introduced consists solely of English words spoken by native American English speakers, with no indication of other languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains recordings only in English; there is no indication of exactly two human languages present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Section 3.1 describes that the training data consists of 1012 recordings of isolated American English nouns spoken by native English speakers. The entire dataset is composed exclusively of English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed exclusively of English words; no non-English languages are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech recordings and their manual annotations, not code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are included in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset focuses on human speech recordings of English words, with no biological sequences or non-human communicative signals involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No fictional or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is explicitly specified as American English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (English) recordings."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7467,
      "completion_tokens": 157,
      "total_tokens": 7624
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.4 Training a Classifier",
          "reasoning": "The paper explicitly states that the code and the final model can be found on GitHub at 'https://github.com/vera-bernhard/stress-detector', indicating availability of code related to the dataset processing and classification pipeline."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper documents the dataset creation process in Section 3.1, describing the collection of 1012 recordings of 92 English words spoken by 6 native speakers, manual annotation of syllable boundaries, and assumptions about stress labels based on dictionaries. This provides transparency and completeness about the dataset construction."
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6499,
      "completion_tokens": 87,
      "total_tokens": 6586
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Experimental Corpus",
          "Reasoning": "The paper states that for the experiments, English speech spoken by Bengali speakers was recorded using a recording app, producing new speech data specifically collected for this study. This data consists of audio recordings of human speakers (L1 Bengali) speaking English."
        }
      ]
    }
  },
  {
    "id": "bharati23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7351,
      "completion_tokens": 208,
      "total_tokens": 7559
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1 Experimental Corpus",
            "reasoning": "The paper states that some portions of the Bengali speaker speech utterances were manually annotated by a linguistics expert using Praat software to create the gold standard dataset, indicating the annotation was performed by a single human expert."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide any specific description or mention of detailed annotation instructions given to the linguistics expert annotator."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of any scoring rubrics or formal criteria provided to the annotator for the manual annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not include or mention any examples provided to the annotator to guide the annotation of the Bengali English speech data."
          }
        }
      ]
    }
  },
  {
    "id": "bharati23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8481,
      "completion_tokens": 218,
      "total_tokens": 8699
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The paper states that some portions of each speaker's speech were manually annotated by a linguistics expert using Praat software to create the gold standard dataset. This indicates quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The majority of the collected speech data were automatically annotated using canonical pronunciation with the help of the Montreal forced aligner, an automated tool. This indicates an automatic verification process as part of quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8099,
      "completion_tokens": 487,
      "total_tokens": 8586
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The paper explicitly states that an English speech corpus spoken by Bengali speakers was recorded as there was no existing dataset for Bengali speakers' English speech. Twenty-four participants were recorded reading designed stimuli (words, sentences, passages) to capture occurrences of similar and new phonemes. The data collection was performed using a recording app with human participants, indicating the data was created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated solely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translating data from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention employing machine translation to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The authors used pre-existing datasets like the TIMIT corpus and a modified subset of the VCTK corpus for L1 English speech data to bootstrap training. These datasets were collected from existing sources without significant modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Data pre-processing and Section 3.1 Experimental Corpus",
          "reasoning": "The paper describes processes such as forced alignment with Montreal Forced Aligner, phoneme sequence generation from word sequences using g2pE, and manual annotation of portions of speech data. The modified VCTK corpus was converted from word-level to phone-level sequences, and annotations were adapted with human expert involvement, indicating derived data from existing sources with some modifications and transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the data origin and processing methods explicitly."
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8617,
      "completion_tokens": 448,
      "total_tokens": 9065
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new Bengali-English speech dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.3",
          "reasoning": "The newly created L2 English speaker (L1 Bengali) speech dataset is used in training the hybrid CNN-LSTM phoneme recognizer model from scratch as described in Section 3.1 (Experimental Corpus) and Section 3.3 (Model Architecture)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate using the new dataset to fine-tune pre-trained models with supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based methods using the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.5",
          "reasoning": "The new Bengali-English speech dataset is used for testing and evaluating the pronunciation error detection system performance as described in Section 3.5."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the new dataset for analyzing trends or characteristics beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the new Bengali-English speech dataset in training and evaluation."
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9340,
      "completion_tokens": 368,
      "total_tokens": 9708
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The newly created dataset consists of English speech spoken by Bengali speakers. Thus, the dataset entries involve two human languages: English (the target L2 language) and Bengali (native L1 language of speakers). The dataset is specifically designed for L2 English speech of L1 Bengali speakers, as described in Section 3.1."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6558,
      "completion_tokens": 173,
      "total_tokens": 6731
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention of code availability in the paper.",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code related to data collection, preprocessing, or dataset generation. The description focuses on the dataset creation and experimental setup but does not indicate code sharing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Experimental Corpus and Section 3.2 Data pre-processing.",
          "reasoning": "The paper describes the creation of the new Bengali English speech dataset, including participant details, recording procedure, annotation methods (manual and automatic), and dataset splitting for training, validation, and testing. Additionally, data preprocessing steps using forced aligner and phoneme conversion are detailed. This provides a transparent and reasonably complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9807,
      "completion_tokens": 173,
      "total_tokens": 9980
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Section 3.3, Section 4",
          "Reasoning": "The INDICYT benchmark consists of audio files collected from YouTube videos, sourced by human language experts using YouTube Search API and filtered for CC-BY-4.0 license. These are human-captured audio segments since they are original audio recordings from videos."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4",
          "Reasoning": "The INDICYT benchmark includes manually transcribed text transcripts for a 2-hour subset of the YouTube audio data. These transcriptions were created by human annotators using the Shoonya platform ensuring high quality transcripts."
        }
      ]
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10659,
      "completion_tokens": 185,
      "total_tokens": 10844
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4",
            "reasoning": "Section 4 describes recruiting language experts proficient in Hindi to collect and transcribe the INDICYT benchmark audio data, indicating multiple expert annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4",
            "reasoning": "The section states transcription was done using the same guidelines as discussed in [19], implying detailed instructions were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "There is no mention of scoring rubrics or quantitative scoring criteria used during transcription in the INDICYT dataset creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not mention providing transcription examples or exemplar annotations to annotators during the INDICYT dataset creation."
          }
        }
      ]
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11789,
      "completion_tokens": 404,
      "total_tokens": 12193
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4, INDICYT Benchmark, Transcription",
          "reasoning": "The transcription of the INDICYT benchmark was carried out by human annotators who were language experts proficient in Hindi. The paper explicitly states that language experts were recruited to collect and transcribe the data, and that the transcriptions followed high-quality guidelines, ensuring expert-level quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only mentions language experts doing the collection and transcription but does not specify that multiple experts worked jointly or corroborated each other's annotations in a manner that constitutes multiple expert QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any single non-expert performed quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention of multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance does not involve AI models judging the quality of the manual transcriptions or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or rule-based algorithmic evaluation being used for annotation quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A documented human expert annotation and QA process for the INDICYT dataset is described explicitly; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11407,
      "completion_tokens": 506,
      "total_tokens": 11913
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 INDICYT Benchmark",
          "reasoning": "The INDICYT benchmark was created by recruiting language experts who curated and transcribed a diverse set of audio data from YouTube. The transcription was done by human annotators following established guidelines, ensuring that the transcriptions are original content created from scratch by human contributors rather than translations or modifications of existing content."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.3 PN-pseudolab",
          "reasoning": "PN-pseudolab dataset was generated by applying the Pratinidhi pseudo-labeling framework which involves multiple base ASR models (pseudo-transcribers) transcribing large amounts of unlabeled YouTube audio. The resulting transcripts are original model-generated outputs not directly derived or translated from existing labeled data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data created by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that machine translation was used to produce any dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 PN-lab and PN-unlab datasets",
          "reasoning": "PN-lab collates publicly available labeled datasets for Hindi from multiple sources, aggregating existing datasets without significant modification. Similarly, PN-unlab includes audio collected from YouTube by automated search and filtering. Thus, these datasets are collected or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 PN-pseudolab",
          "reasoning": "PN-pseudolab is derived by applying pseudo-labeling methods on the PN-unlab unlabeled audio data using base models trained on PN-lab. The pseudo labels are thus transformed versions of unlabeled audio enhanced by model outputs and filtering mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation processes for all datasets introduced are explicitly documented."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11925,
      "completion_tokens": 468,
      "total_tokens": 12393
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced datasets are not described as being used for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 5",
          "reasoning": "The labeled dataset PN-lab, a combination of various labeled Hindi datasets, is used to train ASR models from scratch which serve as pseudo-transcribers (Section 3.2). The combined dataset (PN-lab plus the pseudo-labeled data PN-pseudolab) is used to retrain ASR models with standard recipes (Section 5)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention fine-tuning pre-trained models in a supervised fashion on the introduced datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of reinforcement learning-based post-training techniques involving the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The INDICYT benchmark is introduced as a new evaluation benchmark with human-transcribed high-quality test data to robustly evaluate Hindi ASR systems across diverse domains."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some analysis of pseudo-labeled samples is done qualitatively, the datasets themselves are not introduced primarily for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is clear and documented usage of the introduced datasets in training and evaluation."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12648,
      "completion_tokens": 560,
      "total_tokens": 13208
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced (INDICYT) and pseudo-labeling datasets are focused exclusively on Hindi language content, there is no indication of presence of more than two languages in the entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the newly introduced benchmark INDICYT and the pseudo-labeling datasets as in Hindi only. There is no mention of two languages for the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are for Hindi language ASR and contain transcriptions and audio segments exclusively in Hindi, not in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4",
          "reasoning": "The newly created INDICYT benchmark and the PN-pseudolab datasets contain entries exclusively in Hindi. The paper explicitly states that the focus is on the Hindi language, a low-resource Indian language. The audio-transcript pairs collected and annotated are all in Hindi, thus the datasets are monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described do not contain programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic notations are mentioned as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of human speech audio and transcriptions only; no biological or non-human communication data is involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are part of the dataset entries; only Hindi is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is explicitly specified as Hindi."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain spoken language audio and transcripts, so they include language content."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9866,
      "completion_tokens": 184,
      "total_tokens": 10050
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4",
          "reasoning": "While the abstract mentions that the benchmark, code, and models will be made publicly available, the paper does not provide any explicit links or repositories to the code used for constructing the INDICYT dataset or for its data collection and preprocessing processes. Therefore, based strictly on the paper content, there is no evidence of publicly available code related to dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4",
          "reasoning": "Section 4, titled 'INDICYT Benchmark,' provides detailed documentation of the dataset creation process, including the identification of topics for collection, the collection procedure using YouTube CC-BY-4.0 license content, the transcription process with human annotators following specified guidelines, and statistics of the dataset. This constitutes clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10715,
      "completion_tokens": 100,
      "total_tokens": 10815
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Data",
          "Reasoning": "The paper introduces a new dataset comprising infant vocalizations extracted from long-form audio recordings made by the parents using a portable microphone near the child at home over the first 12 months of life. These are recorded human infant babbling sounds captured via human-operated audio recording equipment, indicating human-generated origin of audio modality data."
        }
      ]
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11567,
      "completion_tokens": 194,
      "total_tokens": 11761
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 (Data)",
            "reasoning": "The dataset consists of vocalizations automatically extracted from long-form audio recordings using an automated extraction method as described (following methodology in [11]). The vocalizations are collected via automated processing of audio rather than manual annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not mention any instructions given to annotators since the vocalizations were automatically extracted without manual labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No scoring rubrics or formal annotation guidelines are mentioned because no manual annotation of vocalization categories was performed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "As there is no human annotation, no examples for annotation are provided."
          }
        }
      ]
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12697,
      "completion_tokens": 355,
      "total_tokens": 13052
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human expert annotators performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that multiple human non-expert annotators checked the dataset for quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While an AI model (Dirichlet process mixture model) is used for clustering, it is not described as a quality assurance method for validating dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 Data and throughout Sections 3 and 4",
          "reasoning": "The infant vocalizations were automatically extracted from long-form audio recordings following the method in [11]. The dataset comprises automatically detected vocalization segments without mention of human verification or annotation. The extraction process is algorithmic and automatic, and quality assurance appears to be based on this automated detection and the statistical modeling (Dirichlet process mixture model) used for clustering and identifying false positives ('garbage cluster'). Hence, quality assurance is conducted via an automatic process and statistical modeling rather than manual validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is suggested via automated detection and statistical identification of false positives, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12315,
      "completion_tokens": 416,
      "total_tokens": 12731
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2. Data",
          "reasoning": "The dataset consists of vocalizations of a single child from birth to one year old, recorded at home by the parents in longitudinal naturalistic settings. These vocalizations were extracted from original long-form audio recordings made by human contributors and are not derived from pre-existing datasets or adapted from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data represent real infant vocalizations collected via audio recordings. Although topological data analysis and modeling are applied to these data, the original data are not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was produced by translating data from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was produced by automatic translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources; it is newly recorded data from a longitudinal study of a single child."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors compute derived features such as Mel Frequency Cepstral Coefficients and apply topological transformations, these are feature representations derived from the original recorded vocalizations for analysis; the original dataset itself is not derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and source of the data are clearly specified."
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12833,
      "completion_tokens": 214,
      "total_tokens": 13047
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The vocalization dataset is used primarily to analyze trends, patterns, and characteristics of infant vocal productions over the first year of life. The authors cluster the vocalizations and compare their temporal distributions and acoustic profiles, providing insights into developmental changes and vocalization categorization rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13556,
      "completion_tokens": 534,
      "total_tokens": 14090
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of vocalizations from a single child acquiring French language (see Section 2 Data), with no indication of multiple languages being present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains vocalizations in only one language environment, French, as described in Section 2. There is no mention of a second language in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is from a French-speaking child; no English language data is present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The dataset contains recordings of vocalizations from a female French child recorded at home over the first year of life, thus it is monolingual in French, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of audio recordings of infant vocalizations, with no programming code or structured code content included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical models and notation to analyze the data, the dataset entries themselves do not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human infant vocalizations; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of constructed, fictional, or artificial languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the infant vocalizations is specified as French; thus the language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken infant vocalizations, hence includes language in the form of human speech sounds."
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10774,
      "completion_tokens": 177,
      "total_tokens": 10951
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 (Data) and throughout the paper",
          "reasoning": "The paper does not provide any URLs, links, or references to publicly available code for data collection, preprocessing, or dataset generation. There is no mention of a public repository or supplementary materials containing code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data)",
          "reasoning": "The paper contains a detailed description of the dataset creation process including the source of the data (recordings by parents of a single child over 12 months), recording conditions, data preprocessing steps such as stereo to mono conversion, rescaling, exclusion criteria for vocalizations longer than 10 seconds, and specific counts and distributions of vocalizations per month. Ethical approvals are also mentioned. This constitutes sufficient documentation for understanding the dataset creation process."
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8792,
      "completion_tokens": 175,
      "total_tokens": 8967
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 'Data setup' and 'Corpus' and 'Dynamic language mixing procedure'",
          "Reasoning": "The new dataset is a simulated multilingual cocktail party database created using audio data from the GlobalPhone 2000 Speaker Package, which consists of human recorded speech audio from 22 languages. The authors apply an adapted dynamic language mixing procedure to generate multilingual mixtures by mixing the intra-language mixtures and creating overall cocktail party scenes on-the-fly during training, using sample SNRs and utterances from speakers. Thus, the base audio data is human generated (human recorded speech), and the mixtures are model generated via procedural mixing simulation within the adapted dynamic language mixing procedure as described, forming the novel dataset used for training and evaluation."
        }
      ]
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9644,
      "completion_tokens": 231,
      "total_tokens": 9875
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 (Data setup) and Figure 2",
            "reasoning": "The paper describes the creation of the new multilingual cocktail party database by applying an adapted dynamic language mixing procedure on GlobalPhone 2000 Speaker Package data to simulate mixtures. The mixtures and intra-language mixtures are constructed by sampling languages, speakers, and utterances and mixing signals algorithmically to create the dataset, not by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No mention of human annotator instructions as the dataset is generated via a deterministic/simulated dynamic mixing procedure rather than manual annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No description or indication of scoring rubrics provided for annotation since dataset creation is automated without subjective scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No examples of annotations or annotation guidelines are given because data generation is automatic and not manual annotation-driven."
          }
        }
      ]
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10774,
      "completion_tokens": 340,
      "total_tokens": 11114
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance being conducted by a single human annotator, nor any indication that such a human annotator was an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple non-expert humans performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention of AI models being used as judges or performing quality assurance on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset generation relies on an adapted dynamic language mixing procedure to simulate multilingual cocktail party mixtures, but the paper does not explicitly state any use of automated verification or algorithmic rule-based quality assurance processes to validate dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe, mention, or document any quality assurance process for the new dataset it constructs (GlobalPhoneMCP mixtures) beyond the data creation procedure. There is no indication that human annotators or automatic verification were used to check or validate the data quality or annotations. Thus, no quality assurance process is documented or applied."
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10392,
      "completion_tokens": 405,
      "total_tokens": 10797
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 (Corpus), Section 3.1 (Dynamic language mixing procedure)",
          "reasoning": "The paper uses the GlobalPhone 2000 Speaker Package, which is a multilingual corpus of speech data recorded from 2,000 native speakers. This data was originally recorded by human speakers, representing original human-produced speech, not derived from existing synthetic or translated data. The authors created new multilingual cocktail party mixtures by mixing these original human speech recordings to simulate overlapping multilingual speech scenarios, indicating that the base data are human-created."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not simply aggregated from existing datasets; the authors generate new mixtures from the existing speech data, thus not purely collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Dynamic language mixing procedure)",
          "reasoning": "The authors create new multilingual cocktail party mixtures by combining, mixing, and adapting the clean speech recordings from the GlobalPhone corpus via an adapted dynamic language mixing procedure. This involves modifications and transformations (mixing speakers and languages at different SNRs) to generate the input and ground truth training data for their blind language separation task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10910,
      "completion_tokens": 506,
      "total_tokens": 11416
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly created dataset for unsupervised or self-supervised pre-training of large models."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Sections 3.3 and 4",
          "reasoning": "The new dataset created via the adapted dynamic language mixing procedure from GlobalPhone 2000 Speaker Package is used to train blind language separation models from scratch. Models, such as Conv-TasNet and SepFormer, are trained from randomly initialized parameters on this dataset as described in Sections 3.3 and evaluated in Section 4."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for fine-tuning pre-trained models through supervised learning. All models reported are trained from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that reinforcement learning or RL-based post-training methods such as RLHF are applied using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 (Test data), Section 4",
          "reasoning": "The paper includes a large evaluation set comprising 231 test sets created statically from the same data source to benchmark the performance of trained BLS models, demonstrating use of the new dataset for rigorous evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The authors analyze performance trends across seen, unseen, and mixed language conditions and discuss how training set composition impacts results, indicating dataset use for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for training, evaluation, and analysis in the paper."
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11633,
      "completion_tokens": 591,
      "total_tokens": 12224
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Data setup - Corpus",
          "reasoning": "The newly simulated dataset created by the authors for Blind Language Separation is based on the GlobalPhone 2000 Speaker Package, which includes speech data for 22 different human languages. The dataset's multilingual cocktail party mixtures each consist of two randomly selected languages from these 22 (e.g., Arabic, Bulgarian, Chinese Mandarin, German, Portuguese, etc.), with multiple speakers per language per mixture (Section 3.1). Hence the dataset contains entries featuring more than two human languages across the whole dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though each mixture contains exactly two languages, the dataset overall spans more than two languages (22 languages). The dataset therefore is not restricted to exactly two languages total, but is multilingual across the entire dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content; it contains multiple non-English languages from GlobalPhone."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of mixtures with multiple languages per sample; not any single monolingual language-only content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio data and does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech audio data; no entries with mathematical or logical notation are described."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human languages and speech; it does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned as part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) in the dataset are clearly specified and documented as the 22 languages from GlobalPhone."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of audio speech data with various human languages, so it includes language content."
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8851,
      "completion_tokens": 179,
      "total_tokens": 9030
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Conclusion sections",
          "reasoning": "The paper states in the abstract and conclusion that 'We will make all information and scripts publicly available,' indicating an intention to release code and scripts, but no explicit link or repository is provided within the paper itself. Therefore, as of the paper content, the code is not made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data setup",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 3.1 (Data setup), describing the data source (GlobalPhone 2000 Speaker Package), the language mixing strategy (adapted dynamic language mixing procedure), the training and test data creation methods, including the random sampling steps and normalization procedures. Hence, the dataset creation process is well documented within the paper."
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6846,
      "completion_tokens": 144,
      "total_tokens": 6990
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Abstract",
          "Reasoning": "UserLibri dataset contains paired audio recordings from LibriSpeech, which are recordings of human speakers reading chapters from Project Gutenberg books, indicating human-generated audio data collected from real speakers."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Abstract",
          "Reasoning": "UserLibri includes additional text-only data sourced from Project Gutenberg (raw book text) corresponding to the same books read in audio. This text data is human-written literature, not generated by models."
        }
      ]
    }
  },
  {
    "id": "breiner22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7698,
      "completion_tokens": 239,
      "total_tokens": 7937
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4",
            "reasoning": "The UserLibri dataset is constructed by processing and reorganizing existing LibriSpeech and Project Gutenberg text data through automated steps such as combining chapters from the same book read by the same speaker into a single 'user' dataset, standardizing encodings, removing boilerplate, splitting text into sentences, and filtering overlapping sentences. No human annotators are mentioned for labeling or annotation tasks in dataset creation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "No mention of annotation guidelines or instructions provided to human annotators, as dataset creation is based on automated processing rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not describe any scoring rubrics or evaluation guidelines applied during dataset creation since it is automatic re-formatting and selection from existing data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "No examples of annotations or annotation outputs are provided since the dataset is generated by automatic procedures rather than human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "breiner22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8828,
      "completion_tokens": 322,
      "total_tokens": 9150
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by an AI model acting as a judge on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4, User Specific LibriSpeech Dataset",
          "reasoning": "The creation of the UserLibri dataset involved automated processing steps such as standardizing text encodings, removing boilerplate, sentence tokenization, filtering out sentences overlapping with test audio examples, and grouping chapters by speaker and book. These processes constitute automated verification and filtering ensuring dataset consistency and domain matching without manual annotation or human checking."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors document the dataset creation process and apply systematic automated processing steps that serve as a form of quality assurance; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8446,
      "completion_tokens": 431,
      "total_tokens": 8877
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset UserLibri is based on existing LibriSpeech audio and Project Gutenberg text data; no indication of entirely new human-created content is present."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the UserLibri dataset being generated by AI or machine learning models; the data is from existing human sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any human translation of data from other languages; the data is English text and audio."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was involved in producing the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (User Specific LibriSpeech Dataset)",
          "reasoning": "The UserLibri dataset is constructed by collecting and aggregating existing paired audio-transcript data from LibriSpeech and additional text-only data from Project Gutenberg books corresponding to the same users, without significant modification to the original data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (User Specific LibriSpeech Dataset)",
          "reasoning": "The authors processed the raw Gutenberg book text by standardizing encodings, removing boilerplate, segmenting into sentences, filtering overlapping text to avoid data leakage, and grouping audio examples by user, thus applying modifications and transformations to create the final UserLibri dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and processing of the UserLibri data are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8964,
      "completion_tokens": 323,
      "total_tokens": 9287
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5, especially 5.1 and 5.2",
          "reasoning": "The UserLibri dataset, which contains paired audio-transcripts and additional text-only data per user, is used to fine-tune personalized language models (LMs) that are then leveraged via shallow fusion to improve ASR performance per user. The personalized LMs are fine-tuned on user-specific text examples extracted from the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 5 and 6",
          "reasoning": "The UserLibri dataset is used to evaluate the effectiveness of the personalized LMs and the ASR models with shallow fusion by measuring word error rates (WER) per user on the test sets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 5.5 and 6",
          "reasoning": "The dataset is used to analyze the impact of personalization on word error rates across users, model sizes, and data amounts, including specific cases of improvements and failures, highlighting trends and characteristics of personalized ASR performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9687,
      "completion_tokens": 530,
      "total_tokens": 10217
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The UserLibri dataset introduced is based on LibriSpeech and Project Gutenberg texts, focusing solely on English content. There is no indication of multiple languages or multiple human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; it is solely English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 (User Specific LibriSpeech Dataset), Abstract",
          "reasoning": "The UserLibri dataset is created from the LibriSpeech dataset which consists of recordings of speakers reading English books from Project Gutenberg. The text-only data and paired audio-transcript data are all English. The paper explicitly states focusing on the English LibriSpeech dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is exclusively English, with no non-English single language data introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical notations; it consists of spoken English and textual English data only."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech and text data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of constructed or fictional languages in the UserLibri dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language\u2014the English text and transcripts\u2014so this label does not apply."
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6905,
      "completion_tokens": 198,
      "total_tokens": 7103
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided in the paper",
          "reasoning": "The paper does not provide any explicit link or reference to publicly available code repositories or tools used to generate or process the UserLibri dataset. Despite mentioning dataset creation details in Section 4, there is no indication that the code for dataset processing is shared or made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (User Specific LibriSpeech Dataset)",
          "reasoning": "The paper provides a fairly detailed description of how the UserLibri dataset was created\u2014explaining that the dataset is a reformatting of LibriSpeech, grouping audio data by user (speaker and book), processing Project Gutenberg texts to extract personalized text-only data corresponding to each user, and describing preprocessing steps such as removing duplicate sentences and formatting text. Section 4 thoroughly documents the dataset construction process, enough for readers to understand the pipeline."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 2811,
      "completion_tokens": 236,
      "total_tokens": 3047
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The new dataset consists of about 600 English video recordings (400 with masks, 200 without) collected from 14 actors portraying 7 emotions. These are real recordings, with human actors captured on video, as stated explicitly in Section 3.2."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The dataset includes audio recorded simultaneously with the video of actors performing emotions, as per Section 3.2 describing the 'recordings' collected. This is human-generated data captured by recording devices."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The text modality consists of ASR (automatic speech recognition) outputs generated by models from the audio recordings. There is no mention of human transcription, the text is produced by ASR models as input to the MER system."
        }
      ]
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 3663,
      "completion_tokens": 158,
      "total_tokens": 3821
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The paper states for German and Korean datasets, the internal collection was annotated by 3 judges, indicating multiple human expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to the annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no explicit mention of scoring rubrics or detailed evaluation criteria in the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No examples of annotations or detailed annotation examples are provided or referenced in the text."
          }
        }
      ]
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 4793,
      "completion_tokens": 278,
      "total_tokens": 5071
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single human expert was involved in quality assurance of the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "For the German and Korean datasets, the paper states that videos were annotated by 3 judges, which implies quality assurance by multiple human experts or annotators belonging to the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as judges, which implies expertise or at least a role indicating informed annotation, rather than non-expert annotation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI model for quality assurance of annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper does describe a quality assurance process involving multiple human annotators for some datasets; thus, QA is evidently documented."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 4411,
      "completion_tokens": 423,
      "total_tokens": 4834
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.2",
          "reasoning": "The authors introduced new datasets collected by themselves: for German and Korean, an internal collection of multimodal data from open license movies was curated and annotated by 3 judges, totaling approximately 3k videos for German and 1.2k for Korean. Additionally, for the mask-wearing experiment, the authors collected about 400 English recordings with masks and 200 without from 14 actors, which are real recordings, not artificially generated or synthesized."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by translating content through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any machine-translated data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "For English, the dataset was assembled by combining two pre-existing datasets: CMU-MOSEI and IEMOCAP. The authors collated these datasets, selecting videos with consistent emotion annotations, effectively aggregating existing data without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets were adaptations or transformations of existing sources beyond collation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are explicitly described."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 4929,
      "completion_tokens": 445,
      "total_tokens": 5374
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced in the paper are not used for pre-training large models; the paper discusses training with supervised methods."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The newly introduced internal datasets for German and Korean languages are used to train separate models from scratch on multimodal emotion recognition tasks as described in Section 3.1."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new datasets for supervised fine-tuning of pre-trained models; instead, models are trained directly on these datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of reinforcement learning or RL-based post-training using these datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The newly collected English masked-face dataset is used for evaluation purposes, measuring the effect of mask wearing on multimodal emotion recognition systems."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The datasets are used to analyze the impact of different modalities and the influence of mask wearing on emotion recognition model performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base or for retrieval-augmented generation according to the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the datasets' usage in training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 5652,
      "completion_tokens": 463,
      "total_tokens": 6115
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The new datasets introduced by the authors include three human languages: English, German, and Korean. Specifically, for English they combined CMU-MOSEI and IEMOCAP; for German and Korean, they used internal collections. The presence of more than two languages in the new datasets makes the dataset multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of video, audio, and text in human languages. There is no indication that programming or structured code content is part of the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the datasets contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain human speech and visual data, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset entries are explicitly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language data in English, German, and Korean."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 2870,
      "completion_tokens": 220,
      "total_tokens": 3090
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories related to dataset construction, collection, or preprocessing of the newly introduced datasets (specifically the internal German and Korean datasets and mask recordings). There is no explicit mention or pointer to code sharing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper documents the dataset creation process for the new datasets introduced by the authors. Specifically, Section 3.1 describes the internal collection for German and Korean datasets: data is collected from open license movies, cut into short videos, and annotated by 3 judges, with statistics on video counts, duration and actors. Section 3.2 details the mask dataset: about 400 masked and 200 unmasked recordings from 14 actors for 7 emotions were collected using actors wearing masks. These descriptions provide transparency on the dataset creation process, although no further protocol or detailed methodology is given."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9477,
      "completion_tokens": 88,
      "total_tokens": 9565
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Table 2",
          "Reasoning": "The paper introduces YueMotion as a newly constructed speech emotion recognition dataset in Cantonese with utterances recorded by 11 speakers using personal recording devices. This indicates the data modality is audio and the data is human-generated through manual recording sessions."
        }
      ]
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10329,
      "completion_tokens": 195,
      "total_tokens": 10524
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "The YueMotion dataset recordings involved 11 speakers each recording 10 sentences with 6 different emotions, indicating that multiple human experts or trained annotators were involved in labeling the emotional content."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not describe any provided detailed annotation instructions for the speakers or annotators of YueMotion dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no mention in the paper of scoring rubrics or systematic grading criteria used during annotation of the YueMotion dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not provide or reference any specific examples or annotation guidelines illustrating labeling for the YueMotion dataset."
          }
        }
      ]
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11459,
      "completion_tokens": 272,
      "total_tokens": 11731
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the newly introduced dataset, YueMotion."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper about quality assurance performed by multiple human experts for the YueMotion dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate quality assurance done by a single non-expert human annotator for the new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving multiple human non-expert annotators for the YueMotion dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used to perform quality assurance on the dataset's annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance performed on the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the YueMotion dataset but does not provide any information about a quality assurance process for validating the dataset's annotations or content."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11077,
      "completion_tokens": 391,
      "total_tokens": 11468
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper states that the YueMotion dataset is newly constructed with utterances recorded by 11 human speakers who spoke 10 predefined sentences with 6 different emotions. This content was created from scratch by human contributors during recording sessions."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets created using machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "While the paper constructs the BiMotion benchmark by collecting six publicly available datasets, BiMotion itself is a benchmark composed of existing datasets and not described as a new dataset. Since the instruction is to assess only new datasets introduced by the authors, BiMotion is not considered a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly constructed YueMotion dataset or any other new dataset is based on existing data transformed or adapted."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and creation process for YueMotion is explicitly documented."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11595,
      "completion_tokens": 605,
      "total_tokens": 12200
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not used exclusively or explicitly for pre-training large models in an unsupervised or self-supervised manner. The paper uses a pre-trained model (XLSR Wav2Vec 2.0) but does not describe the new datasets being used for pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the new datasets. Instead, they fine-tune pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 6 (Methodology - Cross-Group Data Augmentation; Results and Discussion)",
          "reasoning": "The newly constructed YueMotion dataset is used to fine-tune a pre-trained XLSR-53 model using supervised learning methods to evaluate transferability of emotion recognition ability in Cantonese adults. This supervised fine-tuning approach is explicitly described and analyzed in the paper."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the new datasets are used for reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.2 and 6.3 (Benchmark and Dataset - YueMotion Dataset; Results and Discussion - Effect of Language Distance on Transferability)",
          "reasoning": "The YueMotion dataset is used to evaluate and benchmark the transferability of emotion recognition ability, testing how models trained on other languages perform on Cantonese speech emotion recognition."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 6.2 and 6.3 (Results and Discussion - Cross-Group Data Augmentation; Effect of Language Distance on Transferability)",
          "reasoning": "The YueMotion dataset is used for analyzing linguistic distance impact and cross-group data augmentation effects, contributing to understanding speech features and transferability trends across languages."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not used as knowledge bases or for retrieval-augmented generation; their use is focused on fine-tuning, evaluation, and analysis in emotion recognition."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes practical uses of the new dataset YueMotion in supervised fine-tuning, evaluation, and analysis, thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12318,
      "completion_tokens": 655,
      "total_tokens": 12973
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 3.1; Section 3.2; Table 1; Table 2; Section 6.3",
          "reasoning": "The newly constructed dataset 'YueMotion' is introduced in Section 3.2 and contains Cantonese data. Additionally, 'BiMotion' benchmark constructed by the authors includes datasets in exactly two languages: English and Mandarin Chinese from six different public datasets (Table 1 and Section 3.1). YueMotion (Cantonese) is also newly constructed. Together, the datasets cover three human languages\u2014English, Mandarin, and Cantonese\u2014making the combined new datasets multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the BiMotion benchmark covers English and Mandarin, it is a benchmark combining existing datasets, not a newly constructed dataset. Only the YueMotion dataset is newly constructed, which is Cantonese only, hence not bilingual by itself. Therefore, no newly proposed dataset is strictly bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly constructed dataset YueMotion contains Cantonese speech data, i.e., non-English language; hence, it is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2; Table 2",
          "reasoning": "YueMotion is a newly constructed Cantonese speech emotion dataset described in Section 3.2 and Table 2, containing only Cantonese (a non-English language) utterances."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of datasets containing code or programming language content is found; datasets are speech emotion datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets described contain mathematical or formal logical expressions; datasets focus on speech emotion recordings."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets pertain to biological sequences or non-human communication; all focus on human speech emotion."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets involving fictional or artificially created languages are described; all datasets are human natural languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states the languages involved for newly constructed datasets; language identity is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "All newly introduced datasets contain human language speech data."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9536,
      "completion_tokens": 177,
      "total_tokens": 9713
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 7 (Conclusion)",
          "reasoning": "The paper states in the abstract and conclusion sections that the code is publicly released at https://github.com/HLTCHKUST/elderly_ser, implying that the code associated with the dataset construction and experiments is available in a public repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 (YueMotion Dataset) and Section 3 (Benchmark and Dataset)",
          "reasoning": "The paper provides detailed descriptions of the new dataset YueMotion including recording details (number of speakers, gender, sentences, emotion categories) and its statistics in Table 2. The construction of BiMotion benchmark is explained by the collection from six public datasets and how the splits are handled. These descriptions provide transparency and completeness on dataset creation."
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6456,
      "completion_tokens": 97,
      "total_tokens": 6553
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The paper states that the Assamese speech data was collected from 40 voluntary native Assamese speakers reading a translated story passage, recorded at 44.1 kHz and 24-bit resolution, then downsampled. This data was recorded by humans and is audio data captured via human-operated recording equipment."
        }
      ]
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7308,
      "completion_tokens": 263,
      "total_tokens": 7571
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 mentions the speech data was collected from 40 voluntary native Assamese speakers (10 speakers from each of the four regional varieties). The speakers presumably recorded their own speech, which implies annotation in the form of linguistic dialect identification and data collection was done by multiple human non-expert participants (native speakers), rather than labeled by trained experts or AI processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper mentions data collection from voluntary speakers reading a standardized passage, but does not describe any annotation instructions given to annotators related to labeling or classifying dialects. It only states that collected data correspond to specific dialect regions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No mention of scoring rubrics or grading schemes for annotation or labeling of dialect is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There are no annotation examples or exemplar labels provided in the paper for the data annotation process for the new dataset; the data consists of recordings by speakers."
          }
        }
      ]
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8438,
      "completion_tokens": 297,
      "total_tokens": 8735
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied to the dataset annotations or content. Although the speech data is said to have been previously collected, and segmentations and rhythm measures are computed using software tools and scripts, there is no mention of any QA process such as human verification by experts or multiple annotators, non-expert verification, AI-model based or automated QA explicitly performed or documented for the dataset. Therefore, we conclude no quality assurance process is described or performed."
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8056,
      "completion_tokens": 472,
      "total_tokens": 8528
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper explicitly states that the speech data used was collected from 40 voluntary native Assamese speakers across four districts in Assam, India. The data comprises read speech of an Assamese translation of an IPA standard story 'The North Wind and the Sun'. This indicates original content created by human contributors entirely from scratch, not translated or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated solely by AI or machine learning models without referencing or transforming existing data."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The speech data is derived from the Assamese translation of the 'North Wind and the Sun' passage, originally an IPA standard story. The paper mentions that the story was translated into Assamese, implying human translation of the source material before being read aloud by speakers to generate the speech data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation being used to produce the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected from multiple existing sources or aggregated without modification; rather, it was collected from human speakers for this study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The original IPA story was first translated into Assamese and then read aloud by speakers. Thus, the speech data is derived from a pre-existing text (the IPA standard story) through translation and subsequent recording, constituting a transformation of an existing source."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and method of data collection."
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8574,
      "completion_tokens": 294,
      "total_tokens": 8868
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The paper uses the speech dataset collected from Assamese speakers exclusively to evaluate the effectiveness of time- and frequency-domain rhythm measures for dialect classification using quadratic discriminant analysis (QDA). Classification accuracies, confusion matrices, and discriminability between dialects are reported, indicating the dataset's role is solely for evaluation and benchmarking of dialect classification methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The dataset is analyzed extensively to explore distinguishable patterns and trends in rhythm metrics across the Assamese dialects. Kernel density estimations and confusion matrices are used to characterize the linguistic features and discrimination capability of time- and frequency-domain rhythm features, indicating use for analysis of trends and characteristics rather than training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9297,
      "completion_tokens": 593,
      "total_tokens": 9890
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of Assamese language speech recorded from native Assamese speakers from different dialectal regions. There is no mention of any other human language included in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages, but only Assamese. The paper focuses on Assamese dialectal varieties and does not mention inclusion of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves readings of an Assamese translation of the story 'The North Wind and the Sun' and the speech data is exclusively in Assamese, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Data)",
          "reasoning": "The dataset consists of spoken Assamese language data collected from native speakers of four Assamese dialectal varieties. The data is read speech of an Assamese translation of a story, indicating entries are all in Assamese, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although MATLAB and Perl scripts are mentioned as tools used for analysis, the dataset itself does not contain programming or code entries. The dataset is composed of speech audio data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises speech signals and audio data of Assamese speech; it does not include any mathematical or formal logical notations as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech recordings of Assamese dialects; it does not contain any biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset recordings are from human natural language Assamese dialects; no fictional or constructed languages are involved."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is explicitly stated as Assamese, and dialectal varieties thereof. There is no indication that the language is unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data (Assamese speech), therefore it contains language entries."
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6515,
      "completion_tokens": 182,
      "total_tokens": 6697
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories for data collection, preprocessing, or dataset generation. It mentions using MATLAB and Perl scripts and Praat software, but there is no indication that these scripts or code are made available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper provides detailed documentation about the dataset creation process. Section 3.1 describes the data source, number of speakers, dialect regions, recording specifications, and content (Assamese translation of the North Wind and the Sun passage). Section 3.2 explains the preprocessing steps including breath group segmentation, vowel onset and offset detection using specific tools, and feature extraction methods. This constitutes a reasonably transparent and detailed dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6721,
      "completion_tokens": 282,
      "total_tokens": 7003
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and 3.3",
          "Reasoning": "The dataset consists of synthetic spoken instructions generated using off-the-shelf Text-to-Speech (TTS) models from the text instructions in the ALFRED dataset. The spoken audio is thus algorithmically generated by the TTS system, not directly recorded from humans. Noise is artificially introduced via systematic masking of audio segments."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The underlying sub-goal text instructions are derived from the ALFRED dataset, which contains human-annotated textual instructions for task completion in embodied environments."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 3.2",
          "Reasoning": "The visual context associated with each instruction comprises the agent's single-frame visual observations from the ALFRED environment. However, the paper does not explicitly specify the exact origin of these images, but since ALFRED is a simulation environment, these images are rendered scenes, which are synthetic but visually realistic. The paper does not clearly specify if these are human captured or synthetic; hence origin is set as unknown."
        }
      ]
    }
  },
  {
    "id": "chang23c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7573,
      "completion_tokens": 221,
      "total_tokens": 7794
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2",
            "reasoning": "The new dataset consists of synthetic spoken instructions generated from the ALFRED text instructions using an off-the-shelf Text-to-Speech (TTS) model (Silero). This dataset creation is an automatic process without human annotators involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not describe any annotation guidelines, instructions, or manual annotation steps since the dataset is automatically generated using TTS from existing text instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no mention of scoring rubrics or evaluation rubrics provided for annotation processes, as no human annotation is reported for the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No examples of annotation are provided since the spoken instruction dataset is synthesized automatically from existing text and no manual annotation process is indicated."
          }
        }
      ]
    }
  },
  {
    "id": "chang23c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8703,
      "completion_tokens": 316,
      "total_tokens": 9019
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of multiple expert human annotators performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "Quality assurance is not explicitly described; however, the dataset is synthetically generated by applying Text-to-Speech (TTS) models and systematic word-level masking with Gaussian noise according to controlled policies. This procedure is an automated, rule-based approach ensuring consistent generation and corruption of the speech signals, implying an automatic verification process governs the dataset creation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While no explicit human quality assurance is documented, the synthetic and systematic nature of dataset construction constitutes an automated process, thus quality assurance through an automatic process is applicable rather than no QA."
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8321,
      "completion_tokens": 301,
      "total_tokens": 8622
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The dataset of spoken instructions is synthesized by applying Text-To-Speech (TTS) models from the open-source Silero library to existing written instructions from the ALFRED dataset, resulting in speech data generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The spoken instruction dataset is based on the existing ALFRED textual instructions and is transformed by synthesizing speech using TTS and further modified by systematically masking spoken words with Gaussian noise to simulate acoustic degradation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8839,
      "completion_tokens": 359,
      "total_tokens": 9198
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 5",
          "reasoning": "The synthetic spoken instruction dataset derived from ALFRED sub-goal instructions is used to train both unimodal and multimodal ASR models from scratch, including a 4-layer Transformer decoder trained from scratch (Section 3.4 and Section 4.1). This dataset serves as the supervised training data for these ASR models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.4",
          "reasoning": "The dataset is used for evaluation by transcribing the spoken instructions and passing the transcriptions to an off-the-shelf ALFRED embodied agent, measuring task success rates and the impact of noisy speech on downstream embodied task completion (Section 5.4)."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (5.1, 5.2, 5.3)",
          "reasoning": "The dataset is used to analyze the effectiveness of multimodal ASR models in improving word recovery rates and word error rate under varying noise conditions, different speakers, and environments, as well as analyzing the benefit for nouns vs. non-nouns (Sections 5.1 to 5.3)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9562,
      "completion_tokens": 508,
      "total_tokens": 10070
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is derived from the ALFRED benchmark and consists of English language instructions only. There is no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced dataset contains only English spoken instructions synthesized from English text instructions. No second human language is involved."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 \"Building a Dataset of Spoken Instructions\" and throughout the paper",
          "reasoning": "The dataset is constructed by extracting English written instructions from the ALFRED dataset and synthesizing English speech using TTS models. Evaluations are done using English instructions and speech only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English language content; no non-English language data is mentioned."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken natural language instructions, not programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include mathematical or logical symbolic expressions; it contains natural language instructions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence of biological sequences or non-human communication data in the proposed dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses English natural language; no fictional or artificial constructed language is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English throughout the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language entries in English and thus does contain language."
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6780,
      "completion_tokens": 265,
      "total_tokens": 7045
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Footnote 1, footnote 1 references the open-source Silero library; also the abstract provides a GitHub URL: github.com/Cylumn/embodied-multimodal-asr",
          "reasoning": "The paper explicitly provides a GitHub URL in the abstract for the project code (github.com/Cylumn/embodied-multimodal-asr) which presumably includes the code for data construction, and it mentions the use of open-source TTS models from the Silero library, which is publicly accessible. This indicates the code related to dataset construction and processing is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 'Building a Dataset of Spoken Instructions' and Section 3.3 'Injecting Noise into Spoken Instructions'",
          "reasoning": "The paper details the dataset creation process explicitly: it explains how they extract sub-goal text instructions from the ALFRED dataset, synthesize speech using TTS (from Silero), pair instructions with visual context, and then synthetically mask audio segments at the word level to simulate noise using wav2vec 2.0 to identify word boundaries. These procedures are described clearly, demonstrating transparency and documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8832,
      "completion_tokens": 196,
      "total_tokens": 9028
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset and Abstract",
          "Reasoning": "The paper explicitly states that the updated MISP2021-AVSR corpus is a large-scale audio-visual Chinese conversational corpus consisting of 141h of audio data collected from conversations of native Chinese speakers in 34 real-home TV rooms. This data was recorded using multiple microphone arrays, indicating it was human recorded in real scenarios, not synthesized or generated."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset and Abstract",
          "Reasoning": "The updated MISP2021-AVSR corpus also includes video data collected in the same real-home TV rooms using multiple cameras capturing far/middle views of the speakers' lip regions. The video data is explicitly human captured during the conversational recordings and not generated or simulated."
        }
      ]
    }
  },
  {
    "id": "chen22o_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9684,
      "completion_tokens": 227,
      "total_tokens": 9911
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2, Section 3.1",
            "reasoning": "The paper describes the data collection in 34 real-home TV rooms with recordings from multiple microphone arrays and cameras. Lip regions of interest were automatically detected using off-the-shelf face and lip detectors and lip-centered windows were cropped. There is no mention of human annotators for transcription or labeling in the dataset, and ground truth is provided as the true word sequences likely from semi-automatic or automatic transcription processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention any detailed annotation instructions or guidelines provided to annotators, implying none exist for this dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No scoring rubrics or metrics for human annotations are discussed in the paper for dataset annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide any annotation examples or sample labeled data guidelines for the newly introduced dataset."
          }
        }
      ]
    }
  },
  {
    "id": "chen22o_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10814,
      "completion_tokens": 390,
      "total_tokens": 11204
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts or annotators with subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided in the paper about quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of AI models for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 and 4.1",
          "reasoning": "The paper mentions correcting asynchronous samples in training and development sets and applying oracle speaker diarization for segmentation, indicating automated verification and correction processes. However, no explicit human annotation for quality assurance of the dataset transcripts is described, suggesting the QA process relies on automated synchronization and segmentation verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While explicit human QA processes are not described, automated processes for data synchronization and segmentation are applied, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10432,
      "completion_tokens": 434,
      "total_tokens": 10866
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Dataset",
          "reasoning": "The paper states that the updated MISP2021-AVSR corpus contains 141.24 hours of audio and video data collected from 253 native Chinese speakers in 34 real-home TV rooms. The data was recorded in real environments involving multiple variables such as TV and lighting settings. This indicates that the corpus data was created from scratch by human contributors, through actual recordings in controlled settings. There is no indication that the data was translated or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any part of the dataset was generated by AI or machine learning models. The data originates from human recordings rather than model-generated content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information in the paper suggests the data was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details in the paper indicate machine translation was applied in producing the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is newly collected data recorded in real homes; it is not aggregated or collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being based on existing sources with modifications or adaptations. The paper emphasizes the data was recorded anew."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin is explicitly described as recordings from human participants in real environments, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10950,
      "completion_tokens": 544,
      "total_tokens": 11494
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the MISP2021-AVSR corpus for pre-training large models in an unsupervised or self-supervised manner. The focus is on supervised recognition tasks."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Section 3.2 (DNN-HMM Hybrid Acoustic Model)",
          "reasoning": "The dataset is used for training DNN-HMM hybrid acoustic models from scratch, including audio-only, video-only, and audio-visual speech recognition systems. The paper describes training procedures, model architectures, and evaluation over the corpus."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that pre-trained models are fine-tuned on this dataset; the paper rather focuses on models trained specifically on this data."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning-based post-training methods such as RLHF applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Table 3, Table 5",
          "reasoning": "The dataset is used for comprehensive evaluation and benchmarking of various audio-only, video-only, and audio-visual speech recognition models under different noise and clutter conditions, as demonstrated by CER metrics reported."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 (Analysis of errors in noise condition), Section 4.2 (Analysis of errors in overlap condition)",
          "reasoning": "The paper conducts deep analysis of error types (substitution, deletion, insertion) in different scenarios, exploring the complementarity of modalities and the challenges posed by noise and overlap."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base to augment models such as through retrieval-augmented methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple practical uses of the dataset including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11673,
      "completion_tokens": 539,
      "total_tokens": 12212
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2, Dataset",
          "reasoning": "The updated MISP2021-AVSR corpus consists solely of Mandarin Chinese speech data from native Mandarin speakers without strong accents. There is no indication of additional languages present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2, Dataset",
          "reasoning": "The dataset contains only Mandarin Chinese conversational speech; there are no two languages included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2, Dataset",
          "reasoning": "The dataset is explicitly Mandarin Chinese speech; no English content is present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2, Dataset",
          "reasoning": "The dataset consists exclusively of Mandarin Chinese conversational audio-visual speech data collected from native Mandarin-speaking participants (Section 2, Table 1). It is explicitly described as a large-scale Chinese audio-visual corpus."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain programming or structured code content. Code is discussed only in model architecture and experiment descriptions, not as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation appears in the model description and equations but is not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human audio-visual speech only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as Mandarin Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language (Mandarin Chinese), so it is not applicable to mark as containing no language."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8891,
      "completion_tokens": 137,
      "total_tokens": 9028
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 4.3",
          "reasoning": "The abstract mentions that the code is released to promote research and Section 4.3 specifies that more details of their experiments and source code can be found in their released codebase, implying the code associated with dataset processing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "Section 2 describes in detail the dataset including size, number and gender of participants, recording conditions, devices used, corrections made to the dataset, and differences from the challenge version, indicating comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9505,
      "completion_tokens": 115,
      "total_tokens": 9620
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Synthetic DoPaCo generation",
          "Reasoning": "The authors introduce a new dataset of about 160k synthetic doctor-patient conversations (DoPaCos) generated by fine-tuning a DialoGPT model on authentic conversations and then using it to generate synthetic dialog transcripts. The data are thus text modality generated by a language model (DialoGPT), making it model generated, not human generated or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "chen23i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10357,
      "completion_tokens": 239,
      "total_tokens": 10596
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1",
            "reasoning": "The paper states that the synthetic DoPaCos were generated by a fine-tuned DialoGPT model, but for the annotated DoPaCos used in fine-tuning summarization models (which are distinct and separate), human-written summaries exist. This annotated set of 1342 conversations with corresponding human-written HPI summaries implies multiple human annotators with domain expertise created these summaries."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No explicit mention of detailed annotation instructions provided for the human annotators of the summary labels in the annotated DoPaCo dataset is found in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not describe any scoring rubrics or rubric-based guidelines used by human annotators when creating the HPI summaries for the annotated dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No annotation examples or exemplar summaries are described as part of the annotation guidelines for the annotated DoPaCo dataset."
          }
        }
      ]
    }
  },
  {
    "id": "chen23i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11487,
      "completion_tokens": 362,
      "total_tokens": 11849
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that quality assurance was conducted by a single human expert annotator for any of the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that multiple human experts performed quality assurance on the new datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance being conducted by a single non-expert human annotator for any new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple non-expert human annotators performed quality assurance on the newly introduced datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the DialoGPT model is used to generate synthetic data and a RoBERTa-based NER model is used for clinical concept extraction, the paper does not state that these or any AI models were used specifically as a quality assurance judge to validate dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of automated code or rule-based processes for verification or quality assurance of the new datasets introduced."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces synthetic DoPaCo datasets generated by a fine-tuned DialoGPT model and uses existing annotated datasets for fine-tuning; however, it does not describe any explicit quality assurance or validation process applied to the synthetic data or annotated data beyond the generation method and training procedures. No QA process for validating dataset annotations or content is documented for the new synthetic dataset introduced."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11105,
      "completion_tokens": 468,
      "total_tokens": 11573
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset that is original content created entirely by humans from scratch. The annotated set of 1342 DoPaCos used for fine-tuning is a pre-existing dataset, not introduced by the authors."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper clearly describes generating about 160k synthetic doctor-patient conversations (DoPaCos) using a fine-tuned DialoGPT language model on authentic conversations (Section 2.2). This synthetic dataset is newly created by the AI model without direct copying, thus fitting the category of new data generated entirely by AI or machine learning."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating data from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors use existing datasets (authentic conversations and clinical notes), they do not describe creating or releasing these as new collated datasets, and the synthetic dataset is generated rather than collected or aggregated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The synthetic dataset is generated by adapting and fine-tuning the DialoGPT model on an existing authentic DoPaCo dataset and subsequently generating new conversations. Hence, the synthetic data is based on existing data, modified through model generation \u2013 making it derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the synthetic dataset is clearly explained in the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11623,
      "completion_tokens": 475,
      "total_tokens": 12098
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.2, 2.3, 3.1, 3.2",
          "reasoning": "The synthetic Doctor-Patient Conversation (DoPaCo) dataset is generated and used for in-domain pre-training of the Longformer-Encoder-Decoder (LED) model before fine-tuning on annotated data for the summarization task. This pre-training is unsupervised or self-supervised as described in Sections 2.2 and 2.3. The results in Sections 3.1 and 3.2 confirm the use of synthetic data in pre-training improving downstream summarization."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters using the datasets; all models start from publicly available pre-trained weights."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The annotated DoPaCo dataset with human-written summaries is used to fine-tune pre-trained LED models in a supervised manner, as described in Section 2.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any reinforcement learning post-training techniques or use RLHF with the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.4, Section 3",
          "reasoning": "The annotated DoPaCo test set and associated summaries are used for evaluation of model performance using multiple metrics (ROUGE, BERTScore, concept-based metrics) as detailed in Section 2.4 and shown in Section 3."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not primarily used for trend or pattern analysis but rather for pre-training, fine-tuning, and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base or for retrieval-augmented generation in the presented work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple practical uses of the synthetic DoPaCo dataset including pre-training and downstream task evaluation and fine-tuning."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12346,
      "completion_tokens": 550,
      "total_tokens": 12896
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses datasets consisting of English clinical conversations and notes. There is no mention of more than two languages being present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the datasets contain exactly two human languages. The study focuses solely on English medical conversations and notes."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (DoPaCo and clinical note datasets); Section 2.2 (Synthetic DoPaCo generation); throughout the paper",
          "reasoning": "The dataset consists of authentic and synthetic doctor-patient conversations and clinical notes in English only. Conversation examples, summaries, and generated text are all in English. There is no indication of any other language presence."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language datasets are introduced or discussed in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are natural language clinical conversations and notes. There is no mention or example of programming or structured code content in the data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain mathematical or formal logical expressions or symbolic representations. The content focuses on natural language text."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include biological sequences or non-human communication data. The conversations are human doctor-patient dialogues."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of the datasets described in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset entries are well specified and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain natural human language (English) in the form of conversations and clinical notes."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9564,
      "completion_tokens": 209,
      "total_tokens": 9773
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or appendix in the paper provides links to or mentions publicly available code repositories for dataset generation or related processes.",
          "reasoning": "The paper explicitly describes the method used to generate synthetic doctor-patient conversations including fine-tuning a DialoGPT model and the generation strategy. However, there is no mention or provision of any code repository or code release for the synthetic data generation process or other dataset construction scripts."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 \"Synthetic DoPaCo generation\" and Section 2.1 \"DoPaCo and clinical note datasets\"",
          "reasoning": "The paper provides detailed documentation of the dataset creation process for the synthetic dataset, including the source authentic DoPaCo dataset, the fine-tuning of a DialoGPT model, the generation parameters, and heuristics used to create the synthetic conversations. Sampling sizes and generation strategies are described. Thus, the dataset construction pipeline is well documented within the paper."
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10318,
      "completion_tokens": 97,
      "total_tokens": 10415
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The multiPA dataset consists of 50 audio clips collected from about 20 anonymous dialog chatbot users who are native Mandarin speakers practicing English. These audio recordings were created through human speech during real-world open-response scenarios using a dialog chatbot system with users' own headsets, indicating human-generated audio data."
        }
      ]
    }
  },
  {
    "id": "chen24c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11170,
      "completion_tokens": 209,
      "total_tokens": 11379
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The multiPA dataset was collected with annotation by five annotators with high proficiency in English, indicating multiple human experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Annotators were recruited and provided scores on defined scales for sentence-level and word-level labels, implying that detailed instructions for scoring were given."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Scoring criteria for sentence-level scores ranged from 1 to 5 with qualitative descriptions (e.g., 1 as very poor and 5 as excellent), and word-level scores used a 4-level intelligibility scale with clear meanings, indicating the presence of a rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not explicitly mention providing annotation examples in guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "chen24c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12300,
      "completion_tokens": 248,
      "total_tokens": 12548
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was conducted by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The MultiPA dataset annotations were performed by five annotators with high proficiency in English, indicating multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that QA was done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as having high proficiency in English, so not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance based on automated verification is not described for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states expert annotations by multiple annotators for the newly collected multiPA dataset."
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11918,
      "completion_tokens": 462,
      "total_tokens": 12380
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper states that the authors collected a new dataset, referred to as multiPA data, comprising 50 audio clips from about 20 anonymous dialog chatbot users (native Mandarin speakers practicing English). The data was annotated by five expert annotators proficient in English at both sentence and word levels. This information indicates the data was created entirely anew from human subjects and human annotators, representing original content not derived from preexisting datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was generated purely from AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention producing data by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate use of machine translation to produce dataset content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses a pre-existing dataset (speechocean762), this dataset is not newly introduced by the authors and thus excluded from this assessment. The newly introduced data (multiPA dataset) is specifically described as newly collected human data rather than aggregated existing data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly collected dataset is described as collected from real users and annotated from scratch. There is no mention of modifying, transforming, or adapting existing datasets to form the new dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins of the multiPA dataset are explicitly described, so the category of unknown or undocumented data origin does not apply."
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12436,
      "completion_tokens": 352,
      "total_tokens": 12788
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental setup, Data; Section 3 Experimental setup",
          "reasoning": "The authors explicitly state that they use the newly collected multiPA dataset for out-of-domain testing and train their MultiPA model using supervised fine-tuning methods on labeled data with various sentence-level and word-level scores. The paper describes fine-tuning a pretrained model (HuBERT) using multiPA data with annotated labels for both sentence and word levels, indicating supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental setup, Data; Section 3 Experimental setup",
          "reasoning": "The multiPA dataset is used as an out-of-domain evaluation set to test the generalizability and real-world utility of the MultiPA model. The paper states that they collected multiPA data from real-world open-response scenarios and used it for evaluation and benchmarking of their model's performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Experimental setup",
          "reasoning": "The authors conducted correlation analysis using the multiPA dataset to examine relationships between different pronunciation task scores, indicating the dataset is used for analyzing trends and characteristics as part of their study."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13159,
      "completion_tokens": 617,
      "total_tokens": 13776
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The newly collected multiPA dataset consists of audio clips from Mandarin native speakers practicing English, but the speech content is only in English. There is no indication that more than two languages are involved in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The multiPA dataset involves Mandarin native speakers using English for practice; however, the dataset entries themselves contain only English speech samples. The presence of Mandarin as native language background does not imply bilingual speech content in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The multiPA dataset entries are English speech utterances from Mandarin native speakers practicing English. The paper explicitly states that the data are collected from users practicing English, and annotations focus on English utterances. Therefore, the dataset contains entries with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset does not consist of non-English speech only; it is English speech produced by non-native speakers."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists solely of spoken utterances and annotations; there is no mention of any programming or structured code-related content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No parts of the dataset entries contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of human speech utterances only; no biological sequences or non-human communications are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains natural human language (English) spoken by learners; no fictional or artificial languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language content is clearly specified and described as English speech; thus, language(s) are known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken language, so they contain language and are not N/A."
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10377,
      "completion_tokens": 167,
      "total_tokens": 10544
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper mentions that they collected a new dataset (multiPA data) and that they will release the pilot data for other studies, but it does not provide any links or references to code repositories for data collection, preprocessing, or generation. There is no explicit mention or link to code related to dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper provides a detailed description of the new dataset, including the collection process (from dialog chatbot users practicing English), annotators and their proficiency, annotation scales for sentence and word levels, ethical approval (IRB), and features of the data, which constitutes thorough documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7628,
      "completion_tokens": 248,
      "total_tokens": 7876
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Abstract",
          "Reasoning": "The new dataset consists of 346 English videos collected manually from YouTube, including various types of videos like empathy training videos, therapy sessions, TV shows, movies, interviews, and TED Talks. These videos were created and recorded by humans, hence are human-generated videos."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.2",
          "Reasoning": "The audio modality is present as the videos contain spoken content. The audio was obtained from YouTube API and manually corrected and annotated. Since the audio comes from human speech in these videos, the audio data is human-generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The text modality exists as transcripts of the spoken content were manually transcribed or corrected and annotated by human annotators. The transcripts are derived from the human speech in the videos, therefore they are human-generated text."
        }
      ]
    }
  },
  {
    "id": "chen24f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8480,
      "completion_tokens": 219,
      "total_tokens": 8699
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "The paper states that each video is rated by at least three expert annotators for empathy labels, and a subset of 65 videos was annotated by 10 annotators with verification by at least one different annotator, indicating multiple experts involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided to annotators for labeling empathy or segment-level annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no explicit mention or description of scoring rubrics or detailed criteria used for annotation in the guidelines."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 2 in Section 3.2",
            "reasoning": "The paper includes examples of the four stages of empathy at the segment-level annotations (Table 2), which serve as annotation examples to guide annotators."
          }
        }
      ]
    }
  },
  {
    "id": "chen24f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9610,
      "completion_tokens": 315,
      "total_tokens": 9925
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states annotation was conducted by multiple annotators; no indication that only a single human expert performed QA."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Annotations",
          "reasoning": "The paper reports that audio segments were annotated between 2023 and 2024 by 10 annotators and verified by at least one different annotator. The annotators are described as experts (e.g., 'expert annotators'), indicating multiple human experts performed the QA through majority voting and cross-verification."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as experts, not non-experts, and multiple annotators were involved."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication annotators lacked expertise; they are referred to as experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge for quality assurance of the annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated diarization and alignment tools were used, quality assurance specifically involved manual re-alignment and human annotation, not solely automated verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process for annotations is described and performed by multiple expert annotators with cross-validation, so QA is present."
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9228,
      "completion_tokens": 373,
      "total_tokens": 9601
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is collected from publicly available videos, not created originally by human contributors from scratch. The dataset consists of existing YouTube videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation for dataset preparation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection",
          "reasoning": "The dataset is collected by aggregating existing English videos from YouTube using keyword searches. The data are thus aggregated from existing publicly available sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Annotations",
          "reasoning": "The dataset is partially derived in that selected videos are manually transcribed, diarized, and annotated with empathy labels and stages, adding manual modifications and metadata to the original collected videos."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of data generation are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9746,
      "completion_tokens": 266,
      "total_tokens": 10012
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5 (Empathy Classification and Results)",
          "reasoning": "The authors fine-tune a pretrained RoBERTa-base model using supervised learning methods on their new empathy speech dataset for the task of empathy classification."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Empathy Classification and Results)",
          "reasoning": "The new dataset is used for evaluating and benchmarking classification models to detect empathy in speech, demonstrated by the performance metrics reported on a held-out validation set."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Empathy Analysis)",
          "reasoning": "The dataset serves primarily for analyzing acoustic-prosodic and lexical features that characterize empathetic speech, investigating trends and patterns to better understand empathy expression."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10469,
      "completion_tokens": 485,
      "total_tokens": 10954
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset, Table 1; Section 6 Conclusions",
          "reasoning": "The new empathy speech dataset collected consists of 346 English videos totaling about 53 hours. The dataset is explicitly described as English in Section 3.1 and Table 1, and the conclusion reiterates that the corpus is of English empathetic videos."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only introduces an English empathy speech dataset. While there is mention of ongoing Mandarin data collection, this is not part of the newly introduced dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech and transcripts related to empathy detection, with no programming or code-related entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or inclusion of mathematical or formal logical expressions in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses exclusively on human speech data reflecting empathy, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset includes fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the entries is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken language (English) and transcripts, so cannot be considered language-free."
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7687,
      "completion_tokens": 167,
      "total_tokens": 7854
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not provide any link or mention of publicly available code repositories related to the data collection, preprocessing, or generation. While the dataset will be made publicly accessible, there is no indication that code used to collect or annotate the dataset is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Dataset), subsections 3.1 and 3.2",
          "reasoning": "The paper describes in detail the dataset collection process, including source (YouTube videos, keywords used, date range), dataset composition statistics, manual annotation processes, annotator details, and characteristics of the dataset (language, categories, speaker gender, etc.). This provides clear and transparent documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7945,
      "completion_tokens": 289,
      "total_tokens": 8234
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.2 and 2.2.3",
          "Reasoning": "The CNVSRC-Single and CNVSRC-Multi datasets are newly introduced by the authors as development and evaluation data for the challenge. CNVSRC-Single contains over 800 speech videos from one broadcaster, recorded from an online channel, clearly showing human-generated video data. CNVSRC-Multi includes videos from 23 speakers recorded in a studio and from 20 speakers from the Internet, with manual checks and controlled recording setups, confirming the data is human-generated video recordings intended for visual speech recognition."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3 and 2.2.1 to 2.2.3",
          "Reasoning": "Text transcriptions for CNVSRC datasets were generated by applying a Paraformer-based ASR model (model generated) to transcribe the speech in the videos, followed by manual checking to ensure transcription accuracy with less than 2% CER (human generated correction). Thus, the text data is partly model generated and partly human verified (human involvement). This applies to CNVSRC-Single, CNVSRC-Multi, and the CN-CVS dataset adapted for the challenge."
        }
      ]
    }
  },
  {
    "id": "chen24y_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8797,
      "completion_tokens": 205,
      "total_tokens": 9002
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 2.3",
            "reasoning": "The text annotations for all datasets were generated using a Paraformer-based ASR system, which is an automatic speech recognition model, to transcribe the speech in all videos."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "The paper does not mention any annotation instructions provided to human annotators because the transcription was generated by an automatic ASR system rather than humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "There is no indication of scoring rubrics or evaluation criteria for human annotation; only a manual check to ensure transcription error rate below 2% is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "No annotation examples or guidelines are presented, as the annotations were produced automatically and only verified manually for quality."
          }
        }
      ]
    }
  },
  {
    "id": "chen24y_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9927,
      "completion_tokens": 449,
      "total_tokens": 10376
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert human was involved in quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation that multiple non-expert human annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.3 Text annotation",
          "reasoning": "The paper states that a Paraformer-based ASR system was used to transcribe all videos to generate text transcriptions, indicating the use of an AI model for annotation. This constitutes quality assurance performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Text annotation; Section 2.2.3 CNVSRC-Multi",
          "reasoning": "The transcription process used automatic speech recognition (ASR) for initial transcription followed by human check. Additionally, for verifying the integrity of collected videos, a face recognition tool was utilized to ensure only the target face was present, reflecting automated verification processes. These automated and algorithmic verifications characterize quality assurance by automatic processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents quality assurance procedures involving AI transcription and some human checks; thus, it is not accurate to say no quality assurance was performed or documented."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9545,
      "completion_tokens": 509,
      "total_tokens": 10054
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2.2 and 2.2.3",
          "reasoning": "The CNVSRC-Single and CNVSRC-Multi datasets were gathered from online channels and from recording studios respectively. The CNVSRC-Multi recording studio data involved 23 speakers reading prompted sentences, which were recorded with multiple cameras and high-quality microphones, indicating original content creation by human contributors. The internet speech videos were also curated with face recognition and manual checks to ensure integrity, reflecting human involvement in data collection and preparation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any dataset was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention that data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2.3",
          "reasoning": "The CNVSRC-Multi dataset includes speeches downloaded from the internet, collected following the pipeline of [19], suggesting aggregation of existing online speech videos without significant modification, fitting the description of collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2.1 and 2.3",
          "reasoning": "The CN-CVS dataset was originally created for a video-to-speech synthesis task and did not include text transcription. For the CNVSRC challenge, the authors labeled the videos with a semi-automatic pipeline involving ASR transcription and human checking to produce text transcriptions, indicating that the data is derived from existing video-speech data with modifications (adding text labels)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of all datasets are clearly described, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10063,
      "completion_tokens": 399,
      "total_tokens": 10462
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The CN-CVS dataset is used as the primary data for pre-training the baseline systems, as described in Section 3.3. The first training phase (P1 and P2 steps) involves pre-training the models on CN-CVS video data before fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters solely on the proposed datasets; pre-training is explicitly described."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "After pre-training on CN-CVS, the models are fine-tuned on the development datasets CNVSRC-Single and CNVSRC-Multi using supervised learning methods in a fine-tuning step as described in Section 3.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning based post-training methods using the proposed datasets is made in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The CNVSRC-Single and CNVSRC-Multi datasets include evaluation sets with secret transcriptions used exclusively for benchmarking the systems during the challenge, as described in Section 2.2."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the new datasets are used primarily for analyzing trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets serve as external knowledge bases for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets have practical usage described extensively for training, fine-tuning, and evaluation in the challenge."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10786,
      "completion_tokens": 626,
      "total_tokens": 11412
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper explicitly focuses on Chinese language datasets for visual speech recognition and does not mention any datasets containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced (CN-CVS, CNVSRC-Single, CNVSRC-Multi) are all related to Chinese language visual speech recognition. No indication of bilingual content is provided."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets presented are not English language datasets but rather Chinese. No English-only dataset is introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Sections 2.2.1, 2.2.2, 2.2.3, and Section 4.2.4",
          "reasoning": "The paper introduces three new datasets: CN-CVS, CNVSRC-Single, and CNVSRC-Multi, all designed for Chinese visual speech recognition tasks. It specifically mentions the use of Chinese characters as modeling units and that CN-CVS is the largest open-source Chinese visual-speech dataset (Section 2.2.1). This confirms that the datasets contain entries in exactly one language, which is Chinese, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets focus on visual speech data (video and transcriptions) and do not include any programming or code language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the datasets contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets are human visual speech datasets and do not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of fictional or artificially created languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language used in the datasets is explicitly documented as Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain transcribed human language data (Chinese), so they cannot be classified as not containing any language."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8004,
      "completion_tokens": 214,
      "total_tokens": 8218
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and Section 3",
          "reasoning": "The paper describes the dataset collection and processing pipelines used for the newly introduced CNVSRC-Single and CNVSRC-Multi datasets, referencing the original CN-CVS dataset collection method from [19]. However, there is no mention or link to publicly available code repositories for the dataset construction, labeling, or preprocessing. While the organizers published baseline system model and code for system development, code related to data collection and dataset creation is not indicated as available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 and 2.3",
          "reasoning": "The paper provides detailed descriptions of the new datasets CNVSRC-Single and CNVSRC-Multi in Section 2.2, including their source, collection procedures, scenarios, and processing pipelines. Section 2.3 documents the text annotation method via a Paraformer-based ASR system with manual checking. These sections comprehensively document the dataset creation processes."
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7613,
      "completion_tokens": 178,
      "total_tokens": 7791
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 2.1 - Pre-processing",
          "Reasoning": "The MusicLyric dataset consists of over 320k audio sequences corresponding to music songs; the audio data is collected from human-created music available on YouTube and other sources, representing human recorded content rather than synthesized or AI-generated audio."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 2.1 - Pre-processing",
          "Reasoning": "The lyrics text data in the MusicLyric dataset are collected from multiple human-generated sources (e.g., Genius, Musixmatch, LyricFind), which are established lyrics providers containing human created lyrics; thus, the text modality originates from human generated content."
        }
      ]
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8465,
      "completion_tokens": 191,
      "total_tokens": 8656
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 2.4",
            "reasoning": "The annotations (alignments of lyrics to audio) were generated via a teacher-student paradigm with pre-trained alignment models (AutoLyrixAlign) and iterative model training producing pseudo-alignments automatically, not by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention any annotation instructions provided to annotators since annotations are generated by AI models automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or evaluation criteria for human annotation are described; evaluation relates to model outputs and thresholds for automatic filtering."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide annotation examples as the alignment labeling was done automatically by models rather than human annotators."
          }
        }
      ]
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9595,
      "completion_tokens": 344,
      "total_tokens": 9939
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance in the dataset generation or validation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of involvement of multiple human non-expert annotators for QA is provided."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.4 Alignment Framework",
          "reasoning": "Quality assurance is performed by AI models acting as teachers and students in a teacher-student paradigm where the teacher model generates pseudo-alignments and the student model seeks to outperform the teacher, with the agreement between models used to filter high-quality aligned lyric-audio pairs. This AI model-based validation ensures dataset quality without human annotation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.4 Alignment Framework",
          "reasoning": "The dataset QA involves automatic verification processes such as forced alignment via dynamic programming and beam alignment through anchoring to assess and ensure alignment quality. Additionally, filtering is applied based on error rates and consensus measures automatically, which is an algorithmic, rule-based verification method."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A clear quality assurance process is documented using AI models and automated verification methods; therefore, QA is present."
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9213,
      "completion_tokens": 485,
      "total_tokens": 9698
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not original content created entirely from scratch by human contributors; rather, it is obtained from existing sources of audio and lyrics."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content (audio-lyrics pairs) is not generated entirely by AI or machine learning models; instead, models are used to generate alignments and filter data, not to generate original audio or lyrics."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of machine translation to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Methodology), Sections 2.1 and 2.2",
          "reasoning": "The dataset is collected from existing popular music charts (Billboard, Million Song Dataset) and various online sources (YouTube, Genius, Musixmatch, LyricFind). The authors assemble (collate) the audio and lyrics pairs from these sources, which constitutes data collection from existing datasets and web."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.4 (Alignment Framework), Section 2.1 (Pre-processing)",
          "reasoning": "The authors utilize a teacher-student paradigm where models generate alignments and perform filtering/modification on the original lyrics and audio data to create accurate time-aligned labels. This involves adaptations and transformations such as chunking, filtering, pre-processing of lyrics text, alignment corrections, and iterative model-based refinement, making the dataset derived from existing sources through these modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the source and derivation process of the data; hence, the origin is documented."
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9731,
      "completion_tokens": 359,
      "total_tokens": 10090
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments)",
          "reasoning": "The dataset MusicLyric is used to train models for automatic lyrics transcription from scratch. The authors train state-of-the-art models using this dataset exclusively and demonstrate improved performance on evaluation benchmarks, indicating training from scratch on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.4 (Alignment Framework) and Section 5 (Experiments)",
          "reasoning": "The authors describe iterative training of student models fine-tuning on pseudo-labeled data generated from the dataset to improve alignment and transcription quality. This indicates supervised fine-tuning use of the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.5 (Evaluation Set) and Section 5 (Experiments)",
          "reasoning": "A curated subset of the MusicLyric dataset is released as an evaluation benchmark, and models are evaluated on this set to measure performance, supporting usage for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.2 (Properties) and throughout Sections 2 and 5",
          "reasoning": "The paper analyzes properties of the dataset such as genre distribution compared to prior datasets, vocal segmentation, and transcription intelligibility, indicating the dataset is also used for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10454,
      "completion_tokens": 590,
      "total_tokens": 11044
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Pre-processing",
          "reasoning": "The dataset focuses on English language content with a filtering step to exclude non-English pairs, allowing only a limited number of foreign words to increase data size. There is no indication that more than two languages are included in the entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Pre-processing",
          "reasoning": "The dataset is filtered to contain primarily English content; there is no mention of pairs containing exactly two distinct human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Pre-processing",
          "reasoning": "The paper states that pairs that are not in English are filtered out, while allowing a limited number of foreign words. This implies the dataset largely consists of entries with English lyrics."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2.1 Pre-processing",
          "reasoning": "The dataset specifically filters out non-English language pairs, except for a limited number of foreign words, but does not contain monolingual non-English entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains music audio and lyrics aligned data and does not include programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of mathematical or logical symbolic expressions as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of human music audio and lyrics without biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention any inclusion of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset's language content is clearly specified as predominantly English with a filtering procedure applied to ensure the same."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain lyrics (text data), which are in human language; thus, the dataset involves language content."
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7672,
      "completion_tokens": 202,
      "total_tokens": 7874
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 6 Conclusion, and Methodology Section 2",
          "reasoning": "The paper explicitly states that the dataset, toolkit, and pre-trained models are made open-source. Specifically, in the abstract, it says 'We make our dataset, toolkit, and pre-trained models open-source.' In Section 6 (Conclusion), it mentions releasing the dataset and tools fully open to the community. The methodology section describes the process in detail, supporting availability of the toolkit/code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 Methodology (subsections 2.1 to 2.4) and Section 6 Conclusion",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including data collection, filtering, preprocessing, vocal segmentation, and alignment framework. Section 2 covers these steps extensively, and Section 6 summarizes the approach. The thorough description reflects transparency and completeness for reproducibility."
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7048,
      "completion_tokens": 192,
      "total_tokens": 7240
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 and Section 2.1",
          "Reasoning": "The authors introduce a novel dataset of near homophone and synonym word pairs for their analysis. These word pairs are extracted and curated based on existing lexical databases (WordNet and Open Multilingual Wordnet) for synonyms, and phonemicized using phonetic dictionaries (CMU Pronouncing Dictionary and Epitran) to identify near homophones. The dataset uses utterances from existing speech corpora (LibriSpeech and Multilingual Spoken Words based on Common Voice) where audio segments corresponding to words are extracted using forced alignment tools. Thus, the dataset consists of human-recorded audio segments of spoken words paired by their phonetic and semantic similarities, making the modality audio and source human-generated speech recordings curated and segmented by the authors for their new dataset."
        }
      ]
    }
  },
  {
    "id": "choi24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7900,
      "completion_tokens": 223,
      "total_tokens": 8123
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 and 2.3",
            "reasoning": "The dataset of word pairs (near homophones and synonyms) is curated by automated procedures using lexical databases (WordNet, OMW), phonemic transcriptions (CMU dictionary, Epitran), and computed phonetic distances (Levenshtein distance thresholds) without human annotators involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not describe any annotation instructions provided to human annotators, as the dataset creation is automatic via lexical resources and algorithms."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No mention of scoring rubrics for annotation is present; the dataset is formed by computational criteria rather than subjective judgments."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 1",
            "reasoning": "Table 1 provides examples of word pairs and their normalized Levenshtein distances illustrating the near homophone thresholding criteria."
          }
        }
      ]
    }
  },
  {
    "id": "choi24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9030,
      "completion_tokens": 424,
      "total_tokens": 9454
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the newly introduced dataset of word pairs. The dataset construction relied primarily on automated resources and lexical databases rather than manual expert annotation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human expert annotators performing quality assurance on the dataset. The dataset curation process is based on algorithmic and database-driven extraction methods without mention of expert validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human non-expert performed quality assurance on the dataset. All dataset construction steps reported were automatic or based on existing lexical resources."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or statements describing quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge or validator in the quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "Quality assurance for the new dataset of near-homophone and synonym word pairs was performed primarily through automated, algorithmic, and rule-based techniques. Synonyms were extracted using lexical databases WordNet and Open Multilingual Wordnet accessed via the NLTK library. Near homophones were defined using normalized Levenshtein distance on phonemicized words from established pronunciation dictionaries and processing pipelines, with thresholds determined based on statistical analysis of random word pairs. The paper documents these automated, reproducible procedures as the method of dataset curation and validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented as an automatic verification of the dataset construction steps through use of lexical databases and algorithmic distance measures; thus, QA is not absent."
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8648,
      "completion_tokens": 540,
      "total_tokens": 9188
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The paper states in Section 2.1 that the authors curate a novel dataset of near homophone and synonym word pairs. These pairs are created by leveraging human-curated lexical databases such as WordNet and Open Multilingual Wordnet, and by applying defined phonetic distance thresholds to select word pairs. This curation process represents original content created from scratch by humans based on existing linguistic resources, specifically tailored to the study's requirements."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new data entirely by AI or machine learning models. The datasets used are derived or collated from existing resources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Sections 2.1 and 2.2",
          "reasoning": "The datasets such as LibriSpeech, Multilingual Spoken Words (MSW), Common Voice, WordNet and Open Multilingual Wordnet are existing datasets or databases collected from prior sources. The authors collated subsets and combined these existing data sources to facilitate their analysis (e.g., selecting dev-clean and test-clean subsets, intersecting languages supported by OMW and Epitran), but did not significantly modify their contents."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The novel dataset of near homophone word pairs is derived by applying phonetic distance calculations (using CMU pronouncing dictionary, Epitran, and Levenshtein distance) to existing words from existing datasets, setting thresholds (distance \u2264 0.4) to select pairs. This transformation from existing lexical resources constitutes a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and methods of generation for their novel dataset and experimental data."
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9166,
      "completion_tokens": 239,
      "total_tokens": 9405
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.1, 2.3 and Sections 3 and 4",
          "reasoning": "The paper introduces a novel dataset of near homophone and synonym word pairs specifically curated to analyze linguistic properties encoded in self-supervised speech models (S3Ms). The dataset is repeatedly used in similarity analyses and comparisons to understand phonetic versus semantic encoding, rather than for training or evaluation of models. The entire study revolves around analyzing trends and characteristics of representations using this dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9889,
      "completion_tokens": 611,
      "total_tokens": 10500
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper describes extracting synonyms using Open Multilingual Wordnet (OMW) covering multiple languages including English, Chinese, Italian, Spanish, Indonesian, Polish, and Swedish. For crosslingual analysis, word pairs are sampled such that one word is English and the other is from a different language among those listed, involving more than two human languages. Hence, the curated dataset includes entries across multiple languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "In the crosslingual analysis, the word pairs are constructed to contain exactly two languages: English and one other non-English language. Each pair has one English word and one word from another single language, making part of the dataset bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 and 3.3",
          "reasoning": "For the English analysis, the dataset uses English-only subsets of the LibriSpeech corpus and WordNet for synonyms, producing monolingual English word pairs."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not provide word pairs consisting of only one non-English language; non-English words only appear paired with English words in the crosslingual analysis."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of natural language word pairs and does not include any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are included in the curated dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human spoken word pairs and does not include any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions no use of fictional or artificially constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly documented and specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of spoken words in human languages, so language content is present."
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7107,
      "completion_tokens": 144,
      "total_tokens": 7251
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.1, Footnote 1",
          "reasoning": "The paper provides a footnote linking to a GitHub repository containing all the code for reproducibility: 'https://github.com/juice500ml/phonetic_semantic_probing'."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 and 2.2",
          "reasoning": "The paper details the dataset creation process in Section 2.1 (extracting synonyms and near homophones) and Section 2.2 (datasets used and how data is prepared), providing clear descriptions of data sources, processing methods, and sampling procedures."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9282,
      "completion_tokens": 182,
      "total_tokens": 9464
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset",
          "Reasoning": "The dataset is newly introduced in this paper consisting of audio samples (speech) from earnings calls by business analysts and CEOs. The speech was recorded from real conversation audio, which involves human speakers, and the audio is manually segmented into sentence-level samples using forced alignment tools. This indicates human generation via recording humans speaking."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset",
          "Reasoning": "The dataset contains human-generated manual transcripts of the audio speech samples, which are the gold standard transcriptions created by humans. Additionally, flattery annotations were done by human annotators with three expert annotators agreeing on labels, confirming human involvement in generating the textual annotations."
        }
      ]
    }
  },
  {
    "id": "christ24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10134,
      "completion_tokens": 234,
      "total_tokens": 10368
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2 Dataset",
            "reasoning": "The paper states that a team of three expert human annotators labeled the dataset for instances of flattery with full agreement among them indicating multiple experts conducted the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 Dataset and reference [25]",
            "reasoning": "The paper mentions that a detailed account of the annotation guidelines is provided in [25], implying that explicit instructions were given to the annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2 Dataset and reference [25]",
            "reasoning": "The paper refers to developing a reliable, context-sensitive, content-analytical measure for detecting and assessing analyst flattery, which implies the presence of scoring rubrics or structured criteria detailed in [25]."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2 Dataset",
            "reasoning": "While no direct examples are given in the main paper, the reference to detailed annotation guidelines in [25] suggests that examples to illustrate labeling were included to support the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "christ24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11264,
      "completion_tokens": 350,
      "total_tokens": 11614
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being done by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset was labeled by a team of three expert human annotators on a span-level, and an example is considered flattery only if all three annotators agree. This indicates quality assurance was performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that QA was conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of QA being done by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that AI models were used to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic quality assurance process is described for validating the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are explicitly described, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10882,
      "completion_tokens": 428,
      "total_tokens": 11310
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of earnings call recordings and their transcripts from existing public sources, specifically Seeking Alpha. The labeling of flattery was done by expert annotators, but the underlying content was not created from scratch by human contributors for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset is generated entirely by AI or machine learning models; audio and transcripts are sourced from human speech and existing transcription."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is in English and there is no mention of translation from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset is collected from existing sources\u2014the earnings call transcripts and audio recordings from Seeking Alpha covering 2013-2018 earnings calls. The authors aggregated these existing data sources without creating new utterances."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The authors processed the existing data by splitting transcripts into sentences, labeling spans with flattery, projecting subsentential annotations to sentence-level labels, and segmenting audio into sentence-level samples using forced alignment. Thus, the dataset is derived from existing raw data via modifications and annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the data sources and the annotation process, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11400,
      "completion_tokens": 483,
      "total_tokens": 11883
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset being used exclusively for pre-training large models in an unsupervised or self-supervised way."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses fine-tuning pretrained models or training classifiers on features extracted from pretrained models; it does not describe training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2.2",
          "reasoning": "The paper describes fine-tuning pretrained models such as RoBERTa for text and Wav2Vec 2.0 or Whisper for audio on the introduced flattery detection dataset using supervised learning methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RL-based post-training methods using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Results)",
          "reasoning": "The dataset is used for evaluation and benchmarking of the flattery detection models on training, development, and test splits, as evidenced by the detailed reporting of results."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes some discussion and analysis of the dataset's speaker demographics and model performance by gender, these are secondary and the primary use of the dataset is for training and evaluation, not solely for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the dataset as a knowledge base to augment models such as retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used practically for supervised fine-tuning and evaluation as part of the machine learning pipeline."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12123,
      "completion_tokens": 484,
      "total_tokens": 12607
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced consists of speech and transcripts from US business analyst calls, with no mention of multiple languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two languages; all data is in English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset comprises transcripts and audio from earnings calls of US companies and is processed with English-language tools; no other languages are mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset explicitly uses English transcripts and speech; no non-English language data is indicated."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio speech samples and their transcripts, with no programming or code as content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of mathematical or logical notation as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech from humans only, with no biological or non-human communication systems included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English in the dataset description."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language in the form of spoken English and transcripts, so it is not applicable."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9341,
      "completion_tokens": 149,
      "total_tokens": 9490
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 6, Conclusion",
          "reasoning": "The paper explicitly states that the raw data cannot be published due to copyright restrictions and does not mention any link or availability of code related to the dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, Dataset",
          "reasoning": "The paper provides detailed information on the dataset collection: sourcing from earnings calls transcriptions by Seeking Alpha from 2013 to 2018, annotation method involving three expert annotators with unanimous agreement for flattery labeling, sentence-level framing for machine learning, and audio segmentation via Montreal Forced Aligner. Dataset statistics and partitioning details are also given in Section 2."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8501,
      "completion_tokens": 120,
      "total_tokens": 8621
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Dataset Description and Section 2.1 Data Collection",
          "Reasoning": "The MERLIon CCS Challenge dataset is a new dataset introduced by the authors, consisting of over 30 hours of Zoom video call audio recordings of parent-child shared book reading. The audio data captures spontaneous English-Mandarin code-switching child-directed speech recorded in home environments. This data was collected from human participants during video calls and manually annotated by multilingual transcribers."
        }
      ]
    }
  },
  {
    "id": "chua23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9353,
      "completion_tokens": 221,
      "total_tokens": 9574
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.3",
            "reasoning": "Section 2.3 describes that recordings were manually annotated by a team of multilingual transcribers and crosschecked by senior team members, indicating multiple human experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "The transcription protocol used (referred to as an in-house transcription protocol) is described in Section 2.3, which includes specific instructions for segmenting utterances, marking language grains, labeling overlaps, and handling language changes."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "There is no mention of scoring rubrics or formal grading criteria for the annotations described in Section 2.3."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "No explicit mention of annotation examples is provided in the paper; the protocol is referenced but no examples are described or included explicitly."
          }
        }
      ]
    }
  },
  {
    "id": "chua23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10483,
      "completion_tokens": 311,
      "total_tokens": 10794
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Human Annotation",
          "reasoning": "The dataset annotations were performed by a team of multilingual transcribers following an in-house protocol, and to ensure quality, each file was crosschecked by at least one senior member of the transcription team. This indicates a quality assurance process involving multiple human experts, as the transcribers were multilingual and presumably trained, and a senior team member performed crosschecks."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as multilingual transcribers following a detailed protocol and crosschecking by senior team members, indicating expertise; hence not multiple non-experts performing QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models used for quality assurance of annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information provided about automated verification or algorithmic QA of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A clear QA process involving multiple human experts is documented."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10101,
      "completion_tokens": 404,
      "total_tokens": 10505
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The paper specifies that the MERLIon CCS Challenge dataset was collected as part of the Talk Together Study, where parents narrated wordless picture books to their children over Zoom. These are spontaneous spoken interactions recorded in home environments, and the audio recordings were annotated by multilingual human transcribers using a high-fidelity transcription protocol. The dataset thus consists of original audio content created entirely from scratch by human participants, without translation, adaptation, or derivation from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any audio data was generated by AI or machine learning models. All audio data is recorded human speech."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves translation from other languages by human translators. The content is spontaneous speech."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence of machine translation in dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not aggregated from existing sources but collected newly from human participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on existing sources with modifications; it is original from new recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and collection method of the dataset."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10619,
      "completion_tokens": 524,
      "total_tokens": 11143
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MERLIon CCS dataset for pre-training large models in an unsupervised or self-supervised way."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used to train models from scratch with randomly initialized parameters. In fact, baseline models are trained using other existing monolingual datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Results and Discussion)",
          "reasoning": "The paper discusses that large pre-trained models are fine-tuned on the MERLIon CCS dataset (around 30 hours) to adapt to code-switched child-directed speech and improve language identification performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of reinforcement learning post-training techniques (e.g., RLHF) using the dataset is present in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 (MERLIon CCS Challenge), Section 4 (Results and Discussion)",
          "reasoning": "The MERLIon CCS dataset is used as the evaluation benchmark for language identification and language diarization tasks in the challenge, with the Evaluation dataset being a representative subset used for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results and Discussion), Section 5 (Conclusion)",
          "reasoning": "The dataset is used to analyze the impact of short speech segments, local vernacular, rate of code-switching on model performance, and to understand challenges in language diarization, as well as for comparing model effectiveness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset being used as a knowledge base to augment models, like retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple practical uses of the dataset including fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11342,
      "completion_tokens": 562,
      "total_tokens": 11904
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The dataset involves exactly two human languages: English and Mandarin. There are no mentions of more than two languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The MERLIon CCS Challenge dataset contains English-Mandarin code-switched speech. It features spontaneous English and Mandarin utterances and code-switching between these two languages, indicating exactly two human languages are present."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "Only a small subset of recordings (20%) feature one language throughout, but these are not exclusively English; the dataset as a whole is bilingual."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "While some recordings feature only Mandarin, the dataset as a whole is bilingual English-Mandarin; the dataset is not monolingual non-English overall."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset contains speech audio data of human languages only; no programming or structured code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no indication the dataset includes mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset consists of human speech recordings; no biological sequences or non-human communication is involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No constructed or fictional languages are described as part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The languages in the dataset are clearly documented as English and Mandarin; there is no ambiguity or unspecified languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language audio data; it is not without language."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8560,
      "completion_tokens": 140,
      "total_tokens": 8700
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The paper mentions data collection as part of the Talk Together Study and details consent and IRB approval, but does not provide any link or reference to publicly available code for data collection, preprocessing, or dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Dataset Description, including 2.1 Data Collection and 2.3 Human Annotation",
          "reasoning": "The paper provides detailed descriptions of the dataset creation including data collection procedures, languages and accents, and manual human annotation protocol, indicating comprehensive documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7672,
      "completion_tokens": 109,
      "total_tokens": 7781
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2.1 and Table 1",
          "Reasoning": "The Fleurs dataset is described as a new general-purpose massively multilingual evaluation dataset in 102 languages, used for speech recognition, classification, and retrieval tasks. It consists of real recorded speech data (read speech) as indicated by the description of domains and the nature of the tasks, implying that the audio is human-generated speech recordings."
        }
      ]
    }
  },
  {
    "id": "conneau22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8524,
      "completion_tokens": 337,
      "total_tokens": 8861
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3, especially 3.2 and Table 1",
            "reasoning": "The new dataset introduced by the authors is Fleurs, covering 102 languages. It is described as a new multilingual speech understanding evaluation dataset, implying that human experts annotated the speech data for multiple languages for tasks such as speech recognition, language identification (LangID), speech classification, and retrieval. Given the complexity and multilingual nature, multiple human experts were likely involved in annotation to ensure quality and coverage."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 and general benchmark design principles in Section 3.1",
            "reasoning": "The paper emphasizes accessibility, reproducibility, and detailed dataset preparation for tasks in XTREME-S including Fleurs, indicating that annotation instructions were provided to annotators to ensure consistent and accurate labeling across 102 languages."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 and Table 1",
            "reasoning": "Evaluation metrics such as Character Error Rate (CER) for ASR and classification accuracy for LangID on Fleurs are described, indicating the presence of rubrics or scoring criteria to guide annotation and evaluation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 and Appendix (implied though not explicitly stated)",
            "reasoning": "While explicit examples are not directly detailed in the main text, the benchmark's design and emphasis on reproducibility suggest providing annotation examples or sample annotations to ensure consistent understanding across languages. The paper mentions scripts and preprocessing steps which likely include example annotations as references."
          }
        }
      ]
    }
  },
  {
    "id": "conneau22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9654,
      "completion_tokens": 288,
      "total_tokens": 9942
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the Fleurs dataset as a new dataset within XTREME-S, but it does not provide any explicit information or description about a quality assurance process for dataset annotations or validation of content. There is no mention of human annotators or expert reviewers, nor descriptions of automatic verification or AI model validation for data quality. Therefore, based on the paper content, no quality assurance process is documented for the new datasets introduced."
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9272,
      "completion_tokens": 501,
      "total_tokens": 9773
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2.1 and Section 3.2.3",
          "reasoning": "The paper introduces Fleurs, described in Section 3.2.1 and referenced in Section 3.2.3 as having original read-speech data in 102 languages. The paper states that Fleurs is a \"new general-purpose massively multilingual evaluation dataset dubbed Fleurs\" and that it includes data with train/dev/test splits. This implies that the data was generated freshly with human speakers producing speech in these languages, constituting new data created entirely by human contributors, not translations or adaptations of pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset was generated entirely by AI or machine learning models without relying on existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No explicit mention is made of data created by human translations from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper supports that any dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 and Table 1",
          "reasoning": "The XTREME-S benchmark incorporates several pre-existing datasets, such as MLS, VoxPopuli, CoVoST-2, Minds-14, and Fleurs (new but possibly involving some aggregation). The benchmark collects these datasets without significant modification to create a comprehensive evaluation suite. Therefore, the benchmark's overall data can be considered collated from existing publicly available sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While datasets are combined into a benchmark, there is no evidence that the authors significantly modify or transform existing datasets to produce new derived datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources are specified and documented; the paper provides clear references for all datasets included."
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9790,
      "completion_tokens": 315,
      "total_tokens": 10105
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Baselines; Section 4.2 Speech recognition; Section 4.3 Speech translation; Section 4.4 Speech classification",
          "reasoning": "The new dataset Fleurs introduced in the paper is used for supervised fine-tuning of pre-trained models for various downstream tasks such as speech recognition, classification (LangID), and retrieval. Specifically, models like XLS-R and mSLAM are fine-tuned on the Fleurs dataset to evaluate performance, indicating supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 Summary and Table 1; Section 4 Results; Section 5 Discussion on test sets",
          "reasoning": "Fleurs is used as a benchmark evaluation dataset across multiple speech tasks including ASR, LangID, and retrieval. The benchmark is designed for measuring and comparing model performance in a standardized way, indicating exclusive usage for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10513,
      "completion_tokens": 596,
      "total_tokens": 11109
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Section 3, Section 3.2.3, Table 1",
          "reasoning": "The paper introduces XTREME-S, a new benchmark and dataset collection, including the newly created Fleurs dataset covering 102 languages from more than 10 language families (Abstract, Section 3). The dataset entries include speech recognition, speech classification (LangID), and speech retrieval tasks that cover these 102 languages, supporting a multilingual dataset. Minds-14 is also a multilingual dataset with 14 languages in the e-banking domain. These indicate the dataset is multilingual with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced involve multiple languages (102 in Fleurs, 14 in Minds-14, 21 language pairs in CoVoST-2), not exactly two languages. Thus, bilingual label does not apply."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced contain multiple languages, not only English. No evidence suggests a dataset with only English entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced datasets such as Fleurs and Minds-14 cover multiple languages; there is no mention of any new dataset with exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with programming or structured code-related content are introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing mathematical or logical symbolic notation are introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are all human speech and text related; no biological or non-human communications."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence that datasets include constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All datasets introduced have clearly specified languages, e.g., Fleurs with 102 languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Datasets introduced contain human language speech data, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7731,
      "completion_tokens": 197,
      "total_tokens": 7928
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 and elsewhere",
          "reasoning": "The paper discusses the introduction of the XTREME-S benchmark and the new dataset Fleurs, mentioning that datasets and fine-tuning scripts are made accessible via the HuggingFace platform. However, there is no explicit mention or link to publicly available code for constructing or preprocessing the datasets, nor detailed code repositories described for dataset creation or preprocessing. Hence, code for dataset construction does not appear to be provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 and Table 1",
          "reasoning": "The paper provides detailed descriptions of the datasets included in XTREME-S, particularly Fleurs, including size, languages covered, split details, task families, and design principles for dataset and task selection. It includes information on language coverage, domains, and data regimes. Thus, the dataset creation process and its characteristics are well documented within the paper."
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7297,
      "completion_tokens": 105,
      "total_tokens": 7402
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The paper states that they collected a crowdsourced audio dataset for COVID-19 detection via a mobile app which collects three types of audio recordings (cough, breathing, speech). This data is newly collected and thus newly introduced by the authors. This data is human generated since it originates from audio recorded by human participants using a mobile application."
        }
      ]
    }
  },
  {
    "id": "dang22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8149,
      "completion_tokens": 282,
      "total_tokens": 8431
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Data and Section 4.2 Settings",
            "reasoning": "The dataset is a crowdsourced audio dataset collected from a mobile app as described in Section 4.1, with labelled COVID-19 test results and symptoms. The paper does not indicate annotation was performed by crowdsourced non-experts or multiple annotators, nor does it mention automatic or AI-based labelling. Given the clinical nature of COVID-19 test results and symptoms, it is most plausible the annotation (labelling of positive/negative status and symptoms) was conducted by or confirmed with single human experts or clinical staff. There is no evidence of expert consensus or multiple annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not provide any annotation guidelines or detailed instructions given to annotators for labelling the data. COVID-19 test results were presumably collected from participants or clinical confirmation, not requiring additional instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No scoring rubrics or detailed scoring criteria for annotation are mentioned in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No examples of annotation or annotation guidelines with examples are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "dang22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9279,
      "completion_tokens": 309,
      "total_tokens": 9588
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the collected dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about multiple non-expert human annotators involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2 (Settings)",
          "reasoning": "The paper states that all audio recordings were automatically checked using YAMNet to remove unqualified samples (e.g., noisy background). This indicates an AI model was used to perform quality assurance on the raw audio data."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification of codes or formulas for dataset annotation quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "Apart from the AI model based screening using YAMNet to remove noisy data, there is no documented human quality assurance process or other quality assurance methods applied to validate dataset annotations or content. No annotation procedure or experts verifying labels is described."
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8897,
      "completion_tokens": 360,
      "total_tokens": 9257
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The dataset used is a crowdsourced audio database collected via the authors' mobile app, comprising cough, breathing, and speech recordings along with participants' demographics and clinical data. This indicates original content created from scratch by human contributors providing recordings specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated purely by AI or machine learning models. All data described are real human audio recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data involving translation from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data involving machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not described as collected or aggregated from existing sources; rather, it is newly collected from volunteers using their app."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention that the data is modified or adapted from existing datasets or sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as collected via the authors' app from volunteers."
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9415,
      "completion_tokens": 382,
      "total_tokens": 9797
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1.1, Section 4.2",
          "reasoning": "The labelled subset of 1486 audio samples is used to train the supervised model from scratch, as stated in section 3.1.1 where the supervised model f is developed and optimized using cross-entropy loss, and section 4.2 mentions model training details with tuned parameters starting from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is used to train models but not specifically stated as fine-tuning of pre-trained models with supervised methods on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of reinforcement learning or RL-based post-training techniques applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 5",
          "reasoning": "The test partition of the labelled dataset is explicitly used for evaluation of model performance (e.g., ROC-AUC, sensitivity, specificity) as described in Section 4.1 and results are discussed in Section 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.4",
          "reasoning": "The dataset is also used for analysis such as latent space visualization using t-SNE to analyse model generalisation and clustering of positive and negative samples as shown in Section 5.4."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset serves as a knowledge base or is used to augment models through retrieval."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10138,
      "completion_tokens": 544,
      "total_tokens": 10682
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data and Section 1 Abstract",
          "reasoning": "The dataset consists of audio recordings collected via a mobile app as described in Section 4.1 and the Abstract. The paper does not mention multiple languages or bilingual content in the audio recordings. Given the project context from the University of Cambridge and the mobile app, and absence of any language diversity mention, the dataset is best characterized as containing English content only. Hence, it is considered monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper mentions code implementation (Tensorflow) and model structure, these pertain to methodology and not the dataset entries themselves. The dataset entries are audio recordings, so it does not contain code/programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 3.1 FixMatch for COVID-19 detection",
          "reasoning": "Mathematical notation is present in description of methods, but the dataset entries themselves do not contain mathematical or logical symbolic data. The notation is part of the methodology explanation, not dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are human audio recordings (cough, breathing, speech). There is no indication of biological sequences (like DNA) or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the audio recordings contain any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset entries are clearly human audio recordings, implying language content exists."
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7356,
      "completion_tokens": 202,
      "total_tokens": 7558
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.2 Settings",
          "reasoning": "The paper states in Section 4.2 that 'The code was implemented using Tensorflow [23], and can be found here.' Although the exact URL is not included in the excerpt, this explicit mention indicates that the code associated with the dataset and model implementation is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The paper provides a detailed description of the dataset creation in Section 4.1. It explains that the dataset is collected via a crowdsourced mobile app, outlines the types of audio recordings collected (cough, breathing, speech), and demographic and symptom information gathered along with COVID-19 test results. They specify the subset used, and how data was partitioned for training, validation, and testing, including data cleaning steps using YAMNet. This constitutes documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8023,
      "completion_tokens": 118,
      "total_tokens": 8141
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 (Dataset creation), Table 2, and Section 5 (Conclusion)",
          "Reasoning": "The paper describes creating a new dataset by manually adding contextual disfluencies into the existing fluent Vietnamese intent detection and slot filling dataset PhoATIS. This was done by 7 annotators who were undergraduate students, indicating human generation. The dataset consists of written Vietnamese text utterances annotated with disfluency labels along with intent and slot labels."
        }
      ]
    }
  },
  {
    "id": "dao22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8875,
      "completion_tokens": 311,
      "total_tokens": 9186
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2, paragraph beginning 'We split the PhoATIS's training set into 5 equal and non-overlapping subsets and preserve its validation and test sets.'",
            "reasoning": "The paper states that 7 annotators who are undergraduate students strong in linguistics (not experts) manually added contextual disfluencies to the dataset; each annotator annotated a subset. This indicates multiple human non-expert annotators performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2, paragraph beginning 'We split the PhoATIS's training set into 5 equal and non-overlapping subsets and preserve its validation and test sets.'",
            "reasoning": "The annotators were given specific requirements on how to add disfluencies, including semantic equivalence, naturalness, correction by following keywords, inclusion of certain types of disfluencies, and were shown example disfluencies (Table 1). This constitutes detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention any scoring rubrics or formal evaluation criteria used by annotators during annotation; instead, annotations were manually added abiding by instructions and later revised by the authors."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2 and Table 1",
            "reasoning": "The annotators were shown example disfluencies as illustrated in Table 1 to guide their annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "dao22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10005,
      "completion_tokens": 388,
      "total_tokens": 10393
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance was conducted solely by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that multiple human annotators with subject matter expertise or target demographic knowledge conducted quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2, Dataset Creation",
          "reasoning": "The dataset was created by 7 annotators who are undergraduate students strong in linguistics. These annotators manually added disfluencies, which constitutes multiple human non-experts performing annotation (no explicit claim of expertise beyond being linguistically strong undergraduates). The first two authors then manually revisited each utterance to ensure requirements were met, discussed ambiguous cases, and made revisions, indicating a form of quality assurance involving multiple non-expert humans (the authors are presumably experts, but the revisiting is described as manual but not as a formal QA performed by multiple experts on annotations). Thus, the primary QA process involves multiple non-expert annotators and manual checks."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model was used for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automatic verification or rule-based algorithmic techniques were described as part of quality assurance for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving manual checking by the first two authors after annotation was clearly described, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9623,
      "completion_tokens": 427,
      "total_tokens": 10050
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The paper describes manually adding contextual disfluencies into the existing fluent Vietnamese dataset PhoATIS by 7 human annotators who generated disfluent versions of original utterances. This manual annotation process was performed to create a new dataset with disfluency annotations, thus constituting original content created by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset content was generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not produced by translating content from another language through human translators. Although English translations are provided for illustration, the core dataset and its disfluent variant were created in Vietnamese."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that machine translation was used to generate the dataset content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not simply collected or aggregated from existing sources without significant modification; rather, it involves manual modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The new dataset is based on the existing fluent PhoATIS dataset by manually adding disfluent words and annotating them. This constitutes a derived dataset, as it modifies and extends an existing resource."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of data generation are well documented and clearly stated."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10141,
      "completion_tokens": 568,
      "total_tokens": 10709
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for any kind of unsupervised pre-training or self-supervised learning as per the paper. Instead, it is manually annotated for disfluency detection and downstream intent detection and slot filling tasks."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is used to train models from randomly initialized parameters from scratch. The models leveraged are pre-trained language models that are fine-tuned downstream."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Implementation details; Section 4 Experimental results",
          "reasoning": "The paper describes the use of the newly created disfluent Vietnamese intent detection and slot filling dataset to fine-tune pre-trained language models (XLM-R and PhoBERT) using supervised learning methods. They fine-tune a disfluency detection model on the disfluent utterances and fine-tune a joint intent detection and slot filling model on the fluent PhoATIS dataset. This use of supervised fine-tuning for downstream models is explicitly applied."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or usage of reinforcement learning post-training techniques such as RLHF is found in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experimental results",
          "reasoning": "The dataset is used for performance measurement of the disfluency detection model and downstream intent detection and slot filling models. Results in Table 3 report test set metrics, demonstrating the dataset's role in evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Error Analysis",
          "reasoning": "The authors employ the dataset to analyze error types and the impact of disfluencies on downstream tasks, providing detailed error statistics and qualitative analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or to augment models via retrieval-augmented generation or similar mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple practical usages of the dataset in supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10864,
      "completion_tokens": 531,
      "total_tokens": 11395
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset introduced is for Vietnamese intent detection and slot filling with added disfluencies, with no mention of more than two languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries only involve Vietnamese language, and no indication of exactly two human languages used."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is based on Vietnamese language utterances; though English translations are shown for illustration, the dataset entries themselves are in Vietnamese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction and Section 2 Dataset Creation",
          "reasoning": "The proposed dataset is an extension of the Vietnamese PhoATIS dataset with added disfluency annotations, fully composed of Vietnamese language utterances as explicitly described throughout the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of spoken language utterances in Vietnamese; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no presence of mathematical or formal logical expressions in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is entirely based on Vietnamese human language utterances with no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No fictional or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is explicitly stated and documented as Vietnamese; hence the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of natural language utterances and therefore contains language."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8082,
      "completion_tokens": 225,
      "total_tokens": 8307
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Introduction; Dataset creation section; Conclusion",
          "reasoning": "The paper explicitly states: 'We publicly release our dataset with disfluency annotations to facilitate future Vietnamese SLU research and applications. Our dataset is available at https://github.com/VinAIResearch/PhoATIS_Disfluency.' While the link indicates data availability, the paper title the repository as dataset and implies the data is released, it also includes dataset creation process and mentioned code implementations with PyTorch and transformers, suggesting code (likely including preprocessing or creation scripts) are accessible at the provided repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Dataset Creation and Annotation; Paragraphs describing manual annotation and quality control",
          "reasoning": "The paper provides a detailed description of the dataset creation process including how disfluencies were manually added by annotators, the criteria for disfluency inclusion, the annotation process at syllable level, validation and revision steps by authors, and statistics of the created dataset (Table 2). This is extensive documentation about dataset creation."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5803,
      "completion_tokens": 94,
      "total_tokens": 5897
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2",
          "Reasoning": "The authors collected new speech data from literate female speakers of the Lambani and Soliga tribal languages using a software tool in a soundproof studio-quality environment. This data consists of recorded audio speech utterances in these languages, explicitly stated as newly collected dataset for their low-resource TTS work."
        }
      ]
    }
  },
  {
    "id": "dasare23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6655,
      "completion_tokens": 229,
      "total_tokens": 6884
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 Dataset Details",
            "reasoning": "The dataset collection was conducted by literate female speakers from respective tribal communities and recording personnel used a specially developed software tool, indicating human experts from the tribal or related communities involved in annotation and validation of quality and correctness of translations and recordings."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 Dataset Details",
            "reasoning": "The recording tool provides procedures such as playing voice-over for illiterate speakers, verification of utterance correctness, and allowance for rerecording, which indicate clear instructions guiding the annotation/recording process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 Dataset Details",
            "reasoning": "No explicit mention of scoring or rating rubrics guiding the annotation or recording process is provided in the paper for this dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 Dataset Details",
            "reasoning": "The paper does not provide or mention any annotation examples or sample annotated instances for the new dataset recording or transcription in the tribal languages."
          }
        }
      ]
    }
  },
  {
    "id": "dasare23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7785,
      "completion_tokens": 475,
      "total_tokens": 8260
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Details) and Section 3.1 (Evaluation)",
          "reasoning": "The dataset collection involved literate female speakers from the respective tribes with good diction and voice. The translation from English to Kannada, then to Soliga and Lambani languages was performed by literate members of the tribes, which suggests subject matter expertise or membership in the target demographic. Furthermore, subjective evaluations (Mean Opinion Score) were conducted by 20 literate speakers from both languages, implying that quality assurance was performed by multiple humans for evaluation, but the dataset collection itself was validated by a single expert or knowledgeable individual in the tribal language community during translation and recording."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of multiple human experts performing quality assurance on the dataset annotations or content. While 20 speakers participated in subjective evaluation, this does not confirm multiple experts monitored or validated the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset collection and translation were performed by literate members of the tribal communities, implying expertise or membership of the target demographic, not non-experts."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that multiple non-experts performed quality assurance during dataset collection or validation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of using an AI model as a judge for dataset quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process for dataset quality is mentioned; automated evaluations relate to synthetic speech but not dataset annotation quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described via expert translations, speaker consent, and evaluations; thus, not applicable."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7403,
      "completion_tokens": 457,
      "total_tokens": 7860
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Details)",
          "reasoning": "The dataset consists of newly recorded speech data in the Lambani and Soliga tribal languages. Literate female speakers from the respective tribes recorded original spoken sentences in a soundproof studio environment producing 8 hours of Lambani and 6 hours of Soliga voice samples. This data is original and created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset was generated entirely by AI or machine learning models. The data generated by models is synthetic speech, which is not considered a dataset but model output."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Details)",
          "reasoning": "The original transcript sentences were English sentences translated into Kannada and then into Soliga and Lambani languages by literate people from the respective tribes, indicating human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper about the use of machine translation systems for data creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being collected or aggregated from existing sources without significant modification. Instead, it was created through original recordings and translations."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Details)",
          "reasoning": "The dataset is derived in the sense that the English source sentences (from Swadesh list) were translated into Kannada, then into Soliga and Lambani, thus existing content was adapted and transformed into new languages before recording speech samples."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data creation method is explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7921,
      "completion_tokens": 441,
      "total_tokens": 8362
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not used to train a model from randomly initialized parameters. The TTS models were fine-tuned from pre-trained models, not trained from scratch (Section 3)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The Lambani and Soliga datasets are used to fine-tune Nvidia's pre-trained Tacotron2 model with transfer learning, as described in Section 3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning based post-training methods such as RLHF using the proposed datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The datasets are also used for subjective (MOS) and objective (PESQ) evaluation of the TTS models by comparing original and synthetic speech (Section 3.1)."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The datasets are used primarily for analyzing speech characteristics like LP residual, F0 contour, formants to study speech patterns and quality (Section 4)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not serve as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets have documented usages in fine-tuning, evaluation, and analysis as described above."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8644,
      "completion_tokens": 489,
      "total_tokens": 9133
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2: Dataset Details",
          "reasoning": "The dataset is based on translations originating from English sentences which are translated into Kannada and then further into two distinct tribal languages, Lambani and Soliga. Thus, the dataset effectively involves more than two human languages: English, Kannada, Lambani, and Soliga."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not restricted to exactly two human languages; rather, it involves multiple languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes languages beyond English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than one non-English language (Lambani and Soliga) along with English and Kannada used in preparation."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken language recordings and translations, without any programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are included as dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes natural tribal languages (Lambani and Soliga), not constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included in the dataset are explicitly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language data."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5862,
      "completion_tokens": 154,
      "total_tokens": 6016
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section",
          "reasoning": "The paper does not mention any link, repository, or location where code related to data collection or dataset construction for the tribal languages is made available. It only describes the data collection process and TTS system training but provides no indication that the code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Dataset Details)",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 2, including the source of sentences, translation procedure, recording environment, speaker details, and consent process. This documentation covers essential aspects of dataset construction, making it relatively transparent and reproducible in terms of data collection methodology."
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7673,
      "completion_tokens": 140,
      "total_tokens": 7813
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 and 5.2",
          "Reasoning": "The paper introduces a new dataset generated by their proposed finite-state Markov models of turn-taking behavior. The dataset consists of an activity matrix Y, representing speaker activity over time, generated by sampling from the fitted turn-taking models. Utterances from LibriSpeech are then aligned to these generated timings to create mixtures. This simulation process is algorithmic, producing time series data representing speech activity without recorded human audio, thus it is model-generated synthetic data representing temporal turn-taking behavior in conversations."
        }
      ]
    }
  },
  {
    "id": "deadman22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8525,
      "completion_tokens": 209,
      "total_tokens": 8734
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2",
            "reasoning": "The new dataset generation described is achieved through a finite-state generative model trained on timing information from existing datasets (CHiME-5 and AMI). The generation process is a deterministic simulation based on computed transition probabilities and durations, hence annotated by an automatic process (the generative finite-state model)."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of human annotators or instructions guiding manual annotation for the new dataset generation; the data is generated automatically using the model."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or manual evaluation criteria are specified for annotations related to the generation of the new data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided because the dataset is generated algorithmically without manual annotation steps."
          }
        }
      ]
    }
  },
  {
    "id": "deadman22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9655,
      "completion_tokens": 351,
      "total_tokens": 10006
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about any single non-expert human performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an AI model used to perform quality assurance on dataset annotations or contents."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the method uses automated finite-state Markov models and computational training procedures, these are methods for dataset generation and feature extraction rather than explicit automated quality assurance of the dataset annotations themselves. The paper does not describe any automated verification or code-based QA process applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a generative method to simulate data based on real dataset statistics (from CHiME-5 and AMI), but it does not explicitly describe any quality assurance process performed to validate the quality or correctness of dataset annotations or content, nor for the newly generated simulated data. There is no mention of human annotators verifying annotations or any automated QA procedures. Hence, no quality assurance is documented for the introduced simulated datasets."
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9273,
      "completion_tokens": 513,
      "total_tokens": 9786
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset collected entirely from scratch by human contributors. Instead, it relies on existing datasets such as CHiME-5 and AMI for obtaining conversation transcripts and timing information; no original human-created conversation data is introduced."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.3, Section 5.2",
          "reasoning": "The authors generate new multispeaker conversation timing data by sampling from probabilistic finite-state models trained on existing conversation timing data. These generated activity matrices Y simulate turn-taking behaviour with realistic overlap statistics, allowing for arbitrary large datasets to be created. This generated data is entirely produced by the proposed generative Markov models, thus qualifying as new data from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data being translated by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or mention any use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The paper uses existing datasets (CHiME-5, AMI) to obtain transcripts and timing information, which are then used without modification to train models and create representations. This component of the data is aggregated from these existing sources without fundamental modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The generated activity matrices Y are derived by applying the finite-state generative models trained on existing datasets. Moreover, to create audio mixtures, the authors adapt LibriSpeech utterances to match modeled turn durations, which constitutes modifications and transformations of existing data, thus making the generated dataset derived from existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and generation methods of the introduced data, so the data origin is known."
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9791,
      "completion_tokens": 299,
      "total_tokens": 10090
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 and 5.3",
          "reasoning": "The generated datasets from the proposed finite-state Markov models are used to evaluate target-speaker extraction models by creating mixtures that reflect realistic turn-taking patterns. This is elaborated in Sections 5.2 Data Generation and 5.3 Results, where the datasets help measure the performance (SI-SDR) of extraction systems under realistic conversational overlap conditions."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and 5.3",
          "reasoning": "The datasets generated from the proposed models are used primarily to analyze turn-taking behavior and its effect on speaker extraction difficulty. Section 4 describes extracting interpretable vector representations from the models for characterizing parties, and Section 5.3 shows how these representations correlate with speaker extraction performance, highlighting analytical use of the datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10514,
      "completion_tokens": 562,
      "total_tokens": 11076
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the introduction of any dataset containing multiple human languages. The datasets discussed (e.g., CHiME-5, AMI, LibriSpeech) are all primarily English or not specified to include multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the new dataset introduced includes exactly two human languages. The focus is on English conversational speech."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.2 Data Generation",
          "reasoning": "The new simulated dataset is generated by aligning turn-taking behavior learned from datasets CHiME-5 and AMI with utterances from LibriSpeech (dev-clean), which is English speech. Therefore, the dataset is monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is generated using English speech from LibriSpeech; no non-English language is introduced or described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper presents mathematical formulations and discusses models, the dataset entries themselves do not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is used to describe models and methods but the dataset generated does not contain such notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pertains to human speech turn-taking and includes no biological or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no fictional or artificially created languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly states the source language is English from LibriSpeech; therefore, the language is specified, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain English language speech content, so the dataset does contain language."
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7732,
      "completion_tokens": 149,
      "total_tokens": 7881
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code related to the dataset construction or the generative model for turn-taking simulation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 5.2",
          "reasoning": "Sections 3 (Framework for Modelling Turns) and 5.2 (Data Generation) describe the process of creating the simulated datasets, including the training of finite-state models on real transcript data and generation of synthetic activity matrices with LibriSpeech utterances. The process is explained with formulas and methodology, providing sufficient documentation to understand the dataset creation process."
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6163,
      "completion_tokens": 142,
      "total_tokens": 6305
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Experimental Setup)",
          "Reasoning": "The new dataset is a prepared Bengali speech corpus consisting of recorded utterances by 2 male and 2 female native speakers recorded in a soundproof room, explicitly described as audio speech data collected from human participants."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Experimental Setup)",
          "Reasoning": "The dataset includes transcriptions of the Bengali speech utterances which are text data, created by human transcription aligned with the recorded audio samples."
        }
      ]
    }
  },
  {
    "id": "deb23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7015,
      "completion_tokens": 257,
      "total_tokens": 7272
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3 Experimental Setup",
            "reasoning": "The dataset was prepared by recording 85 utterances spoken by 4 native L1 Bengali speakers (2 male, 2 female) who were instructed to produce speech acts of request, question, and order. This indicates involvement of multiple native human speakers as annotators/contributors for the dataset creation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 Experimental Setup",
            "reasoning": "Participants were instructed to carefully read each of the sentences and comprehend their meanings before recording. Sentences for each category were recorded one after another, indicating instructions were provided to ensure correct expression of speech acts."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 Experimental Setup",
            "reasoning": "The paper does not mention any scoring rubrics or formal criteria used to annotate or label the speech acts in the dataset."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3 Experimental Setup and Figure 4",
            "reasoning": "The paper provides example sentences expressing the same utterance in Bengali differently interpreted as question or request, and their English translations illustrating class distinctions, which can serve as annotation examples in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "deb23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8145,
      "completion_tokens": 256,
      "total_tokens": 8401
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not explicitly describe any quality assurance process or validation procedure conducted on the prepared Bengali speech corpus annotations or content. There is no information about how the utterances were validated or verified for correctness or consistency. Therefore, no quality assurance process is documented or applied."
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7763,
      "completion_tokens": 417,
      "total_tokens": 8180
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Experimental Setup",
          "reasoning": "The authors explicitly mention preparing a dataset of 85 utterances in Bengali, recorded by four native Bengali speakers in a soundproof room. These utterances were created for the purpose of classifying speech acts (request, question, order) and were recorded specifically for this study. This constitutes original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any part of the dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data was produced by human translators translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 4.2 Opus-MT",
          "reasoning": "The text transcripts in Bengali were translated to English using the Opus-MT model, a machine translation system based on the Marian-NMT transformer architecture. This English translation text input was used as an additional modality for classification, implying machine-translated data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was newly recorded and not merely collected or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is original and not based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is clearly documented."
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8281,
      "completion_tokens": 252,
      "total_tokens": 8533
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 Experimental Setup and Section 4 Methodology",
          "reasoning": "The paper describes using the newly prepared Bengali speech acts dataset to fine-tune pre-trained models such as wav2vec2.0 and Marian-NMT for supervised classification of speech acts. Specifically, in Section 3, the dataset preparation is described and in Section 4, the authors discuss fine-tuning these pre-trained models (wav2vec2.0 and Opus-MT) using the dataset with labeled samples to perform supervised learning for speech act classification."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9004,
      "completion_tokens": 508,
      "total_tokens": 9512
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Setup), Section 4.2 (Opus-MT), and throughout",
          "reasoning": "The prepared dataset consists of Bengali speech utterances, where each utterance is associated with a Bengali audio and its transcription, which is further translated into English text. The dataset thus effectively includes two languages: Bengali (the original speech language) and English (the translated text). The Bengali speech corpus entries are paired with English translational text, allowing bilingual content per entry."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken Bengali utterances and their textual translations; no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical symbolic notation is present in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed exclusively of human spoken languages; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural languages Bengali and English only; no constructed or fictional languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are clearly documented and identified as Bengali (spoken) and English (translated text)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human linguistic content in Bengali and English; therefore, it is not applicable to consider as containing no language."
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6222,
      "completion_tokens": 174,
      "total_tokens": 6396
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link in the paper provides code or repository information for dataset construction or processing.",
          "reasoning": "The paper mentions a project page URL (https://soumitri2001.github.io/BeAts) but does not explicitly state that the code for dataset construction or data preprocessing is available there or elsewhere. No direct reference to dataset code availability is made in the paper text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Experimental Setup)",
          "reasoning": "The paper provides a description of the dataset preparation in Section 3, including details on the number of utterances per class, speaker demographics, recording conditions, data augmentation strategy, and preprocessing (standard amplitude normalization). This constitutes documentation of the dataset creation process necessary to understand and potentially reproduce the dataset."
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5394,
      "completion_tokens": 214,
      "total_tokens": 5608
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.4",
          "Reasoning": "The dataset consists of speech recordings of 20 human participants recalling self-defining autobiographic memories. These audio recordings were captured directly from human speakers as they orally recounted memories, as described in the methods section."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The speech recordings were manually transcribed into orthographic text by the authors. These transcriptions form a new text dataset derived from the participants' spoken memories."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "Memories were coded a posteriori based on video recordings of participants during the experiment. The video data were captured from human participants and used for coding the self-defining memories."
        }
      ]
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6246,
      "completion_tokens": 235,
      "total_tokens": 6481
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2",
            "reasoning": "The coding of self-defining memories (SDM) was performed a posteriori by three independent investigators who classified SDM according to specificity, integrative meaning, and affective valence using Singer and Blagov's classification system and scoring manual, indicating involvement of multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "The investigators performed the coding based on Singer and Blagov's classification system and scoring manual for self-defining autobiographical memories, which implies the presence of detailed instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "The coding used a published classification system and scoring manual which includes rating scales, indicating the presence of scoring rubrics for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not mention the provision of annotation examples or sample annotations in the guidelines for coding the SDM by the investigators."
          }
        }
      ]
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7376,
      "completion_tokens": 332,
      "total_tokens": 7708
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Coding of SDM",
          "reasoning": "The paper states that three independent investigators coded the self-defining memories (SDM) according to specific criteria. These coders can be considered multiple human experts as they performed coding according to an established scoring manual. Interrater agreement was reported, indicating a rigorous and reliable QA process by multiple experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of QA by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of QA by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "AI models were used for text analysis (EMOTAIX software), but this served for content analysis, not for quality assurance of annotation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Content analysis",
          "reasoning": "The EMOTAIX text analysis software automatically detects and classifies emotional items in the transcripts. This automatic detection is part of an algorithmic verification process complementing manual verification by experts, constituting an automatic process for QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described and documented in the paper, including multiple expert annotation and automated text analysis verification."
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 6994,
      "completion_tokens": 437,
      "total_tokens": 7431
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.3",
          "reasoning": "The dataset consists of recordings of 20 human participants aged 25 to 35 recounting five successive self-defining autobiographical memories each. These recordings, including speech production, were collected directly from human subjects specifically for this study, constituting original data created entirely by human contributors. The memories and speech are original, not derived, translated, or generated by any model."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by any AI or machine learning model; all speech data originates from human participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from pre-existing datasets; it was recorded anew for this study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 and 2.4",
          "reasoning": "Derived data involves features extracted from the original speech recordings, such as acoustic parameters (MeanF0, MedianF0, SDF0, etc.) and emotional lexicon analysis with EMOTAIX performed on manually transcribed texts. These transformations and analyses constitute derived data based on the original human-generated recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7512,
      "completion_tokens": 266,
      "total_tokens": 7778
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Entire paper, especially Sections 2 and 3",
          "reasoning": "The newly introduced dataset consisting of 99 self-defining memories recorded from 20 participants is used primarily for analyzing vocal and emotional patterns in naturalistic speech. The dataset is manually transcribed, annotated for emotional content using EMOTAIX, and acoustic features are extracted to study relationships between speech parameters and emotional properties. There is no mention of using this dataset for training, fine-tuning, evaluation, or serving as a knowledge base; rather, it serves as the empirical foundation for analyzing trends and characteristics of emotional speech in autobiographical memory recall."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8235,
      "completion_tokens": 371,
      "total_tokens": 8606
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.3 and throughout the paper",
          "reasoning": "The dataset consists of self-defining autobiographical memories recounted orally by participants and manually transcribed. The emotional lexicon is analyzed using the French Emotaix software, indicative that the dataset is in French. However, the paper's text is in English, but the dataset itself is not described as containing English content. The paper does not explicitly state the language of the speech data, but Emotaix is a French emotional lexicon tool. Thus, the dataset contains only one human language, which is French (a non-English language). Therefore, the appropriate label is Monolingual (Non-English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.3 (Content Analysis) and methods sections",
          "reasoning": "The dataset contains transcriptions of speech in one language analyzed using Emotaix, a French emotional lexicon tool, implying the data are in French only. No mention is made of any other languages present in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5453,
      "completion_tokens": 178,
      "total_tokens": 5631
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any link or reference to publicly available code repositories or scripts for dataset construction, data collection, transcription, or preprocessing. There is no mention of code sharing or accessibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 to 2.4",
          "reasoning": "The paper provides thorough documentation on the dataset creation process, including details on participant recruitment and characteristics (Section 2.1), task and data collection methods (recall of self-defining memories, speech and video recordings), transcription and scoring methods (Section 2.2), content analysis using EMOTAIX software (Section 2.3), and speech analysis parameters and tools (Section 2.4). This detailed methodological description documents the dataset creation process clearly in the paper."
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6615,
      "completion_tokens": 161,
      "total_tokens": 6776
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.2; Section 4",
          "Reasoning": "The new dataset includes speech data recorded during port-catheter placement operations, captured via multiple microphone channels. This is human-generated data as it originates from humans performing the surgeries and speaking, recorded by human-operated equipment."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.2; Section 4",
          "Reasoning": "The dataset includes X-Ray images captured during the surgical procedures (port-catheter placement). These images are human generated as they are medical imaging acquired through clinically operated X-Ray equipment during the intervention."
        }
      ]
    }
  },
  {
    "id": "demir23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7467,
      "completion_tokens": 236,
      "total_tokens": 7703
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2, especially 2.1 and 3.3",
            "reasoning": "The paper describes a dataset of 31 port-catheter placement operations collected by medical personnel, implying annotations or phase labels were provided or curated by clinical experts involved in the intervention or analysis. There is no indication that annotation was performed by multiple annotators or non-expert crowds, and the context suggests expert annotation for surgical phases."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not describe any detailed annotation instructions or guidelines provided to annotators for labeling the surgical phases in the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "There is no indication of scoring rubrics or formal evaluation criteria provided as part of annotation guidelines in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not mention providing annotation examples or sample annotations to annotators in the dataset creation process."
          }
        }
      ]
    }
  },
  {
    "id": "demir23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8597,
      "completion_tokens": 288,
      "total_tokens": 8885
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human subject matter expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided about multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by a single human annotator without subject matter expertise is made in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple human annotators lacking subject matter expertise."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance is not reported to be performed by an AI model as a judge for the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any automated verification or algorithmic/rule-based quality assurance process for validating dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new dataset of 31 port-catheter placement operations but does not document any quality assurance process conducted for the dataset annotations or content. No details about annotation, validation, or verification procedures are provided."
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8215,
      "completion_tokens": 417,
      "total_tokens": 8632
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3: Evaluation & Discussion, and Section 1: Introduction",
          "reasoning": "The dataset comprises 31 port-catheter placement operations collected by authors at Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg, involving audio (speech) and X-ray imaging data recorded during actual surgical interventions. This indicates original data recorded from human subjects in real clinical settings, created entirely from scratch by human contributors (medical personnel and researchers) without translation or transformation from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset was generated entirely by AI or machine learning models; instead, models were trained and evaluated on real recorded data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by human translators converting content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as merely collected or aggregated from existing public sources without significant modification; it is newly recorded in this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived or adapted from existing datasets; raw audio and X-Ray images were recorded during interventions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of the dataset as newly recorded human surgical intervention data."
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8733,
      "completion_tokens": 347,
      "total_tokens": 9080
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3.2",
          "reasoning": "The paper describes using the newly collected dataset of port-catheter placement operations to train a Multi-Stage Temporal Convolutional Network (MS-TCN) with self-supervised feature extractors (wav2vec 2.0 XLSR-53 for speech and Densenet121 for X-Ray images held frozen). The training is supervised to recognize surgical phases, indicating supervised fine-tuning of pre-trained feature extractors on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The dataset is used for evaluation of the proposed surgical phase recognition framework, as results such as frame-wise accuracy and F1 scores are reported using this dataset in various experiments."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The authors analyzed temporal modeling choices and the class imbalance problem using the dataset. They evaluated different loss functions, temporal connections, and visualized phase duration distributions, indicating the dataset's use for analysis of trends and challenges in surgical phase recognition."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9456,
      "completion_tokens": 556,
      "total_tokens": 10012
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that the dataset uses German language speech audio during the port-catheter placement operations (Section 2.2: 'including German which is the language of the experiment corpus'). English is used only for the paper and references, not as the language of the dataset recordings."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 Feature Extraction Backbone",
          "reasoning": "The dataset contains speech audio in German only, as stated in Section 2.2: 'A large variant XLSR-53 is trained with 50k hours of public data from 53 languages, including German which is the language of the experiment corpus.' There is no mention of other human languages included in the speech data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes speech and X-Ray images; it does not contain programming or code-related content. Although model code is mentioned, the dataset itself does not contain programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of mathematical or formal logical expressions or symbolic representations as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio and X-Ray images; there is no biological sequence data or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states the language of the speech data as German; thus, language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech audio in German (human language) and X-Ray images; therefore, it does contain languages."
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6674,
      "completion_tokens": 182,
      "total_tokens": 6856
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.3 Temporal Model, near Figure 1",
          "reasoning": "The authors explicitly provide a link to their source code at https://github.com/kubicndmr/PoCaPNet, indicating that the code related to the model and dataset processing is publicly available, which supports reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Intervention and early parts of Section 2 Proposed Method",
          "reasoning": "The paper documents the dataset creation process, including detailed description of the intervention (port-catheter placement operation), data modalities collected (speech from three microphone channels and X-Ray images), sampling rates, windowing approaches, and feature extraction backbones used. The surgical phases and their durations as well as class imbalance distributions are also documented, providing transparency about the dataset creation and its characteristics."
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7530,
      "completion_tokens": 142,
      "total_tokens": 7672
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Affect Production Task",
          "Reasoning": "The Affect Production Task uses audiovisual capture to record participants' facial affect production, involving human participants directly providing video data during the task under the guidance of a virtual dialogue agent."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Affect Production Task",
          "Reasoning": "The Affect Production Task records participants' vocal affect production via their spoken responses during the task, collected as audio data directly from human participants during recordings."
        }
      ]
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8382,
      "completion_tokens": 192,
      "total_tokens": 8574
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2.2 Human ratings of affect production ability",
            "reasoning": "Two human raters classified each participant's responses; raters had intact affect recognition abilities and were effectively experts in affect recognition."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2.2",
            "reasoning": "Raters were not instructed on any formal coding system or operationalized stimuli, and thus no detailed instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2.2",
            "reasoning": "No mention of scoring rubrics or formal rating scales was made; ratings were based on subjective impression of the rater."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2.2",
            "reasoning": "No annotation examples or sample items were described or provided to raters."
          }
        }
      ]
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9512,
      "completion_tokens": 456,
      "total_tokens": 9968
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance was conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although two human raters classified each participant's responses, the raters were not described as experts; the paper only mentions that they had intact affect recognition abilities, but not subject matter expertise or specialized training."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper indicates there were two human raters, not a single annotator, so this label does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2.2, Human ratings of affect production ability",
          "reasoning": "Two human raters classified participants\u2019 facial and vocal responses blinded to the prompted emotion. They were selected based on intact affect recognition abilities (average or higher on DANVA-2), but were not instructed on any operationalized coding system or described as experts. This suggests that multiple non-expert human raters performed the quality assurance by classifying affect production."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated metrics were used to predict affect production ability, the AI model was not described as performing quality assurance or annotation validation; human raters provided the primary ground truth."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes automated extraction of facial and vocal features, but no automated verification or validation steps for annotation quality were described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving human raters is described, so QA is documented."
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9130,
      "completion_tokens": 419,
      "total_tokens": 9549
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Affect Production Task",
          "reasoning": "The paper describes collection of original audiovisual data where human participants (children and adolescents with and without autism) produced specific facial and vocal affect expressions in response to prompts. This data is created entirely from scratch by human contributors and does not represent translated, adapted, or derived data from existing sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data collected was generated by human participants, not by models. While automated metrics and machine learning methods are mentioned for analysis, the dataset itself consists of human-generated audiovisual affect production data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the data involves machine translation of content from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources but collected directly from new experimental procedures."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated facial and vocal metrics were derived from raw audiovisual data, these are features extracted for analysis, not the original dataset itself. The original data consists of newly collected human participant data, not transformed or adapted existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data source and collection method. Thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9648,
      "completion_tokens": 276,
      "total_tokens": 9924
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The novel Affect Production Task (APT) dataset is primarily used to evaluate affect production ability in children and adolescents with autism spectrum disorder and neurotypical controls. The study involves psychometric validation including criterion, ecological, and discriminant validity assessments of the dataset, demonstrating its use for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.4 and 4",
          "reasoning": "The dataset is used to analyze psychometric properties across various demographic factors (age, sex, race/ethnicity) and task demands, as well as to explore patterns in affect production ability. These analyses go beyond mere evaluation and focus on understanding characteristics and trends within the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10371,
      "completion_tokens": 387,
      "total_tokens": 10758
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Affect Production Task",
          "reasoning": "The Affect Production Task dataset contains participants producing emotions using vocal and facial expressions where the spoken utterances are in English only. For example, the monosyllabic utterance 'oh' and the neutral sentence 'I'll be right back' are used consistently as prompts. No other human languages are mentioned or used in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes spoken language (English). Thus, language is present in the dataset."
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7589,
      "completion_tokens": 162,
      "total_tokens": 7751
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section",
          "reasoning": "The paper does not provide any link, mention, or indication that the code related to data collection, preprocessing, or generation for the Affect Production Task (APT) dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.2 and 2.2.1",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the design of the Affect Production Task (APT), participant procedures, the three task conditions, computation of facial and vocal metrics, and how data was collected and processed, particularly in Sections 2.2 and 2.2.1. This documentation presents sufficient information on dataset construction for understanding and reproducibility purposes."
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8059,
      "completion_tokens": 108,
      "total_tokens": 8167
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.1., Section 2.1.2.",
          "Reasoning": "The evaluation dataset for the ProsAudit benchmark is created from the Boston University Radio News Corpus, which consists of professionally-read English news speech recordings. The authors selected and processed these human-recorded audio stimuli, inserting artificial pauses for the evaluation pairs, resulting in audio stimuli for the protosyntax and lexical tasks."
        }
      ]
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8911,
      "completion_tokens": 283,
      "total_tokens": 9194
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2.3",
            "reasoning": "The paper states that 389 participants were recruited via Mechanical Turk, a crowd-sourcing platform, to perform the human evaluation on the new ProsAudit benchmark. These participants are non-expert humans recruited for the task, thus categorized as multiple human non-experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2.3",
            "reasoning": "The description of the human evaluation specifies that participants were presented examples at the beginning of the session and instructions on the task (e.g., choosing the more natural audio in pairs). This implies the existence of instructions given to annotators for the new dataset evaluation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2.3",
            "reasoning": "Participants were asked not only to pick the more natural audio but also to indicate their confidence level (slightly, moderately, strongly), and controls were included to filter inattentive participants, indicating an annotated scoring rubric for quality control and grading."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2.3",
            "reasoning": "Two examples were explicitly presented at the beginning of each session to participants as part of the annotation guidelines for the human evaluation, supporting the presence of examples."
          }
        }
      ]
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10041,
      "completion_tokens": 432,
      "total_tokens": 10473
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any single human expert performing quality assurance on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts conducting quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human non-expert performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2.3 Human evaluation",
          "reasoning": "The dataset created for human evaluation involved 389 participants recruited via Mechanical Turk, who are likely non-experts, conducting assessments to filter and obtain human baseline data. Multiple non-expert humans performed evaluations on the dataset stimuli to provide a human baseline."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not performed by an AI model as a judge for dataset validation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.1 Material and data preprocessing; Section 2.1.2 Creating the protosyntax and lexical tasks",
          "reasoning": "The dataset construction involved automated processing steps such as automatic deletion of existing pauses, applying crossfading, and sampling stimuli pairs constrained by similarity losses to balance distributions. Thus, automated algorithmic/rule-based techniques were used in the dataset generation and validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described involving multiple non-expert human annotators and automated processing; thus, 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9659,
      "completion_tokens": 500,
      "total_tokens": 10159
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The evaluation dataset is created based on the existing Boston University Radio News Corpus, which is a pre-existing human-annotated dataset. While stimuli are modified by the authors (such as artificially inserting pauses), there is no indication that new human-created speech recordings or entirely new annotations were made from scratch by humans specifically for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models directly. AI models are evaluated using this benchmark, but the dataset itself is not AI-generated."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is in English and there is no mention of any data translated from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is mentioned or performed in the creation of the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Material and data preprocessing)",
          "reasoning": "The base dataset used for the evaluation set is the Boston University Radio News Corpus, an existing professionally-read news speech corpus with prosodic and linguistic annotations. The authors select segments following specific criteria and remove existing pauses, but the underlying data is collected from this existing source without significant modification to the original speech content."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and 2.1.2",
          "reasoning": "The authors create new evaluation stimuli by modifying the original corpus, such as deleting existing annotated pauses and artificially inserting 400ms pauses with crossfade at different locations to create pairs of natural and unnatural prosodic conditions. These transformations adapt the existing data to create a new benchmark dataset, thus it is derived data based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are well documented in the paper."
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10177,
      "completion_tokens": 376,
      "total_tokens": 10553
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset, ProsAudit, is not used for pre-training models; it is introduced as an evaluation benchmark."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of the dataset being used to train models from scratch; it serves as an evaluation benchmark."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of the ProsAudit dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No RL-based post-training or reinforcement learning methods are described using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 1 (Abstract, Introduction), Section 2 (Methods 2.1), Section 3 (Results 3.1)",
          "reasoning": "ProsAudit is explicitly introduced as an evaluation benchmark to assess structural prosodic knowledge in self-supervised speech models, providing two subtasks with corresponding metrics. The dataset is used exclusively for evaluation and benchmarking, as demonstrated by the evaluation of various SSL models and human baselines on the tasks."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 (Further analyses)",
          "reasoning": "The paper uses the ProsAudit dataset to analyze trends and characteristics in model performances, including effects of training data size and model nativeness on performance, thereby employing the dataset for analysis purposes beyond sole evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10900,
      "completion_tokens": 535,
      "total_tokens": 11435
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and throughout the paper (e.g., Abstract, Introduction, Methods 2.1.1)",
          "reasoning": "The proposed ProsAudit benchmark dataset is created using the Boston University Radio News Corpus, which is an English dataset with annotated prosodic boundaries. The benchmark and tasks are developed solely in English, as explicitly stated (e.g., Abstract: 'Our benchmark, in English, consists of two subtasks', Section 2.1.1: 'We used the Boston University Radio News Corpus ... based on the American English ToBI system'). There is no mention of inclusion of other languages in the new dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the work involves models and evaluation metrics, the dataset itself contains speech stimuli and annotations, not code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The benchmark dataset entries consist of speech audio with prosodic boundary annotations; no mathematical or formal logical expressions are contained in the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data comprises human speech from English news corpus, not biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or constructed languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are explicitly described and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken English language utterances, so language is clearly present."
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8118,
      "completion_tokens": 186,
      "total_tokens": 8304
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "not explicitly specified",
          "reasoning": "The paper does not provide any explicit link or mention of publicly available code repository for constructing the ProsAudit dataset or the scripts used for data preprocessing and pause insertion. While it states the dataset is part of the Zero-Resource Speech challenge evaluation metrics and leaderboard, no direct code repository is referenced in the paper text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Methods - ProsAudit benchmark (especially 2.1.1 and 2.1.2)",
          "reasoning": "The paper provides a detailed description of the dataset creation process including data source (Boston University Radio News Corpus), criteria for selecting segments, methodology for removing and inserting pauses, and constraints used for sampling stimuli pairs. This serves as thorough documentation of how the benchmark dataset is constructed, enabling reproducibility in principle."
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8009,
      "completion_tokens": 202,
      "total_tokens": 8211
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 2.3",
          "Reasoning": "The WavText5K dataset consists of audio recordings sourced from two websites providing royalty-free sound effect libraries (BigSoundBank and SoundBible), which are human-recorded natural audio files. The paper states these are unique sources not used in other datasets. The audio data is resampled and processed but originally collected from human-recorded sounds."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 2.2",
          "Reasoning": "The dataset contains free-form natural language descriptions provided by the uploaders of the audio recordings. These captions focus on isolated audio events and are manually written by humans, unlike automatically generated or model-generated text. They differ from curated or auto-extracted captions in other datasets."
        }
      ]
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8861,
      "completion_tokens": 236,
      "total_tokens": 9097
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 2.1 and 2.2",
            "reasoning": "WavText5K dataset captions are described as free-form descriptions provided by the uploader of the audio recordings, implying that the annotation was performed by individual non-expert uploaders rather than experts or a formal annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section describing annotation instructions",
            "reasoning": "The paper explicitly states that the descriptions are free-form and provided by the uploader, implying there was no formal instruction given for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of scoring rubrics or criteria in the paper regarding annotations",
            "reasoning": "The paper does not describe any rubric or consistent evaluation criteria for the annotation of the WavText5K dataset captions."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2 and Table 2",
            "reasoning": "The paper provides examples of captions from WavText5K and comparisons with captions from other datasets, thus showing examples of annotations made by uploaders."
          }
        }
      ]
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9991,
      "completion_tokens": 393,
      "total_tokens": 10384
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance conducted by a single human expert for the WavText5K dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance done by multiple human experts nor provide details about expert annotation or verification for WavText5K."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information describing quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or report any quality assurance process involving multiple human non-expert annotators for WavText5K."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process is not described as being performed by an AI model acting as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automatic verification or algorithmic/rule-based quality assurance process is described for the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any formal quality assurance processes for the WavText5K dataset annotations. It mentions that descriptions are 'free-form descriptions provided by the uploader of the audio recording' without mention of review or validation. No QA or annotation validation is described in the paper."
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9609,
      "completion_tokens": 496,
      "total_tokens": 10105
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset collection",
          "reasoning": "The WavText5K dataset introduced by the authors consists of approximately 5000 audio-text pairs sourced from two sound effects libraries: BigSoundBank and SoundBible. These pairs include free-form descriptions provided by the uploaders of the audio recordings, indicating original human-generated content created from scratch rather than translations or adaptations. This is explicitly mentioned in Section 2.1 and Section 2.2, where the authors emphasize the natural language descriptions focusing on isolated audio events. Thus, the dataset represents new data from human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset collection",
          "reasoning": "WavText5K is compiled by collecting audio-text pairs from existing online sources, specifically BigSoundBank and SoundBible websites. The data are collected without significant modifications mentioned besides resampling audio files to 44.1 kHz. The descriptions are those originally provided by uploaders on these platforms. Therefore, this dataset could also be considered as collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors resampled audio and combined title and description for caption construction, these transformations are minimal and do not constitute substantial derivation or modification of existing sources. Hence, the data is not considered derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of dataset generation is clearly documented and described."
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10127,
      "completion_tokens": 335,
      "total_tokens": 10462
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, 4.2, 5.2",
          "reasoning": "The WavText5K dataset is explicitly used as part of the training data for the CLAP contrastive learning model from scratch with randomly initialized parameters, as described in the training methodology in Section 3.1 and experimental setup in Section 4.2. The authors demonstrate in Section 5.2 that including WavText5K in the training data improves audio-text retrieval performance, indicating that it is directly used to train models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "WavText5K is not used exclusively for evaluation; instead, evaluation is performed on other datasets such as AudioCaps and Clotho."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The paper provides quantitative analysis and discussion of the characteristics of WavText5K in Section 2.3, including statistics about the audio and text data, such as duration, word counts, and common terms, demonstrating its use for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10850,
      "completion_tokens": 546,
      "total_tokens": 11396
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces the WavText5K dataset with audio-text pairs described in natural language. There is no mention or evidence that multiple human languages are included; the descriptions are consistently in English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain exactly two human languages. The text descriptions focus on English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 2.2 Description of isolated events and 2.3 Data analysis",
          "reasoning": "The dataset WavText5K contains natural language descriptions provided by uploaders, which are in English as indicated by example captions and comparisons with other datasets. All provided example captions and analysis are in English, supporting its monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's textual content is not in any non-English language; it is consistently in English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or evidence that the dataset entries include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of audio-text pairs with natural language descriptions without any mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural language descriptions of audio events but does not contain biological sequences, signals, or non-human communication systems as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No references or descriptions suggest the use of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language descriptions, so it is not without any language."
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8068,
      "completion_tokens": 156,
      "total_tokens": 8224
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.1 and Footnote 1",
          "reasoning": "The paper explicitly mentions that WavText5K is available online with a URL https://github.com/microsoft/WavText5K indicating that the dataset and related code or at least data is publicly accessible, facilitating reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (entire)",
          "reasoning": "The paper thoroughly documents the dataset creation process for WavText5K including data sources (BigSoundBank and SoundBible), data formats, resampling procedures, the nature of descriptions (free-form from uploaders), and distinctions from other datasets. This comprehensive description supports reproducibility and transparency."
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8891,
      "completion_tokens": 126,
      "total_tokens": 9017
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 (e.g., 5.1 Text-to-Audio generation, 5.2 Text-to-Music generation, 5.3 Speech Synthesis, 5.4 Noise suppression)",
          "Reasoning": "The authors collected human listening scores on audio samples generated from various models across tasks including Text-to-Audio, Text-to-Music, Text-to-Speech, and Deep Noise Suppression; these audio samples and corresponding scores represent new dataset contributions created for their evaluation."
        }
      ]
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9743,
      "completion_tokens": 327,
      "total_tokens": 10070
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 5.1 (Text-to-Audio generation), Section 5.2 (Text-to-Music generation), Section 5.3 (Speech Synthesis), Section 5.4 (Noise suppression)",
            "reasoning": "The paper describes human listening tests where multiple participants (e.g., 10 participants per audio) rated samples using Likert scales or MOS scores. Participants were recruited via Amazon Mechanical Turk, indicating they are non-expert crowd workers. Multiple human non-experts performed the annotation across the various introduced datasets for quality evaluation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 5.1 (Text-to-Audio generation)",
            "reasoning": "To ensure quality annotations, participants were given tasks to rate overall quality on a five-point Likert scale with randomized audio presentation. The paper mentions exclusion criteria (e.g., exclude workers with uniform responses or too fast completion), indicating some instructions and quality control in the annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5.1 and 5.2",
            "reasoning": "The rating scales used are structured, such as five-point Likert scales (Overall Quality) and MOS scores for acoustic and musical quality, serving as rubrics defining the score levels and interpretation guidelines for ratings."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not mention providing annotation examples or sample ratings to annotators in the listening tests or human evaluation sections."
          }
        }
      ]
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10873,
      "completion_tokens": 408,
      "total_tokens": 11281
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance conducted by a single human expert on the collected datasets or human listening scores."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 5.1, 5.2, 5.3",
          "reasoning": "Human listening experiments were conducted with multiple human raters. For example, in Section 5.1, each audio was rated by 10 participants recruited via Amazon Mechanical Turk, with quality controls to exclude poor-quality annotations. This indicates multiple human non-experts participated in scoring; however, there is no explicit indication these raters were subject matter experts, so this label reflects human raters as multiple annotators involved in scoring."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that quality assurance was conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5.1, 5.2, 5.3",
          "reasoning": "Human listening tests involved multiple annotators recruited via Amazon Mechanical Turk without mention of expert status, indicating they were likely non-experts. The paper describes quality control to exclude inconsistent ratings, confirming multiple human non-expert annotators rated samples for the human listening scores collected."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process where an AI model was used as a judge to validate dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification of dataset annotations or content quality is described; the paper's quality assurance relies on human listening tests rather than algorithmic verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a quality assurance process involving multiple human annotators for the human listening scores collected in new datasets relevant to each task."
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10491,
      "completion_tokens": 525,
      "total_tokens": 11016
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5, subsections 5.1 Text-to-Audio generation and 5.2 Text-to-Music generation",
          "reasoning": "The paper describes the authors conducting new human listening experiments for multiple audio generation tasks. Specifically, in Section 5.1 and 5.2, they detail collecting human subjective scores (Mean Opinion Scores) from participants recruited via Amazon Mechanical Turk to assess audio quality of generated audio and music samples. These listening tests involved original human evaluations created from scratch for this study, not adaptations or translations of existing annotations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any entirely model-generated datasets as new datasets; rather, it uses existing datasets or human evaluations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being produced by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation from another language is described in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 Experiments and Section 4.1 Effect of Distortions",
          "reasoning": "The paper uses multiple existing well-known datasets such as AudioCaps, MusiCaps, LibriTTS, NISQA, and others as source audio data for evaluation and distortion experiments. These data are collected (collated) from existing sources without modification. The paper relies on these existing datasets for analysis."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Effect of Distortions",
          "reasoning": "The paper derives new data by adding controlled distortions (e.g., Gaussian noise, Tanh distortion, Mu-Law compression, Reverb) to existing audio samples from professionally recorded sound effects and datasets. These modifications transform the existing data, producing derived datasets used in experiments."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the origin and method of generation for all datasets it introduces or uses."
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11009,
      "completion_tokens": 333,
      "total_tokens": 11342
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 and Section 5",
          "reasoning": "The newly introduced dataset in the paper consists of human listening scores collected for various audio generation tasks (Text-to-Audio, Text-to-Music, Text-to-Speech, and Deep Noise Suppression). These datasets are used to evaluate the proposed PAM metric by measuring its correlation with human perception and comparing with existing metrics. This usage is explicitly described in sections 3 (Experiments) and 5 (PAM for audio tasks), where listening tests are conducted and PAM's performance is benchmarked."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The collected human listening scores and the related datasets are used for analysis of PAM's sensitivity to distortions as well as to evaluate prompting strategies. The dataset enables detailed analysis to understand the metric's behavior under different conditions, as described in sections 4.1 (Effect of distortions) and 4.2 (Prompting strategy)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11732,
      "completion_tokens": 626,
      "total_tokens": 12358
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset containing entries in more than two human languages. All datasets used are English-centric or do not specify multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing exactly two human languages in the newly introduced datasets. The datasets referenced are either monolingual or not explicitly stated to be bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.3 (Speech Synthesis), Section 3 (Experiments), and related dataset descriptions",
          "reasoning": "The newly collected human listening scores and datasets used for evaluation, such as LibriTTS, NISQA, VoiceMOS, Blizzard Challenge Chinese TTS samples, and human evaluations for various audio tasks, are predominantly English or derived from English datasets. The paper does not report introducing new datasets in non-English languages for audio-text pairs. The text prompts used for PAM are in English, and the datasets for human evaluation appear English-centric."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset with exactly one non-English language. Although the Blizzard Challenge dataset contains Chinese TTS samples, it is used only for out-of-domain evaluation and is not introduced by the authors."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code-related content is included in the new datasets introduced by the authors."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions are present in the description of PAM computation and experimental metrics, but these are not part of any new dataset introduced by the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is included in the new datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created language data is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of entries in the datasets are specified or clearly English, no unknown language origin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The proposed datasets contain human languages (English) in the speech and text prompt data, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8950,
      "completion_tokens": 176,
      "total_tokens": 9126
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Sections 5 and 7",
          "reasoning": "The paper mentions releasing code and human listening scores, but does not provide a link, nor details about code for dataset creation or processing. The primary new datasets are the human listening scores collected for different tasks, but no code repository or explicit description of dataset construction code availability is provided in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5 (Experiments) and Abstract",
          "reasoning": "The paper documents the collection of new human listening scores for four audio tasks (TTA, TTM, TTS, DNS). It describes the listening test protocols, rating scales, participant recruitment, and quality control measures. However, the exact procedures or code for dataset creation are not described in detail beyond these experimental protocol summaries."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8173,
      "completion_tokens": 158,
      "total_tokens": 8331
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.1.1 Dataset construction",
          "Reasoning": "The new dataset introduced is the INTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge dataset. It comprises audio clips derived from public domain podcast recordings (LibriVox Community Podcasts) which are human-generated audio recordings. Packet loss traces extracted from real Microsoft Teams calls (human communication data) are applied to these audio clips by zeroing out lost packet regions to simulate packet loss. Thus, the primary modality is audio, originating from human-generated recordings (podcast audio), combined with real-world packet loss traces also originating from human communication data."
        }
      ]
    }
  },
  {
    "id": "diener22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9025,
      "completion_tokens": 231,
      "total_tokens": 9256
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1.3",
            "reasoning": "The evaluation used crowdsourced raters on Amazon Mechanical Turk with 421 raters participating, indicating multiple human non-expert annotators provided ratings for the dataset clips."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1.3",
            "reasoning": "Raters were given the reference audio file for each clip and asked to rate on a 7 point categorical scale from -3 to 3 using a comparative category rating methodology, indicating detailed instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1.3",
            "reasoning": "The rating scale is specifically described as a 7 point categorical scale ranging from -3 to 3 with defined categorical labels for comparative category rating, functioning as a rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1.3",
            "reasoning": "The paper does not describe any examples or sample ratings provided to annotators as part of the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "diener22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10155,
      "completion_tokens": 415,
      "total_tokens": 10570
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention a single human non-expert performing QA on dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.3 Evaluation",
          "reasoning": "The evaluation of the dataset and challenge results involved crowd-sourced ratings using Amazon Mechanical Turk with 421 raters. The raters, likely non-experts, provided multiple ratings per clip for the subjective Comparative Mean Opinion Score (CMOS), representing a quality assurance process involving multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 PLCMOS",
          "reasoning": "The paper describes PLCMOS, a neural network model trained to estimate human mean opinion scores, which was used to rapidly evaluate PLC system performance. This represents an AI model employed for quality assurance of audio quality, acting as an automated judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is found regarding automated verification of dataset annotations through code or formal algorithmic rule checks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is clearly described, both through crowd-sourced human ratings and AI model evaluation."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9773,
      "completion_tokens": 401,
      "total_tokens": 10174
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset audio content is sourced from an existing public domain podcast dataset, not created from scratch by human contributors specifically for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by AI or machine learning models; it consists of real audio clips with packet loss patterns applied."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No translation from another language by humans is mentioned regarding the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data is referenced in the dataset construction."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset construction",
          "reasoning": "The dataset is built by collecting and aggregating audio segments from an existing public domain podcast dataset and packet loss traces from actual calls, without significant modification to the original audio besides applying packet loss. This constitutes collating data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset construction",
          "reasoning": "The dataset involves applying packet loss traces sampled from real calls to segments of podcast audio by zeroing out lost packets, representing a transformation of the collated data, making it derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is documented clearly in Section 3.1.1; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10291,
      "completion_tokens": 366,
      "total_tokens": 10657
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is provided with training and validation splits specifically for participants to train Deep PLC models from scratch or using other datasets, as indicated by the description of the training split intended as a quick-start set for training PLC models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset includes supervised data (clean audio and corresponding lossy audio with packet loss regions), allowing participants to fine-tune models in a supervised manner to predict or reconstruct missing audio segments."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any usage of reinforcement learning or RL-based post-training techniques using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The blind test split of the dataset is used exclusively for evaluation and benchmarking of submitted models in a blind manner, as detailed in the description of the challenge evaluation procedure."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used primarily for analysis of trends or characteristics but rather for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models in retrieval-augmented generation or similar settings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical uses of the newly introduced dataset in training, supervised fine-tuning, and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11014,
      "completion_tokens": 395,
      "total_tokens": 11409
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset construction",
          "reasoning": "The dataset audio clips are sampled from a public domain podcast dataset (LibriVox Community Podcasts), which is an English-language podcast dataset. Furthermore, in Section 4 (Results), it is mentioned that Ground truth transcriptions for speech recognition were created and human-corrected, implying English text. No mention is made of other languages being present in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech audio, thus it contains language and cannot be marked as not applicable."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8232,
      "completion_tokens": 186,
      "total_tokens": 8418
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.1 Dataset and evaluation; specifically 3.1.1 Dataset construction",
          "reasoning": "The paper states that the dataset is now available as a fully open source dataset for use in further evaluations by any interested researcher, and provides a link to a GitHub repository https://github.com/microsoft/PLC-Challenge/tree/main/PLCMOS for the dataset and related code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1.1 Dataset construction",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including the sampling of actual packet loss traces from Microsoft Teams calls, stratified sampling based on burst loss length and packet percentage loss quantiles, and the application of these traces to audio clips from a public podcast dataset. This thorough description serves as documentation of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10621,
      "completion_tokens": 99,
      "total_tokens": 10720
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection",
          "Reasoning": "The authors created a new dataset by recording audio speech data from 42 healthy volunteers reading aloud in two controlled rooms with low and high reverberation. Recordings were made simultaneously with a condenser microphone and three smartphones, capturing human speech audio signals generated by human participants under the supervision of a trained researcher."
        }
      ]
    }
  },
  {
    "id": "dineley23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11473,
      "completion_tokens": 198,
      "total_tokens": 11671
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 Data Collection",
            "reasoning": "The data collection involved 42 healthy volunteers with recording overseen by a trained researcher, indicating expert human involvement for data annotation or supervision."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Data Collection",
            "reasoning": "Participants were given instructions on recording procedure, such as reading aloud The Rainbow Passage and positioning in front of microphones; this reflects detailed instructions in the annotation or data collection guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 Data Collection",
            "reasoning": "The paper does not mention any scoring rubrics or explicit criteria for annotation scoring related to this dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 Data Collection",
            "reasoning": "No examples of annotations or detailed annotation samples are provided in the paper for this dataset."
          }
        }
      ]
    }
  },
  {
    "id": "dineley23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12603,
      "completion_tokens": 225,
      "total_tokens": 12828
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The dataset creation involved 42 healthy volunteers recruited and the collection was conducted with oversight by a trained researcher, implying quality assurance was done by a single human expert who supervised the recordings and ensured data integrity."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Feature extraction and analysis",
          "reasoning": "The paper describes automated transcription using OpenAI Whisper model and automated forced alignment with the Montreal Forced Aligner. These tools perform automated verification and extraction of features without human intervention, implying an automatic process quality assurance step in feature extraction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12221,
      "completion_tokens": 368,
      "total_tokens": 12589
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The authors created a new dataset by recording speech from 42 healthy volunteers reading aloud a standard passage in two different reverberation environments. This data was newly generated from human participants during controlled recording sessions, overseen by trained researchers. There is no indication that this data was derived, translated, or aggregated from previous datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on human speech recordings; no data was generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being translated from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being translated from other languages using machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected by aggregating or combining existing datasets; it was newly collected."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on or adapted from existing sources; it is original human speech recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method are explicitly documented."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12739,
      "completion_tokens": 422,
      "total_tokens": 13161
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was used to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe usage of the dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training methods using the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset is recorded and analyzed, it is not exclusively used for evaluation or benchmarking of models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.2 Feature extraction and analysis; Abstract and Conclusion",
          "reasoning": "The dataset was primarily used to analyze the effects of room reverberation on various speech features across different recording devices and environments. The study reports detailed analyses of acoustic feature variability rather than using the data for model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as serving as a knowledge base or for augmenting models via retrieval."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly documented and used within the study for analysis purposes."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13462,
      "completion_tokens": 491,
      "total_tokens": 13953
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The dataset consists of recordings from 42 volunteers whose recordings involved reading aloud English texts, specifically The Rainbow Passage. Participants whose first language was not English were required to have sufficient English proficiency, and all transcriptions and analyses were conducted on English speech. Thus, the dataset contains only English language entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper mentions usage of software tools (e.g., Praat, Montreal Forced Aligner, OpenAI Whisper model) for automated transcription and feature extraction, the dataset itself does not contain entries of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses speech features and statistical analyses but does not indicate that the dataset contains mathematical or logical expressions as part of the data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech recordings only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are mentioned or included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is specified explicitly as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset comprises human speech recordings in English and thus contains language."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10680,
      "completion_tokens": 124,
      "total_tokens": 10804
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link regarding code availability in the paper",
          "reasoning": "The paper does not mention or provide any links to code repositories or indicate that code related to data collection, preprocessing, or dataset generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 - Data Collection and Feature Extraction and Analysis",
          "reasoning": "The paper describes in detail the data collection procedures including participant recruitment, ethics approval, recording environments, devices used, and the feature extraction methodology, demonstrating transparency and completeness in the dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7197,
      "completion_tokens": 119,
      "total_tokens": 7316
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction and Section 3.1 Datasets and Evaluation Protocols",
          "Reasoning": "The ST-EMO dataset is newly introduced by the authors. It consists of 153 hours of Chinese Mandarin speech audio data recorded by 50 male actors and 50 female actresses in various scenarios. The data were collected using Android smartphones in controlled environments and manually labeled by experts, indicating human involvement in recording and annotation, making it human generated audio data."
        }
      ]
    }
  },
  {
    "id": "ding23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8049,
      "completion_tokens": 204,
      "total_tokens": 8253
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The paper states that after collecting the raw ST-EMO data, they consulted four experts to judge its emotion category, and data is considered valid only if at least two experts' labels are consistent with the original label."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to the experts for labeling ST-EMO dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no description of specific scoring rubrics or guidelines for labeling quality standards for the ST-EMO dataset annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No examples or annotation samples are provided or referenced in the paper for annotators labeling the ST-EMO dataset."
          }
        }
      ]
    }
  },
  {
    "id": "ding23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9179,
      "completion_tokens": 220,
      "total_tokens": 9399
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1, Dataset description of ST-EMO",
          "reasoning": "The ST-EMO dataset was validated by a quality assurance process where four experts judged the emotion category of each piece of data. The data was considered valid only if at least two experts agreed with the original label, demonstrating that multiple human experts with subject matter expertise performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents a quality assurance process for the ST-EMO dataset involving multiple human experts; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8797,
      "completion_tokens": 390,
      "total_tokens": 9187
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 (Datasets and Evaluation Protocols)",
          "reasoning": "The ST-EMO dataset was collected by the authors with 50 male actors and 50 female actresses in different scenarios, recording original Chinese Mandarin speech data. The paper states the data were recorded using Android smartphones in controlled settings, with scripts designed for text-independent, text-dependent, and improvised scenarios. The data are original human speech recordings created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data were produced by translating content from another language using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation being used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly introduced ST-EMO dataset was collected originally, not aggregated or compiled from existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ST-EMO dataset was not derived from existing sources with modifications; it is described as a newly collected corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection process are clearly described."
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9315,
      "completion_tokens": 279,
      "total_tokens": 9594
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.2",
          "reasoning": "The authors collected a new Chinese Mandarin speech emotion dataset ST-EMO and used it to train their proposed GLRF model from scratch, as explicitly described in Section 3.1 and Section 3.2 where the dataset is split into training and test sets, and models are trained and fine-tuned for optimal performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The ST-EMO dataset is used to evaluate and benchmark the performance of the proposed GLRF model and various baselines in Section 3.3, including performance comparisons on metrics like weighted accuracy, unweighted accuracy, and micro-F1 score."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10038,
      "completion_tokens": 544,
      "total_tokens": 10582
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The only new dataset introduced, ST-EMO, contains Chinese Mandarin speech data exclusively, with no mention of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "ST-EMO dataset is composed solely of Chinese Mandarin speech without entries in a second language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "ST-EMO dataset does not contain English entries; the English IEMOCAP dataset is pre-existing and not newly introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "ST-EMO is described as a Chinese Mandarin speech emotion dataset exclusively, indicating monolingual non-English content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset ST-EMO contains human speech audio data only, with no programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the ST-EMO dataset contains mathematical or formal logical expressions within its data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "ST-EMO dataset contains human speech recordings and does not include biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any fictional or artificially created languages; it contains natural Chinese Mandarin speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The language of the dataset ST-EMO is explicitly stated as Chinese Mandarin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language data, so it is not without language content."
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7256,
      "completion_tokens": 158,
      "total_tokens": 7414
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 and Conclusion",
          "reasoning": "The paper does not include any mention or link to publicly available code repositories related to the ST-EMO dataset creation, collection, or preprocessing. There is no indication that the code for data collection or processing is released or accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Datasets and Evaluation Protocols",
          "reasoning": "The paper provides detailed documentation of the ST-EMO dataset collection process, including the number of speakers, recording conditions, devices used, duration of utterances, emotional categories, data cleaning procedures involving multiple experts determining labels, and statistics about the dataset distributions, demonstrating sufficient transparency on the dataset creation."
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8661,
      "completion_tokens": 383,
      "total_tokens": 9044
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 ARCOM's Corpus and 6. Acknowledgements",
          "Reasoning": "The dataset consists of a large corpus of 32,000 hours of French TV and radio broadcasts from 2023 provided and digitized by human-operated institutions (ARCOM and INA). The audiovisual materials were recorded from actual TV and radio broadcasts, thus human-generated data captured by broadcast and digitization methods."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 ARCOM's Corpus and 6. Acknowledgements",
          "Reasoning": "The dataset includes audio from French TV and radio channels during 2023 collected as broadcast recordings, which is human-generated audio data captured from real media sources rather than synthesized or model-generated audio."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Augmented speech transcriptions",
          "Reasoning": "The audio data was transcribed automatically using Whisper\u2019s large-v3 multilingual model and WhisperX framework. These transcriptions and timestamps are generated by AI models, constituting model-generated textual data derived from the audio modality."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.5 Face Detection and Categorization",
          "Reasoning": "From the video frames sampled at one image per second, faces were detected and classified using a model (Resnet50 DNN trained on FairFace). The inputs are human-generated images, but the processed face gender annotations are model-generated results applied to images extracted from the human-recorded broadcasts. The dataset contains these classified face data as new annotations generated by models on the original images."
        }
      ]
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9513,
      "completion_tokens": 261,
      "total_tokens": 9774
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The ARCOM's Corpus, newly introduced in the study for analyzing women's representation, is based on manual reports provided by French TV and radio channels, which themselves rely on human annotations of speaking characters categorized by gender and role. Given the large scale (29,707 programs, 135,385 individuals), it implies multiple human experts participated in the manual annotation process used by ARCOM for the reports."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Programs are classified using a hierarchical taxonomy and gender and roles are specified, indicating that detailed annotation instructions and criteria were given to annotators to maintain consistency across the large dataset."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The annotation involves categorization by gender and specific roles (presenter, journalist, political guest, expert, other), representing a rubric or structured scoring scheme for annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention providing annotation examples or samples to annotators as part of the guideline for the ARCOM manual reports."
          }
        }
      ]
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10643,
      "completion_tokens": 414,
      "total_tokens": 11057
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used to analyze and classify data (e.g., speaker gender classification, face detection and classification), these models are used to generate dataset annotations rather than to perform quality assurance or validation of annotations as a judge."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 Method, especially Sections 3.2 (Speaker Gender Segmentation), 3.3 (Augmented speech transcriptions), 3.4 (References to Men and Women), and 3.5 (Face Detection and Categorization)",
          "reasoning": "The dataset annotations and descriptors are produced using automated algorithms and models with described validation and performance metrics (e.g., speaker gender segmentation accuracy, face gender classification accuracy), indicating an automated and algorithmic approach to data validation. The paper discusses benchmarks and accuracy of models used, reflecting automated verification of methods rather than human quality assurance. No explicit manual QA process is described, but the description and benchmarking of tools suggests reliance on automated verification techniques for QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes the use of automated tools with benchmarking and performance evaluations for dataset annotation, indicating some form of quality assurance beyond no QA process."
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10261,
      "completion_tokens": 540,
      "total_tokens": 10801
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created original data entirely from scratch by human contributors. The data consists mainly of broadcast content from TV and radio channels, which are existing materials, rather than newly created human-generated content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors applied AI and machine learning models (inaSpeechSegmenter, Whisper, inaFaceAnalyzer) to generate speech transcripts, gender segmentations, face detections, and first name extractions, these outputs are derived from existing broadcast data. The data itself is not newly generated content but processed from existing sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the data was produced by human translation from another language. The corpus is French broadcast content recorded in France, with no indication of cross-language translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate the dataset content from another language. The data originates from French TV and radio broadcasts and transcripts."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 ARCOM\u2019s Corpus",
          "reasoning": "The dataset analyzed is a large corpus of existing French TV and radio broadcasts provided by ARCOM, comprising 32,000 hours of audiovisual material from 2023. This data consists of collected and aggregated broadcast content from multiple channels without substantial modification, representing an aggregation of pre-existing media sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.2 - 3.5 Methodology; Sections 4 and 5 Results and Discussion",
          "reasoning": "The authors derived new datasets by applying various automatic processing methods\u2014speaker gender segmentation, automatic speech transcription, first name extraction for gender estimation, and face detection and categorization\u2014on the pre-existing broadcast data. These processed outputs constitute new representations derived from existing sources with transformations and adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of data generation are clearly specified and documented; hence N/A does not apply."
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10779,
      "completion_tokens": 216,
      "total_tokens": 10995
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The ARCOM's Corpus introduced by the authors is used primarily for analyzing gender representation trends, patterns, and characteristics in French TV and radio broadcasts. The paper employs this dataset to perform detailed statistical analyses and comparisons between manual channel reports and automatic descriptors to understand disparities, rather than for any model training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11502,
      "completion_tokens": 504,
      "total_tokens": 12006
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on French TV and radio broadcasts from 2023, exclusively in French as indicated throughout the paper."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries in exactly two languages; it is focused on French broadcasts only."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content is in French, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (ARCOM\u2019s Corpus), Abstract, Introduction",
          "reasoning": "The dataset consists of 32,000 hours of French TV and radio broadcasts from 2023, explicitly stated as French audiovisual media; thus it is monolingual in French, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While machine learning and analysis methods are described, the dataset itself does not include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is statistical modeling and effect size calculations mentioned, but the dataset itself does not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audiovisual human speech and facial data from broadcasts, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly stated as French and well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human spoken French language and related audiovisual contents."
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8720,
      "completion_tokens": 172,
      "total_tokens": 8892
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories for the dataset construction or associated processing code. Although methods and tools used are described, no code is shared or indicated as accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Sections 3 (Method) and 4 (Results), including the data sources, sampling periods, classifications, processing tools, and algorithms used for speaker gender segmentation, speech transcription, face detection and classification, and first name gender attribution. This documentation enables reproducibility of the described dataset creation within the limits of access to source audiovisual materials, although the materials themselves are not publicly available due to copyright."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6828,
      "completion_tokens": 173,
      "total_tokens": 7001
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.3 Recording",
          "Reasoning": "The MD3 dataset includes more than 20 hours of audio recordings of conversational speech from human speakers in India, Nigeria, and the United States. The recordings were made using human participants conversing via virtual meetings, recorded through human-operated microphones and speakers without any AI generation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.4 Transcription",
          "Reasoning": "The dataset contains orthographically transcribed tokens of the recorded conversations. Transcriptions were performed by human speech transcription professionals and reviewed by a second worker, thus the text data originates from manual human annotation of the recorded audio."
        }
      ]
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7680,
      "completion_tokens": 215,
      "total_tokens": 7895
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.4",
            "reasoning": "Orthographic transcription was performed by speech transcription professionals, which suggests trained non-expert annotators; each transcription was reviewed by a second worker, implying multiple annotators participated."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.4",
            "reasoning": "Transcribers were provided audio segmented by prompt and likely had instructions to segment and transcribe orthographically; the mention of professionals and review implies guidelines including instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.4",
            "reasoning": "The paper does not mention any scoring rubrics or graded scales used during transcription, only a review for accuracy, implying no formal rubrics for annotation quality assessment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.4",
            "reasoning": "No explicit mention of providing annotation examples or sample transcriptions in the transcription process or guidelines is present in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8810,
      "completion_tokens": 410,
      "total_tokens": 9220
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that transcription was performed by individual workers who were speech transcription professionals but does not specify that they were subject matter experts or members of the target demographic individually responsible for QA."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.4 Transcription",
          "reasoning": "The transcription was performed by speech transcription professionals and each transcription was reviewed by a second worker for accuracy. Both transcribers were from the same locale as the speakers which suggests they are members of the target demographic, and the dual review indicates multiple human experts performed QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting QA was performed by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as speech transcription professionals and appear to belong to the target demographic, so they are treated as experts rather than non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "AI models were used for speech recognition experiments (e.g., Whisper) but not for quality assurance of the annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated or algorithmic verification methods for quality assurance applied to the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a QA process involving transcription by professionals and double review, so quality assurance was performed and documented."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8428,
      "completion_tokens": 520,
      "total_tokens": 8948
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Elicitation) and Section 4 (Dataset)",
          "reasoning": "The dataset was collected by recruiting human speakers from three locales (India, Nigeria, and the United States) to participate in conversational information-sharing tasks (guessing games). These dialogues were recorded as they naturally occurred during the task, resulting in original content created entirely from scratch by human contributors. The paper details the recruitment, recording, and transcription of these conversations, indicating the data is newly created human speech, not a translation or adaptation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or models. All data was produced by human participants during conversations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced from translations of other language data into English by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine-translated data in the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While image prompts were drawn from existing public datasets, the speech and conversational data was newly recorded and not aggregated without modification. The core dataset is original human speech data, not a collation of existing speech corpora."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2.1 (Image prompts) and Section 3 (Elicitation)",
          "reasoning": "The elicitation used prompts partly drawn from existing image datasets (FoodX-251, CalTech-UCSD Birds, Stanford Dogs) to stimulate natural conversation. Although the speech content is new, the stimuli for the conversations derive from existing sources, adapting those images into the context of a guessing game. Thus, while the conversational data is new, part of its elicitation is derived from prior existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data collection method and origins; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8946,
      "completion_tokens": 469,
      "total_tokens": 9415
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the MD3 dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MD3 dataset to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that MD3 is used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No discussion or evidence in the paper supports the use of MD3 for reinforcement learning-based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset could be used for evaluation, the paper does not demonstrate or describe exclusive use of MD3 for benchmarking or performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Dialect features, Section 4 Dataset, Section 5 Conclusion",
          "reasoning": "The paper primarily uses MD3 for analyzing dialectal variation, trends, and linguistic characteristics across English dialects from India, Nigeria, and the United States. The authors present statistical analyses of dialect features, discourse markers, and syntactic patterns, illustrating its utility for linguistic study and dialect robustness in spoken language processing."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that MD3 is used as a knowledge base for augmenting models or enabling retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents usage of the MD3 dataset for linguistic and dialect analysis."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9669,
      "completion_tokens": 573,
      "total_tokens": 10242
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of English conversational speech samples from three dialects (India, Nigeria, United States) but does not include other human languages in the dataset entries themselves."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although speakers' first languages include Telugu (India) and Yoruba (Nigeria), the dataset content itself is exclusively English speech from these dialects; no entries contain two distinct human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 3 Elicitation; Section 4 Dataset",
          "reasoning": "The dataset MD3 contains conversational speech in English from speakers in India, Nigeria, and the United States. The data is described as English dialogues only; while speakers' first languages may vary, the dataset entries themselves are solely English conversational speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries consist exclusively of English content, without any non-English language-only entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset comprises conversational speech transcripts and audio; no programming or code-related content is indicated."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication of inclusion of mathematical or logical expressions in the dataset samples."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human conversational speech and contains no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of use of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language content is well documented as English; there is no uncertainty about the language(s) present in the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains conversational speech and transcripts, therefore language is present."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6887,
      "completion_tokens": 176,
      "total_tokens": 7063
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper states that the dataset will be made publicly available with the publication, but there is no mention or link to any code repository or specific code used for data collection, preprocessing, or generation. Therefore, the code is not made publicly available according to the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including speaker recruitment (Section 3.1), the elicitation setup and tasks (Sections 3, 3.2), recording conditions (Section 3.3), transcription methods (Section 3.4), and detailed dataset statistics and dialect feature analysis (Section 4). This comprehensive description ensures transparency and completeness of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9383,
      "completion_tokens": 134,
      "total_tokens": 9517
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Tasks and Datasets, Table 1",
          "Reasoning": "The paper explicitly introduces five novel speech datasets (Cog1, Cog2, Cog3, Cog4, and Phys) collected through recording voice samples from volunteers undergoing cognitive or physical load induction tasks. These datasets are audio recordings manually captured from human speakers under controlled experimental protocols, confirming the origin is human generated audio data. The descriptions include details on speaker counts, languages, tasks, and duration, indicating these datasets are original and human generated speech audio."
        }
      ]
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10235,
      "completion_tokens": 229,
      "total_tokens": 10464
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1",
            "reasoning": "The datasets (Cog1, Cog2, Cog3, Cog4, and Phys) were labeled by the experimental condition (with load / without load) based on task design and timing (e.g., before/after exercise or tasks inducing cognitive load), implying the labels were assigned programmatically or derived directly from experimental conditions rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not mention any specific instructions given to annotators for labeling, as labeling is based on experimental design rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "No scoring rubrics or formal criteria for annotation are described since labels correspond directly to known task conditions, requiring no subjective scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "No annotation examples are provided in the paper, as labeling relies on experimental condition timing."
          }
        }
      ]
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11365,
      "completion_tokens": 377,
      "total_tokens": 11742
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert annotator for the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance performed by multiple human experts or annotators with subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance carried out by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert human annotators were involved in quality assurance of the annotations or dataset content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of AI models to perform quality assurance or judgement on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated or algorithmic verification processes applied as quality assurance to dataset labels or recordings."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process for validating the dataset annotations or content. The speech data were labeled according to experimental conditions (with/without load) based on the protocol, but no explicit QA procedure is reported."
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10983,
      "completion_tokens": 380,
      "total_tokens": 11363
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Tasks and Datasets",
          "reasoning": "The paper introduces five novel speech datasets collected from human participants performing tasks under induced cognitive or physical load. The speech data was recorded directly from volunteers specifically for this study, using various experimental protocols with original tasks such as counting audio stimuli, memorizing digits, describing images while multitasking, and before/after physical exercise. These are original data created entirely from scratch by humans as per the experiment descriptions."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not generated by AI or machine learning models but collected from human recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is described as part of the data creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not stated as being collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not derived from existing sources but are newly collected data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection methodology are fully documented and described in Section 2.1."
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11501,
      "completion_tokens": 222,
      "total_tokens": 11723
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.3 and 3",
          "reasoning": "The four novel cognitive load datasets and the physical load dataset introduced in the paper are used exclusively as downstream tasks for evaluation and benchmarking of different audio representation models. The evaluation pipeline involves splitting these datasets into training and test sets, training linear classifiers on the representations without fine-tuning, and reporting performance metrics on unseen speaker-independent test sets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12224,
      "completion_tokens": 546,
      "total_tokens": 12770
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 Tasks and Datasets, Table 1",
          "reasoning": "The Cog4 dataset contains speech data in 9 different languages, making the overall dataset multilingual. The Cog1 and Phys datasets include English and French (2 languages), Cog2 and Cog3 include Mandarin Chinese only. Cog4's inclusion of 9 languages makes the combined proposed dataset multilingual."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 Tasks and Datasets, Table 1",
          "reasoning": "The Cog1 and Phys datasets include data recorded in exactly two languages: English and French."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset exclusively contains English language entries; English is only present along with other languages in Cog1, Phys, and others."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Tasks and Datasets, Table 1",
          "reasoning": "Cog2 and Cog3 datasets contain only Mandarin Chinese, a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain speech audio recordings; there is no indication that programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the datasets contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Datasets consist of human speech recordings under cognitive and physical load; no biological sequences or non-human communication are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "None of the datasets contains fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the datasets are clearly documented and specified in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "All datasets contain human language speech recordings, so language is present."
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9442,
      "completion_tokens": 181,
      "total_tokens": 9623
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or explicit mention found",
          "reasoning": "The paper does not provide any explicit mention or link to code repositories related to the construction, preprocessing, or generation of the newly introduced datasets. The methodology describes the dataset collection protocols and methods but does not state that code or scripts used during data collection or processing are shared publicly."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Tasks and Datasets) and Table 1",
          "reasoning": "The paper provides detailed descriptions of the five new datasets in Section 2.1 and summarizes them in Table 1, including information on languages, number of utterances, speakers, duration, and specific protocols used for cognitive and physical load induction. The description includes experimental setups, task designs, and speech recording conditions, providing sufficient documentation about dataset creation."
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5443,
      "completion_tokens": 116,
      "total_tokens": 5559
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Dataset",
          "Reasoning": "The new dataset introduced is a corpus of TV and radio speech samples from the French National Institute of Audiovisual (INA), containing speech recordings of 1025 speakers over four time periods (1955-56, 1975-76, 1995-96, 2015-16). These are human speech recordings collected from media broadcasts, thus audio data of human origin."
        }
      ]
    }
  },
  {
    "id": "elie24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6295,
      "completion_tokens": 193,
      "total_tokens": 6488
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1",
            "reasoning": "The paper states that four people manually transcribed a subset of 81 segments without overlap to evaluate automatic transcription quality, indicating multiple human experts performed the manual transcription annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not mention providing detailed annotation instructions for the manual transcription process; no annotation guidelines are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no mention of scoring rubrics or evaluation rubrics provided to annotators; only evaluation metrics (WER, PER) are reported after annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not report providing annotation examples or guidelines illustrating annotation decisions to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "elie24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7425,
      "completion_tokens": 395,
      "total_tokens": 7820
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset (Paragraph 3)",
          "reasoning": "The paper states that the transcription quality was estimated on a subset of segments manually transcribed by four people without overlap. While no explicit mention is made about their expertise, the manual transcription for the evaluation of the automatic transcription was done by multiple human annotators, suggesting multiple human annotators were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that quality assurance was conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify the annotators as non-experts; thus, this label is not applicable."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset (Paragraph 3)",
          "reasoning": "The dataset transcription was done automatically using Whisper, an AI model, and the quality was assessed by comparing Whisper's transcriptions to manual transcriptions. The use of Whisper as an AI model demonstrates that quality assurance involved an AI model for automatic transcription."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset (Paragraph 3) and Section 2.2 Acoustic feature extraction",
          "reasoning": "Quality assurance includes automatic processing steps such as forced alignment using Montreal Forced Aligner and estimation of formants using Praat with optimized parameters. These automated and algorithmic methods contribute to quality assurance by systematic verification of acoustic feature extraction and segment annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented with manual transcription evaluation, AI model transcription, and automated acoustic processing."
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7043,
      "completion_tokens": 477,
      "total_tokens": 7520
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset",
          "reasoning": "The new dataset consists of speech samples collected from French media archives spanning 60 years, including 1025 speakers with metadata on gender and age. The data represents human-produced speech recordings, curated and selected by the authors for their analysis. Thus, the original speech data was created entirely by human speakers, not translated or adapted from other datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors apply an acoustic-to-articulatory inversion model to estimate articulatory parameters, the generated articulatory data are derived features based on the original speech recordings rather than original data generated entirely by models without input data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced by human translation from another language. The dataset consists solely of French speech from media archives."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine translation applied to the data. The corpus is French media speech."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset",
          "reasoning": "The speech data was collected from existing French media archives (INA). Thus, the dataset is an aggregation of existing recordings without significant modification of the audio data itself."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 2.2 Acoustic feature extraction and 2.3 Acoustic-to-articulatory inversion",
          "reasoning": "The articulatory parameters used for analysis are derived from the original acoustic signals through feature extraction (formant estimation) and inversion with Maeda\u2019s model, representing transformed and adapted data generated from the original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation methods are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7561,
      "completion_tokens": 310,
      "total_tokens": 7871
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2.1-4",
          "reasoning": "The introduced dataset, derived from French media archives with acoustic and articulatory feature extraction, is used primarily to analyze trends and characteristics in articulatory configurations across genders and time periods. The study employs statistical models to interpret changes in parameters like larynx height and lip protrusion, focusing on patterns rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8284,
      "completion_tokens": 542,
      "total_tokens": 8826
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes a dataset composed solely of French media speech recordings. No mention of any other language is made."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes two languages. All data is French speech."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of French speech, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset",
          "reasoning": "The corpus collected is explicitly described as based on French TV and radio archives with automatic transcription and alignment done using French phonetic dictionaries and models. The entire dataset and analysis focus on French speech data only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper mentions algorithms and software (e.g., Whisper, Montreal Forced Aligner), the dataset itself consists of speech data and is not composed of programming code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 2.2 Acoustic feature extraction, Section 2.3 Acoustic-to-articulatory inversion",
          "reasoning": "The paper presents mathematical formulas and modeling to describe the methods used, but the dataset being evaluated consists of speech samples and their associated extracted features, not mathematical expressions as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned; the dataset is real French speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries (French) is clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (French speech), so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5502,
      "completion_tokens": 136,
      "total_tokens": 5638
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link present",
          "reasoning": "The paper does not mention any link, repository, or location where the code for data collection, preprocessing, transcription, forced alignment, acoustic feature extraction, or articulatory inversion is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 to 2.4",
          "reasoning": "The paper provides detailed descriptions of the dataset construction and processing steps including corpus origin, speaker demographics, transcription method and evaluation, forced alignment, acoustic feature extraction, inversion procedure, and statistical analysis, documenting the dataset creation process transparently."
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10950,
      "completion_tokens": 145,
      "total_tokens": 11095
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2, 2.1 and 2.2",
          "Reasoning": "The Lombard-GRID-2mix dataset is derived from the Audio-Visual Lombard GRID speech corpus which contains audio recordings of 54 human speakers speaking sentences in both normal and Lombard speech styles (human generated). Two-speaker mixtures are then simulated using scripts from Isik et al. [27] to create mixture audio data (model generated). The mixture creation process programmatically combines the human-recorded audio signals, thus the final dataset includes both human-generated base signals and model-generated mixtures."
        }
      ]
    }
  },
  {
    "id": "ewert24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11802,
      "completion_tokens": 185,
      "total_tokens": 11987
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 and 2.3",
            "reasoning": "The annotation involves generating two-speaker mixtures using scripts from Isik et al. [27] to simulate mixing based on the Audio-Visual Lombard GRID Corpus speech data. This is a deterministic simulation process without human labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly discussed",
            "reasoning": "There is no mention of specific annotation instructions given, as the data mixture simulation is performed automatically via scripts."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly discussed",
            "reasoning": "No scoring rubrics or manual scoring is involved in the automatic data simulation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly discussed",
            "reasoning": "No annotation examples are provided; the process is scripted and automated."
          }
        }
      ]
    }
  },
  {
    "id": "ewert24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12932,
      "completion_tokens": 258,
      "total_tokens": 13190
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 Audio-Visual Lombard GRID Speech Corpus",
          "reasoning": "The Lombard speech was recorded by 54 native speakers who read sentences while receiving feedback from a human listener. Specifically, the human listener asked the speaker to repeat sentences while pretending not to understand, providing direct human feedback to ensure intelligibility, which implies quality assurance by a human expert involved in the recording process."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 and 2.3 Dataset Simulation and Addition of speech-shaped noise",
          "reasoning": "The dataset mixtures were simulated using scripts from Isik et al. [27] employing an automated procedure for combining utterances and adding noise with controlled SNR values, ensuring standardized and reproducible mixture generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12550,
      "completion_tokens": 501,
      "total_tokens": 13051
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset Lombard-GRID-2mix is derived from the Audio-Visual Lombard GRID Speech Corpus, which contains speech utterances recorded by 54 human native British English speakers in both normal and Lombard speaking conditions, triggered by noise. The paper states that the Lombard speech was recorded via a carefully designed experimental setup involving human speakers responding to noise stimuli and a human listener. Thus, the speech data are original recordings of human speech, created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated or synthesized by AI or machine learning models but rather from human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset content consists of translated data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset content consists of machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely aggregated from existing sources but involved novel recordings and generation of speech mixtures; thus, it is not simply collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 and 2.3",
          "reasoning": "The Lombard-GRID-2mix dataset is derived from the Audio-Visual Lombard GRID Speech Corpus by forming two-speaker mixtures using scripts originally designed for WSJ0-2mix dataset creation. The mixtures combine speech utterances, sometimes with added speech-shaped noise generated from existing WSJ0 corpus speech data, processed through phase randomization and mixing methods. This represents transformation and adaptation of existing speech data into new mixture datasets for the tasks at hand."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method are well specified in the paper."
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13068,
      "completion_tokens": 439,
      "total_tokens": 13507
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The Lombard-GRID-2mix dataset is explicitly used to train speech separation systems from scratch using supervised learning, as described in Section 3.2 where the authors detail the training procedure from randomly initialized parameters with a supervised loss (negative SI-SDR). Several systems are trained from scratch using different subsets of Lombard-GRID-2mix."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4",
          "reasoning": "The dataset is also used for supervised fine-tuning of pre-trained separation models, as detailed in Section 3.2. The authors perform finetuning experiments on Lombard-GRID-2mix subsets and discuss their efficacy in Section 4."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or demonstrate reinforcement learning post-training techniques such as RLHF applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The Lombard-GRID-2mix dataset is used for performance evaluation and benchmarking of speech separation systems, as described in Section 4 through detailed experiments evaluating models on normal and Lombard speech test sets and noisy versions."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Abstract, Section 4",
          "reasoning": "The dataset is used to analyze the impact of Lombard speech on separation performance and the benefits of including such data during training, which constitutes an analysis of trends and characteristics of speech styles in speech separation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset serves as a knowledge base to augment models through retrieval or related mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has practical usage demonstrated in the paper through training, finetuning, evaluation, and analysis as described above."
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13791,
      "completion_tokens": 256,
      "total_tokens": 14047
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 The Audio-Visual Lombard GRID Speech Corpus",
          "reasoning": "The proposed dataset Lombard-GRID-2mix is derived from the Audio-Visual Lombard GRID Speech Corpus, which contains recorded utterances by native speakers of British English. The corpus uses the Grid corpus syntax, which is in English. Thus, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11009,
      "completion_tokens": 186,
      "total_tokens": 11195
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 (Construction of Lombard-GRID-2mix) and abstract",
          "reasoning": "The paper explicitly states in the abstract that all scripts to simulate the Lombard-GRID-2mix dataset are made publicly available. This indicates that code related to dataset construction and simulation is accessible for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Construction of Lombard-GRID-2mix)",
          "reasoning": "Section 2 provides comprehensive details about the dataset construction process, including the data source (Audio-Visual Lombard GRID Speech Corpus), the grouping and mixing strategy, SNR sampling, use of simulation scripts from Isik et al. [27], consideration of noise addition and splitting of data for training/validation/testing. This level of detail constitutes good documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7678,
      "completion_tokens": 108,
      "total_tokens": 7786
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The authors introduce a new in-house meeting code-switching dataset collected from real meeting recordings, manually transcribed, comprising 3000 hours of monolingual English training data, 3000 hours of monolingual Mandarin training data, and 3000 hours of Mandarin-English code-switching training data, indicating human-recorded speech audio data."
        }
      ]
    }
  },
  {
    "id": "fan23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8530,
      "completion_tokens": 183,
      "total_tokens": 8713
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1",
            "reasoning": "The in-house meeting code-switching dataset is manually transcribed from real meetings, implying that multiple human experts performed the transcription annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper states the in-house dataset is manually transcribed but does not provide any detailed annotation instructions in the text."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No mention or description of scoring rubrics or detailed criteria used to evaluate annotator performance or transcription quality are given."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No examples or sample annotations of the manual transcription process or labeling guidelines are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "fan23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9660,
      "completion_tokens": 359,
      "total_tokens": 10019
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human expert annotators performing quality assurance on the datasets; the datasets used include SEAME and an in-house meeting dataset collected and manually transcribed, but details on the QA process or annotator expertise are not provided."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single non-expert human conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions that the in-house meeting code-switching dataset is manually transcribed but does not describe any quality assurance procedures involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that an AI model was used specifically as a judge for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automatic verification or algorithmic/rule-based quality assurance processes applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper provides no explicit description of quality assurance procedures applied to either the SEAME dataset or the in-house meeting code-switching dataset beyond mentioning that the in-house dataset is 'manually transcribed.' There is no information about annotator expertise, the number of annotators, or any verification process. Therefore, no quality assurance process is documented or detailed in the paper."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9278,
      "completion_tokens": 405,
      "total_tokens": 9683
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper states that the authors evaluated their method on two datasets: SEAME, which is an existing public corpus, and an in-house meeting code-switching dataset. The in-house meeting dataset is described as collected from recordings of real meetings and manually transcribed, indicating that the data was newly created by human contributors from scratch rather than derived or translated from pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The in-house meeting dataset is described as newly recorded and manually transcribed; there is no indication it was collated or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data does not appear to be derived from existing sources with modifications or adaptations; rather it was newly collected and transcribed."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of data generation for the in-house meeting dataset is documented in Section 4.1."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9796,
      "completion_tokens": 489,
      "total_tokens": 10285
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The newly introduced in-house meeting code-switching dataset consisting of 3000 hours monolingual and 3000 hours code-switching speech is used as training data to train the model architecture from scratch, as evidenced by the training details in Section 4.2 and the improvement of results in Section 4.4 when trained on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using the new dataset for fine-tuning a pre-trained model; instead, training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that reinforcement learning or RL-based post-training methods such as RLHF are used with the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.4",
          "reasoning": "The in-house meeting code-switching dataset has dedicated test sets (test ma, test n, test cs) used to evaluate the model\u2019s performance, showing MER reductions and assessing effectiveness in real-world scenarios."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the dataset solely for analysis of trends or characteristics but for model training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using the dataset as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The in-house meeting code-switching dataset is clearly used for training from scratch as well as for evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10519,
      "completion_tokens": 537,
      "total_tokens": 11056
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced contain exactly two human languages, Mandarin and English, and do not include more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper introduces an in-house meeting code-switching dataset comprising Mandarin and English speech data, as well as the SEAME dataset which consists of Mandarin-English code-switching speech. Both datasets contain exactly two human languages, Mandarin and English."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No new datasets with only English content exclusively are introduced; the dataset contains bilingual speech with Mandarin and English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No new datasets with only a non-English language exclusively are introduced; the dataset contains bilingual speech with Mandarin and English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or evidence of datasets containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not include mathematical or formal logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets focus on human speech in Mandarin and English; no biological or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are present in the described datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages included in the datasets are explicitly specified as Mandarin and English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain human language (Mandarin and English) transcriptions; thus, this label does not apply."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7737,
      "completion_tokens": 211,
      "total_tokens": 7948
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The paper mentions the use of two datasets: SEAME and an in-house meeting code-switching dataset. The SEAME dataset is a publicly available corpus referenced by [28], but the in-house meeting dataset is proprietary and collected by the authors. There is no mention or link to any code repository for data collection, preprocessing, or dataset construction for the in-house dataset or for SEAME within this paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The paper provides some descriptive details about the datasets used, including data amounts and evaluation sets, but does not provide any documentation or detailed methodology on how the in-house meeting code-switching dataset was constructed, collected, or processed. The SEAME dataset is referenced from another paper but not described in detail here. Therefore, the documentation of the dataset creation process is not presented in this paper."
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7594,
      "completion_tokens": 190,
      "total_tokens": 7784
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 and Section 3",
          "Reasoning": "The novel dataset includes audio recordings from the Image Description Task, where participants describe an image while their microphone records their speech. This data is human generated as it is manually spoken by participants and recorded during the study conducted on the Thymia Research platform."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.2 and Section 3",
          "Reasoning": "The dataset includes behavioral performance metrics from the n-Back Task, such as precision, recall, false-positive rates, and reaction times. These are tabular data derived from participant interactions (clicks and responses) during the cognitive task, making it human generated through participant input during the study."
        }
      ]
    }
  },
  {
    "id": "fara22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8446,
      "completion_tokens": 252,
      "total_tokens": 8698
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2.2, Section 3.1",
            "reasoning": "The n-Back Task data consists of participants' responses collected via a computerized card memory game implemented on the Thymia Research platform, where performance metrics such as reaction times and accuracy were calculated automatically. Similarly, speech data was collected via an online platform with automated audio quality checks and features extracted using automated tools."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Participants received task instructions as part of the online study setup, including reminders about recording and task description (e.g., participants are encouraged to describe images while being recorded; instructions for n-Back task given in Section 2.2.2)."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific mention",
            "reasoning": "The paper does not describe any scoring rubrics or qualitative annotation scales used for the collected data; performance metrics were quantitative and computed algorithmically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific mention",
            "reasoning": "The paper does not provide any annotation examples or annotated samples as part of guideline documentation."
          }
        }
      ]
    }
  },
  {
    "id": "fara22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9576,
      "completion_tokens": 288,
      "total_tokens": 9864
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple non-expert annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 (Quality Controls)",
          "reasoning": "Quality assurance for audio recordings was performed using an automated audio quality pipeline that flagged files based on extracted feature quality and speech activity detection. This automated process was used to accept or reject audio data prior to analysis, thus constituting an automatic verification method."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented in the paper, specifically for audio data, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9194,
      "completion_tokens": 438,
      "total_tokens": 9632
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3",
          "reasoning": "The dataset comprises original content collected directly from human participants through tasks such as the Image Description Task and n-Back working memory task, conducted remotely using the Thymia Research Platform. Participants are native English-speaking adults who performed these tasks on their personal devices, and the data consists of speech recordings and behavioral responses newly generated in this study, not translated, adapted, or derived from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any part of the dataset being generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any use of machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as aggregated or collected from existing databases or sources; instead it was newly collected for this study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The raw human-collected data (speech recordings and n-Back task behavioral data) was processed to extract acoustic, linguistic, and performance features using established feature extraction tools and algorithms, creating derived features for analysis and modeling purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and generation method of the data is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9712,
      "completion_tokens": 275,
      "total_tokens": 9987
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3.1 and 5 (Models and Results)",
          "reasoning": "The dataset is used to train binary Random Forest classifiers from scratch to predict depression presence or absence. The models are trained and cross-validated from randomly initialized parameters with this dataset, indicating supervised training rather than fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Results and Discussion)",
          "reasoning": "The dataset is used for evaluating model performance on depression classification, including on test hold-out sets, to benchmark predictive effectiveness."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Results and Discussion)",
          "reasoning": "The authors perform detailed feature importance analyses and regression to analyze associations between dataset features and specific depression symptoms, using the dataset primarily for analytic insights."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10435,
      "completion_tokens": 575,
      "total_tokens": 11010
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes only a single language, English, as stated in Section 2.2: participants were adult, native English speakers."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains no indication of two languages present; only English is specified as the language of participants (Section 2.2)."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 'Online Study Setup', and Section 3 'Dataset'",
          "reasoning": "The dataset consists solely of native English-speaking participants (Section 2.2), and speech data recorded from them in English (Image Description Task, Section 2.2.1). This is explicitly stated and implies all the speech data is in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset exclusively contains English speech, as participants were native English speakers (Section 2.2)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain code or programming languages; although code was used for feature extraction and modeling, the dataset itself does not contain programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech and behavioral data, but no entries with mathematical or formal logical notation are present as data points."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is human speech and behavioral task performance; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed or fictional languages in the dataset; all speech recorded is natural English from native speakers."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the speech data is well documented as English spoken by native English speakers (Section 2.2)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains linguistic data in the form of English speech, thus language is present."
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7653,
      "completion_tokens": 162,
      "total_tokens": 7815
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention regarding code availability for dataset construction.",
          "reasoning": "The paper does not include any links, references, or statements indicating that the code used for data collection, preprocessing, or dataset construction is publicly available. No repository or codebase is mentioned."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3 (Data Collection and Dataset).",
          "reasoning": "The paper documents the dataset creation process extensively, including detailed descriptions of the data collection platform (Thymia Research Platform), participant recruitment criteria, task designs (Image Description Task and n-Back Task), data modalities collected, quality control procedures, and dataset splits. This information provides transparent and relatively complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8368,
      "completion_tokens": 155,
      "total_tokens": 8523
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1: Introduction; Section 5.1: Datasets; Table 2; Table 3",
          "Reasoning": "The paper explicitly states the creation of a new dataset of 6,681 instances of voice-friendly hints, consisting of triples: initial question, related questions, and follow-on hint, across 9 different domains. The dataset includes annotations and is used for pretraining and training hint generation models. The hints are human-written, as indicated by 'human written hints' in the abstract and annotation guidelines in the methodology. There is no indication that the data were generated by models or derived from unknown sources."
        }
      ]
    }
  },
  {
    "id": "faustini23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9220,
      "completion_tokens": 307,
      "total_tokens": 9527
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4, Section 5.3.2, Section 7",
            "reasoning": "The paper describes human evaluation studies involving multiple human annotators judging syntactic correctness, question coverage, and naturalness of hints (Sections 5.3.2 and 7). The dataset of 6,681 instances was created using carefully constructed annotation guidelines and quality checks (Section 8), implying expert annotators were involved to ensure high quality."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4, Section 8",
            "reasoning": "Section 4 mentions providing guidelines to annotators to create voice-friendly hints. Section 8 also states the dataset was created using carefully constructed annotation guidelines, indicating instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5.3.2, Section 7",
            "reasoning": "Section 5.3.2 describes human evaluation criteria (e.g., syntactic correctness, question coverage) used in evaluations, which implies the existence of scoring rubrics for annotation. Section 7 details consistent evaluation metrics applied during human studies."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4, Section 8",
            "reasoning": "Section 4 and 8 indicate preliminary experiments and careful construction of annotation guidelines, which typically include examples. The paper also provides example hints and questions (e.g., Table 7) demonstrating annotation outputs."
          }
        }
      ]
    }
  },
  {
    "id": "faustini23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10350,
      "completion_tokens": 315,
      "total_tokens": 10665
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple human experts or annotators with subject matter expertise in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5.3.2 Human Evaluation and Section 7 Human Evaluation Studies",
          "reasoning": "The paper describes human evaluation studies where human annotators assess syntactic correctness, question coverage, hint naturalness, and retention in voice modality. Although the expertise level of annotators is not explicitly stated, the labeling guidelines and annotations imply the involvement of multiple non-expert human annotators performing quality assessment."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that AI models were used as judges for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes automated metrics (BLEU, ROUGE, BERTScore) for evaluation of model outputs but not as a quality assurance process for dataset annotation verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is documented via human evaluations; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9968,
      "completion_tokens": 480,
      "total_tokens": 10448
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Abstract; Section 5.1 (Datasets)",
          "reasoning": "The paper introduces a new dataset of 6,681 instances comprising triples of initial questions, related questions, and human written follow-on hints created following detailed annotation guidelines and quality checks, as described in the Abstract and Section 5.1. The data is original content created from scratch by human annotators specifically for this task, not translated or adapted from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset is generated entirely by AI or machine learning models without reference to existing data. Generated hints by models are outputs during experiments, not datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset involved human translation from another language; the dataset appears to be originally created in English."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is described or used in the dataset creation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not merely collected or aggregated from existing sources; it was constructed via annotation from related questions and human authored hints."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets); Section 2 (Linguistic Task and Background)",
          "reasoning": "The dataset includes related questions retrieved/generated by a voice assistant system (i.e., existing questions), from which human annotators derived the follow-on hints by applying linguistic transformation principles such as converting direct questions into indirect content clauses. Thus, the data is based on existing question sources but involves transformations and adaptations by human annotators."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of dataset creation is explicitly described, so the origin is specified and documented."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10486,
      "completion_tokens": 533,
      "total_tokens": 11019
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets), and Section 5.2 (Baselines and Approaches)",
          "reasoning": "The paper explicitly states that the SINGLE-HINTS dataset (also called RS data) of voice-friendly hints is used for pretraining the hint generation approaches to learn the syntactic transformations required to convert questions into inquisitive content clauses. This pretraining helps the model to perform rewriting operations foundational to the task before fine-tuning for voice-friendly hint generation."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the proposed dataset. All Transformer models used (BART, T5) are pre-trained models further fine-tuned or pretrained on the dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2 (Baselines and Approaches) and Section 6 (Evaluation)",
          "reasoning": "The dataset is clearly used to fine-tune pre-trained Transformer models (BART, T5) in a supervised setting for the main task of generating voice-friendly hints from related questions. The fine-tuning is done after the pretraining stage or independently (direct hint generation)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of reinforcement learning based post-training methods (such as RLHF) being applied on the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3 (Evaluation Metrics), Section 6 (Evaluation), and Section 7 (Human Evaluation Studies)",
          "reasoning": "The dataset or derived splits (test set of the hint generation dataset) is used to evaluate the quality of generated hints using automatic metrics (BLEU, ROUGE, BERTScore) as well as human evaluation on correctness, naturalness, coverage, and retention."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as used primarily for analysis of trends, patterns, or other characteristics without training or evaluation. The focus is on supervised modeling and assessment."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used or described as a knowledge base to augment models such as in retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes and demonstrates multiple uses of the new dataset in pretraining, supervised fine-tuning, and evaluation stages."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11209,
      "completion_tokens": 537,
      "total_tokens": 11746
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or indicate that the dataset includes more than two human languages. All dataset examples and linguistic discussion focus solely on English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention that the dataset contains exactly two human languages. The dataset is described as dealing with English questions and hints only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, particularly Abstract and Section 2 (Linguistic Task and Background)",
          "reasoning": "The dataset consists of input questions, related questions, and human-written hints all in English. The linguistic desiderata and syntactic analysis focus exclusively on English, and all examples provided are in English. There is no mention of other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries that are in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural language questions and hints; it does not include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of mathematical or formal logical expressions in the dataset entries. The content is natural language questions and hints."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include non-human communication data such as biological sequences or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages appear in the dataset or paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly English; it is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains natural language (English) data, so it is not without language."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8427,
      "completion_tokens": 227,
      "total_tokens": 8654
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention indicating code availability is present in the paper.",
          "reasoning": "The paper states that the dataset will become publicly available but does not mention releasing any code related to dataset construction, data collection, or preprocessing. There is no link, footnote, or appendix referencing code repositories associated with the dataset or experiments."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 5.1 (Datasets) and 6 (Evaluation with Automated Metrics) provide dataset descriptions and statistics; Sections 2 and 3 outline linguistic desiderata and dataset construction rationale.",
          "reasoning": "The paper provides detailed descriptions of the new dataset, including its composition (6,681 instances), domains covered (9 domains), and the process for collecting voice-friendly hints with annotation guidelines and quality checks. Tables 2 and 3 summarize dataset statistics for training, development, and testing splits. The linguistic desiderata and objectives guiding dataset construction are articulated in Section 2. This information documents the dataset creation process sufficiently for understanding and reproducing the dataset at a high level."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7827,
      "completion_tokens": 178,
      "total_tokens": 8005
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.2",
          "Reasoning": "The paper describes recordings of speech audio signals of 5 subjects who produced 600 logatomes at different speaking rates, recorded in a soundproofed audio studio with microphones and audio interfaces, indicating human-generated audio data."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.2",
          "Reasoning": "Tongue movements were measured using Electro-Optical Stomatography (EOS) which includes optical distance sensors mounted on individualized artificial palates worn by 5 subjects. These articulatory measurements are sensor data recorded from human subjects, hence human generated signal/sensor data."
        }
      ]
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8679,
      "completion_tokens": 224,
      "total_tokens": 8903
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.4",
            "reasoning": "The paper describes that the start and end times of the acoustic diphthong intervals were manually marked from the audio signal, which indicates human expert annotation. The manual alignment of audio and articulatory data, and the specific selection and exclusion of diphthongs based on sensor data quality, further suggests expert human judgment rather than automatic or multiple annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not mention any detailed annotation instructions or guidelines provided to annotators for marking acoustic or articulatory diphthong intervals."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No mention of scoring rubrics or formal criteria used to evaluate or score the annotations is made in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not provide any examples of annotations or annotated data segments to guide annotators."
          }
        }
      ]
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9809,
      "completion_tokens": 323,
      "total_tokens": 10132
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human expert annotators performing quality assurance or validation of the dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human non-expert performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is given about multiple human non-experts performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any AI model being used to perform quality assurance on the dataset or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the study involved automated processing such as smoothing and regression analysis, these are data analysis methods and not described as quality assurance or verification steps for dataset annotation quality. The paper does not describe any automated verification process for the dataset quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not explicitly describe any quality assurance or validation procedure applied to the dataset annotations or content. Data segmentation such as acoustic diphthong intervals was manually marked (Section 2.4), but no explicit QA or double annotation procedure is described. Therefore, no documented quality assurance process for the dataset is present."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9427,
      "completion_tokens": 375,
      "total_tokens": 9802
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Subjects, corpus and experimental procedure",
          "reasoning": "The data is collected from five healthy male subjects who produced logatomes containing German diphthongs at various speaking rates, while articulatory and acoustic signals were recorded. This new data collection involved human participants and original recordings, not derived from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation of data or content between languages performed by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate or adapt any data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected by aggregating or collecting existing data sources; rather, it was newly recorded from experimental sessions."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some calculations and processing were performed on the recorded data, the dataset itself is original and not based on pre-existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and method of data generation."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9945,
      "completion_tokens": 325,
      "total_tokens": 10270
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2 (Method), 2.4 (Data analysis), and 3 (Results)",
          "reasoning": "The dataset of tongue movement and acoustic recordings of German diphthongs is primarily used for analyzing articulatory-acoustic relationships, characterizing trends and patterns in tongue movements relative to acoustic intervals, rather than for training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates usage of the dataset for analysis of articulatory and acoustic characteristics, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10668,
      "completion_tokens": 542,
      "total_tokens": 11210
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper's proposed dataset consists exclusively of German diphthongs spoken by native German speakers. There is no indication that more than two languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only German language data; no second language is present."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although English translations appear in the text, the dataset entries are German language utterances only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (Subjects, corpus and experimental procedure)",
          "reasoning": "The dataset consists of recordings of 5 native German speakers producing logatomes containing German diphthongs, spoken within German carrier phrases. Thus, the dataset is monolingual and non-English (German)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or code-related content is included in the dataset; the data consists of speech and articulatory signals only."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the paper uses mathematical notation for analysis and modeling, the dataset entries themselves do not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech production data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset only contains naturally occurring German language utterances; no fictional or constructed languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is clearly specified as German."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains linguistic content in German and thus is not without language."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7886,
      "completion_tokens": 163,
      "total_tokens": 8049
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No code repository or link is provided in the paper.",
          "reasoning": "The paper does not mention the availability of any code related to data collection, preprocessing, or analysis, nor does it provide any repository link or URL for code access."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method) and subsections 2.1, 2.2, 2.3, 2.4 describe the data collection, corpus, measurement setup, and data processing.",
          "reasoning": "The paper provides a detailed account of the dataset creation process, including the recording setup, subjects, corpus composition, recording procedure, sensor calibration, and data postprocessing steps, giving transparency about how the new dataset was constructed."
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7252,
      "completion_tokens": 165,
      "total_tokens": 7417
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The new dataset consists of about 1000 hours of an in-house dataset containing medical conversations in English. The audio recordings were collected from conversations involving doctors and patients with varying acoustic conditions, recorded using a device carried by the doctor, thus involving human participants and human-operated recording devices."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The transcribed text of the in-house medical conversations was manually transcribed and annotated for PII sequences by human annotators, confirming human generation of the textual modality associated with the new dataset."
        }
      ]
    }
  },
  {
    "id": "flechl22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8104,
      "completion_tokens": 231,
      "total_tokens": 8335
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 De-identification and Section 4.1 Data",
            "reasoning": "The paper states the de-identification process starts with manual annotation of the transcribed text, where PII is tagged and given one of roughly 30 labels, indicating skilled annotation likely by experts given the medical domain sensitivity and manual detailed tagging."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 De-identification",
            "reasoning": "The manual annotation of PII with about 30 fine-grained labels implies detailed annotation guidelines and instructions to identify and tag various PII categories accurately."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention the presence of any scoring rubrics or formal criteria for scoring annotations; it mainly describes labeling categories without discussing scoring or evaluation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no indication or reference to annotation examples being provided to annotators within the paper."
          }
        }
      ]
    }
  },
  {
    "id": "flechl22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9234,
      "completion_tokens": 225,
      "total_tokens": 9459
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 De-identification and Section 4.1 Data",
          "reasoning": "The paper states that the training data recordings are manually transcribed and that PII sequences are annotated afterwards. This manual annotation implies human annotators performed the identification and tagging of PII. Although the paper does not specify the number or expertise of annotators, the use of 'manual annotation' suggests expert annotators familiar with the domain (medical conversations) carried out this task, as it involves complex medical data and sensitive information requiring domain expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8852,
      "completion_tokens": 478,
      "total_tokens": 9330
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any completely new original content from scratch by human contributors. The training data used is an in-house dataset of medical conversations that are existing data, not newly created from scratch."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 ('Text-to-speech')",
          "reasoning": "The paper describes generating artificial audio for surrogate PII tokens using text-to-speech (TTS) synthesis with state-of-the-art neural TTS models. This synthetic audio is newly generated by AI models without direct reference or transformation of existing audio data for those tokens."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that data was produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 ('Data')",
          "reasoning": "The original training data is an in-house medical conversation corpus collected and transcribed previously, thus collated from existing real-world recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 ('Splicing using audio from the corpus') and Sections 3.1 to 3.3 in general",
          "reasoning": "The paper describes generating training data variants by replacing PII tokens with surrogate tokens and creating corresponding audio either by splicing together fragments of existing audio corpus recordings (possibly combining audio pieces from different speakers) or by TTS synthesis. This process creates adapted or transformed data derived from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and methods of generating augmented data, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9370,
      "completion_tokens": 286,
      "total_tokens": 9656
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The in-house medical conversation dataset is used to train end-to-end ASR models from scratch, as described in Section 4.2. Various models are trained on the de-identified and augmented data subsets for 80 epochs until convergence."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The dataset is used for evaluation in testing the ASR models' performance by measuring word error rates and F1 scores on the held-out test data with and without de-identification and augmentation methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.3 and 5",
          "reasoning": "The paper analyzes trends in recognition performance degradation due to de-identification and recovery by augmentation methods, with detailed analysis of recall, precision, and diarization error metrics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10093,
      "completion_tokens": 515,
      "total_tokens": 10608
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as containing medical conversations conducted in English only (Section 4.1), with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages; it is exclusively English as described in Section 4.1."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The proposed dataset consists of English medical conversations exclusively, as explicitly stated in Section 4.1: 'The training data consist of a subset of about 1000 hours of an in-house dataset containing medical conversations conducted in English covering about twenty different medical specialties.'"
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English datasets or languages are mentioned in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio and transcriptions; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech conversations only, no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned or included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the dataset are clearly stated as English; no uncertainty is indicated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain natural language (spoken English), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7311,
      "completion_tokens": 174,
      "total_tokens": 7485
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section provides code or code repository links.",
          "reasoning": "The paper does not mention or provide any link to publicly available code for data collection, de-identification, surrogate generation, TTS processing, or splicing used in constructing the dataset or generating augmented training data."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Data, Section 3 De-identification, Section 3 Strategies for performance recovery",
          "reasoning": "The paper includes detailed descriptions of the training and test datasets, the manual annotation and labeling of PII, the forced alignment for audio intervals, surrogate generation methods, and two audio generation methods (TTS and splicing). These sections document the dataset creation and augmentation processes thoroughly, providing transparency on how the datasets were constructed and de-identified."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7575,
      "completion_tokens": 152,
      "total_tokens": 7727
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The Earnings21 dataset consists of audio recordings of earnings calls from public companies, which are recordings of real human speech, hence human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The authors created and released contextual biasing lists (word and phrase lists compiled from manually curated PERSON and ORG entities tagged by spaCy) accompanying the Earnings21 dataset. These biasing lists are text data manually curated for this work, hence human-generated text."
        }
      ]
    }
  },
  {
    "id": "fox22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8427,
      "completion_tokens": 197,
      "total_tokens": 8624
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Data",
            "reasoning": "The paper states that biasing lists were manually curated from PERSON and ORG entities auto-tagged by spaCy, indicating annotation was done by a single human expert reviewing and curating the named entity lists for the Earnings21 dataset bias lists."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not provide explicit annotation instructions for the manual curation of the biasing lists from named entities."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "There is no mention of scoring rubrics or guidelines used during the annotation or curation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No examples of annotation guidelines or exemplar annotations are provided for the manual curation of the bias term lists."
          }
        }
      ]
    }
  },
  {
    "id": "fox22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9557,
      "completion_tokens": 387,
      "total_tokens": 9944
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by multiple human experts on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single non-expert human annotator performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation about multiple non-expert human annotators performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While an alternate spelling prediction AI model is used for generating alternate spellings, the paper does not describe the AI model as being used for quality assurance or judging data quality of the released datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe any automated verification or algorithmic/rule-based quality assurance performed on the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces contextual biasing lists accompanying the Earnings21 dataset but does not document any quality assurance process applied to these biasing lists or other new datasets. The lists were manually curated from automatically tagged entities but no QA process or validation is reported."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9175,
      "completion_tokens": 478,
      "total_tokens": 9653
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The authors created new contextual biasing lists for the Earnings21 dataset, manually curated from named entities detected in the Earnings21 references using spaCy. The lists include a 'best case scenario' list and a 'distractor' list. This is original content created by human contributors from scratch relevant to this research, as described in Section 4.1."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any dataset generated solely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data produced by human translation of text from another language is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of existing data is described or referenced in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The Earnings21 contextual biasing lists are compiled from existing named entities automatically tagged by spaCy on the Earnings21 references, with manual curation. Hence, the biasing lists are aggregated from existing sources (the Earnings21 dataset's reference annotations) without fundamental modification, fitting the definition of collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The biasing lists derive from automatic entity recognition outputs which are then manually curated to produce final biasing lists. This involves filtering and selecting entities, thus representing data based on existing sources with transformations or adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the source and method of generation of the new biasing lists; thus, the data origin is documented."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9693,
      "completion_tokens": 338,
      "total_tokens": 10031
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Data, Section 5.1 Earnings21 Benchmark Baselines",
          "reasoning": "The authors introduce contextual biasing lists created to accompany the Earnings21 dataset to form a public benchmark for contextual ASR evaluation. They report results on the Earnings21 test set using these biasing lists to measure model performance, particularly recall of bias terms, rare, and OOV words to evaluate recognition improvements."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The Earnings21 contextual biasing lists are clearly used for evaluation purposes as a benchmark; thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10416,
      "completion_tokens": 543,
      "total_tokens": 10959
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper introduces the Earnings21 dataset used for evaluation, which contains earnings calls from a variety of public companies in English only. There is no mention of multiple human languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only English content, with no indication of entries in exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The Earnings21 dataset, introduced in this paper with contextual biasing lists, consists of earnings calls from public companies and is described as containing proper nouns and phrases in English only. The paper does not mention the presence of any other languages, indicating the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain non-English language content exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper discusses models and code implementations, the dataset entries themselves are natural language transcripts, not programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are human language transcripts from earnings calls without presence of mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human language speech transcripts; there is no reference to biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of dataset entries is clearly identified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language transcripts, so language is definitely present."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7634,
      "completion_tokens": 360,
      "total_tokens": 7994
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.1 Data and Section 6 Conclusions",
          "reasoning": "The paper states that for the Earnings21 dataset, they created contextual biasing lists and that both the lists and Earnings21 dataset are publicly available in the Earnings21 repository [3]. The paper mentions that these resources enable replication by other researchers. Also, the ASR model used is open-source from the WeNet toolkit and datasets like GigaSpeech are open-source, implying that the required code and data are accessible. Although an explicit link to code generating the biasing lists is not directly given in the provided excerpt, the mention that both lists can be found in the Earnings21 repository supports that at least the dataset and the biasing lists are publicly provided. There is no explicit mention of code for dataset construction being publicly available, but since the biasing lists are constructed by manual curation from spaCy entity tagging and are released, it suggests at least partial reproducibility support. Hence, considering the code used to generate the contextual biasing lists is likely shared or easily replicable, the code availability is assessed as true."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The paper provides descriptive detail about how the contextual biasing lists were created: they were manually curated from PERSON and ORG entities automatically tagged by spaCy from the Earnings21 references. The authors describe the creation of a best case list and a distractor list by adding Fortune 500 companies and famous CEOs. The sizes of these lists and examples are provided. Also, procedural details regarding audio segmentation and scoring against references are given. This constitutes clear and sufficient documentation of how the dataset and biasing lists were created and used."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7996,
      "completion_tokens": 190,
      "total_tokens": 8186
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "The paper states that approximately 436 hours (around 200,000 utterances) of reading speech by Chinese L2 adult learners were collected and used for the masked language modeling (MLM) pre-training, indicating this is a new, human-recorded non-native speech dataset introduced by the authors for pre-training."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "Along with the non-native speech, the authors collected the associated prompt text used in the read aloud tasks for the MLM pre-training. This prompt text is human-generated as it was prepared for eliciting learner speech during data collection, representing a new dataset introduced by the authors for pre-training purposes."
        }
      ]
    }
  },
  {
    "id": "fu23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8848,
      "completion_tokens": 219,
      "total_tokens": 9067
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1",
            "reasoning": "Section 4.1 states that the fine-tuning dataset 'ByteRead' is an internal dataset of 14,000 utterances collected from Bytedance's education product and these are human-annotated fluency scores, implying multiple human experts provided the annotations for fluency scoring."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to the human raters for fluency scoring in ByteRead dataset or any new dataset introduced by the authors."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention any scoring rubrics or criteria used for annotation in the new ByteRead dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No examples of annotated data or annotation guidelines including examples for the new ByteRead dataset are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "fu23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9978,
      "completion_tokens": 304,
      "total_tokens": 10282
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple human expert annotators were involved in quality assurance for the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models as judges for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of quality assurance conducted via automated verification or rule-based algorithmic techniques on the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process applied to the human-annotated fluency scores or other dataset annotations. No details are provided on annotator expertise, number of annotators, or verification procedures. Therefore, no quality assurance process is considered present based on the information in the paper."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9596,
      "completion_tokens": 483,
      "total_tokens": 10079
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper states that the authors collected approximately 436 hours (about 200,000 utterances) of reading speech by Chinese L2 adult learners and prompt text for MLM pre-training. Furthermore, the ByteRead internal dataset of 14,000 English utterances is described as collected from Bytedance's education product, indicating original content created by humans. This data was created from scratch by human contributors for the purpose of this research."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used to generate datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The authors also use pre-existing datasets such as LibriSpeech and Speechocean762. The LibriSpeech dataset (960 hours) and the Speechocean762 dataset are publicly available datasets collected previously; incorporating these without significant modification constitutes collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1 and Section 4.2",
          "reasoning": "The authors derive phone-level raw features (deep features, phone sequence, and duration) from the raw audio and forced alignment using an acoustic model and forced aligner. These features are created by transforming existing raw audio data into new representations used for model training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly details dataset sources and methods of generation, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10114,
      "completion_tokens": 604,
      "total_tokens": 10718
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1 and Table 1 (Data splitting), Section 4.1 (Speech corpora)",
          "reasoning": "The authors introduce a new unlabeled dataset consisting of approximately 436 hours (about 200,000 utterances) of reading speech by Chinese L2 adult learners and prompt text used exclusively for self-supervised phonetic and prosody-aware pre-training. This dataset is used to pre-train their model by masking phones and durations and reconstructing them, as described in Section 3.1."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new pre-training dataset to train models from scratch; instead, it uses it for pre-training only."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Fine-tuning for fluency scoring; Section 4.1 and Table 1 (Data splitting); Section 5 Results",
          "reasoning": "The authors use human-annotated labeled datasets, including ByteRead and Speechocean762, which are new or internal datasets, to fine-tune the pre-trained model using supervised learning with fluency scores."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence of reinforcement learning based post-training (e.g., RLHF) using the dataset is provided."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Speech corpora); Section 5 Results and analyses",
          "reasoning": "The Speechocean762 and ByteRead datasets, including their splits, are used to evaluate and benchmark the fluency scoring performance of models pretrained and fine-tuned with the new datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2 Ablation studies",
          "reasoning": "The authors perform an analysis of the contributions of phonetic and prosodic components in the pre-training dataset to fluency scoring performance, using ablation studies."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new datasets introduced in the paper are used in multiple stages of the machine learning pipeline, including pre-training, supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10837,
      "completion_tokens": 461,
      "total_tokens": 11298
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1, 'Speech corpora', and throughout the paper",
          "reasoning": "The new dataset introduced by the authors for pre-training and fine-tuning is described as consisting of English speech data from Chinese L2 adult learners reading English prompts (Section 4.1). The pre-training corpus contains approximately 436 hours of reading speech by Chinese L2 adult learners, and the fine-tuning uses ByteRead, an internal English dataset, and Speechocean762, a public English non-native speech corpus. All data is English speech produced by second language learners, and the prompt texts are in English. There is no mention of any other human language being present in the proposed datasets. Therefore, the datasets introduced are monolingual with English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language (English), so the N/A category does not apply."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8055,
      "completion_tokens": 223,
      "total_tokens": 8278
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper does not provide any links or mention availability of code related to dataset construction, preprocessing, or generation. There is no indication that the code used for constructing the new datasets or for the data processing is publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Speech corpora) and Section 4.2 (Feature extraction)",
          "reasoning": "The paper documents the dataset collection and preparation processes to some extent. It describes the collection of a large unlabeled non-native speech dataset (~436 hours, about 200,000 utterances) for pre-training and an internal dataset named ByteRead of 14,000 English utterances used for fine-tuning and evaluation. It also details the data splitting in Table 1. Additionally, feature extraction procedures and forced alignment details are described in Section 4.2. However, it lacks comprehensive step-by-step details about how the datasets were collected or constructed and does not provide exhaustive code or scripts for reproducibility."
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7205,
      "completion_tokens": 113,
      "total_tokens": 7318
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract and Section 3.1",
          "Reasoning": "The paper introduces an in-house 2.6k-hour Chinese dataset with labeled Mandarin-Cantonese speech recorded for ASR model training, explicitly mentioned as 'our in-house 2.6k-hour Chinese dataset' used in experiments. It consists of human-recorded speech signals used for training ASR models, thus the data is audio modality with human-generated origin."
        }
      ]
    }
  },
  {
    "id": "fu23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8057,
      "completion_tokens": 237,
      "total_tokens": 8294
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1",
            "reasoning": "The labels for the datasets, such as the 2.6k-hour Chinese labeled speech dataset and 1k-hour English labeled dataset, are used for fine-tuning SL and SSL models but the process of annotation is not described explicitly. The datasets appear to be pre-annotated or labeled externally, and no human annotator or annotation process is described for creating new labels in this paper. The paper primarily focuses on model fusion techniques using existing labeled data rather than performing new annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not describe any annotation guidelines, instructions, or protocols related to labeling data for the newly introduced datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No scoring rubrics or criteria for annotation quality or labeling correctness are discussed in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not provide annotation examples or samples demonstrating the labeling process for these datasets."
          }
        }
      ]
    }
  },
  {
    "id": "fu23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9187,
      "completion_tokens": 277,
      "total_tokens": 9464
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or description of multiple human experts performing quality assurance on the dataset introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that multiple human non-experts were involved in quality assurance for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description that dataset quality assurance was conducted via automated verification of code or formulas."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces an in-house 2.6k-hour Chinese dataset but provides no details on any quality assurance process applied to the dataset annotations or content. No explicit mention of human or automated quality assurance is provided."
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8805,
      "completion_tokens": 423,
      "total_tokens": 9228
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any original speech data was created entirely from scratch by human contributors. The datasets used are existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was generated by AI or machine learning models; rather, it uses existing speech datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by human translation between languages in the provided paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine-translated data for training the models or evaluation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (implicit in descriptions of datasets), Section 1 Introduction",
          "reasoning": "The paper uses existing datasets such as the 1k-hour English LibriSpeech dataset and an in-house 2.6k-hour Chinese dataset. The Chinese dataset is described as in-house but no indication it is newly created; rather it appears collected from existing sources. Therefore, the data appears to be collected and aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any extensive transformations or modifications to existing datasets to create new datasets; it focuses on model fusion methods rather than data adaptation or derivation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are clearly specified as existing datasets, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9323,
      "completion_tokens": 339,
      "total_tokens": 9662
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The proposed datasets, including the in-house 2.6k-hour Chinese labeled dataset and the 1k-hour English labeled dataset, are used to fine-tune the fused model after model fusion via moderate supervised training. This is described in Sections 4.1 and 4.2 where the ASR systems are fine-tuned on these labeled datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The datasets, including the original LibriSpeech test-clean set and the in-house Chinese test sets, are used for evaluation of recognition performance (WER and CER) of the individual and fused models as shown in Section 4.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4.3",
          "reasoning": "Datasets are used for analysis of the complementarity and optimization properties of SL and SSL models via error analysis and loss landscape visualization, as discussed in Section 3.1 and the ablation studies in Section 4.3."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10046,
      "completion_tokens": 530,
      "total_tokens": 10576
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4 (Main experiment), Abstract",
          "reasoning": "The newly introduced datasets are the in-house 2.6k-hour Chinese dataset which includes Mandarin and Cantonese languages, and the 1k-hour English LibriSpeech dataset. Since the Chinese dataset includes two different languages and the study involves both English and Chinese datasets, the combined new dataset spans more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the Chinese dataset consists of Mandarin and Cantonese (two languages), the overall proposed dataset considered in the study also includes English data. Therefore, the dataset is not limited to exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is not only English but also includes Chinese languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced includes English and Chinese, so it is not monolingual in non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are speech recognition datasets and do not include mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of human speech data and do not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in the proposed datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the datasets are explicitly stated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human languages (English, Mandarin, Cantonese)."
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7264,
      "completion_tokens": 219,
      "total_tokens": 7483
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part in the paper mentions code availability for dataset construction or processing.",
          "reasoning": "The paper does not mention or provide any link or information regarding the availability of code repositories related to data collection, preprocessing, or dataset construction. The experimental section describes datasets used but does not refer to releasing code for dataset creation or processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 and 4.2 (Experimental Setup and Main experiment)",
          "reasoning": "Although the paper does not introduce a completely new dataset, it uses an in-house 2.6k-hour Chinese dataset and a 10k-hour unlabeled Mandarin-Cantonese dataset described with details such as data composition and labeling ratio. The paper provides some information about the dataset labels, language ratio, and evaluation setup, which indicates some documentation about the dataset creation and usage process. However, the details are limited and do not constitute a complete dataset construction description, but the available details are enough to mark documentation as present."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7251,
      "completion_tokens": 137,
      "total_tokens": 7388
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 and 3.1.2 (Data collection and Data annotation)",
          "Reasoning": "The authors collected a novel Mandarin Cued Speech corpus consisting of videos recorded by five volunteers using their personal mobile phone front-facing cameras. These videos are human-recorded data capturing lip and hand movements in Mandarin CS, annotated manually with ELAN and Praat tools. The data collection and annotation processes are explicitly described, confirming the videos are human-generated and captured through human involvement as part of this new dataset introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "gao23c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8103,
      "completion_tokens": 232,
      "total_tokens": 8335
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1.2",
            "reasoning": "The paper states that the annotations were all manually done and double-checked, with hand annotations completed using ELAN and acoustic annotations using Praat. There is no mention that annotators are experts, only that volunteers recorded videos and annotations were manually done, implying multiple human non-experts likely performed annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1.2",
            "reasoning": "The paper does not provide any details or references to specific annotation instructions given to annotators. Thus, there is no evidence of detailed instructions provided in the annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1.2",
            "reasoning": "No mention of scoring rubrics or specific evaluation criteria for annotation quality is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1.2",
            "reasoning": "The paper does not discuss or show any annotation examples or sample annotations for guiding annotators."
          }
        }
      ]
    }
  },
  {
    "id": "gao23c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9233,
      "completion_tokens": 351,
      "total_tokens": 9584
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.2 Data annotation",
          "reasoning": "The annotations were all manually done and double-checked, indicating that multiple human annotators performed the quality assurance. Although the paper does not explicitly mention their level of expertise, since the annotation involved phoneme temporal segments for lip and hand movements related to Cued Speech, it is reasonable to consider the annotators as experts or belonging to the target demographic, given the specialized nature of the annotation task."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that an AI model was used as a judge in the quality assurance process for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of automated verification or algorithmic validation of annotation quality was found in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described involving manual annotation and double-checking, indicating that QA was performed."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8851,
      "completion_tokens": 369,
      "total_tokens": 9220
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The paper states that the authors collected a new multi-cuer Mandarin cued speech corpus consisting of videos recorded by five volunteers (three normal hearing and two hearing-impaired) who learned Mandarin CS and recorded themselves. The videos were manually annotated as well. This data is original content created entirely from scratch by human contributors and not derived from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any new data was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention producing data through human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating data by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was collected newly from participants rather than collated from existing datasets or sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is newly created and not derived by modifying or transforming existing corpora."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as collected directly from newly recorded videos by the authors."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9369,
      "completion_tokens": 459,
      "total_tokens": 9828
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Vowel recognition",
          "reasoning": "The Mandarin CS corpus was used to train a Slowfast video classification network for multi-modal vowel recognition. The network was initialized randomly and trained from scratch using the collected corpus data (Section 4.2)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the dataset for supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning based post-training methods involving the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Results and Discussion",
          "reasoning": "The dataset was used to evaluate the performance of hand preceding time (HPT) prediction models via metrics such as MSE, MHCD, and vowel recognition accuracy, thus serving an evaluation purpose."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Hand preceding analysis and Section 5.2 Comparison between normal and hearing-impaired",
          "reasoning": "The dataset was used primarily to analyze lip-hand asynchrony phenomena, differences between normal and hearing-impaired cuers, and statistical characteristics of hand preceding time."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset being used as a knowledge base for augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is demonstrably used for training, evaluation, and analysis purposes."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10092,
      "completion_tokens": 526,
      "total_tokens": 10618
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a Mandarin Cued Speech (CS) corpus containing Mandarin language data only; no other human languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset contains only Mandarin CS data and does not involve exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced does not contain any English language content; it is focused on Mandarin Chinese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Data collection",
          "reasoning": "The newly developed corpus is a Mandarin Cued Speech corpus, as explicitly stated in Section 3.1.1. The dataset contains Mandarin Chinese videos only, with no mention of any other human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of annotated video data with lip and hand gesture temporal segments; it does not contain programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation for modeling and analysis, the dataset itself does not contain entries with mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human Cued Speech video data and annotations, not biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mandarin CS encodes Mandarin Chinese phonemes; it is not a constructed or fictional language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as Mandarin Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data (Mandarin Chinese) and thus does not fall under N/A."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7310,
      "completion_tokens": 175,
      "total_tokens": 7485
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any URL, link, or mention of a public repository for code related to dataset construction, data collection, or annotation procedures. The methodology and experimental setup describe data collection, annotation tools, and preprocessing, but no code availability is stated."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Mandarin CS Corpus) and Section 3.1.1 and 3.1.2",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including number and profile of cuers, recording conditions, corpus size, annotation methodology, annotation tools used (ELAN and Praat), and manual checking procedures. This documentation sufficiently describes the dataset creation process for understanding and potential reproduction."
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9922,
      "completion_tokens": 202,
      "total_tokens": 10124
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Source; Abstract; Section 4.1 Experiment Setup; Table 1",
          "Reasoning": "The dataset, LibriCrowd, contains 100 hours of English speech audio recordings collected from the LibriVox project, which is a public resource of audio books read by human volunteers. These audio recordings are human speech recorded by humans reading books."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Source; Section 4.1 Experiment Setup; Table 1; Abstract",
          "Reasoning": "The associated transcriptions in LibriCrowd are crowdsourced human transcriptions collected via Amazon Mechanical Turk workers transcribing the audio recordings. The text data is manually transcribed by 4433 human transcribers, as explicitly stated in the paper."
        }
      ]
    }
  },
  {
    "id": "gao23f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10774,
      "completion_tokens": 234,
      "total_tokens": 11008
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 Data Collection",
            "reasoning": "The transcriptions were collected via crowdsourcing from Amazon MTurk where annotators are required to have an approval rate >= 95%, but no specialized training or entrance exam. This qualifies them as multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Transcription Task Design",
            "reasoning": "The paper explicitly mentions that instructions are provided to annotators, such as ignoring punctuation marks, transcribing digits as words, and allowing question marks where uncertain, indicating detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 Transcription Task Design",
            "reasoning": "No explicit mention or description of scoring rubrics or numeric scoring guidelines being provided to annotators is made in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 Transcription Task Design",
            "reasoning": "There is no indication in the paper that example transcriptions or annotated examples were provided as part of the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "gao23f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11904,
      "completion_tokens": 511,
      "total_tokens": 12415
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was conducted by a single human annotator who is an expert. In fact, multiple annotators and automated models are involved in QA processes."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that multiple human experts performed quality assurance. The human annotators are crowdsourced workers from MTurk without specified expert status."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves multiple annotators per utterance, not a single annotator; thus, this category does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2: Worker Selection, Number of Workers per Utterance; Section 4.1: Experiment Setup",
          "reasoning": "The quality assurance involves multiple human annotators (five different workers per utterance) recruited via Amazon Mechanical Turk without requirement of expert training or certification. These are non-expert annotators performing transcription. The paper explicitly states that workers need only meet a general approval rate threshold but no expert qualifications."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1: Confidence Estimation; Section 3.2: Error Correction",
          "reasoning": "AI models are used as quality assurance judges. Specifically, the Confidence Estimation Model (CEM), a fine-tuned ELECTRA model, detects word-level errors and assigns confidence scores to transcriptions. Additionally, the Error Correction Model (ECM) automatically corrects detected errors using voting mechanisms enhanced by confidence scores, thus streamlining the quality assurance process."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1: Confidence Estimation (gradient boosting tree model); Section 3.2: Error Correction (voting and alignment algorithms)",
          "reasoning": "Quality assurance incorporates automated algorithmic processes, such as the gradient boosting trees (LightGBM) model used for utterance-level error detection based on multiple meta features, as well as algorithmic alignment methods (e.g., Needleman\u2013Wunsch) for text-text alignment during error correction. These constitute automated verification procedures enhancing annotation quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple quality assurance processes combining human non-expert annotators, AI models, and automated algorithmic methods. Therefore, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11522,
      "completion_tokens": 500,
      "total_tokens": 12022
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2. Data Collection; Section 4.1 Experiment Setup",
          "reasoning": "The paper describes the collection of the LibriCrowd dataset consisting of 100 hours of English speech transcribed by 4433 human transcribers via crowdsourcing on Amazon MTurk. The transcriptions are original human-created content, produced entirely from scratch, not translated or adapted from pre-existing material. This is explicitly stated in Section 2.1 where the authors mention starting from raw audios and collecting human transcriptions for building the speech corpus."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated entirely by AI or machine learning models without human transcription. Models are only used for quality estimation and error correction of human transcriptions, not to generate original transcriptions."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is English speech transcriptions collected by humans; there is no mention of the data being produced via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Source",
          "reasoning": "The audio files are collected from the LibriVox project, an existing source of English speech audio books. The authors collected and aggregated audio from this existing source without significant modification, then collected new human transcriptions for them. Thus, the base audio data is collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes processing such as confidence estimation and error correction to improve transcription quality, the dataset itself is not described as derived or adapted from existing transcription datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12040,
      "completion_tokens": 340,
      "total_tokens": 12380
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The LibriCrowd dataset is used to fine-tune pre-trained ASR models such as Wav2Vec2 and WavLM using supervised learning methods. The paper presents experiments in Section 4.3 demonstrating that using LibriCrowd with improved transcription quality leads to a significant reduction in word error rate (WER) of these ASR models, indicating its utility in supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "LibriCrowd is used for evaluating the quality of human transcriptions by measuring Transcription Word Error Rate (TWER) against ground truth, as well as for evaluating the impact of transcription error on ASR performance. The dataset serves to benchmark transcription quality and ASR model performance under different transcription noise conditions."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The dataset is employed to analyze the correlation between transcription errors and ASR model performance, exploring trends and characteristics of transcription quality impact on downstream tasks."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12763,
      "completion_tokens": 509,
      "total_tokens": 13272
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset LibriCrowd contains only English speech transcriptions; there is no mention of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are exclusively English speech transcriptions; there is no indication of exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment Setup; Table 1 Dataset Statistics",
          "reasoning": "The LibriCrowd dataset consists of 100 hours of English speech transcribed by human annotators. The data source is LibriVox audiobooks with English text as ground truth, which confirms the dataset is purely monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any non-English language transcriptions; it is exclusively English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech transcriptions and does not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are human speech transcriptions and do not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is explicitly specified as English, based on source and annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains linguistic content (English speech transcriptions)."
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9981,
      "completion_tokens": 149,
      "total_tokens": 10130
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3",
          "reasoning": "The abstract states \"We release the dataset and code to benefit the research community.\" The paper also describes ML-in-the-loop modules (CEM and ECM) and quality improvement code. Thus, code related to data collection and processing is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "Sections 2 (Data Collection) and 3 (Quality Improvement) provide detailed documentation on dataset creation, including transcription task design, worker selection, confidence estimation model, error correction model, and quality metrics. The paper thoroughly documents all aspects of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7044,
      "completion_tokens": 153,
      "total_tokens": 7197
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, especially 2.1 Statistics and 2.2 Collection",
          "Reasoning": "The Mandarin Dysarthria Speech Corpus (MDSC) is introduced as a new dataset consisting of 17 hours of audio recordings from 21 dysarthric and 25 control speakers. Participants read wake-up words and non-wake-up words recorded in quiet indoor environments at 16 kHz sampling rate. This data was collected through direct human involvement, i.e., recordings of actual human speech from dysarthric and non-dysarthric individuals, clearly stated in Sections 2.1 and 2.2."
        }
      ]
    }
  },
  {
    "id": "gao24c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7896,
      "completion_tokens": 166,
      "total_tokens": 8062
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.3",
            "reasoning": "Section 2.3 states that subjective intelligibility evaluation was performed by five expert annotators who transcribed the recordings and calculated annotation accuracy."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided to the five expert annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No explicit mention of scoring rubrics or criteria beyond calculating annotation accuracy (percentage of accurately transcribed words)."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide or reference annotation examples for transcription."
          }
        }
      ]
    }
  },
  {
    "id": "gao24c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9026,
      "completion_tokens": 458,
      "total_tokens": 9484
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.3 Intelligibility Evaluation",
          "reasoning": "The paper states that five expert annotators transcribed the recordings and compared them to the standard text to calculate annotation accuracy (subjective intelligibility). The annotators are described as experts, indicating they have subject matter expertise. This multi-expert annotation supports quality assurance by multiple human experts rather than a single expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Intelligibility Evaluation",
          "reasoning": "Five expert annotators were employed to transcribe and evaluate the recordings against the standard text. The use of multiple experts explicitly supports this QA category."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any single non-expert annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All the annotators mentioned are described as experts; no indication is given of non-expert annotators involved in QA."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.3 Intelligibility Evaluation",
          "reasoning": "The paper uses open-source ASR models (Paraformer and WENET) to transcribe dysarthric speech and calculate word error rates as an objective intelligibility measure, indicating the use of AI models as a judge in the QA process."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification through algorithmic or rule-based processes independent of human or AI model annotation is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a thorough quality assurance process involving multiple expert human annotators and AI models, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8644,
      "completion_tokens": 397,
      "total_tokens": 9041
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (MDSC), particularly 2.2 Collection",
          "reasoning": "The Mandarin Dysarthria Speech Corpus (MDSC) was collected directly from 21 dysarthric and 25 non-dysarthric human speakers using original recording sessions conducted in a quiet indoor environment with specific recording protocols. The data consists of utterances spoken by these human participants and is not described as translated, adapted from existing datasets, or generated by models. Hence, the data is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models. All datasets are from human recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that machine translation was used for the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources; it was newly collected."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as modified or transformed from existing data; it is newly recorded speech."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly documented."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9162,
      "completion_tokens": 581,
      "total_tokens": 9743
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe training models from randomly initialized parameters using the Mandarin Dysarthria Speech Corpus (MDSC). Instead, it uses pre-trained models fine-tuned later."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The MDSC is used to fine-tune pre-trained Wake-up Word Spotting (WWS) models. Section 3.1 describes fine-tuning the speaker-independent control model (SIC) with the dysarthria training set (D-train) to form the SID model. Section 3.2 describes further fine-tuning the SID model with personalized enrollment utterances from individuals to form speaker-dependent dysarthria models (SDD). This demonstrates supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning post-training methods or RLHF in relation to using the MDSC."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The MDSC is used for extensive evaluation and benchmarking of different wake-up word spotting models (SIC, SID, SDD) on dysarthric speech samples. Section 4.2 discusses testing and performance results on the MDSC test sets to assess model robustness and challenges."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and Section 5 (Conclusion)",
          "reasoning": "The paper conducts comprehensive analysis of trends and characteristics of dysarthric speech such as significant intra-domain variance and limited data volume. It also analyzes intelligibility scores correlation to system performance, highlighting the dataset's utility in understanding speech disorder characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for retrieval-augmented generation or similar tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The MDSC dataset is clearly used for training (fine-tuning), evaluation, and analysis as described in multiple sections. Hence, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9885,
      "completion_tokens": 538,
      "total_tokens": 10423
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The dataset, MDSC, is specifically described as a Mandarin dysarthria speech corpus, with all participants being native Mandarin speakers and no mention of any additional languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The paper only specifies one human language, Mandarin; there is no indication of two languages present in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The dataset is explicitly described as Mandarin and not English; no English-only data is indicated."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset MDSC is described as a Mandarin Dysarthria Speech Corpus, with all speakers being native Mandarin speakers and the wake-up words and commands recorded are in Mandarin."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations; such notation appears only in analysis and equations outside the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech recordings only; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains native Mandarin speech only; no fictional or artificially created languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly specified as Mandarin; it is not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists entirely of spoken language data; it does contain language."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7103,
      "completion_tokens": 175,
      "total_tokens": 7278
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section explicitly provides code availability for dataset construction.",
          "reasoning": "The paper does not mention any code repository or link to code related to data collection, preprocessing, or dataset generation. The corpus is stated to be released at a website for data download, but no associated code for reproducing the dataset is indicated."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (MDSC), subsections 2.1, 2.2, and 2.3.",
          "reasoning": "The paper provides detailed documentation of dataset statistics including number of speakers, demographics, recording conditions, utterance types, and intelligibility evaluation methodology. The data collection process is described (e.g., recording environment, microphone distance, text transcription using a mobile application), providing transparency on dataset creation."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8052,
      "completion_tokens": 172,
      "total_tokens": 8224
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The new lexical stress corpus was compiled using human recorded speech audio from the Bulgarian Parliament plenary sessions (2010-2018) and professional audiobook recordings. These recordings were made by human speakers and professionally recorded, thus the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Section 3.4",
          "Reasoning": "The corpus includes human-generated manual transcriptions aligned to the speech audio from the Parliament sessions and audiobooks, as well as standard Bulgarian dictionary entries added as separate utterances. These texts originate from human created manual transcriptions and curated dictionaries."
        }
      ]
    }
  },
  {
    "id": "geneva23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8904,
      "completion_tokens": 199,
      "total_tokens": 9103
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 (Lexical stress corpus)",
            "reasoning": "The large lexically diverse text corpus annotated with lexical stress marks was automatically compiled using an ASR system with a specialized acoustic model that detects stressed and unstressed vowels automatically. This automatic annotation process explicitly relies on decoding with weighted paths for stress assignment, as described in Section 3.3."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No explicit mention or description of detailed annotation instructions for the automatic process is given in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not describe any scoring rubrics related to the automatic annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No annotation examples or illustrative examples are provided for the automatic annotation process in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "geneva23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10034,
      "completion_tokens": 461,
      "total_tokens": 10495
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance of the dataset annotations was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments), Conclusions",
          "reasoning": "The evaluation of the synthesized speech's lexical stress correctness was performed manually by a group of 5 native Bulgarian speakers, supervised by a professional linguist. This implies that multiple human experts (subject matter experts or target demographic members) were involved in quality assurance of the annotations indirectly through manual checking of stress placements in synthesized speech."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention in the paper of quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert annotators were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ASR model is used to automatically annotate the text corpus, but it is not described as a quality assurance judge for verifying the accuracy of annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 (Lexical stress corpus), Section 3.3 (Automatic annotation)",
          "reasoning": "The large lexical stress corpus was automatically compiled by employing a specialized ASR acoustic model that decodes aligned audio-transcription pairs to assign lexical stress labels. This automatic annotation process constitutes an automatic quality assurance step over the aligned data, ensuring annotation accuracy at scale without manual intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described both via automatic annotation by ASR and manual evaluation by multiple human experts; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9652,
      "completion_tokens": 460,
      "total_tokens": 10112
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was created entirely from scratch by human contributors without reference to existing data. Instead, the data is compiled and annotated using a mixture of existing audio and texts."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated entirely by AI or machine learning models without using any existing data. Instead, machine learning models are used to annotate existing aligned data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The lexical stress corpus was automatically compiled by collecting large amounts of data from existing sources: recordings and manual transcriptions from the Bulgarian Parliament plenary sessions (2010-2018) and professional audiobooks paired with texts from the Chitanka library. This constitutes collating data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The collected aligned data is automatically annotated with lexical stress marks by leveraging an ASR system trained on a phonetic lexicon with stress information. Moreover, the corpus is extended by adding dictionary word types with canonical stress patterns. This indicates modifications and transformations applied to the existing collected data, categorizing the resulting corpus as derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are well described in the paper, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10170,
      "completion_tokens": 437,
      "total_tokens": 10607
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the lexical stress corpus being used to train a dedicated Accentor model, not for unsupervised or self-supervised pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The Accentor model is trained from scratch on the large lexically diverse lexical stress corpus (Section 4) to learn to explicitly model lexical stress placement in text."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being used to fine-tune pre-trained models via supervised methods; models are trained directly on the datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning or RL-based post-training techniques applied to their datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The lexical stress corpus is explicitly used for training the Accentor model, not exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper analyzes lexical stress phenomena and dataset characteristics, the compiled lexical stress corpus is used primarily for model training rather than merely analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation; it is directly used for training models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The lexical stress corpus is actively utilized in model training and thus has practical utility."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10893,
      "completion_tokens": 416,
      "total_tokens": 11309
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1.3, Section 3, Section 6",
          "reasoning": "The lexical stress corpus introduced in this paper is compiled from Bulgarian parliamentary sessions and Bulgarian audiobooks specifically for Bulgarian text-to-speech (TTS) systems. The paper explicitly states the focus on Bulgarian language data (e.g. Section 1.3 states demonstration on Bulgarian TTS, Section 3 details data sources from Bulgarian parliament and Bulgarian audiobooks). Hence, the proposed new dataset is monolingual, containing only Bulgarian language entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language text and corresponding speech data; thus, it is language-relevant."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8111,
      "completion_tokens": 208,
      "total_tokens": 8319
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any explicit link, URL, or mention of publicly available code repositories for the dataset construction, automatic alignment, or annotation code used to compile the lexical stress corpus. There is no indication that code for data processing or stress annotation is shared publicly."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Lexical stress corpus)",
          "reasoning": "Section 3 of the paper provides detailed documentation on the dataset creation process, including data sources (Bulgarian Parliament videos and audiobooks), the automatic alignment method using a hybrid DNN-HMM ASR system, alignment challenges and techniques, and the automatic annotation procedure using an ASR acoustic model with stressed and unstressed vowels phonemes. Further information on extended corpus creation by adding dictionary word types is also given, along with detailed statistics in Table 2. This constitutes clear and relatively complete documentation of dataset construction."
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7698,
      "completion_tokens": 100,
      "total_tokens": 7798
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Gathering",
          "Reasoning": "The new dataset is a corpus consisting of nearly five hours of irony-annotated naturalistic conversational speech collected from comedy podcast recordings provided in WAV format. This data originates from human-generated speech recordings, as it consists of naturalistic conversations from podcast hosts and guests, confirmed by the paper's data gathering description."
        }
      ]
    }
  },
  {
    "id": "gent22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8550,
      "completion_tokens": 221,
      "total_tokens": 8771
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1 Data Gathering",
            "reasoning": "The paper states that a team of annotators was trained to perform irony annotation, with each episode labelled by two annotators, indicating multiple human experts involved in the annotation process."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Data Gathering",
            "reasoning": "Annotators were trained to perform annotation using a discourse-based irony labeling method introduced in [29], which implies detailed instructions were provided as part of the annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 Data Gathering",
            "reasoning": "The paper does not mention specific scoring rubrics or grading criteria in the annotation guidelines for irony labeling\u2014only agreement between annotators was used for inclusion."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 Data Gathering",
            "reasoning": "No explicit mention is made of annotation examples being provided in the guidelines, such as annotated sample utterances or illustrative cases."
          }
        }
      ]
    }
  },
  {
    "id": "gent22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9680,
      "completion_tokens": 362,
      "total_tokens": 10042
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being conducted by a single human annotator with subject matter expertise or being part of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Gathering",
          "reasoning": "The dataset's irony annotation was performed by a team of annotators trained using a discourse-based labeling method. Each episode was labeled by two annotators, and only samples with agreed labels were included, indicating quality assurance by multiple human annotators. Although the paper does not explicitly state their expert status, the annotators were trained for irony annotation and annotation required interpretation of conversational responses, implying expertise or familiarity with the task."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that only a single non-expert annotated or quality assured the data."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators were trained and applied a discourse-based irony annotation method requiring conversational understanding, suggesting they were not simply non-experts. The use of multiple annotators does not mean they were non-experts; rather, the paper suggests some level of expertise or training."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model is described as performing quality assurance or judging the annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic quality assurance of the annotations is described; only human agreement is mentioned."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving multiple trained annotators with agreement requirement is described, so QA was performed and documented."
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9298,
      "completion_tokens": 408,
      "total_tokens": 9706
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Gathering",
          "reasoning": "The paper describes the collection of a new corpus of nearly five hours of irony-annotated, naturalistic conversational speech from comedy podcasts. The recordings were provided directly by the podcast creator, and human annotators labeled the data for irony based on conversational context. This corpus is original, created from scratch by human contributors, and is not derived from existing datasets. This is explicitly stated in the description of the data gathering process."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated artificially by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that machine translation was used to generate any part of the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although data was sourced from podcasts, the paper treats this as original data collection rather than collating previously published datasets without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not based on existing datasets with modifications or transformations; rather, it was newly collected and annotated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method is clearly documented."
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9816,
      "completion_tokens": 305,
      "total_tokens": 10121
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.4 Model Architectures and Section 3 Results",
          "reasoning": "The new corpus constructed of naturalistic irony-annotated speech (Section 2.1) is used as training data to develop and train deep learning models from scratch, as detailed in the model architecture description and training procedures in Section 2.4 and reported performance results in Section 3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Results and Section 4 Discussion",
          "reasoning": "The corpus is also used for evaluation of model performance through cross-validation under speaker-dependent and speaker-independent conditions, with metrics including F1, AUC, and accuracy reported."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Discussion",
          "reasoning": "The dataset is analyzed to examine the impact of prior speaker knowledge on irony classification performance, as well as to compare acoustic and textual modalities, which constitutes an analytical usage beyond pure training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10539,
      "completion_tokens": 532,
      "total_tokens": 11071
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a new corpus from the Sad Boyz Podcast featuring naturalistic conversational speech. There is no indication that the dataset includes more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only one language, English, with no mention of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Gathering",
          "reasoning": "The dataset consists of naturalistic conversational speech from English-language comedy podcasts (Sad Boyz Podcast). All the speech and transcriptions used in the corpus are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly English speech; no non-English languages are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Section 2.2 Acoustic Feature Selection and Extraction",
          "reasoning": "While the study uses programming tools and algorithms such as Keras and Python for preprocessing and modeling, the dataset itself contains speech data, not programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations; it only includes natural spoken language data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists entirely of human speech from podcasts; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of all entries in the dataset is clearly documented and explicitly English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of naturalistic speech data in English, so it clearly contains human language."
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7757,
      "completion_tokens": 128,
      "total_tokens": 7885
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.4 Model Architectures",
          "reasoning": "The paper states that model implementation, preprocessing, training, and evaluation code can be found at https://github.com/helengent/Irony-Recognition, indicating code accessibility including preprocessing steps related to the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Data Gathering",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including data source selection, annotation methodology, annotator agreement, utterance balancing, and statistical properties of the resulting corpus."
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8065,
      "completion_tokens": 110,
      "total_tokens": 8175
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Dataset",
          "Reasoning": "The paper introduces a novel dataset comprising speech recordings from 20 patients (10 women and 10 men) recorded in a clinical setting. The speech data are human speech recordings collected using a Zoom Q8 recorder during the patients reading neutral texts and other speaking tasks. The data are clearly stated as recorded human speech samples, hence 'audio' modality with human-generated origin."
        }
      ]
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8917,
      "completion_tokens": 222,
      "total_tokens": 9139
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2",
            "reasoning": "The study doctor assessed the near-term suicide risk of each participant on a Likert scale. Only one expert annotator is explicitly mentioned for the suicide risk labeling."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "The assessment was made using a Likert scale of [1\u20136] with clarified category definitions (e.g., 1\u20132: no risk, 3\u20134: suicidal without intention to act, 5\u20136: high suicide risk with intention to act), indicating the use of instructions for consistent annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "The suicide risk was assessed on a standard Likert scale with specified risk categories serving as a rubric guiding the scoring process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no explicit mention of annotation examples being provided in the paper for the suicide risk assessment."
          }
        }
      ]
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10047,
      "completion_tokens": 169,
      "total_tokens": 10216
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The suicide risk labels were assigned by the study doctor, who assessed the near-term suicide risk of each participant on a Likert scale. This indicates quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9665,
      "completion_tokens": 381,
      "total_tokens": 10046
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset consists of speech recordings collected from 20 patients diagnosed with behavioral or mental disorders, recorded during emergency admission to a psychiatric department. The speech samples were recorded by human subjects reading neutral texts and spontaneous speech, thus the data represents original recordings created from scratch by human contributors for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not consist of data generated entirely by AI or machine learning models. Although features are extracted using models, the raw dataset consists of human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced by translation from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation applied to the data is found in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated or collected from existing sources; it was collected by the authors specifically for this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is original human speech recordings and not derived or adapted from existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly specified as recordings from human patients during emergency psychiatric admissions."
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10183,
      "completion_tokens": 315,
      "total_tokens": 10498
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4 \"Suicide Risk Classification\" and Section 4.1 \"Classification Results\"",
          "reasoning": "The paper describes training classifiers (SVMs) on the novel speech dataset to classify high vs. low suicide risk using supervised learning methods, thus fine-tuning or training models leveraging this data for the specific task."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 \"Classification Results\"",
          "reasoning": "The dataset is used to evaluate classification performance via leave-one-speaker-out cross-validation for suicide risk assessment, measuring balanced accuracy, indicating its use for evaluation and benchmarking the models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 \"Experiments and Results\" and Section 5 \"Conclusions and Future Work\"",
          "reasoning": "The authors analyze gender-specific speech patterns and acoustic feature correlations with suicide risk using the dataset, providing insights into paralinguistic markers, thus the dataset is employed for analytic purposes beyond pure classification."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10906,
      "completion_tokens": 362,
      "total_tokens": 11268
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset consists of speech recordings of 20 patients reading three neutral German short stories, explicitly indicated as German texts: \u201cDer Nordwind und die Sonne\u201d, \u201cGleich am Walde\u201d, and \u201cDer Hund und das St\u00fcck Fleisch\u201d. No other languages are mentioned, and the speech content is monolingual German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8124,
      "completion_tokens": 153,
      "total_tokens": 8277
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No direct references to code availability are found in the paper.",
          "reasoning": "The paper does not provide any links, URLs, or mentions of publicly available code repositories for dataset construction, data collection, preprocessing, or generation. Therefore, the code associated with the dataset is not publicly available according to the paper content."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2. Dataset",
          "reasoning": "The paper documents the dataset creation process, including participant demographics, clinical assessments, recording devices, speech tasks, and ethical approval details in Section 2. This provides transparency about how the dataset was collected and the characteristics of the subjects, indicating sufficient documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7824,
      "completion_tokens": 115,
      "total_tokens": 7939
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data Collection",
          "Reasoning": "The paper reports the collection of a new clinical pediatric audio dataset for asthma detection. The audio data were recorded using far-field non-contact microphones in a physician's office under real conditions, with each participant performing specific tasks. The recordings were manually annotated and verified by human annotators and a pediatric specialist for cough and wheezing presence, confirming human generation and involvement in the data creation process."
        }
      ]
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8676,
      "completion_tokens": 195,
      "total_tokens": 8871
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1 Data Collection",
            "reasoning": "The data labeling was performed by two human annotators for segmentation and labeling of each task, followed by quality verification by an independent annotator. Additionally, a pediatric specialist labeled the cough samples for presence of wheezing."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1 Data Collection",
            "reasoning": "The paper does not mention any provided annotation instructions or detailed guidelines given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 Data Collection",
            "reasoning": "No description or mention of rubric or formalized scoring systems for annotation is provided in the text."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1 Data Collection",
            "reasoning": "The paper does not provide annotation examples or mention inclusion of examples in annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9806,
      "completion_tokens": 472,
      "total_tokens": 10278
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection",
          "reasoning": "The paper states that the audio recordings were segmented and labeled for each task by two human annotators, and the quality of the segments was then verified by an independent annotator. Furthermore, a pediatric specialist specifically labeled the cough samples for the presence of wheezing. These annotators had expert domain knowledge, indicating quality assurance was conducted by multiple human experts. However, for wheezing presence labeling, it was done by a single pediatric specialist, showing a single human expert quality assurance for that annotation."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection",
          "reasoning": "The initial segmentation and labeling of the audio recordings for each task were performed by two human annotators, followed by verification from an independent annotator. These annotators are implied to be domain experts or qualified for the task, indicating multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single non-expert annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses AI models (PANNs, DevNet) for automatic segmentation and anomaly detection, these are part of the data processing pipeline, not quality assurance of annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process for quality assurance of annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a documented quality assurance process involving human expert annotators and validation steps."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9424,
      "completion_tokens": 484,
      "total_tokens": 9908
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection",
          "reasoning": "The paper states that the authors collected clinical pediatric audio data for identifying pediatric asthma in collaboration with healthcare institutions. The data was recorded using a non-contact microphone in physician offices from 27 participants (healthy and asthma diagnosed children). The recordings were manually segmented and labeled by human annotators and a pediatric specialist labeled the cough samples for the presence of wheezing. This indicates the data was created entirely from scratch by human contributions in a clinical setting."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new audio data was generated entirely by AI or machine learning models. Although they use pre-trained models for segmentation and active learning for annotation, the core audio data itself is recorded from human subjects, not synthesized by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset involves machine translation from another language, as the data is audio recordings of respiratory sounds collected directly."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as aggregated from existing datasets without modification. Although the paper uses pre-existing datasets like AudioSet or COUGHVID for evaluation or background synthesis, the main dataset was newly collected."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived or transformed from existing sources. The authors collected new original recordings and labeled them, rather than adapting or modifying existing datasets for this asthma detection task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is specified and documented clearly as newly collected clinical audio data recorded from pediatric participants."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9942,
      "completion_tokens": 499,
      "total_tokens": 10441
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly collected dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used to train models from scratch; models used include pre-trained audio neural networks (PANNs) and few-shot anomaly detection methods."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The collected dataset is used to fine-tune models such as DevNet and a Linear Regression model on MFCC features in the active learning pipeline to detect wheezing events using supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of reinforcement learning post-training techniques such as RLHF with the introduced dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 3.3",
          "reasoning": "The dataset is used for evaluation purposes by assessing the performance of automatic segmentation (Section 3.2) and the active learning pipeline's effectiveness in retrieving wheezing cases (Section 3.3)."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 3.3",
          "reasoning": "The paper provides analysis of segmentation precision and recall, and annotation cost in the active learning iterations, to characterize the dataset's strengths and the system's performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset as a knowledge base to augment models, such as through retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has multiple practical uses in training (fine-tuning), evaluation, and analysis as described above."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10665,
      "completion_tokens": 510,
      "total_tokens": 11175
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains data from pediatric patients speaking English only, no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention of exactly two human languages in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection",
          "reasoning": "The paper repeatedly mentions tasks such as reciting ABCs, numbers, vowels and naming colors and foods all in English. The participants are in a clinical setting in the USA and produce spoken English. There is no mention of any other spoken human language being collected."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset entries are clearly in English, no non-English languages described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of clinical audio recordings of human speech and lung sounds, no programming language or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions are present in the dataset; the paper only discusses model architectures and algorithms separately."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio recordings of human lung sounds and speech, but no biological sequences or non-human communication systems like animal signals or DNA."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language is explicitly described and observable as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset does contain human language (English) in the form of speech tasks, thus not applicable."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7883,
      "completion_tokens": 171,
      "total_tokens": 8054
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention found in the paper",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code or code repositories related to the data collection, preprocessing, or dataset construction. There is no indication that the dataset creation or active learning framework code has been made accessible for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data Collection and Section 2 Proposed System",
          "reasoning": "The paper documents the dataset creation process in detail, including the data collection setup, participant demographics, recording equipment and environment, tasks performed, annotation process including human annotators and validation, and the overall pipeline for audio data segmentation and annotation. This constitutes sufficient documentation of the dataset creation process for reproducibility and ethical assessment purposes."
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8983,
      "completion_tokens": 208,
      "total_tokens": 9191
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Table 1",
          "Reasoning": "The DeToxy dataset introduced by the authors consists of spoken utterances collected from various openly available speech databases (e.g., CMU-MOSEI, Common Voice, IEMOCAP). These are human speech recordings manually annotated for toxicity, thus the audio modality is human generated and explicitly stated as sourced from human-recorded speech."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Table 1",
          "Reasoning": "DeToxy includes textual transcripts associated with the spoken utterances, either originally available or obtained via ASR (wav2vec-2.0) and subsequently manually annotated for toxicity by three professional annotators. The annotations and transcripts are human generated or verified, so the text modality is human generated and explicitly described in the paper."
        }
      ]
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9835,
      "completion_tokens": 198,
      "total_tokens": 10033
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The paper states that all utterances filtered through the first step were manually annotated by 3 professional annotators, indicating multiple human experts conducted the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention providing detailed annotation instructions or guidelines to the annotators beyond the definition of toxicity, so no clear evidence of instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no indication in the paper that formal scoring rubrics were provided or used during annotation; it only mentions majority voting and inter-annotator agreement."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not report providing any annotation examples or sample annotations to guide annotators."
          }
        }
      ]
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10965,
      "completion_tokens": 336,
      "total_tokens": 11301
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies usage of multiple annotators for the dataset annotation, thus not a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset Collection and Annotation",
          "reasoning": "DeToxy dataset annotations were performed by three professional annotators, indicating multiple human experts annotated and quality assured the dataset. They used majority voting to determine final toxicity labels, and the inter-annotator agreement measured by Cohen's Kappa Score is 0.76, which reflects rigorous and reliable annotation quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as professional annotators, implying subject matter expertise rather than non-expert status."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotators described were professionals, which suggests expert status rather than non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an AI model (BERT classifier) was used initially for filtering data, it was not used as a quality assurance judge for final annotations; rather, human annotators performed QA."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of automated verification techniques or algorithmic/rule-based QA was described for final annotation verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a quality assurance process involving multiple professional annotators with majority voting and inter-annotator agreement statistics; thus, QA is documented and applied."
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10583,
      "completion_tokens": 505,
      "total_tokens": 11088
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset DeToxy is constructed by aggregating utterances from various openly available speech databases and manually annotating them for toxicity. However, the speech utterances themselves are not created from scratch by human contributors; they are sourced from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any utterances or data samples are generated entirely by AI or machine learning models as original content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is restricted to English spoken utterances and the paper does not mention producing data by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data creation via machine translation from another language in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3: DeToxy and DeToxy-B, especially Section 3.1 Dataset Collection and Annotation",
          "reasoning": "DeToxy is explicitly described as a subset collated from various existing openly available speech databases. The core audio data was collected from multiple public datasets such as Common Voice, Switchboard, CMU-MOSEI, and others, aggregating them into a large unified dataset without generating new audio recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset Collection and Annotation",
          "reasoning": "The authors applied manual annotations to the collected utterances to label toxicity, using a 2-step approach involving a BERT-based toxicity classifier for pre-selection followed by professional human annotations. Additionally, for datasets lacking transcripts, they used ASR models to generate transcripts. These modifications and additions are transformations applied to existing source data, thus the data can be considered derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origins and methods of generating the dataset, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11101,
      "completion_tokens": 301,
      "total_tokens": 11402
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 and 6",
          "reasoning": "The DeToxy-B dataset is used to fine-tune pre-trained models such as BERT BASE and wav2vec-2.0 for toxicity classification, as described in Section 4 with baselines provided, and experimental results in Section 6 demonstrate model fine-tuning and evaluation on different splits of DeToxy-B."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6",
          "reasoning": "The dataset is used for performance measurement and benchmarking of toxicity classification models and baselines, with explicit test splits (Test and Test-T) used for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4, 6",
          "reasoning": "The dataset is used to analyze the advantages and disadvantages of different approaches (2-step vs End-to-End) for speech toxicity classification, including trends in model performance on trigger terms and dependency on gold transcripts."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11824,
      "completion_tokens": 606,
      "total_tokens": 12430
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 Introduction and Section 3.1 Dataset Collection and Annotation",
          "reasoning": "The dataset DeToxy is described as the first publicly available toxicity annotated dataset for the English language only. There is explicit mention that it contains English spoken utterances, with no indication of inclusion of other human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 Introduction and Section 3.1 Dataset Collection and Annotation",
          "reasoning": "The dataset is described exclusively as English language data. There is no mention or indication of any second human language being included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3 Introduction",
          "reasoning": "The dataset DeToxy is explicitly described as a toxicity annotated dataset for the English language. The source datasets, annotation process, and baselines all focus on English spoken utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset is not stated to include any single non-English language; it focuses purely on English data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset comprises spoken utterances with text transcripts focused on toxicity classification; there is no mention of code or programming language contents."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "There is no indication that the dataset contains entries involving mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset involves human spoken utterances only and does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No mention of fictional or constructed languages in the dataset; it is composed entirely of natural English speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.1 Dataset Collection and Annotation",
          "reasoning": "The language of the dataset entries is explicitly specified as English; therefore, language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken and transcribed utterances in the English language, thus it contains language and is not applicable to this category."
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9042,
      "completion_tokens": 137,
      "total_tokens": 9179
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 5 Experimental Setup",
          "reasoning": "The paper explicitly states: 'We make all our code and data publicly available on GitHub.' This indicates that the code related to dataset construction, preprocessing, and generation is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 DeToxy and DeToxy-B",
          "reasoning": "Section 3 provides detailed information on dataset collection, annotation process, filtering strategy using a textual toxicity classifier, manual annotation procedure with inter-annotator agreement, data sources, and dataset statistics. This comprehensive description documents the dataset creation process clearly."
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7252,
      "completion_tokens": 118,
      "total_tokens": 7370
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2.1 Data Collection",
          "Reasoning": "The BhavVani dataset is described as a novel Hindi Speech Emotion Recognition dataset consisting of approximately 13 hours of audio from 8,734 utterances curated from the popular Indian sitcom. The dataset was created and annotated by human annotators noted in the acknowledgments, indicating human involvement in recording and annotation. Therefore, the modality is audio, and the data origin is human generated."
        }
      ]
    }
  },
  {
    "id": "goel24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8104,
      "completion_tokens": 220,
      "total_tokens": 8324
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 6 Acknowledgments",
            "reasoning": "The paper states in the Acknowledgments (Section 7) that 17 individuals from IIIT Delhi contributed to annotating the BhavVani dataset; no mention suggests these are subject-matter experts, implying they are multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide any explicit description or reference to detailed annotation instructions given to annotators for the BhavVani dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "There is no explicit mention or detail about scoring rubrics or a rubric-based guideline used during annotation in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not mention or provide annotation examples or exemplar annotations for the BhavVani dataset."
          }
        }
      ]
    }
  },
  {
    "id": "goel24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9234,
      "completion_tokens": 354,
      "total_tokens": 9588
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance for the new BhavVani dataset was conducted by a single human expert annotator or a subject matter expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2.1 Data Collection and Acknowledgments",
          "reasoning": "The paper states that the BhavVani dataset was annotated by multiple individuals from IIIT Delhi (Adamya, Aditya, Akshat, etc.), implying multiple human annotators were involved. Given these annotators are from an academic institution and the explicit mention acknowledging their role in annotation, it is reasonable to infer they are skilled annotators and members of the target demographic, acting as human experts for this task."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates that QA was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators mentioned are multiple, but they are affiliated with IIIT Delhi and acknowledged, indicating expertise or appropriate skills; no indication they were non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate use of AI models for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated algorithms or rule-based techniques used for verifying the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly details the annotation process involving multiple annotators, so quality assurance was conducted and documented."
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8852,
      "completion_tokens": 429,
      "total_tokens": 9281
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.2.1",
          "reasoning": "The paper introduces BhavVani, a novel Hindi Speech Emotion Recognition dataset, which comprises 8,734 utterances from audio clips curated from a popular Indian sitcom. It was annotated by human contributors (acknowledged in Section 7), indicating the data was created/collected and annotated by humans from scratch. This original dataset is not described as being translated or derived from existing datasets, but collected and annotated specifically for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was generated entirely by AI or models without reference to pre-existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset obtained through human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data used or introduced."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper benchmarks five existing datasets, it does not indicate any new dataset created by collecting or aggregating existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new Hindi dataset BhavVani is described as a new collected dataset, not derived or adapted from existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the novel dataset BhavVani is clearly documented as collected and annotated by humans, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9370,
      "completion_tokens": 414,
      "total_tokens": 9784
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.2, 4, and 5",
          "reasoning": "The BhavVani dataset is used to fine-tune pre-trained models using supervised learning methods for speech emotion recognition, as demonstrated in the benchmarking experiments (Table 1) where models including CAMuLeNet are trained and evaluated on BhavVani for unseen speaker emotion recognition."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using reinforcement learning or RL-based post-training techniques with the BhavVani dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 and Table 1",
          "reasoning": "The BhavVani dataset is also used for evaluation and benchmarking purposes to measure performance of SER models on unseen speakers, as part of the comprehensive benchmarking alongside other datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the BhavVani dataset is primarily used for analysis of trends or characteristics independent of training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10093,
      "completion_tokens": 585,
      "total_tokens": 10678
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The novel dataset introduced by the authors, BhavVani, contains Hindi utterances only and does not include more than two languages. Other datasets mentioned are existing and not novel."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "BhavVani dataset is described as Hindi-only with no indication of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The introduced BhavVani dataset is specifically a Hindi language dataset and does not contain English-only utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Our Novel Hindi SER Dataset: BhavVani)",
          "reasoning": "The authors introduce the BhavVani dataset solely in the Hindi language with approximately 8,734 utterances. It is explicitly stated that it is a Hindi SER dataset designed to enrich research in Indian languages, thus strictly monolingual (Hindi)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No relevant section",
          "reasoning": "The dataset contains speech utterances in Hindi language and no indication of code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No relevant section",
          "reasoning": "No part of the dataset contains mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No relevant section",
          "reasoning": "The dataset relates to human speech emotions and contains utterances only, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No relevant section",
          "reasoning": "BhavVani consists of natural human Hindi language only, no constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The language of the dataset is explicitly stated as Hindi, so the language is known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language entries (Hindi utterances), so it does contain language."
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7311,
      "completion_tokens": 183,
      "total_tokens": 7494
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 'Proposed Methodology' and footnote 1",
          "reasoning": "The paper provides a URL to a GitHub repository (https://github.com/arnav10goel/CAMuLeNet) in Section 2, indicating that code relevant to the methodology, which likely includes data preprocessing and possibly dataset handling, is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 'Our Novel Hindi SER Dataset: BhavVani' and subsections",
          "reasoning": "The paper documents the dataset creation process for the new BhavVani dataset in Section 3.2, describing the data collection (curated from an Indian sitcom), duration, number of utterances, average clip length, and annotation contributors. This provides transparency and some completeness regarding the dataset creation."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7718,
      "completion_tokens": 108,
      "total_tokens": 7826
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data",
          "Reasoning": "The paper introduces a new dataset containing speech recordings from 55 speakers (39 pwMS and 16 HC) collected with a microphone system (AKG 555L headset microphone with audio interface) at clinical sites in Switzerland. Audio recordings are human speech collected through supervised recording sessions involving patients and controls, explicitly described as newly recorded data for this study."
        }
      ]
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8570,
      "completion_tokens": 215,
      "total_tokens": 8785
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 Data",
            "reasoning": "The dataset was collected in a clinical study supervised by a study nurse; participants provided informed consent. Data labeling consists primarily of participant group (pwMS or HC). No indication of crowdsourced or multiple annotators is given, and clinical oversight suggests expert annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1 Data",
            "reasoning": "The paper does not mention any detailed annotation instruction guidelines provided to annotators for labeling or segmenting data; the focus is on data collection, not annotation per se."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 Data",
            "reasoning": "No mention of scoring rubrics or rating guidelines used for annotations is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 Data",
            "reasoning": "No annotation examples or guidelines with examples are described or referenced for the data annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9700,
      "completion_tokens": 330,
      "total_tokens": 10030
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human experts or annotators with subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single non-expert human performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple non-expert human annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any AI model was used to perform quality assurance or validation of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Data processing",
          "reasoning": "The paper describes a process where audio files were downsampled and segmented using an automated voice activity detection (VAD) algorithm, and manual audio quality checks were done before analysis. The VAD segmentation is based on an algorithmic process, indicating an automatic verification step in data processing and quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process was implicitly applied through automated voice activity detection and manual audio quality checks, so it is not correct to say no QA process was performed or documented."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9318,
      "completion_tokens": 448,
      "total_tokens": 9766
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data",
          "reasoning": "The paper describes data collected at the Department of Neurology, Inselspital, Bern, Switzerland and the University of Bern, Switzerland, obtained through a clinical observational study with participants providing consent. The dataset consists of original speech recordings of 39 patients with relapsing MS and 16 healthy controls. The recordings include multiple speech tasks such as free speech and read speech, recorded using a microphone setup. This indicates the data was created entirely from scratch by human contributors (the patients and controls) and is original content, not translated or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated or synthesized by AI or machine learning models. All data originates from human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the data was produced by translation from another language using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate or create the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as aggregated from existing sources; rather, it was newly recorded from human participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While acoustic features were extracted from the raw audio, the dataset itself (raw speech recordings) is not derived or adapted from existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clearly the origin of the data as new human speech recordings collected for the study."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9836,
      "completion_tokens": 491,
      "total_tokens": 10327
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of the new dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Predictive Modelling)",
          "reasoning": "The dataset is used to train machine learning models from scratch, specifically support vector machines (SVM) and k-nearest neighbors (KNN), to classify people with MS and healthy controls based on acoustic features extracted from the speech data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of the dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RLHF techniques applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 (Predictive Modelling)",
          "reasoning": "The dataset is split into training and a fixed hold-out test set to evaluate and report the performance of machine learning classification models, serving as evaluation data for benchmarking model accuracy."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 (Statistical Feature Analysis) and the Discussion section",
          "reasoning": "The dataset is used to analyze and identify statistically significant acoustic features that distinguish people with MS from healthy controls, providing insights into speech characteristics related to MS."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset as a knowledge base for augmenting models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used for training, evaluation, and analytical purposes, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10559,
      "completion_tokens": 487,
      "total_tokens": 11046
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper presents a dataset of speech recordings from German-speaking participants only, with no mention of entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset only contains entries in German; no indication of exactly two human languages is given."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is in German, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data",
          "reasoning": "The dataset consists of speech recordings from a German-speaking cohort only (explicitly stated in the abstract and Section 2.1). Thus, it contains exactly one non-English human language, German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio recordings with no programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are contained in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns human speech recordings and does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are natural human language speech (German), with no mention of fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly stated as German."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language, so language is present."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7777,
      "completion_tokens": 137,
      "total_tokens": 7914
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.1 Data",
          "reasoning": "The paper states that the recorded data cannot be published due to ethics committee restrictions, and there is no mention of any publicly available code or repository related to data collection or processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Data and Section 2.2 Data processing",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including participant recruitment criteria, ethical approval, recording setup, speech tasks used, data processing steps such as downsampling and voice activity detection, and partitioning strategy for train/test sets."
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7611,
      "completion_tokens": 92,
      "total_tokens": 7703
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Datasets",
          "Reasoning": "The paper introduces a new manually transcribed dataset called the MPS dataset, consisting of audio recordings of elementary school children reading L2 English text aloud. These recordings were collected from nearly 2500 students in government schools, making it a human-generated audio dataset captured via recordings."
        }
      ]
    }
  },
  {
    "id": "gothi24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8463,
      "completion_tokens": 285,
      "total_tokens": 8748
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 Datasets, paragraph describing manual transcription",
            "reasoning": "The dataset comprises recordings of nearly 2500 students with manual transcriptions underway. The errors contain various types of substitutions and disfluencies transcribed phonemically. This suggests multiple human annotators, likely trained but not necessarily experts, performed detailed word-level manual transcriptions including error labels."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 Datasets, description of manual transcription process",
            "reasoning": "The manual transcription process is detailed enough to label the reading disfluencies including types such as substitution, deletion, insertion, indicating the use of annotation instructions guiding how to mark these distinctions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2 Datasets, Table 1 and text on labeling error types",
            "reasoning": "The error types are explicitly categorized into COR, INS, DEL, SUB-1, and SUB-2 in Table 1, showing clear rubric definitions for annotators to classify each word-level deviation, which qualifies as annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not mention the provision of annotation examples or exemplar cases in guidelines or appendices for the manual transcribers to follow."
          }
        }
      ]
    }
  },
  {
    "id": "gothi24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9593,
      "completion_tokens": 326,
      "total_tokens": 9919
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of multiple human experts performing quality assurance or annotation validation on the MPS dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human annotator performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human non-experts involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for miscue detection, the paper does not describe AI models used as judges for the quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic/rule-based quality assurance process for the correctness of manual annotations is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper states that manual transcription at the word level is underway but does not provide any details about the quality assurance process used for these annotations or who performed the transcription or verification. There is no discussion on annotation validation, multiple annotators, expert review, or inter-annotator agreement. Therefore, no quality assurance process is documented for the new MPS dataset annotations."
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9211,
      "completion_tokens": 426,
      "total_tokens": 9637
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The paper states that audio recordings of oral reading by approximately 2,500 elementary school children in India were collected in summer 2023, with manual transcription underway. The dataset, termed the MPS dataset, consists of manually transcribed and labeled connected text readings with ground-truth miscue annotations. This data was recorded and transcribed by human contributors entirely from scratch, representing original content and not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the new dataset is generated entirely by AI or machine learning models without reference to existing data. The paper uses models for analysis but not for dataset generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involves human translation of content from another language. Although subjects are L2 English learners with diverse home languages, the data consists of oral reading in English, not translated text."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by any machine translation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not simply collected or aggregated from existing sources; it is newly recorded from the field."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the dataset is derived or transformed from an existing dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and collection procedures for the new dataset."
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9729,
      "completion_tokens": 215,
      "total_tokens": 9944
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2 and 4",
          "reasoning": "The newly introduced Maharashtra Primary Schools (MPS) dataset is explicitly used as a test dataset for evaluation purposes. It is manually transcribed and labeled for reading miscues and is used to benchmark the performance of various ASR-based miscue detection methods in the experiments described in Sections 2 and 4."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10452,
      "completion_tokens": 564,
      "total_tokens": 11016
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2 Datasets",
          "reasoning": "The MPS dataset comprises oral reading recordings of elementary school children with diverse home languages in India. Although the reading texts are in L2 English, the children use a variety of home languages including Hindi and potentially others, as stated: 'The students, who come with diverse home languages, are introduced to both Hindi and English reading and writing in Grade 1.' Additionally, reading errors include several non-English words transcribed phonemically, indicating presence of multiple languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While English and Hindi are mentioned, the dataset includes diverse home languages beyond Hindi, thus exceeding exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset texts are in L2 English, but contain reading errors including non-English words and phonemic transcriptions for those, indicating other languages are present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "English is present in the reading texts and transcripts, so dataset is not strictly monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio and transcriptions, no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset contains human oral reading recordings without biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages are described explicitly: English reading prompts with diverse home languages and non-English words transcribed phonemically."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes human speech and transcripts which qualify as language content."
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7670,
      "completion_tokens": 153,
      "total_tokens": 7823
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any links or references to publicly available code pertaining to the dataset construction, data collection, preprocessing, or annotation processes. No repository or supplementary material regarding code is indicated in the text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Datasets",
          "reasoning": "The paper provides detailed documentation about the dataset creation process in Section 2, including how audio recordings were collected from elementary school children across multiple schools, the ethical clearance obtained, the nature of the reading prompts, the transcription process, error labeling schema, and statistics of the dataset. This documentation provides clarity on dataset composition and annotation, supporting reproducibility and assessment."
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7301,
      "completion_tokens": 122,
      "total_tokens": 7423
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Open track; Section 5 Conclusions",
          "Reasoning": "The paper states that the authors curated a new speech dataset of Singaporean dialects from YouTube videos featuring Singaporean English and Mandarin spoken speech. This data consists of spoken audio recorded by humans (Singaporean speakers), hence audio modality and human-generated data. No evidence suggests the data was generated or simulated by models. The data source is explicitly described and curated, so its origin is known."
        }
      ]
    }
  },
  {
    "id": "gupta23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8153,
      "completion_tokens": 226,
      "total_tokens": 8379
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2",
            "reasoning": "The authors curated the new Singaporean dialect English-Mandarin speech dataset from YouTube videos using a Voice Activity Detection (VAD) model (marblenet) to generate speech segments, and then filtered segments using their trained model to select those where errors were made. This indicates an automated, model-based annotation/selection process rather than human labeling or expert annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "There is no mention or description of annotation instructions provided for this automated data curation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not describe any scoring rubrics or guidelines applied during this curation or annotation of the new YouTube dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "No examples of annotations or annotation guidelines are provided or referred to in the paper regarding this dataset."
          }
        }
      ]
    }
  },
  {
    "id": "gupta23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9283,
      "completion_tokens": 353,
      "total_tokens": 9636
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper describes using an open-source Voice Activity Detection (VAD) AI model (marblenet from Nvidia NeMo Toolkit) to segment speech data (Section 2.1 for National Speech Corpus and Section 2.2 for additional YouTube and Mozilla Common Voice data). Additionally, to filter the YouTube segments, the authors used a model trained on the closed task to select segments on which the model made prediction errors, effectively applying an AI model-based filtering and quality control step on the dataset. Thus, quality assurance of dataset content and segmentation was performed using AI models."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly mentions the use of AI models for data segmentation and filtering, indicating some quality assurance processes are performed."
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8901,
      "completion_tokens": 434,
      "total_tokens": 9335
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the new dataset was recorded or created directly from human contributors in a new recording session or original content creation. Instead, the data is curated from existing YouTube videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was generated by any AI or machine learning model in a generative manner. The data originates from real-world audio recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translating the data by human translators from another language into the target languages or any translation process."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper states that the new Singaporean dialect English-Mandarin speech dataset was curated by collecting speech data from existing YouTube videos. Speech segments were extracted using a voice activity detection model but no significant modification is described, indicating data collection or aggregation from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The data was processed by applying voice activity detection to generate speech segments and then filtered by using a model trained on the closed task to select segments where the model made errors, thus applying transformations and filtering on the original data, making it derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and process of generation of the new dataset is documented explicitly in the paper."
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9419,
      "completion_tokens": 235,
      "total_tokens": 9654
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 3.5.3",
          "reasoning": "The new dataset curated from YouTube videos with Singaporean dialect English and Mandarin speech is used in the open track to fine-tune a pre-trained speaker recognition model for the language identification task. This usage is described in Section 2.2 where the data is introduced and Section 3.5.3 where fine-tuning on this dataset along with others is detailed."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10142,
      "completion_tokens": 532,
      "total_tokens": 10674
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The introduced new dataset contains only English and Mandarin languages, therefore it is not multilingual in the sense of containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.2 and Abstract",
          "reasoning": "The paper explicitly states that the newly curated dataset from YouTube contains English and Mandarin speech data spoken by Singaporean speakers (Section 2.2 and Abstract). This dataset contains exactly these two human languages, making it bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is not exclusively English; it includes Mandarin as well."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes both Mandarin and English, not just a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of spoken language audio segments and does not include programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are speech audio segments; no mathematical or logical notation is present."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is human speech audio, no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No fictional or artificial languages are mentioned as part of the curated dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages in the curated dataset are explicitly stated as English and Mandarin; there is no ambiguity or unspecified language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language (English and Mandarin); it is not language-free."
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7360,
      "completion_tokens": 244,
      "total_tokens": 7604
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.2, Section 3 and Footnotes 1 and 2",
          "reasoning": "The paper states that the curated Singaporean dialect English-Mandarin speech dataset prepared from YouTube videos will be released for public use on their project repository (footnotes 1 and 2 with URLs). Also, Section 3 mentions that changed code files and training/fine-tuning scripts used in experiments are included in the supplementary material and are available on their project repo. This implies that code related to dataset creation, including VAD segmentation and filtering, is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2",
          "reasoning": "Section 2.2 clearly documents the dataset creation process for the new Singaporean dialect dataset curated from YouTube. It details the source channels, use of VAD (marblenet) to segment speech, filtering samples on which the base model made errors to increase diversity, and the total hours and sample counts per language. Additionally, they provide the complete list of YouTube video links in supplementary material for reference. This constitutes sufficient documentation on dataset construction."
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10970,
      "completion_tokens": 213,
      "total_tokens": 11183
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
          "Reasoning": "DreamVoiceDB consists of recordings of 900 speakers sampled from existing datasets (LibriTTS-R and VCTK), which are human speech recordings. The dataset was annotated by speech and language experts. Therefore, the audio data is human-generated, i.e., recorded from human speakers."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
          "Reasoning": "The dataset contains natural language descriptors generated by OpenAI's GPT4 API based on combinations of expert-annotated keywords for each speaker, used to create text prompts describing voice timbre. Thus, the text descriptions are model-generated. Additionally, the annotation keywords were selected by human speech experts and collected through expert annotators' surveys, representing human-generated text annotations."
        }
      ]
    }
  },
  {
    "id": "hai24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11822,
      "completion_tokens": 212,
      "total_tokens": 12034
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2",
            "reasoning": "The paper states that a total of 8 expert annotators with speech-related expertise (speech and language pathology, coaching, transcription) annotated all 900 speakers once each, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "The survey questions were designed with reference audio examples to facilitate objective annotations using structured approaches including Likert scales and binary scales, indicating detailed instructions were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "Annotations for subtler attributes were conducted using Likert scales and binary scales, which serve as scoring rubrics to guide annotators in making consistent assessments."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "The paper explicitly mentions using reference audio examples in the survey design to assist annotators in consistent and objective category annotations."
          }
        }
      ]
    }
  },
  {
    "id": "hai24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12952,
      "completion_tokens": 299,
      "total_tokens": 13251
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2, DreamVoiceDB: Voice Timbre Dataset",
          "reasoning": "The paper states that 8 expert annotators with expertise in speech-related fields such as speech and language pathology, speech and accent coaching, singing coaching, and transcription work annotated the dataset. All 900 speakers were annotated once by each expert annotator. This indicates that multiple human experts conducted the quality assurance process for the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The quality assurance process is clearly described and involves multiple human experts annotating the dataset."
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12570,
      "completion_tokens": 495,
      "total_tokens": 13065
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The paper introduces DreamVoiceDB, a new dataset of voice timbre annotations for 900 speakers. These annotations were performed by 8 expert human annotators with speech-related expertise, who evaluated speakers from existing datasets using expert-designed surveys with reference audio and scales to provide high-quality human-created labels and natural language descriptors generated from these annotations using GPT-4. This process involves original human judgment and annotation, not merely translation or direct aggregation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speaker samples from existing datasets with human annotations. There is no indication that data samples themselves are generated entirely by AI models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language via human translators in the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that content in the dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The DreamVoiceDB dataset is constructed by sampling 900 speakers from existing multi-speaker datasets LibriTTS-R and VCTK. The audio data is thus collected from pre-existing sources, though with extensive new annotation. Therefore, the audio samples themselves are collated from existing datasets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The original audio data from LibriTTS-R and VCTK is enhanced with newly created detailed human annotations for voice timbre characteristics, plus prompt generation using GPT-4 based on these annotations. This constitutes a derived dataset as it modifies and enriches existing data with additional metadata and labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the dataset origin and annotation process."
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13088,
      "completion_tokens": 322,
      "total_tokens": 13410
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Settings",
          "reasoning": "The DreamVoiceDB dataset of 900 speakers with annotated text prompts is used as training data to train the proposed text-guided voice conversion and generation models (DreamVC and DreamVG) from scratch, as described in Section 4.1."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using DreamVoiceDB to fine-tune pre-trained models in a supervised manner."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training techniques applied on DreamVoiceDB."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used exclusively for evaluation; the paper uses other speakers from LibriTTS-R dev set for validation and testing purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 DreamVoiceDB: Voice Timbre Dataset",
          "reasoning": "The authors analyze agreement scores among annotators and perform keyword distribution and label alignment analyses to ensure dataset quality and richness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The DreamVoiceDB is not described or used as a knowledge base for augmenting models via retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13811,
      "completion_tokens": 618,
      "total_tokens": 14429
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The DreamVoiceDB dataset is constructed by sampling speakers from LibriTTS-R and VCTK, which are primarily English speech datasets. There is no indication of inclusion of multiple languages beyond English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The dataset entries are derived only from English speech corpora (LibriTTS-R and VCTK). No second language presence is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (DreamVoiceDB: Voice Timbre Dataset), Introduction, and Experimental Settings (Section 4.1)",
          "reasoning": "DreamVoiceDB is composed of 900 speakers sampled exclusively from LibriTTS-R and VCTK datasets, both of which are English multi-speaker corpora. The paper explicitly references these sources and does not mention inclusion of other languages, confirming the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The dataset is derived solely from English-language datasets, with no indication of any non-English languages included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries consist of speech audio files and their corresponding text-based annotative labels, not programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain mathematical or logical symbolic expressions; it contains natural language annotations describing voice timbre."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is based on human speech data only, with no inclusion of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language source of the dataset is clearly documented as English speech from LibriTTS-R and VCTK."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain English language audio and text annotations; hence it is not non-linguistic data."
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11029,
      "completion_tokens": 150,
      "total_tokens": 11179
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and Introduction",
          "reasoning": "The paper references an official website for the dataset but does not provide any link or mention of the code used for data collection, preprocessing, or generation. There is no indication that code has been publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including expert consultation, keyword selection, survey design, data collection by expert annotators, and analysis for alignment and label integration. This detailed process is described in Section 2 with a figure and description of methodology, indicating comprehensive documentation."
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5587,
      "completion_tokens": 99,
      "total_tokens": 5686
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Dataset",
          "Reasoning": "The CNVVE dataset consists of 950 audio samples of non-verbal voice expressions collected from 42 human participants who voluntarily donated their voice recordings, as explicitly described in Section 3.3. The data are human-generated recordings captured via smartphone microphones, demonstrating clear human involvement in recording the audio modality."
        }
      ]
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6439,
      "completion_tokens": 260,
      "total_tokens": 6699
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The dataset was collected from 42 anonymous participants who voluntarily donated their voice recordings by using a dedicated website. These participants are not described as experts but as general volunteers, implying multiple human non-expert annotators recording the data."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The data collection website defines the purpose and type of voice data sought, provides example recordings to participants and displays the written equivalent of expressions (e.g. 'Uh-huh'), thus constituting clear instructions for participants on how to perform the annotations (recordings)."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no indication in the paper of any scoring rubrics or criteria used by annotators to rate or classify the recordings since the data consists of participant recordings rather than rated annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The website provides example recordings of the non-verbal voice expressions to participants to guide the type of samples desired, serving as annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7569,
      "completion_tokens": 386,
      "total_tokens": 7955
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The dataset was collected from 42 participants who anonymously and voluntarily donated their voice recordings. The participants are not described as domain experts but as members of the target demographic or general population. The recordings and labels correspond directly to participants' own utterances of predefined non-verbal voice expressions. The quality assurance appears to consist of reviewing and removing corrupted files as mentioned in Section 3.3, which implies some verification but no expert annotation. Hence, the quality assurance was conducted by multiple human non-expert participants (annotators) who provided the data themselves and possible simple review steps by the authors."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model was used for quality assurance or annotation verification; AI models were used for classification only."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The recorded audio files were automatically saved in .wav format and reviewed to remove corrupted files, indicating some automatic or rule-based verification of data integrity was performed as part of quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents that corrupted files were removed and recording was controlled through the website, indicating some form of quality assurance took place."
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7187,
      "completion_tokens": 405,
      "total_tokens": 7592
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Collecting non-verbal voice expressions), specifically Sections 3.1 and 3.3",
          "reasoning": "The dataset was created from recordings donated by 42 human participants who recorded non-verbal voice expressions themselves through a dedicated website. The paper explicitly states that these recordings are original audio samples collected from human contributors, not derived or translated from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was generated by AI or machine learning models; all data are human voice recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any process involving translation of content by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation usage in the data creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources but newly collected recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.3 (Data augmentation)",
          "reasoning": "Data augmentation techniques were applied to the original recordings, resulting in transformed audio files (e.g., background noise addition, pitch shifting). These augmented data are based on existing human-recorded samples with systematic modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data collection process and augmentation methods, so the data origin is well specified."
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7705,
      "completion_tokens": 368,
      "total_tokens": 8073
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Section 5.1",
          "reasoning": "The CNVVE dataset is used to train a convolutional neural network model from scratch to classify non-verbal voice expressions, as described in Section 4 (Method for classifying NVVEs) and evaluated in Section 5.1. The model is trained using the dataset without mention of pre-training, indicating training from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1, Section 5.4",
          "reasoning": "The dataset is used for evaluation through 5-fold cross-validation testing and the presentation of confusion matrices in Section 5.1 and 5.4, benchmarking the classification performance of the models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2, Section 5.3, Section 5.4",
          "reasoning": "The dataset is used for analysis to study feature sets (MFCC vs. Mel Spectrogram), effects of number of mel bands (Section 5.2), the impact of data augmentation techniques on model accuracy (Section 5.3), and in analyzing model performance confusion matrices (Section 5.4). These uses indicate analytical purposes to understand the dataset and model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8428,
      "completion_tokens": 417,
      "total_tokens": 8845
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Table 1",
          "reasoning": "The dataset consists of recorded non-verbal voice expressions labeled using English representations (e.g., \u201cUh-huh\u201d, \u201cmm-hmm\u201d, \u201cHush\u201d, \u201cPsst\u201d, \u201cAhem\u201d, etc.). There is no mention of recordings in other human languages or bilingual/multilingual content. The expressions and instructions provided to participants were in English, indicating the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of non-verbal voice expressions which are vocal sounds produced by humans, therefore they contain language-related audio signals and cannot be classified as having no language."
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5646,
      "completion_tokens": 168,
      "total_tokens": 5814
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Introduction and footnotes",
          "reasoning": "The paper states in the Introduction that the CNVVE dataset, code for creating the voice recognition system, trained models, and other voice processing tools are publicly available at https://github.com/hedeshy/CNVVE. This indicates that the code associated with the dataset and processing is made publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Collecting non-verbal voice expressions) and 3.3 (Dataset)",
          "reasoning": "The paper provides detailed documentation about the dataset creation process, including the data collection procedure via a dedicated website, participant demographics and consent, recording specifications, dataset composition, and associated metadata. This thorough description confirms that dataset creation process is well documented."
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6559,
      "completion_tokens": 245,
      "total_tokens": 6804
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3: Data",
          "Reasoning": "The paper describes selecting 27 files from the CORAAL corpus containing about 10 hours of African American English speech audio. This audio data was originally recorded human speech from sociolinguistic interviews, thus is human generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3: Data",
          "Reasoning": "The authors produced multiple human transcript versions (CORAAL original transcripts, Rev human transcripts, Rev (+AA tag) human transcripts, Amberscript transcripts) of the same 10 hours of audio. These transcripts were manually created by human transcribers from the audio, so are human generated text data."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3: Data",
          "Reasoning": "Two ASR-produced transcript versions (Rev ASR model and OpenAI Whisper model outputs) of the same audio were generated by automated speech recognition systems, indicating model generated text data."
        }
      ]
    }
  },
  {
    "id": "heuser24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7411,
      "completion_tokens": 256,
      "total_tokens": 7667
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3 Data",
            "reasoning": "The new dataset consists of newly produced transcripts (Rev, Rev (+AA tag), Amberscript) created by professional transcribers recruited and managed through platforms like Rev.com and Amberscript. These transcribers are professionals but are not described as experts in linguistics or the specific dialect, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 6 Discussion",
            "reasoning": "The paper mentions that Rev transcribers were trained on a style guide with specific instructions on transcription style, including guidelines about English grammar conventions and verbatim vs non-verbatim distinctions, indicating the presence of instructions in the annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper discusses a style guide with instructions but does not mention any scoring rubrics or criteria for evaluating annotations or transcriptions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no mention in the paper of examples being provided within the annotation guidelines or style guide for transcribers."
          }
        }
      ]
    }
  },
  {
    "id": "heuser24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8541,
      "completion_tokens": 479,
      "total_tokens": 9020
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3 Data; Section 6 Discussion",
          "reasoning": "The original CORAAL transcripts were produced and corrected by multiple researchers familiar with African American English, indicating involvement of subject matter experts in quality assurance. This suggests that at least some quality assurance was conducted by single human experts in the target demographic on the original CORAAL transcripts."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3 Data",
          "reasoning": "The CORAAL transcript versions were produced by a team of linguistic researchers familiar with AAE, with multiple stages of transcription and editing, indicating multiple human experts participated in quality assurance. This shows that multiple human experts were involved in producing and quality assuring the CORAAL transcripts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3 Data",
          "reasoning": "The Rev, Rev (+AA tag), and Amberscript transcripts were produced by professional transcribers who are not described as subject matter experts but are professionals in transcription, indicating multiple human non-experts performed transcription and quality verification by senior transcribers, thus acting as quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used for quality assurance of the transcripts."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although algorithmic analyses were performed to categorize transcription differences, there is no indication that automated verification was used as a quality assurance step for the dataset annotations themselves."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple quality assurance processes involving human experts and professional transcribers, so QA was performed and documented."
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8159,
      "completion_tokens": 501,
      "total_tokens": 8660
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3. Data",
          "reasoning": "The paper describes that the authors produced multiple human transcript versions of the same 10 hours of African American English speech from the CORAAL corpus. These transcripts include newly created human transcripts by professional transcribers solicited specifically for this study (e.g., Rev, Rev (+AA tag), Amberscript). The CORAAL transcripts themselves were produced previously, but the other human transcripts were newly created for this study, representing original content created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3. Data",
          "reasoning": "Two ASR-produced transcripts were generated using machine models: Rev ASR and OpenAI's Whisper. These machine-generated transcripts are newly created data outputs from speech recognition models applied to the CORAAL audio files, representing original content generated by AI models distinct from existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the data was produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the data was generated by machine translating content from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not simply collected or aggregated from existing sources without modification; rather, new transcriptions were generated either by humans or ASR models."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the transcripts are all aligned to and derived from the same original audio, the new transcripts represent new transcription efforts rather than transformed existing transcripts; thus, they are not considered derived in the sense of transformations of existing written data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origins and generation methods of all new datasets, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8677,
      "completion_tokens": 335,
      "total_tokens": 9012
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1, Figures 1 and 2",
          "reasoning": "The newly introduced dataset consists of six transcript versions (four human-produced and two ASR-produced) of the same 10 hours of African American English speech from CORAAL. These transcripts are used to compute word error rates (WER) between pairs of transcripts, both human-human and ASR-human comparisons, to evaluate and benchmark ASR system performance under different transcription styles. The dataset serves as reference transcripts to measure and compare ASR performance, as shown in their WER analysis results."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 5.2 and 6",
          "reasoning": "The dataset is used to analyze the nature and sources of transcription differences between transcript versions, including hypotheses about verbatim versus non-verbatim styles, morpho-syntactic differences characteristic of African American English, and reduction variations. The analysis examines these differences to understand stylistic variability, biases in transcription and ASR systems, and the implications for fair evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9400,
      "completion_tokens": 532,
      "total_tokens": 9932
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a dataset consisting of transcripts of African American English speech only; no indication of multiple languages being included is given."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves only African American English speech and transcripts, with no mention of two distinct languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 Data and throughout the paper",
          "reasoning": "The dataset consists of transcripts of African American English (AAE) speech, which is a variety of English. The paper explicitly discusses transcription of AAE, a variety of English, and no other languages are mentioned in the dataset. All transcripts and analyses pertain to English content within AAE."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is clearly English (specifically AAE), so it is not a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human and ASR-produced transcripts of speech; no programming or code-related content is present."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or logical symbolic representations, only natural language transcripts."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns human speech transcripts only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any constructed or fictional languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are clearly described as African American English; the language is specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language transcripts, so it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6618,
      "completion_tokens": 179,
      "total_tokens": 6797
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4 Methods, footnote 6 and 5",
          "reasoning": "The paper explicitly links to open source tools and code repositories used for alignment and testing (fstalign, and https://github.com/revdotcom/speech-datasets/tree/main/coraal-multi), indicating that code used for processing and analysis of the dataset is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Data and Section 4 Methods",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including selection of audio files from the CORAAL corpus, descriptions of the six transcripts versions produced (four human, two ASR), the transcription style guides used, and the methods for alignment and difference testing. This constitutes clear documentation of the dataset construction and annotation process."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5625,
      "completion_tokens": 193,
      "total_tokens": 5818
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data set",
          "Reasoning": "The new dataset consists of unscripted speech recordings from 8 native speakers of Southern British English collected through storytelling, map tasks, and informal discussions. The speech data were manually recorded from human speakers, thus the modality is audio and the source is human generated."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Preprocessing",
          "Reasoning": "The raw audio F0 contours were extracted and processed and subsequently transformed into grayscale PNG images of F0 contours (28 by 28 pixels) through algorithmic methods such as smoothing, interpolation, normalization, and image conversion. This transformation was fully automated and generated by computational processing, without human authorship, thus the image data are model generated."
        }
      ]
    }
  },
  {
    "id": "hu24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6477,
      "completion_tokens": 219,
      "total_tokens": 6696
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1",
            "reasoning": "The dataset was annotated for pitch accents by one expert annotator, and a second expert independently annotated 12% of the accents, with high inter-annotator agreement reported, indicating multiple expert annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1",
            "reasoning": "The annotators used the same criteria based solely on the F0 shape as specified (L+H* if a deliberate F0 dip at syllable onset, H* if not), indicating existence of clear annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper describes annotation criteria but does not mention any formal scoring rubric or rubric-based scoring used in annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not provide or mention any examples of annotated data or example annotations given to annotators in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "hu24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7607,
      "completion_tokens": 202,
      "total_tokens": 7809
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1. Data set",
          "reasoning": "The dataset annotations for pitch accents were conducted by one expert annotator based on F0 shape, demonstrating that quality assurance was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1. Data set",
          "reasoning": "A second expert annotator independently annotated 12% of the accents to assess inter-annotator agreement, indicating involvement of multiple human experts for quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7225,
      "completion_tokens": 425,
      "total_tokens": 7650
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset consists of unscripted speech collected from 8 native speakers of Southern British English, recorded through specifically designed tasks (storytelling, map task, informal discussions). The speech data were manually annotated for pitch accents by human experts based on well-defined phonetic criteria, resulting in 2,025 accents. This indicates that the dataset was newly created from scratch by human contributors and is not adapted or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any data solely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any data produced by translating content from another language through human translators is made."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any machine-translated data is present."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources; rather it was freshly recorded and annotated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The raw speech data were processed through multiple steps including F0 extraction, normalization, smoothing, interpolation, and conversion into grayscale images to be used for classification. These transformation and preprocessing steps represent derived data based on the original recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly specified and documented."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7743,
      "completion_tokens": 433,
      "total_tokens": 8176
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.3, Model structure",
          "reasoning": "The dataset of pitch accent images is used to train a basic neural network model from randomly initialized parameters to classify pitch accents. The paper describes the model training on the dataset without mention of pre-training or transfer learning, indicating training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that the dataset is used for fine-tuning a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using reinforcement learning or RL-based post-training techniques on the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used solely for evaluation; it is central to training and testing the model, thus not exclusive to evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4 (Results and Discussion)",
          "reasoning": "The dataset is analyzed to assess model prediction accuracy differences between classes and to interpret discrepancies between model and human annotations, which involves analyzing patterns and characteristics of the pitch accent data."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as serving as a knowledge base to augment models through retrieval or other methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used practically for training and analysis within the paper."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8466,
      "completion_tokens": 560,
      "total_tokens": 9026
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset contains only Southern British English speech from native speakers. No indication of multiple languages involved."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset contains only one language, English (Southern British English). No second language is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset consists of unscripted speech from native speakers of Southern British English, explicitly indicating English language content only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset is exclusively English, specifically Southern British English, so it is not non-English monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "General paper content",
          "reasoning": "The dataset entries consist of images representing F0 contours of speech; no programming or code content is part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "General paper content",
          "reasoning": "While data processing involves statistics and smoothing techniques, the dataset itself contains images of pitch contours, not explicit mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "General paper content",
          "reasoning": "The dataset pertains to human speech pitch accents and contains no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset involves natural language (English) only; no constructed or fictional languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The language of the dataset entries is clearly documented as Southern British English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries represent pitch accent images derived from speech data and thus involve language; therefore, dataset does contain language."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5684,
      "completion_tokens": 188,
      "total_tokens": 5872
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.3 Model structure",
          "reasoning": "The paper explicitly states that the code used for training the neural network model is available at https://osf.io/esb4g/?view_only=4409bab750b24dcd9be308bc2d66, indicating that at least part of the code related to data processing and model training is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Data set and Section 2.2 Preprocessing",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including information about the participants, the speech tasks, pitch accent annotation procedures, F0 extraction, normalization, smoothing, time registration, and image generation steps. This thorough description spans sections 2.1 and 2.2, offering transparency and completeness necessary for reproducibility."
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9120,
      "completion_tokens": 93,
      "total_tokens": 9213
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Evaluation; Section 1 Introduction",
          "Reasoning": "The internal dataset is introduced as a new dataset with 8 keywords uttered 10 times by 50 speakers, recorded in a clean environment. This dataset contains recorded human speech audio for keyword spotting evaluation, confirming it is audio modality and human generated."
        }
      ]
    }
  },
  {
    "id": "huang22l_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9972,
      "completion_tokens": 243,
      "total_tokens": 10215
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and Section 3.2",
            "reasoning": "The paper describes the use of the Montreal Forced Aligner tool to generate word-level annotated segmentations on Librispeech for training and details on dataset preparation that include augmentations and selection criteria. The internal dataset consists of recorded keywords by speakers, but annotation is done via forced alignment and automated segmentations rather than manual labeling by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No explicit annotation instructions for human annotators are described; annotation is performed using forced alignment tools and data selection criteria without mention of human annotator instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no indication of scoring rubrics for the annotations since automated forced alignment is used rather than manual scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section provides annotation examples; annotation is automated.",
            "reasoning": "No examples of annotation guidelines or annotated data samples are presented, as the dataset annotations are generated automatically using forced alignment."
          }
        }
      ]
    }
  },
  {
    "id": "huang22l_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11102,
      "completion_tokens": 293,
      "total_tokens": 11395
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information regarding quality assurance performed by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving multiple human experts for the introduced internal dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance procedures involving multiple non-expert human annotators for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any AI model to perform quality assurance on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic/rule-based quality assurance applied to dataset annotations or content in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available datasets (Hey-Snips) and an internal dataset, but does not document or describe any quality assurance process applied to annotations or content of the datasets. Therefore, no QA process is considered present or documented."
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10720,
      "completion_tokens": 489,
      "total_tokens": 11209
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 Evaluation",
          "reasoning": "The paper introduces a new internal dataset with 8 keywords named after actual home appliances, uttered 10 times by 50 speakers, collected by the authors themselves (Section 3.2). This dataset is original content created entirely from scratch by human contributors, as it records human speakers uttering these keywords uniquely for the study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any datasets generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was produced via machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Training and Section 3.2 Evaluation",
          "reasoning": "The authors use publicly available datasets such as Librispeech, Hey-Snips, and the MS-SNSD noise datasets, which were collected from existing sources without significant modification; they are aggregated and utilized for training and testing. Specifically, Librispeech and Hey-Snips are existing datasets used 'as is'."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Training and Section 3.2 Evaluation",
          "reasoning": "The authors apply modifications and transformations such as segmenting Librispeech audio into 1s clips using forced alignment, data augmentation by adding noise and simulating far-field effects on both training and evaluation datasets (Hey-Snips and the internal dataset). Thus, datasets derived from existing sources with applied transformations are used."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the datasets used is specified and documented."
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11238,
      "completion_tokens": 286,
      "total_tokens": 11524
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The authors describe using the Librispeech dataset to train their QbyE-MLPMixer model from randomly initialized parameters using supervised learning with cross-entropy loss. This indicates training from scratch on this labeled dataset for keyword spotting."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4",
          "reasoning": "The authors evaluate their proposed MLPMixer model using the publicly available Hey-Snips dataset and an internal dataset. These datasets are used exclusively for benchmarking and performance measurement, as described in Sections 3.2 and 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The paper includes ablation studies analyzing input representations and activation functions using datasets to understand their impact on performance, indicating usage for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11961,
      "completion_tokens": 590,
      "total_tokens": 12551
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed datasets described (Hey-Snips and the internal dataset) contain only English language speech as evident from the descriptions and training on the Librispeech dataset which is English-only (Section 3.1 and Section 3.2). There is no indication of multiple languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of datasets containing exactly two languages. The datasets are either English or not specified as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Training), Section 3.2 (Evaluation)",
          "reasoning": "The new internal dataset created by the authors contains utterances of eight English keywords (e.g., 'LG Styler', 'Hey LG') spoken by 50 speakers, as stated in Section 3.2. The publicly available Hey-Snips dataset used is also English-only. The training data (Librispeech) is English audiobooks, confirming the monolingual English nature of the datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with non-English language content are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain audio speech data; there is no programming or code content included in the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain mathematical or symbolic expressions. Mathematical notations appear only in the methods section for model description, not as part of dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain human speech audio, not biological sequences or other non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The proposed datasets are composed of spoken human language audio; hence, language is present."
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9179,
      "completion_tokens": 160,
      "total_tokens": 9339
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section in the paper",
          "reasoning": "The paper does not provide any URL, footnote, or mention indicating that the code for data collection, preprocessing, or dataset construction is publicly available in any accessible repository."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 3.1 and 3.2 describe dataset usage only",
          "reasoning": "The paper uses publicly available datasets (Hey-Snips, Librispeech) and an internal dataset for evaluation but does not provide documentation on dataset creation or data collection procedures. There is no description of how the internal dataset was collected, generated, or constructed; only the number of speakers and keywords is mentioned. Therefore, no documentation on dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8636,
      "completion_tokens": 187,
      "total_tokens": 8823
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The ConEC dataset is described as consisting of earnings calls audio, which are real-world audio recordings of human speech, collected along with supplementary materials. It is introduced by the authors as a real-world contextual ASR test set and used to evaluate the proposed methods, thus it is a new dataset introduced by the authors."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The ConEC dataset also includes real-world supplementary materials such as presentation slides, earnings news releases, and lists of meeting participants' names and affiliations. These textual contexts are human-generated textual data and are introduced as part of the ConEC dataset by the authors."
        }
      ]
    }
  },
  {
    "id": "huang24f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9488,
      "completion_tokens": 210,
      "total_tokens": 9698
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets",
            "reasoning": "The new dataset introduced is ConEC, which consists of earnings calls and associated real-world contexts. The paper describes usage of this dataset but does not mention manual annotation by humans; the dataset is used as-is for evaluation and training, indicating automatic or pre-existing dataset usage without manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no information in the paper indicating that annotation instructions were provided for ConEC or any new annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not discuss any scoring rubrics or guidelines related to annotation for the ConEC dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide examples or annotation guidelines related to the ConEC dataset annotation, implying no new annotations with examples were conducted."
          }
        }
      ]
    }
  },
  {
    "id": "huang24f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10618,
      "completion_tokens": 293,
      "total_tokens": 10911
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert to validate dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance performed by multiple human experts for the introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human performed quality assurance on the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided about multiple non-expert humans performing QA on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models to perform quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses use of hand-crafted linguistic rules for alternative spellings and algorithmic techniques, there is no explicit description of an automatic quality assurance process verifying dataset correctness or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces or uses datasets (LibriSpeech, SPGISpeech, ConEC) but does not document any quality assurance process performed specifically on these datasets by the authors. Quality assurance details for data annotation or verification are absent in the paper."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10236,
      "completion_tokens": 414,
      "total_tokens": 10650
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any entirely new datasets created from scratch by human contributors were introduced by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generation of original data purely by AI or models; it focuses on methods to improve ASR using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of data produced by human translation from another language described as new dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss creation of new data via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors used existing public datasets (LibriSpeech, SPGISpeech, ConEC), which are pre-existing and not newly collated by the authors as datasets themselves."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The authors use existing datasets (LibriSpeech, SPGISpeech, ConEC) and generate artificial biasing lists for contexts by extracting rare words and adding distractors. They also perturb reference transcriptions with alternative spellings based on hand-crafted linguistic rules as a form of data augmentation. These modifications indicate derived data based on existing datasets with transformations applied, rather than entirely new datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and modifications are documented in the paper; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10754,
      "completion_tokens": 500,
      "total_tokens": 11254
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset exclusively for pre-training large models on general patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets) and Section 4.3 (Results)",
          "reasoning": "The new datasets SPGISpeech and ConEC are used to train contextual ASR models from scratch or with contextual biasing training with the proposed techniques, as indicated by the descriptions of training on these datasets and evaluation results on them."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new datasets for supervised fine-tuning of pre-trained models. The training described is end-to-end contextual ASR model training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training methods applied to the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets) and Section 4.3 (Results)",
          "reasoning": "The ConEC dataset is used as a real-world evaluation set for measuring performance of the contextual ASR models; similarly, test parts of SPGISpeech and LibriSpeech contexts are used for benchmarking. Thus, the datasets serve an evaluation purpose."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the datasets for analysis of trends or characteristics; the focus is on training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as serving as a knowledge base for model augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is clear documented usage of the introduced datasets for training and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11477,
      "completion_tokens": 570,
      "total_tokens": 12047
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets that contain more than two human languages. The datasets used and introduced (LibriSpeech, SPGISpeech, ConEC) are all English-based."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset contains exactly two human languages. The datasets are English-only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The new dataset introduced in the paper for experiments includes LibriSpeech, SPGISpeech, and ConEC, all described to be English datasets. The ConEC dataset, a real-world earnings call dataset with associated English supplementary materials, is also English. The paper explicitly states these datasets are English and uses English text for training, text perturbation, and contexts."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset in a non-English language. All datasets used for training and evaluation are English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are speech and text datasets for ASR. No programming or structured code-related content is included in the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses model architecture and uses mathematical notation in descriptions, the datasets themselves do not contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are speech and text from human languages only; no biological or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset includes constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the language (English) of the datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of speech and text data in English and thus do contain human language."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8695,
      "completion_tokens": 168,
      "total_tokens": 8863
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Introduction section, sentence 'Our implementation and experiment results are available in the ConEC repository: https://github.com/huangruizhe/ConEC.'",
          "reasoning": "The paper explicitly states that their implementation and experiment results are available in the ConEC repository, providing a link to the GitHub repository where code related to the experiments and dataset use can be accessed."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets and Introduction section",
          "reasoning": "The paper provides detailed information on the datasets used (including LibriSpeech, SPGISpeech, and ConEC), their statistics, and how contexts and biasing lists are generated for training and evaluation, thus documenting the dataset creation and processing procedures relevant to their experiments."
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7352,
      "completion_tokens": 162,
      "total_tokens": 7514
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset",
          "Reasoning": "The FisheyeMeeting dataset is introduced as newly collected data by the authors using a fisheye panoramic camera in multi-party meeting scenes, resulting in fisheye panoramic video recordings. This represents the video modality captured by human-operated equipment."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset",
          "Reasoning": "The FisheyeMeeting dataset includes corresponding multi-channel audio data captured by a circular multi-channel microphone array. This multi-channel audio data is human-recorded through physical microphone arrays during meetings, thus originating from human involvement."
        }
      ]
    }
  },
  {
    "id": "huang24g_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8204,
      "completion_tokens": 216,
      "total_tokens": 8420
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 Dataset",
            "reasoning": "Section 3.1 states that multiple annotators with a basic understanding of the ASD task performed annotations and also cross-checked and corrected the annotations, indicating multiple human non-experts conducted the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset",
            "reasoning": "The paper describes standardized annotation requirements including definitions of visibility and active speaking segments with timing precisions and pause thresholds, indicating the presence of detailed instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 Dataset",
            "reasoning": "No explicit mention of scoring rubrics or systematic scoring criteria is provided in the annotation description; only guidelines on labeling visibility and timing are given."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1 Dataset",
            "reasoning": "The paper does not mention providing specific annotation examples or sample annotations to guide annotators."
          }
        }
      ]
    }
  },
  {
    "id": "huang24g_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9334,
      "completion_tokens": 305,
      "total_tokens": 9639
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that the annotators are subject matter experts or belong to the target demographic, so this label does not apply."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper states that multiple annotators with basic understanding of the ASD task annotated the segments, cross-checked, and corrected the annotations. There is no indication that these annotators were experts, implying they are non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described involving multiple annotators and cross-checking; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8952,
      "completion_tokens": 385,
      "total_tokens": 9337
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The FisheyeMeeting dataset was collected by the authors themselves using a fisheye panoramic camera and a six-channel microphone array in multi-party roundtable meeting scenes. The paper explicitly states that multiple annotators manually annotated the data with cross-checking and corrected the annotations, confirming that the dataset is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation used in generating the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected directly from new recordings and is not aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is original recordings and annotations; it is not derived by modifying or adapting existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the dataset creation procedure and thus the data origin is documented."
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9470,
      "completion_tokens": 318,
      "total_tokens": 9788
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 (Loss function), Section 4.1 (Comparison with other methods)",
          "reasoning": "The paper describes training ASD models on the newly introduced FisheyeMeeting dataset from scratch by defining loss functions and reporting training results, demonstrating the dataset's use in training models with randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 (Evaluation metric), Section 4.1 (Comparison with other methods)",
          "reasoning": "The FisheyeMeeting dataset is used for evaluation purposes by employing mean average precision (mAP) to measure and compare the performance of the proposed and state-of-the-art ASD methods on its validation set."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Ablation study)",
          "reasoning": "The dataset is used to analyze contributions of different features and encoders by conducting ablation studies, which assess the impact of scene images, spatial spectrum heatmaps, and feature encoder architectures on performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10193,
      "completion_tokens": 501,
      "total_tokens": 10694
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any linguistic content or annotations in English or any other language for the proposed FisheyeMeeting dataset; the focus is on audiovisual data and audio from meetings, without mention of spoken language content or its type."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audiovisual and audio recordings; it does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes mathematical techniques and algorithms for audio processing (e.g., SRP-PHAT), the actual dataset described (FisheyeMeeting) does not contain mathematical or logical notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human audiovisual and audio data, not biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificial languages in the dataset."
        },
        "Unknown": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The dataset is described as audiovisual recordings and multi-channel audio from meeting scenes with multiple participants, but the paper does not specify the language(s) spoken or annotated in the dataset. Therefore, the linguistic content is unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human communication (audio and video), so language is involved, though unspecified."
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7411,
      "completion_tokens": 200,
      "total_tokens": 7611
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Conclusion",
          "reasoning": "The paper mentions at the end of Section 5 (Conclusion) that 'The source code will be released.' No explicit link, repository URL, or details about the availability of the code related to dataset construction or processing are provided in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "There is detailed documentation on the dataset creation process in Section 3.1. The paper describes the recording setup (fisheye panoramic camera and six-channel circular microphone array), data collection duration (42 hours recorded, 6 hours selected for the dataset), participant details, segmentation, face detection, trajectory forming, annotation process with multiple annotators and annotation instructions, as well as dataset statistics and splits."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6931,
      "completion_tokens": 108,
      "total_tokens": 7039
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The paper introduces the use of the USC 75-Speaker Speech MRI Database, which consists of real-time MRI video data capturing vocal tract articulation of speakers. This dataset was recorded from human speakers producing continuous English speech, collected through MRI imaging processes requiring human speakers, thus human-generated video data is present in this new dataset introduced by the authors for analysis."
        }
      ]
    }
  },
  {
    "id": "huang24i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7783,
      "completion_tokens": 224,
      "total_tokens": 8007
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3",
            "reasoning": "The paper describes an automated pipeline for extracting articulatory setting frames using ASR transcription with Whisper-large-v3 and forced alignment with the Montreal Forced Aligner, followed by automated frame extraction based on interspeech pauses. This indicates that the annotation (frame selection) was performed automatically using a deterministic pipeline rather than manual labeling by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No explicit annotation instructions for human annotators are described in the paper because the frame selection was done via an automated pipeline."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No scoring rubrics or evaluation criteria for human annotation are described; instead, parameters for frame selection are fixed in the automated pipeline."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No annotation examples are provided since annotations were performed automatically without human annotators following example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "huang24i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8913,
      "completion_tokens": 318,
      "total_tokens": 9231
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset; thus, this label does not apply."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that a single non-expert conducted quality assurance on the data."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an AI automatic speech recognition model (whisper-large-v3) was used for generating transcriptions, the paper does not state that an AI model was used specifically as a quality assurance judge for dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Frame Selection, and Section 3.4 Feature Extraction",
          "reasoning": "The dataset processing involves a fully automated multi-step pipeline for frame selection using AI-based ASR (whisper-large-v3) and forced alignment, followed by automatic contour segmentation and computation of articulatory features without mention of human verification. The paper acknowledges potential errors but relies primarily on automated verification steps to ensure data quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8531,
      "completion_tokens": 375,
      "total_tokens": 8906
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new data collection conducted by the authors themselves. Instead, it uses a publicly available dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data analyzed is not generated by a model, but is based on real MRI imaging of speech production."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation of any data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The study uses the publicly available USC 75-Speaker Speech MRI Database, which was collected by others, and aggregates data from speakers of various demographic backgrounds for analysis."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.2, 3.3, and 3.4 Methods",
          "reasoning": "The authors process the existing MRI data by applying an automated pipeline including forced alignment, segmentation, and feature extraction to derive articulatory constriction features representing articulatory settings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is specified and documented as a publicly available database."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9049,
      "completion_tokens": 249,
      "total_tokens": 9298
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results) and Section 5 (Discussion)",
          "reasoning": "The dataset, a large real-time MRI corpus of vocal tract articulation from speakers of various linguistic backgrounds speaking English, is used primarily to analyze articulatory setting differences statistically across different speaker groups. The paper conducts multiple linear regression analyses and examines patterns and trends in articulatory postures rather than using the data for model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the dataset for scientific analysis of articulatory settings, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9772,
      "completion_tokens": 565,
      "total_tokens": 10337
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The dataset contains English speech data spoken by speakers with different L1 backgrounds (American English, Indian English, L2 English speakers from China and Korea), but all utterances are in English. No other languages are spoken in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The dataset is not bilingual; it contains only English speech, though speakers have different native languages. There are no entries with exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "All speech data in the dataset are English. Speakers are from different language backgrounds, but the speech samples analyzed are exclusively English utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset does not contain non-English speech samples; all speech is English regardless of speaker L1."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech MRI video data and derived articulatory measurements, not programming code or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 3.4 Feature Extraction",
          "reasoning": "Mathematical notation is used to describe the method for feature extraction but the dataset entries themselves do not contain mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pertains to human speech articulated during English speaking; it does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset are clearly described and the content is exclusively English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken English language data; thus the dataset is not devoid of language."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6990,
      "completion_tokens": 216,
      "total_tokens": 7206
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention in the paper",
          "reasoning": "The paper does not include any mention or link to code repositories or software for the automated pipeline, data processing, or data extraction used in the construction or analysis of the dataset. There is no indication that the code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Methods), including subsections 3.1 Data, 3.2 Preliminary Data Analysis, 3.3 Frame Selection, 3.4 Feature Extraction",
          "reasoning": "The paper provides detailed documentation of the dataset creation process: it describes the data source (USC 75-Speaker Speech MRI Database), the criteria for subject selection, the cleaning and filtering steps such as removal of non-linguistic tasks, the automated pipeline steps including speech transcription, forced alignment, pause detection parameters, and feature extraction methods with mathematical definitions. This thorough description facilitates understanding of the dataset creation and preprocessing steps, enhancing reproducibility even though code is not available."
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 4552,
      "completion_tokens": 119,
      "total_tokens": 4671
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Production test, Sections 2.1 and 2.2",
          "Reasoning": "The new dataset consists of recordings from twenty Japanese-learning children during a picture-naming task conducted by the authors. Recordings were made with a Marantz digital recorder and microphone capturing the children's spoken utterances in a controlled experimental setting. This audio data is human-generated as it originates from real children speaking, captured by human-operated recording devices during the study."
        }
      ]
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 5404,
      "completion_tokens": 194,
      "total_tokens": 5598
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1",
            "reasoning": "The production experiment involved analysis of children's speech recordings by the authors, who are experts, to determine vowel devoicing occurrences; no indication of use of multiple annotators or non-experts is given."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not describe any detailed annotation instructions given to annotators for identifying devoicing in the child's utterances."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention any rubric or scoring system for annotation; it only indicates calculation of devoicing occurrence rates."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper provides example words used in the experiment but does not provide concrete annotation examples or guidelines for identifying devoicing."
          }
        }
      ]
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 6534,
      "completion_tokens": 188,
      "total_tokens": 6722
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not provide any information about a quality assurance process applied to validate dataset annotations or content. There is no mention of human or automated review, multiple or single annotators, expert or non-expert involvement, or any AI-based or algorithmic verification steps. Hence, no documented quality assurance process exists for the dataset."
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 6152,
      "completion_tokens": 364,
      "total_tokens": 6516
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper describes a production experiment conducted by the authors with twenty Japanese-learning children aged 3 and 4 years, recording their speech in a controlled environment via a picture-naming task. The recorded speech data are original and created entirely from scratch by human participants, not derived or adapted from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention in the paper of any AI or machine learning model generating data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data produced through human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data produced through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated or collected from existing sources; rather, it was freshly collected through the experiment."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is original and not based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection method are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 6670,
      "completion_tokens": 302,
      "total_tokens": 6972
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2, 3, and 4 (Production test, Results, and Discussion)",
          "reasoning": "The paper introduces a new dataset consisting of production recordings of Japanese-learning children aged 3 and 4 years, collected via a picture-naming task. This dataset is used primarily to analyze patterns and trends in high vowel devoicing (HVD) development, such as devoicing rates by age and position in words. There is no indication that the dataset is used for training, pre-training, fine-tuning models, or evaluation benchmarks. Instead, the dataset serves to provide empirical evidence for linguistic analysis and understanding the developmental trajectory of HVD in children."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A clear practical usage of the dataset is described, namely its use for analyzing linguistic developmental patterns of high vowel devoicing in children."
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 7393,
      "completion_tokens": 250,
      "total_tokens": 7643
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset consists of speech data collected from twenty Japanese-learning children from Tokyo and surrounding areas, who named pictures using Japanese words. All recorded speech materials are in Japanese, and the paper explicitly discusses features of phonetics and phonology of Japanese, indicating the dataset is solely in Japanese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 4611,
      "completion_tokens": 200,
      "total_tokens": 4811
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention or provide any link or reference to code related to the dataset creation, data collection, preprocessing, or analysis. No code repository or software is referenced anywhere in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Production test), subsections 2.1 Participants and procedure and 2.2 Material",
          "reasoning": "The paper documents the dataset creation process in detail in Section 2. It describes the participants (20 Japanese-learning children, age and demographics), the recording procedure (picture-naming task, recording environment, recording equipment), and the materials used (35 pictures representing familiar words). It also explains which data were analyzed (only words involving devoiceable vowels) and criteria for exclusion (noise, laughter, disfluencies, whispers). Thus, the dataset creation and collection process is described clearly and transparently within the paper."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6503,
      "completion_tokens": 476,
      "total_tokens": 6979
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Stimuli and data recording; Section 2.3 Data analysis and measurements",
          "Reasoning": "The new dataset consists of recordings of four native speakers of Tripolitanian Libyan Arabic producing a list of 108 utterances, including 30 target words with minimal or near-minimal pairs involving singleton and three types of geminate consonants. The data was collected through human speech recording sessions and manually segmented and annotated for phonetic analysis. Thus, it is audio data generated by human speakers as part of the study."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Stimuli and data recording",
          "Reasoning": "The stimuli used for recording were compiled lists of written words in the local Arabic dialect with specified phonological properties. The word lists and their transcriptions are human-created text data specifically crafted for this study's experiments, used as prompts for speakers."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Table 2 in Section 3.1 Results",
          "Reasoning": "The study presents tables showing aggregated statistical results of durational measurements (means, standard deviations, and counts) of the singleton and geminate consonants from the recorded data. These tables are human-generated summaries based on the processed human-recorded data of the study's new dataset."
        },
        {
          "Modality": "graph",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Figures 1, 2 and 3 in Section 3 Results",
          "Reasoning": "The paper includes graphical representations (bar charts and frequency plots) of the acoustic measurements such as durations, RMS amplitudes, and formant frequencies from the new dataset analyzed. These graphs illustrate analysis of the recorded human speech data."
        }
      ]
    }
  },
  {
    "id": "issa23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7355,
      "completion_tokens": 188,
      "total_tokens": 7543
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.2 and 2.3",
            "reasoning": "The study uses data collected from four native speakers who were recorded reading prepared stimuli; the segmentation and labeling of the data were done semi-automatically but checked and corrected by hand by the author, indicating expert human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not detail any annotation guidelines or instructions given to annotators for the segmentation or labeling process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No mention of scoring rubrics or structured assessment criteria for annotations is made in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide examples of annotations or guidelines illustrating annotation decisions."
          }
        }
      ]
    }
  },
  {
    "id": "issa23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8485,
      "completion_tokens": 386,
      "total_tokens": 8871
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided about multiple non-expert annotators participating in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Data analysis and measurements",
          "reasoning": "The paper describes that durational measurements were obtained using a script and then manually checked, and additional acoustic measurements were obtained automatically using specifically designed scripts. This indicates that some quality assurance involved automated verification through algorithmic techniques, supplemented by manual checking to ensure accuracy of the automated measurements. However, no further human annotation checking process is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated (scripted) measurement and checking process for the acoustic data, indicating some level of quality assurance."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8103,
      "completion_tokens": 365,
      "total_tokens": 8468
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper describes a new dataset constructed by the authors specifically for this study, comprising 30 real minimal or near-minimal utterances designed for the investigation of geminate types in Tripolitanian Libyan Arabic. These utterances were produced by four native human speakers in controlled recording sessions, indicating original data collection and human-generated content created entirely from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the data was created by translating existing data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of machine-translated data in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was newly recorded by the authors rather than collected or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived from existing datasets with modification; it is newly elicited data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and collection method are clearly documented."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8621,
      "completion_tokens": 258,
      "total_tokens": 8879
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Results and Section 4 Discussion and Conclusion",
          "reasoning": "The dataset compiled and analyzed in this study is used primarily to analyze acoustic characteristics and phonetic correlates of geminate and singleton consonants in Tripolitanian Libyan Arabic. The paper presents detailed statistical analyses of durational and non-durational correlates using the dataset, focusing on trends and patterns rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the use of a new dataset collected and analyzed specifically to examine phonetic properties of gemination, indicating practical usage beyond no-utility."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9344,
      "completion_tokens": 434,
      "total_tokens": 9778
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of utterances in Tripolitanian Libyan Arabic, a single language."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that exactly two languages are included; only one language, TLA, is documented."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English utterances; it is based on Tripolitanian Libyan Arabic."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (Speakers) and Section 2.2 (Stimuli and data recording)",
          "reasoning": "The new dataset introduced consists solely of utterances in Tripolitanian Libyan Arabic (TLA), a non-English language. This is confirmed by descriptions of native speakers of TLA and stimuli consisting of TLA words only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain programming or structured code content; only recorded spoken utterances in TLA are used."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any mathematical or formal logical expressions as part of the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human spoken language data only; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on a natural language dialect (Tripolitanian Libyan Arabic), not a constructed or fictional language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly identified and documented as Tripolitanian Libyan Arabic; thus, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of spoken utterances in TLA, which is a human language, so the dataset contains language."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6562,
      "completion_tokens": 179,
      "total_tokens": 6741
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not provide any links, URLs, or references to publicly available code repositories or software scripts used for data collection, preprocessing, or analysis. Although it mentions use of PRAAT and scripts, these appear to be custom or standard scripts not provided or linked in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method)",
          "reasoning": "The paper provides detailed documentation on dataset creation including speaker recruitment (Section 2.1), stimuli compilation and description (Section 2.2), and detailed recording procedure with sentence carrier and repetitions. It also describes data analysis and measurement procedures (Section 2.3), including annotation and acoustic measurements methods. Hence, the dataset creation and processing is transparently documented in the Method section."
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 3032,
      "completion_tokens": 182,
      "total_tokens": 3214
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Introduction; Section 2",
          "Reasoning": "The dataset RUSAVIC introduced by the authors contains audio recordings of 20 drivers in natural driving conditions (in-the-wild recordings). These audio recordings are captured via microphones during driving, implying direct human generation and recording of natural speech."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Introduction; Section 2",
          "Reasoning": "The RUSAVIC dataset also contains video recordings of the drivers, simultaneously recorded at 30 frames per second with resolution 1280\u00d7720 pixels as described in Section 2. The videos capture the driver's face, especially the mouth region during speech, and are recorded with human involvement (drivers speaking naturally in a vehicle)."
        }
      ]
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 3884,
      "completion_tokens": 208,
      "total_tokens": 4092
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2",
            "reasoning": "The RUSAVIC dataset comprises in-the-wild audio and video recordings of 20 drivers captured via an audio-visual signal acquisition module that records audio and video simultaneously under natural driving conditions. The annotation process involves automated steps such as voice activity detection and mouth region cropping without mention of human annotators performing labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not describe any detailed annotation instructions provided for human annotators since the data collection and labeling appear to be done via automated processes."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No mention is made of scoring rubrics or formal guidelines to assess annotation quality or label assignment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no description or inclusion of annotation examples in the paper related to the dataset annotation."
          }
        }
      ]
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 5014,
      "completion_tokens": 259,
      "total_tokens": 5273
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the RUSAVIC dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or indication that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single human non-expert conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic verification process for quality assurance of the dataset is reported in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process applied to the newly introduced RUSAVIC dataset or any other dataset introduced by the authors."
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 4632,
      "completion_tokens": 401,
      "total_tokens": 5033
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 3 Conclusions",
          "reasoning": "The paper states that the authors trained their models on the RUSAVIC dataset containing in-the-wild audio and video recordings of 20 drivers. The corpus was collected by the authors themselves as original content recorded in natural driving conditions, indicating data created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as being collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications, transformations, or adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origin and creation method of the new dataset (RUSAVIC), which was recorded by the authors, so the data source is documented."
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 5150,
      "completion_tokens": 208,
      "total_tokens": 5358
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 1 and 2",
          "reasoning": "The paper states that the authors trained audio and video models on their new RUSAVIC dataset, which contains in-the-wild recordings of 20 drivers. This indicates the dataset is used to train models from scratch for audio-visual speech recognition in a driving context."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 5873,
      "completion_tokens": 251,
      "total_tokens": 6124
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 2 DAVIS Architecture",
          "reasoning": "The RUSAVIC dataset used for training contains Russian language audio-visual speech data from 20 different drivers (Section 1). The dataset and system recognize the most frequent drivers' requests in Russian, indicating the dataset contains only one non-English human language \u2014 Russian."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 3091,
      "completion_tokens": 209,
      "total_tokens": 3300
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 Conclusions, footnote 1",
          "reasoning": "The paper states that the source code, dataset, and trained models are available 'by request' via a provided URL (https://mobiledrivesafely.com). However, there is no indication that all code related to data collection, preprocessing, and dataset generation is publicly available in a repository such as GitHub. Hence, the code is not openly accessible for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Introduction (Section 1) and Section 2 DAVIS Architecture",
          "reasoning": "The paper describes the creation of their RUSAVIC dataset and gives references to prior publications [6,7] that discuss the corpus. Additionally, Section 2 details audio and video recording specifications, signal acquisition, preprocessing steps such as mouth region extraction and spectrogram computation. This provides documentation on the dataset creation process and preprocessing, sufficient to understand the data characteristics and experimental setup."
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7023,
      "completion_tokens": 125,
      "total_tokens": 7148
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Svarah and Conclusion",
          "Reasoning": "Svarah is a new benchmark dataset introduced by the authors. It contains 9.6 hours of transcribed English audio recorded from 117 Indian speakers with diverse accents. The audio data was collected by having speakers record read speech, extempore speech, and utterances corresponding to everyday use cases using a mobile app. This process involves direct human participation and manual recording of voice samples, confirming the data is human-generated audio."
        }
      ]
    }
  },
  {
    "id": "javed23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7875,
      "completion_tokens": 211,
      "total_tokens": 8086
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3",
            "reasoning": "The paper states that undergraduate and postgraduate students who are fluent in English and work on various research projects in the institute performed transcription. These annotators verify quality and segment and transcribe data, indicating multiple human non-expert annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "Transcribers were instructed to verify audio quality and correctness of responses and to use the same transcription guidelines as used for the Switchboard corpus, indicating the existence of written instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No mention of scoring rubrics or explicit criteria for annotation quality is found in the paper; only instructions on transcription methodology are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not mention providing example annotations or explicit transcription examples in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "javed23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9005,
      "completion_tokens": 265,
      "total_tokens": 9270
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3 (Svarah), paragraph 'Transcription of voice samples'.",
          "reasoning": "The dataset transcription was performed by multiple undergraduate and postgraduate students who are fluent in English but not described as subject matter experts. They followed transcription guidelines but are not indicated to have expert status, implying QA by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8623,
      "completion_tokens": 427,
      "total_tokens": 9050
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Svarah",
          "reasoning": "The paper explicitly states that the Svarah dataset was collected by recruiting human speakers from diverse geographic locations across India who read, spoke extempore on given topics, and responded to prompts in various everyday use cases. The audio data was recorded directly from these speakers, and transcriptions were created by recruited human transcribers. This process indicates original data created entirely from scratch by human contributors, not derived, translated, or adapted from any existing dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not consist of translated content. The data collected is English speech from native speakers of regional Indian languages but is not translated from any other language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used in data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions collecting text prompts from Wikipedia for reading tasks, the audio data itself is original human speech recordings. The dataset is not merely collated from existing speech sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not described as modified or adapted from existing datasets; it was freshly recorded."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation method is clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9141,
      "completion_tokens": 303,
      "total_tokens": 9444
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Results & Discussions) and Section 6 (Conclusion)",
          "reasoning": "The Svarah dataset is introduced as a benchmark specifically designed for evaluating ASR systems for Indian accents. It is used in the paper to benchmark the performance of six open-source and two commercial ASR models, as detailed in Section 5. The authors emphasize that their dataset is intended for robust evaluation and highlight performance gaps across different models and accents."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Results & Discussions)",
          "reasoning": "The dataset is used not only for evaluation but also for analyzing trends and performance characteristics, such as comparing WERs across different models, accents, and speech types (read speech, extempore, use cases). The detailed analysis of model performance and accent-based variation indicates the dataset's utility for analytical purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9864,
      "completion_tokens": 500,
      "total_tokens": 10364
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains English speech from speakers who are native speakers of various Indian languages, but the speech content itself is only in English. The native languages are metadata about speakers, not languages present in the speech data entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain exactly two human languages; speech content is only in English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (Svarah), multiple paragraphs",
          "reasoning": "Svarah dataset contains only English speech data, as stated: it contains Indian-accent English data collected from speakers whose native languages are Indian languages, but all utterances are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is exclusively English speech, not monolingual in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only, with no biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly stated as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries do contain language (English)."
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7082,
      "completion_tokens": 147,
      "total_tokens": 7229
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Conclusion",
          "reasoning": "The paper mentions in both the abstract and conclusion that the benchmark as well as their code have been made publicly available. This indicates that the code related to the dataset creation and evaluation scripts is accessible, implying code availability for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Svarah)",
          "reasoning": "Section 3 of the paper provides detailed steps involved in creating the Svarah dataset, including recruitment of speakers, text and prompt collection, use case creation, data collection method via a mobile app, and transcription procedures. This comprehensive description serves as documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8466,
      "completion_tokens": 500,
      "total_tokens": 8966
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Dataset Construction; Section 4.1 Dataset details; Abstract",
          "Reasoning": "The CI-MOEI dataset is constructed by annotating text from the CMU MOSEI and IEMOCAP datasets, which are real speech-based multimodal emotion datasets with human-provided transcriptions. The annotation process involves human annotators identifying sentiment expressions in the textual modality, confirming that the text data is human generated. This is explicitly described in Section 2.2 and Section 4.1 and summarized in the Abstract."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Dataset Construction; Section 4.1 Dataset details; Abstract",
          "Reasoning": "The CI-MOEI dataset includes audio recordings from the CMU MOSEI and IEMOCAP datasets, which consist of real scenes and actor performances providing raw speech. This speech data is originally human recorded and not artificially synthesized. Hence, the audio modality of the CI-MOEI dataset is human generated as per the paper's description in Section 2.2 and Section 4.1, along with the Abstract."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Dataset Construction; Section 4.1 Dataset details; Abstract",
          "Reasoning": "The CIM-MOEI dataset is formed by augmenting the MPQA dataset's text data, which is human authored as a well-known opinion mining corpus. The textual modality remains human generated since it comes from MPQA sentences labeled with sentiment polarity, as described in Section 2.2 and 4.1 and in the Abstract."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Dataset Construction; Section 4.1 Dataset details; Abstract",
          "Reasoning": "The audio modality in the CIM-MOEI dataset is synthesized speech generated by Text-to-Speech (TTS) technology (Azure TTS with multiple speakers and emotional styles) based on the text from MPQA. Therefore, this audio data is model generated, as explicitly stated in Section 2.2 Dataset Construction and Section 4.1 Dataset details, and summarized in the Abstract."
        }
      ]
    }
  },
  {
    "id": "jia24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9318,
      "completion_tokens": 225,
      "total_tokens": 9543
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2",
            "reasoning": "The paper states that three university students majoring in English and one native English speaker served as annotators, with the native speaker making the final decision among the student annotators. This indicates that the annotations were performed by multiple human non-experts guided by a single native speaker, who is not explicitly labelled as an expert."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "The paper mentions establishing annotation guidelines aligned with MPQA for identifying sentiment expressions and their polarities, indicating that instructions were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "There is no mention of scoring rubrics or formalized scoring criteria in the annotation guidelines, only annotation guidelines inspired by MPQA."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not mention providing annotation examples within the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "jia24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10448,
      "completion_tokens": 372,
      "total_tokens": 10820
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators include three university students and one native English speaker as final reviewer, but there is no explicit indication that the native speaker is a subject matter expert or a member of the target demographic expert in OEI. Experts are consulted only for disputed items, so single human expert QA is not the primary process."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Dataset Construction",
          "reasoning": "The QA process involved three university students majoring in English providing annotations independently, with a native English speaker making final decisions, and in cases of uncertainty, an expert in OEI is consulted. Multiple annotators with relevant linguistic background and consultation with a subject matter expert indicate that multiple human experts performed QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotation was performed by multiple annotators, not a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotators were university students majoring in English and a native English speaker. Although not explicitly stated as subject matter experts initially, they have relevant background, and expert consultation was used, indicating they are treated as experts rather than non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that AI models were used to perform quality assurance on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description suggests that automatic or algorithmic verification was used as a QA step for annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a thorough quality assurance process involving multiple annotators with relevant expertise and expert adjudication, so QA is indeed present and described."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10066,
      "completion_tokens": 478,
      "total_tokens": 10544
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Dataset Construction",
          "reasoning": "The CI-MOEI dataset was constructed by recruiting human annotators (three university students and one native English speaker) to annotate existing multimodal emotion datasets (CMU MOSEI and IEMOCAP) according to opinion expression identification guidelines. This process involved manual annotation with human judgment and expert adjudication, producing original annotated data specific for the MOEI task."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entirely generated by AI or machine learning models without reference to existing data. Although synthesized speech was generated, it was from existing text sentences, not new content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by translating content via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation systems according to the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Selection",
          "reasoning": "The CI-MOEI dataset's source text and audio are derived from existing open-source datasets CMU MOSEI and IEMOCAP. The paper indicates using these datasets and annotating them, indicating collating existing sources as a base for their dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Dataset Construction",
          "reasoning": "The CIM-MOEI dataset is derived by applying Text-to-Speech (TTS) synthesis technology on existing OEI dataset MPQA sentences to create corresponding synthetic speech, thus modifying existing data by transformation and augmentation to create a multimodal dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The sources and methods of dataset generation are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10584,
      "completion_tokens": 496,
      "total_tokens": 11080
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly introduced datasets (CI-MOEI and CIM-MOEI) for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets were used to train models from randomly initialized parameters. The models are fine-tuned from pre-trained LLMs."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 4.4",
          "reasoning": "The paper explicitly describes fine-tuning pre-trained LLMs (e.g., Vicuna and LLaMA2) on the constructed datasets CI-MOEI and CIM-MOEI using supervised learning to improve OEI task performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence in the paper that the datasets are used for reinforcement learning post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.1 and 4.4",
          "reasoning": "The datasets are used not only for training but also for performance evaluation and benchmarking the proposed STOEI method and baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 (Fine-grain analysis of joint speech-text)",
          "reasoning": "The datasets enable analysis of model performance variations (e.g., sentence length effects) and the role of speech-text multimodality in OEI."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base for retrieval-augmented generation or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets have documented uses in fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11307,
      "completion_tokens": 492,
      "total_tokens": 11799
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets CI-MOEI and CIM-MOEI introduced in the paper are constructed from English datasets (CMU MOSEI, IEMOCAP, and MPQA) and contain only English content, with no mention of other languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced do not contain exactly two human languages; only English is used as per the datasets constructed and described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2, Dataset; Abstract; Conclusion",
          "reasoning": "The newly constructed datasets CI-MOEI and CIM-MOEI are based on English datasets CMU MOSEI, IEMOCAP, and MPQA, all of which consist of English speech and text. The paper explicitly mentions in Section 2 that these datasets are derived from English data, and Section 5 notes that experiments are conducted only on English datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are based exclusively on English language resources; no non-English languages are included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets pertain to multimodal opinion expression identification involving text and speech; no programming or structured code content is included or mentioned."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical formulas describing model architectures and losses, the datasets themselves do not contain mathematical or logical notation as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain human speech and text data; no biological sequences or non-human communication systems are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve English natural language data; there is no mention or use of fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets' language is explicitly stated as English, derived from known English datasets such as MPQA, CMU MOSEI, and IEMOCAP."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain human language data (English text and speech), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8525,
      "completion_tokens": 185,
      "total_tokens": 8710
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract; Conclusion",
          "reasoning": "The paper states in the abstract and conclusion that the authors will publicly release the code and datasets to facilitate future research, but it does not provide any actual links or explicit details confirming the public availability of the code at the time of writing. Hence, there is no evidence within the paper that the code used for dataset construction is currently publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Dataset)",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 2, including data selection strategies, annotation guidelines, annotator details, consensus procedures, and the process of using TTS to synthesize speech for MPQA. It describes the datasets CI-MOEI and CIM-MOEI construction steps clearly, enabling understanding and reproducibility of the dataset creation methodology."
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6927,
      "completion_tokens": 109,
      "total_tokens": 7036
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The dataset described is a dysarthric speech dataset constructed by the authors, consisting of Chinese speech data collected from 64 patients with dysarthria and 20 healthy participants, including about 31 hours and 18 hours respectively. The data collection tasks involved reading Chinese characters, words, and sentences, indicating that the audio data was recorded with human involvement."
        }
      ]
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7779,
      "completion_tokens": 201,
      "total_tokens": 7980
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1",
            "reasoning": "The dataset is constructed from Chinese speech data collected from patients and healthy participants following the approach in [23], which suggests expert involvement in data collection and annotation with speech and severity level labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for transcribing or labeling the new dysarthric speech dataset."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1",
            "reasoning": "Severity levels are assigned according to the Frenchay Dysarthria Assessment (FDA) scale with specific score thresholds defined, indicating a rubric for severity classification."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not provide examples of annotated data or transcription to illustrate annotation guidelines or practices for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8909,
      "completion_tokens": 330,
      "total_tokens": 9239
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being performed by a single human expert annotator for the new dysarthric speech dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit description in the paper indicating that multiple human experts performed quality assurance on the new dysarthric speech dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single non-expert human annotator conducted quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that multiple non-expert human annotators were involved in quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any AI model as a judge or quality assurance mechanism for dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic rule-based techniques for quality assurance of the dataset are described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new Chinese dysarthric speech dataset (Section 4.1) but does not provide any explicit details on quality assurance procedures for the dataset annotations or content. There is no mention of human annotators, expert involvement, multiple raters, AI-based adjudication, or automated verification steps for quality assurance. Therefore, no quality assurance process is documented for the dataset."
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8527,
      "completion_tokens": 374,
      "total_tokens": 8901
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 - Dataset",
          "reasoning": "The paper states that the authors constructed a dysarthric speech dataset comprising Chinese speech data collected from 64 patients with various degrees of dysarthria and 20 healthy participants. The collection involved reading tasks of Chinese characters, words, and sentences, indicating original content created entirely from human contributors and collected from scratch for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human translation involved in the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation involved in the dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper indicates the dataset was collected from patients and healthy participants for this study, not aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as constructed from original collection efforts rather than being derived or adapted from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin is clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9045,
      "completion_tokens": 305,
      "total_tokens": 9350
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The newly constructed Chinese dysarthric speech dataset is used to fine-tune the pre-trained Whisper model using LoRA as a supervised fine-tuning step to improve dysarthric speech recognition performance. This fine-tuning is explicitly described in Section 4.1 (Experimental Setup) and results are given in Section 4.2 (General Result Analysis)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The same dysarthric speech dataset is used to evaluate and benchmark model performance after adaptation, showing Character Error Rate reductions compared to baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "Further analysis is conducted on the dataset to study the correlation between the Perceiver-Prompt effectiveness and speaker discrimination or dysarthria severity levels, as detailed in Section 4.3."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9768,
      "completion_tokens": 558,
      "total_tokens": 10326
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset introduced consists exclusively of Chinese speech data from dysarthric patients and healthy participants. There is no mention of multiple languages or multilingual content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset contains only Chinese speech data. No data from exactly two human languages is included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The data is explicitly Chinese speech and not English. The paper focuses on Chinese dysarthric speech recognition."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The new dataset is constructed for Chinese dysarthric speech recognition (Section 4.1). The dataset comprises only Chinese speech from patients and healthy speakers, explicitly stated and used for experiments."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No mention of code-related dataset",
          "reasoning": "The paper does not introduce any dataset containing programming or structured code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No dataset with mathematical or logical notation introduced",
          "reasoning": "While some mathematical notation is used to explain methods, no datasets contain such content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No biological or non-human communication datasets mentioned",
          "reasoning": "The dataset introduced focuses on human speech; no mention of biological sequences or animal communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No constructed language dataset mentioned",
          "reasoning": "No references to fictional or constructed languages in the new dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Language is documented in Section 4.1 Dataset",
          "reasoning": "The language used in the dataset is clearly indicated as Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken language data, so it is not language-absent."
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6986,
      "completion_tokens": 168,
      "total_tokens": 7154
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 Experimental Setup - Dataset",
          "reasoning": "The paper describes the constructed Chinese dysarthric speech dataset including the number of participants, hours of speech, collection tasks, and severity levels, but does not provide any links, references, or indications that code used to build, preprocess, or manage the dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experimental Setup - Dataset",
          "reasoning": "The dataset creation process is described in Section 4.1 with details of number of patients and controls, amount of data collected, types of speech tasks, severity level assessments, training and test splits. This provides documentation of the dataset construction process, though no further protocol details or collection scripts are included."
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8898,
      "completion_tokens": 254,
      "total_tokens": 9152
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction; Section 2 Data Simulation",
          "Reasoning": "The LibriheavyMix dataset introduced in this paper is a 20,000-hour synthesized corpus for multi-talker overlapping speech scenarios with reverberation. The audio data is based on Libriheavy, which contains human-recorded speech audio. However, the multi-talker overlapping reverberant audio is simulated programmatically by mixing source utterances and applying reverberation to simulate real-world far-field conditions, as described in Section 2. Thus, the audio data modality is both human generated (original speech recordings) and model generated (simulated mixtures and reverberation)."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction",
          "Reasoning": "The dataset includes transcripts with punctuation, casing, and text context, inherited from the Libriheavy ASR corpus. These transcripts are produced by humans as part of the original dataset. There is no indication that the transcripts were generated or modified by models for the new dataset; therefore, the text data modality is human generated only."
        }
      ]
    }
  },
  {
    "id": "jin24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9750,
      "completion_tokens": 212,
      "total_tokens": 9962
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2",
            "reasoning": "The paper describes the creation of LibriheavyMix as a simulated dataset generated by algorithmic processes (see Algorithm 1 in Section 2), using distributions derived from target sessions to simulate overlapping speech, pauses, and reverberation without mention of human annotators manually labeling or transcribing data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "There is no mention of annotation instructions provided for human annotators since the dataset is synthesized automatically via simulation algorithms without manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "No evaluation or scoring rubrics for annotation are provided in the context of dataset generation since no manual annotation process is involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "No examples are provided for annotation guidelines as the dataset is produced by automatic simulation methods, not manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "jin24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10880,
      "completion_tokens": 290,
      "total_tokens": 11170
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence of multiple human experts performing quality assurance in the dataset creation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human annotator conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert humans performing any quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using AI models to perform quality assurance for the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 Data Simulation",
          "reasoning": "The dataset is created by algorithmic data simulation methods involving sampling from statistical distributions and mixing of speech signals. The process is programmatic and involves automated procedures to generate overlapped speech with reverberation, which implies an automated verification or generation mechanism rather than manual annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes an automated data simulation process to construct the dataset, therefore quality assurance is performed through automatic procedures rather than being absent."
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10498,
      "completion_tokens": 408,
      "total_tokens": 10906
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as original recordings created entirely from scratch by human contributors; rather, it is based on existing speech data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the dataset content is generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any human translation process in the creation of this dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve machine translation of content as described in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset is large-scale and involves existing speech segments, it is not merely collected or aggregated without modifications; the paper details a simulation process."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2: Data Simulation",
          "reasoning": "The LibriheavyMix dataset is derived from the existing Libriheavy corpus by simulating overlapped speech using distributions extracted from target sessions, adding reverberation for far-field scenarios, and creating mixtures with multiple speaker turns. Thus, it is based on existing data with transformations and adaptations applied as described in the data simulation methodology."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation method is clearly documented in the paper, particularly in Section 2, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11016,
      "completion_tokens": 232,
      "total_tokens": 11248
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The paper presents baseline experiments on speech separation and multi-talker ASR conducted by training models on the proposed LibriheavyMix dataset, indicating usage for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The dataset is used as a benchmark to evaluate the performance of speech separation, multi-talker ASR, and speaker diarization models by reporting metrics such as word error rate and diarization error rate."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11739,
      "completion_tokens": 410,
      "total_tokens": 12149
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced, LibriheavyMix, is based on Libriheavy which is an ASR corpus of English audiobooks. No evidence in the paper states that the data contains multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of exactly two languages in the dataset; the dataset is derived from English audiobook data only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Introduction and Section 2 - Data Simulation",
          "reasoning": "LibriheavyMix is constructed based on Libriheavy, which is an ASR corpus derived from English audiobooks (LibriSpeech and related). The dataset contains English speech and transcripts with punctuation and casing. There is no mention of other languages included."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of any monolingual non-English data in the dataset; all data is English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries contain speech and transcripts, no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although mathematical notations appear in method descriptions and algorithms, they are not part of dataset entries but are part of methodological presentation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are mentioned in the dataset description."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly specified as English; not undocumented or unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken language (English); hence this is not applicable."
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8957,
      "completion_tokens": 128,
      "total_tokens": 9085
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link provided",
          "reasoning": "The paper discusses the dataset creation methods and simulation process in Section 2, but does not provide any links or references to publicly available code repositories for the dataset generation or related code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data Simulation)",
          "reasoning": "The paper details the data simulation process in Section 2, providing algorithms and explanations on how overlapped speech is simulated, including relevant distributions and sampling strategies. This documentation of dataset creation is transparent and detailed within the paper."
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8372,
      "completion_tokens": 277,
      "total_tokens": 8649
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Training Dataset",
          "Reasoning": "The MSP-Podcast Release 1.9 dataset is used as the training dataset for ParaCLAP. It consists of natural English speeches extracted from podcast recordings, totaling over 110 hours of human-recorded speech across 55,283 utterances from over 1,200 speakers. This dataset is introduced as the source of real human-generated audio data used to create training pairs for the new ParaCLAP model."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Query generation, Section 3.1 Training Dataset",
          "Reasoning": "The text queries paired with the MSP-Podcast audio data are generated by the authors using a novel templating method. These textual queries are constructed based on dataset labels (emotion categories, gender, and binned dimensional attributes) and expert acoustic features (e.g., pitch, intensity, jitter, shimmer, duration) extracted from audio samples. These features and labels are converted into natural language sentences using predefined templates and then combined (e.g., using 'and'). Thus, the text data for training is algorithmically generated by the authors rather than sourced from human-generated text."
        }
      ]
    }
  },
  {
    "id": "jing24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9224,
      "completion_tokens": 186,
      "total_tokens": 9410
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "The MSP-Podcast dataset used for training is annotated using crowdsourcing, implying multiple human non-expert annotators contributed to the emotional labels."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The dataset was annotated via crowdsourcing which typically requires detailed instructions to guide non-expert annotators towards consistent labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any scoring rubrics or formal scales used to guide annotation, only majority agreements are mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No annotation examples or illustrations are provided or referenced within the paper for the corpus annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "jing24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10354,
      "completion_tokens": 365,
      "total_tokens": 10719
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert annotator on the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of multiple human experts performing quality assurance on the annotations of the new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention a single human non-expert annotator performing quality assurance on the annotations for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the MSP-podcast dataset used for training is annotated via crowd-sourcing, the paper does not specify any quality assurance by multiple non-expert human annotators conducted or reported by the authors themselves for the new dataset creation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any AI model being used as a judge or for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated code or formula verification as a QA step for the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses the MSP-podcast dataset, which was annotated via crowd-sourcing by others, but does not describe any quality assurance process performed or documented by the authors on these annotations or on any new annotations. The new dataset (the constructed (audio, query) pairs) is generated via templating based on existing labels and extracted expert features, with no additional QA process detailed. Thus, no QA process is applied or documented by the authors for the new dataset construction."
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9972,
      "completion_tokens": 459,
      "total_tokens": 10431
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MSP-podcast dataset used to train the ParaCLAP model is an existing dataset extracted from podcasts and annotated via crowdsourcing. The paper does not indicate that the authors created any new dataset entirely from scratch by human contributors themselves."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper suggests that any new data was generated entirely by AI or model generative processes without reference to existing data. The authors generate text queries based on templates and existing labels/features rather than generating original data samples from scratch via models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation or translation processes."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 (Training Dataset)",
          "reasoning": "The MSP-podcast dataset is an existing, publicly available dataset composed of natural English speech from podcasts. It was collected and annotated previously; thus, it is collated data that the authors used for training."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 (Query generation)",
          "reasoning": "The authors create new text queries for training by deriving them from existing dataset labels and expert acoustic features (eGeMAPS features) extracted from the MSP-podcast utterances. They generate templated sentences that combine these labels and features into textual prompts, thus modifying and adapting existing data to form new training pairs."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation method are specified clearly."
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10490,
      "completion_tokens": 331,
      "total_tokens": 10821
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate training models from randomly initialized parameters using the new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.3",
          "reasoning": "The MSP-Podcast dataset, constructed with novel audio-text pairs via query templates, is used for training the ParaCLAP model in a supervised manner with contrastive learning, fine-tuning both audio and text encoders along with projection layers, as detailed in Sections 3.1 and 3.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training methods such as RLHF using the new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not used exclusively for evaluation or benchmarking; rather, it is used for training as well."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analyzing trends or characteristics but for training the model."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used or described as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11213,
      "completion_tokens": 632,
      "total_tokens": 11845
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Training Dataset and Section 3.2 Test Datasets",
          "reasoning": "The new dataset introduced for training, MSP-Podcast, contains only English speech data, and the queries generated are in English. Although test datasets include some German datasets, these are used for evaluation only and are pre-existing datasets, not newly introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Training Dataset and Section 3.2 Test Datasets",
          "reasoning": "The proposed training dataset MSP-Podcast is monolingual English and does not contain two human languages. The test datasets include German language data, but these are pre-existing, not newly introduced datasets."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Training Dataset: 'MSP-podcast Release 1.9 [13] is utilised to train our ParaCLAP models. The MSP-podcast data comprises natural English speeches ...'",
          "reasoning": "The newly introduced dataset for training (MSP-Podcast) consists entirely of natural English speech, and the generated text queries are all in English, indicating monolingual English dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly introduced dataset includes only non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain any programming or structured code-related content. The text queries are natural language sentences related to paralinguistic features."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation in methodology sections, the dataset entries themselves do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human speech audio and natural language queries; it does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language (English text queries and spoken English audio)."
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8431,
      "completion_tokens": 229,
      "total_tokens": 8660
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 1",
          "reasoning": "The paper states that their code and resources are publicly available at https://github.com/KeiKinn/ParaCLAP, but this repository contains code and resources related to the ParaCLAP model training and evaluation. However, they do not introduce any new datasets; instead, they use an existing dataset (MSP-Podcast) for training and various existing datasets for evaluation. They create text queries for this existing dataset but do not present new data collected or constructed as a dataset. Therefore, there is no code associated with new dataset construction or generation described or linked in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not describe or document the creation of any new dataset; it uses the MSP-Podcast dataset (which is publicly available) for training. Their method involves generating text queries from existing labels and expert features, but this is part of data preprocessing for training rather than a new dataset creation. Therefore, no documentation of new dataset creation is present."
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7586,
      "completion_tokens": 138,
      "total_tokens": 7724
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and 3.3",
          "Reasoning": "The paper explicitly states the SASV protocols use the ASVspoof 2019 LA database, which includes bona fide utterances recorded from 107 human speakers (Human Generated) and spoofed utterances generated via 19 state-of-the-art voice conversion (VC), text-to-speech (TTS), and hybrid TTS-VC algorithms (Model Generated), thus the new SASV challenge data is based on this audio data with both human speech and model-generated spoofing."
        }
      ]
    }
  },
  {
    "id": "jung22c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8438,
      "completion_tokens": 269,
      "total_tokens": 8707
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Sections 3.3 and 3.4",
            "reasoning": "The SASV protocols and evaluation metrics are based on well-defined, disjoint subsets of ASVspoof 2019 LA data and are applied automatically to trials (target, bona fide non-target, spoofed non-target) without manual human annotation. The paper describes protocols (Section 3.3) and metrics (Section 3.4) but does not mention any manual annotation process for labeling dataset trials."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "The paper does not describe any manual annotation tasks or instructions given to human annotators related to labeling the new SASV protocols or trials, as labels derive from existing ASVspoof dataset partitions and their metadata."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "There is no mention of scoring rubrics or standards for human annotators since no manual scoring or labeling was involved for the new SASV protocols or challenge data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable",
            "reasoning": "No example annotations or labeling examples are provided since the SASV protocols are derived automatically and no manual annotation was performed or required."
          }
        }
      ]
    }
  },
  {
    "id": "jung22c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9568,
      "completion_tokens": 299,
      "total_tokens": 9867
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any manual annotation or quality assurance performed by a single human expert for the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation of quality assurance conducted by multiple human experts in the paper for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any involvement of a single non-expert human annotator for quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert human annotators performing quality assurance on the introduced datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by AI models acting as judges for the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that automated verification algorithms or rule-based techniques were used for quality assurance of the datasets in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces or utilizes datasets but does not describe any quality assurance process applied to validate dataset annotations or content. Specifically, for the SASV protocols and datasets built upon ASVspoof 2019 LA and VoxCeleb2, no explicit QA process is documented in the paper."
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9186,
      "completion_tokens": 470,
      "total_tokens": 9656
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper utilizes existing public databases (VoxCeleb2 and ASVspoof 2019) and does not introduce any new original data created entirely from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses spoofed utterances generated by various VC, TTS, and hybrid algorithms in the ASVspoof database, no new data generated entirely by AI or ML models without reference to existing data is introduced by the authors themselves."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by human translation of content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The SASV challenge data uses publicly available VoxCeleb2 and ASVspoof 2019 LA databases, which are existing datasets collected from online videos and speech corpora. The SASV protocols collate these existing data sources without significant modification for the challenge."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The SASV protocols derive a new trial structure combining bona fide target, bona fide non-target, and spoofed non-target trials from the ASVspoof 2019 LA data. They adapt the existing data for a new integrated evaluation scenario (spoofing-aware speaker verification)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origins and sources of the data used, including how the data is derived or collated. Thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9704,
      "completion_tokens": 277,
      "total_tokens": 9981
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Baselines",
          "reasoning": "The ASVspoof 2019 LA database is used to train the DNN back-end fusion model (B2 baseline) using supervised learning methods as described in Section 4.2, where the model is trained using the training partition of the ASVspoof 2019 LA dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 SASV protocols and Section 5 Challenge results",
          "reasoning": "The SASV protocols are designed for evaluation involving target, bona fide non-target, and spoofed non-target trials. The dataset is used exclusively for evaluation and benchmarking of SASV solutions as detailed in Sections 3.3 and 5."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10427,
      "completion_tokens": 610,
      "total_tokens": 11037
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "The dataset is generated from the VCTK source database, which consists of speech data from 107 speakers, but all content is in English. The paper does not mention multiple languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "There is no indication that the dataset includes entries in exactly two human languages. Only English is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "The ASVspoof 2019 LA dataset is generated from the VCTK database, which is an English multi-speaker corpus. The dataset consists of speech data in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "No mention of any non-English language; the dataset is English only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section describing new datasets containing code",
          "reasoning": "The paper describes dataset for speech and spoofing detection but does not include any programming or structured code content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No mention in the dataset description sections",
          "reasoning": "The dataset consists of speech utterances and labels. No mathematical or logical symbolic notation is part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No section discussing any such data",
          "reasoning": "Datasets contain human speech data only, no biological sequences or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No section regarding fictional or constructed languages",
          "reasoning": "Dataset entries are natural speech only; no fictional languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "Languages in the dataset are explicitly known and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes human language speech data, so it is not the case that there is no language data."
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7645,
      "completion_tokens": 228,
      "total_tokens": 7873
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section explicitly mentions code availability for dataset construction",
          "reasoning": "The paper discusses datasets used in the challenge, such as VoxCeleb2 and ASVspoof 2019 LA, both publicly available existing datasets. These datasets were not introduced or constructed by the authors themselves. The paper provides open source baseline models and systems but does not mention any code or scripts related to dataset construction or generation being released. Therefore, the code used for constructing any dataset is not provided or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and throughout the paper",
          "reasoning": "Although the authors did not introduce new datasets, the paper describes in detail the characteristics of the utilized datasets (VoxCeleb2 and ASVspoof 2019 LA) in Section 3, including their sources, composition, and partitions. It also details the protocols derived for SASV evaluation based on these datasets. This constitutes clear documentation of dataset usage and creation process for the protocols, contributing to reproducibility of experiments using these datasets with the described protocols."
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7511,
      "completion_tokens": 125,
      "total_tokens": 7636
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 SpeechEQ Dataset",
          "Reasoning": "The SpeechEQ Dataset (SEQD) introduced in the paper is a new Mandarin SER dataset consisting of 2.3 hours of speech audio recorded from 20 human speakers using a Huawei phone in a controlled environment. The dataset is explicitly described as consisting of audio clips recorded by humans and judged by human judges ensuring the emotion matches the utterance. Therefore, the data modality is audio, and the origin is human-generated recordings."
        }
      ]
    }
  },
  {
    "id": "kang22d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8363,
      "completion_tokens": 233,
      "total_tokens": 8596
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2",
            "reasoning": "The annotation of the SpeechEQ Dataset (SEQD) involved each utterance being judged independently by three judges, indicating multiple human experts performed the annotation to ensure quality and agreement."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Speakers independently wrote sentences to express specific emotions, and the process included instructions such as the style of speaking (talking to someone in everyday conversation), and the judges also had criteria to judge the emotion correctness before accepting the recording."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Annotation involved judges independently evaluating the recorded speech emotion accuracy, requiring all three judges to agree on the emotion label before acceptance, indicating presence of scoring or acceptance rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not mention providing specific examples or sample annotations in the annotation guidelines for the annotators or judges for the SpeechEQ Dataset."
          }
        }
      ]
    }
  },
  {
    "id": "kang22d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9493,
      "completion_tokens": 319,
      "total_tokens": 9812
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human expert performed quality assurance on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "Section 2.2 SpeechEQ Dataset",
          "reasoning": "The annotations were done by three judges independently for each utterance, but there is no information confirming these judges as subject matter experts or members of the target demographic, so they cannot be assumed to be experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of a single non-expert annotator performing QA."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 SpeechEQ Dataset",
          "reasoning": "Each utterance was judged independently by three judges, and if more than one judges the emotion as inaccurate, the speaker was requested to re-record. There is no information indicating that these judges are experts, thus they are considered multiple human non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic quality assurance process is described for the dataset annotation quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented for the SpeechEQ dataset which involves multiple judges reviewing each utterance; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9111,
      "completion_tokens": 374,
      "total_tokens": 9485
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 SpeechEQ Dataset",
          "reasoning": "The SpeechEQ dataset was created by human speakers who independently wrote sentences describing emotions for each of the 25 emotions in the SpeechEQ Metric and performed recordings in everyday conversational tones. Each utterance was judged independently by three human judges for accuracy, and re-recorded if the emotion was not correctly reflected, indicating original data creation by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translation using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of data generated through machine translation is provided."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SpeechEQ Dataset is newly constructed; it is not described as being collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset does not appear to be derived or adapted from existing datasets; rather, it is newly created."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly specified and documented."
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9629,
      "completion_tokens": 285,
      "total_tokens": 9914
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3.3",
          "reasoning": "The SpeechEQ Dataset (SEQD), a new Mandarin SER dataset constructed by the authors, is used to train models from scratch within a multitask learning framework. This is shown in Section 2.2 where the dataset construction is described, and in Section 3.3 where ablation studies and experimental results demonstrate training on this dataset from the ground up to improve emotion recognition performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The SpeechEQ Dataset is also used for evaluation as part of the experiments presented in Section 3.3, where the model's performance on this dataset is measured and compared under different configurations, demonstrating its use for benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10352,
      "completion_tokens": 566,
      "total_tokens": 10918
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The SpeechEQ Dataset (SEQD) introduced by the authors is a Mandarin SER dataset only, with 1648 audio clips from 20 Mandarin-speaking speakers, indicating only one language."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The SEQD is explicitly described as Mandarin only. Although the paper performs experiments on English datasets such as IEMOCAP, these are not newly introduced datasets by the authors."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The only new dataset introduced by the authors is the SEQD, which is in Mandarin; English datasets mentioned are pre-existing."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The newly introduced SpeechEQ Dataset (SEQD) consists exclusively of Mandarin language speech clips, confirming it is monolingual and non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech audio clips; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are speech audio clips labeled with emotion classes and intensities, not containing math or logic notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech for emotion recognition, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No indication of fictional or artificial languages in the dataset; data is natural Mandarin speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the SpeechEQ Dataset is clearly stated as Mandarin; it is not unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human spoken language, so it cannot be classified as having no language."
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7570,
      "completion_tokens": 122,
      "total_tokens": 7692
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided",
          "reasoning": "The paper does not mention any link to code repositories or provide information about the availability of code related to the SpeechEQ dataset or its construction process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2",
          "reasoning": "The paper provides detailed documentation on the construction of the SpeechEQ Dataset in Section 2.2, describing its size, recording conditions, speaker information, labeling process involving three judges, and emotion categories and intensities covered by the dataset."
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7200,
      "completion_tokens": 358,
      "total_tokens": 7558
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 Recording Environment",
          "Reasoning": "The corpus includes recorded speech of subjects and teleoperators during persuasive dialogues using a teleoperated android, collected via speech microphones, involving human audio recordings."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 Recording Environment",
          "Reasoning": "The corpus contains frontal face images captured by a webcam of the human dialogue participants during the persuasive dialogues, thus these images were captured via human-operated devices."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 Recording Environment and Section 3.3 Action Unit",
          "Reasoning": "The corpus includes facial features in the form of Action Units (AUs) extracted automatically from images of participants' faces recorded by human-operated cameras; the AU signals represent sensor data derived from human behavior recordings."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract and Section 3. Corpus Annotation",
          "Reasoning": "Transcriptions of the recorded speech were manually transcribed and annotated with dialogue-act labels by trained annotators, indicating human-generated textual data."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.5 Questionnaires to Subjects and Section 2.6 Follow-up Survey",
          "Reasoning": "Structured questionnaire data were collected from participants regarding their personality, awareness, attitudes, and behavioral changes, which are represented as tabular data originating from direct human input."
        }
      ]
    }
  },
  {
    "id": "kawano22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8052,
      "completion_tokens": 218,
      "total_tokens": 8270
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.4, Section 3.2",
            "reasoning": "The paper states that one trained annotator transcribed and annotated the recorded speech with dialogue-act labels based on the ISO-24617-2 dialogue-act tag standard, indicating a single expert annotator performed this task."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The annotator used the extended ISO-24617-2 Dialogue-Act tag standard, implying detailed annotation instructions and guidelines were followed for consistent labeling."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "Using the ISO-24617-2 standard as basis provides a structured rubric for dialogue-act annotation, suggesting scoring rubrics or label definitions were provided and utilized."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention providing specific annotation examples or exemplars within guidelines for the dialogue-act annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "kawano22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9182,
      "completion_tokens": 268,
      "total_tokens": 9450
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.4 and Section 3.2",
          "reasoning": "A single trained annotator transcribed and annotated the recorded speech with dialogue-act tags based on the extended ISO-24617-2 dialogue-act tag standard (Section 2.4, 3.2). There is no indication of multiple annotators or expertise beyond 'trained annotator,' so it is likely a single human expert performed quality assurance on annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.4 and 3.3",
          "reasoning": "The facial action units (AUs) were automatically annotated using the open-source library PyFeat to extract facial features (Section 2.4, 3.3). This represents automated annotation, which can be regarded as automated quality assurance of these features."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8800,
      "completion_tokens": 437,
      "total_tokens": 9237
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Data Collection of Persuasive Dialogues), Section 2.2 (Instructions to Subjects)",
          "reasoning": "The dataset consists of dialogues recorded from interactions between human subjects and a teleoperated android robot (controlled by human operators). The speech and facial expressions were collected from real human participants engaging in persuasive dialogues, and accompanying questionnaires provide user personality and behavioral data. The data was generated explicitly for this study, involving human participants creating original dialogue and multimodal behavior data during experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by AI or machine learning models; the dialogues were produced through human participants interacting via teleoperation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any translation by humans was performed to produce the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used in generating the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as aggregated from existing sources but rather as newly collected experimental data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some annotations like action units and dialogue-act tags were derived from the recorded data, the underlying data (dialogues and multimodal recordings) are original and not derived from prior datasets. The dataset itself is not derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are clearly documented in the paper; therefore, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9318,
      "completion_tokens": 219,
      "total_tokens": 9537
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 - Corpus Analysis",
          "reasoning": "The dataset is extensively used to analyze trends and factors affecting persuasion success through various feature analyses (e.g., personality traits, impressions, emotions, action units, dialogue-acts) and training classifiers to predict persuasion outcomes. The primary usage described is to understand and analyze patterns in persuasive dialogues rather than to train or evaluate models directly."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10041,
      "completion_tokens": 644,
      "total_tokens": 10685
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The corpus is described as collected with dialogues involving Japanese participants and is not stated to contain multiple languages. No mention is made of multiple languages being involved or transcribed."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "There is no evidence or statement in the paper that indicates the data includes exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.2 (Instructions to Subjects), Section 3 (Corpus Annotation)",
          "reasoning": "The paper does not indicate the use of English language for the corpus. Instead, the corpus appears to be Japanese-based dialogue, with references to annotation with the Japanese dialogue-act standard and facial AU extraction for Japanese participants."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (Instructions to Subjects), Section 3.2 (Dialogue-Act)",
          "reasoning": "The corpus consists of persuasive dialogues collected from 60 subjects in Japan, with annotation according to the extended ISO-24617-2 dialogue-act tag standard applied to Japanese. The presence of Japanese is strongly implied by references to Japanese dialogue corpus and settings, making the dataset monolingual and non-English (Japanese)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset does not contain entries of programming or structured code content as described. Although machine learning models were trained on features, the dataset itself is dialogue and audio-visual data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No indication that the dataset entries contain formal mathematical or logical symbols. The features used are linguistic and multimodal but do not include symbolic math expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset includes human speech and facial action units but no biological sequences or non-human communication such as animal signals or DNA."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The language of the dataset is well documented as Japanese (non-English)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (dialogue transcription and annotations), so it is not language-free."
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7259,
      "completion_tokens": 156,
      "total_tokens": 7415
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not mention or provide any URL or repository link for code related to data collection, preprocessing, or generation. No indication is given that code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 Data Collection of Persuasive Dialogues and 3 Corpus Annotation",
          "reasoning": "The paper documents the dataset creation process in detail, including data collection methods (persuasion domains, instructions to subjects and teleoperators, recording environment), data annotation (dialogue-act labels, facial features), and questionnaires. These parts describe how the multimodal persuasive dialogue corpus was constructed and annotated, revealing transparency and completeness in the documentation."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 16247,
      "completion_tokens": 108,
      "total_tokens": 16355
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction",
          "Reasoning": "The paper introduces a new benchmark setting named \"split GSC,\" which is described as a subset of Google Speech Commands dataset version 2 (GSC v2). This dataset consists of audio recordings of spoken keywords, which are human-generated recordings. The paper explicitly states that split GSC is introduced as a benchmark for few-shot open-set keyword spotting."
        }
      ]
    }
  },
  {
    "id": "kim22h_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 17099,
      "completion_tokens": 243,
      "total_tokens": 17342
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 1 Introduction, Section 3 Method",
            "reasoning": "The new dataset introduced is the split GSC, a new benchmark setting that is a subset of the existing GSC ver2 dataset. The data preparation involves splitting GSC categories into known and unknown classes for few-shot open-set keyword spotting episodes. There is no indication of human annotation or manual labeling; rather, the data is prepared by algorithmic partitioning and episode construction based on existing labeled data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "The paper does not mention providing annotation instructions for human annotators, as the dataset is derived by splitting an existing labeled dataset rather than collecting new annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "No scoring rubrics or annotation quality control procedures are described since no manual annotation or subjective labeling was involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "No annotation guideline examples are provided as the dataset construction is automatic and based on existing labels from GSC ver2."
          }
        }
      ]
    }
  },
  {
    "id": "kim22h_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 18229,
      "completion_tokens": 295,
      "total_tokens": 18524
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for the new dataset or its annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human expert annotators performing quality assurance for the new dataset introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human annotator conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert human annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based quality assurance process applied to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new benchmark setting named split GSC, a subset of Google Speech Commands dataset ver2, but does not describe any quality assurance process performed on this new split or the data. The paper focuses on methodology and evaluation metrics, with no mention of annotation validation or QA procedures."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 17847,
      "completion_tokens": 416,
      "total_tokens": 18263
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of new datasets entirely generated by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset introduced was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any dataset created by human translation of content from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data used or produced in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 1 Introduction, Section 3 Method",
          "reasoning": "The authors introduce a new benchmark setting named 'split GSC,' which is described as a subset of the Google Speech Commands (GSC) version 2 dataset. This indicates that the new dataset is collated from an existing source without significant original data creation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the new dataset is a subset (split) of GSC ver2, the paper does not detail any modifications, transformations, or adaptations applied to the original data beyond selection, so it is not considered 'derived' per the rubric."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origin of the new dataset (split GSC) clearly from the existing GSC ver2, so the data source is documented."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 18365,
      "completion_tokens": 456,
      "total_tokens": 18821
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 and various experimental sections",
          "reasoning": "The split GSC dataset, newly introduced as a benchmark setting in this paper, is used to fine-tune and train few-shot open-set keyword spotting models, including the proposed Dummy Prototypical Networks (D-ProtoNets). The dataset is utilized in supervised learning settings where labeled support and query samples are provided to train models for classification and open-set detection."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning methods or usage of the dataset for RL-based post-training."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Abstract, Section 3, Experimental results",
          "reasoning": "The split GSC benchmark is used for evaluation and benchmarking of the proposed method against baselines for few-shot open-set keyword spotting. It serves to measure the performance, including detection rates and AUROC metrics, of models on unseen keywords and open-set classes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No explicit usage of the new dataset solely for analytical or trend analysis purposes is described."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base for retrieval augmentation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The split GSC dataset is explicitly used for both supervised fine-tuning and evaluation, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 19088,
      "completion_tokens": 556,
      "total_tokens": 19644
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset, split GSC, is a subset of Google Speech Commands (GSC) version 2 and is designed for keyword spotting in English only; there is no indication of multiple human languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset containing exactly two human languages; it focuses on English keyword spotting."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction",
          "reasoning": "The introduced dataset, split GSC, is a subset of the Google speech commands dataset (GSC) which contains English language keywords. The paper focuses on keyword spotting for English keywords such as 'Hey, Google' and 'Hey, Siri', indicating the dataset entries are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the dataset entries are in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio utterances for keyword spotting; there is no mention of programming or structured code being part of the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions to describe the method, these do not pertain to the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio samples (spoken keywords) and does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the dataset includes artificial or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as English; it is not unspecified or unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language audio and therefore includes entries with language."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 16306,
      "completion_tokens": 219,
      "total_tokens": 16525
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper about code availability for dataset creation.",
          "reasoning": "The paper does not include any link or reference to code repositories related to the construction or processing of any new dataset introduced. There is no indication that the authors have made dataset-related code publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 1 and Section 3 (Method), especially around the introduction of split GSC benchmark setting.",
          "reasoning": "The paper introduces a new benchmark setting named split GSC, which is a subset of the Google Speech Commands (GSC) version 2 dataset, for few-shot open-set keyword spotting. The process and rationale for the split and the construction of episodes for FSOSR are described in the text, particularly focusing on how the known and unseen classes are separated for training and evaluation. Although the dataset is derived from an existing one (GSC ver2), the paper documents the newly established split and benchmark formulation clearly to enable reproducibility of the experimental setting."
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7103,
      "completion_tokens": 353,
      "total_tokens": 7456
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.2",
          "Reasoning": "The paper introduces new datasets of user annotations of ASR hypotheses, where annotators listen to audio recordings and provide quality judgments or side-by-side hypothesis choices. This audio data is captured and annotated with human involvement, as explicit in Section 3.1 (HypRating) and 3.2 (HypChoice)."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.2",
          "Reasoning": "The ASR output hypotheses and reference transcriptions used in the user annotation datasets are textual data corresponding to recognized speech and manual references. These textual transcriptions are human-generated or derived from human-created references as indicated by the user rating and choice studies described in Sections 3.1 and 3.2."
        },
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Table 1 and Table 3",
          "Reasoning": "The paper includes tabular data summarizing the correlation coefficients and model performance metrics, but this is a presentation format rather than a dataset introduced by the authors. No new tabular datasets with uncertain provenance are explicitly introduced; thus, no new tabular dataset with known origin is introduced."
        }
      ]
    }
  },
  {
    "id": "kim22p_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7955,
      "completion_tokens": 278,
      "total_tokens": 8233
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "Section 3.1 describes HypRating with 73k user-ratings where annotators listened to audio and rated hypotheses; Section 3.2 describes HypChoice with 38k user annotations choosing better hypotheses. The annotators listened to audio and provided judgments, indicating human annotators presumably recruited for user evaluation, fitting 'Multiple Human Non-Experts' rather than experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Annotators were given a rating scheme with four levels ('exact match', 'useful hyp', 'wrong hyp', 'nonsense hyp') with definitions such as 'useful hyp' being errors that do not hurt downstream tasks, implying instructions were provided to guide annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The four rating levels correspond to integer scores (0 to 3), quantifying the annotation scale, which acts as a rubric for consistent scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not mention providing annotation examples to annotators or in appendices for these user judgment tasks."
          }
        }
      ]
    }
  },
  {
    "id": "kim22p_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9085,
      "completion_tokens": 360,
      "total_tokens": 9445
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single human expert conducted quality assurance for the user annotation datasets. Annotators are described as users but their expertise is not detailed."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the data. The annotators are described simply as 'users' without mention of professional or domain expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single non-expert human annotator specifically."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The datasets 'HypRating' (73k user ratings) and 'HypChoice' (38k user annotations) involve multiple human annotators who are users asked to listen to audio and rate or compare hypotheses. The annotators are referred to as users rather than experts, indicating they are non-experts. Thus, quality assurance is performed by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses AI models used to compute SemDist and to predict user judgment, it does not describe AI models performing quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process for the quality assurance of dataset annotations is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is documented via multiple human non-expert annotators providing user ratings and choices, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8703,
      "completion_tokens": 410,
      "total_tokens": 9113
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper describes user annotation data collected through human annotators who listened to audio and rated ASR hypotheses (HypRating with 73k ratings and HypChoice with 38k pair annotations). These annotations reflect original content created from scratch by human contributors, specifically created for this study, and are not translations or adaptations of pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset that was generated entirely by models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that data was produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset was merely collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset uses existing concepts like ASR outputs and user ratings, the new datasets (user annotations) are collected newly and do not appear to be based on adaptations or transformations of existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin/method is explicitly documented in Sections 3.1 and 3.2 for user ratings and choices."
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9221,
      "completion_tokens": 311,
      "total_tokens": 9532
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.2, 4, 5.1",
          "reasoning": "The paper introduces two new datasets: 'HypRating' with 73k user-ratings of ASR hypotheses and 'HypChoice' with 38k user annotations for ASR hypothesis pairs. These datasets are used exclusively to evaluate and benchmark the correlation between proposed SemDist metrics and user perception of ASR quality, as well as their relation to downstream NLU tasks. The datasets serve solely for performance measurement and correlation analysis."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3, 5.4",
          "reasoning": "The datasets are also used for analyzing trends and characteristics of ASR evaluation metrics. In particular, analysis of ranking gaps between WER and SemDist and modeling user judgment based on the datasets provide insights into how different metrics reflect semantic correctness and user satisfaction."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9944,
      "completion_tokens": 486,
      "total_tokens": 10430
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly introduced datasets contain exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, and 4",
          "reasoning": "The dataset entries involve English utterances used for automatic speech recognition evaluation and user annotation. There is no mention of any other language in the datasets; the examples and all transcriptions are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as containing only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech utterances and their transcriptions, not programming code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses metrics and formulas, the dataset itself does not contain mathematical or logical symbolic expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset relates to human speech data and does not include biological or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language (English) as they are transcripts of speech and user ratings."
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7162,
      "completion_tokens": 203,
      "total_tokens": 7365
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section mentions code availability for dataset construction.",
          "reasoning": "The paper does not provide any link, repository, or explicit mention of code being publicly available related to the construction, collection, or annotation process of the user-annotated datasets (71k user ratings and 36k hypothesis choices). There is no supplementary material or appendix describing code availability for the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.1 and 3.2 describe dataset collection and annotation process.",
          "reasoning": "The paper documents the creation and annotation details of the datasets in Sections 3.1 (Hypothesis Rating) and 3.2 (Side-by-Side Hypothesis Choice). It explains the annotation protocols, the size of the datasets (71k and 36k annotations), the rating/scoring criteria used for user judgement, and how these annotations were quantified, which provides transparency into the dataset creation process."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10538,
      "completion_tokens": 151,
      "total_tokens": 10689
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2 (Data and Synthesis)",
          "Reasoning": "The new dataset introduced is the synthesized speech samples created using a neural text-to-speech (TTS) system trained on the ThinkComputers Corpus (TCC). The TCC itself is human-generated audio from recordings of a podcast by two male speakers. The synthesized samples were generated by the TTS model (Tacotron 2 variant) trained on this human-recorded data, thus the samples are computer-generated audio but based on human speech recordings. Therefore, modality is audio, with dual origin: human-generated source recordings and model-generated synthesized samples."
        }
      ]
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11390,
      "completion_tokens": 239,
      "total_tokens": 11629
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The perception experiments recruited self-reported native English speakers via Prolific, a crowdsourcing platform, to rate synthesized speech stimuli; these participants are multiple human non-expert annotators providing subjective ratings on speaker competence, confidence, sincerity, and friendliness."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 and 3.2",
            "reasoning": "Participants were given specific rating tasks (e.g., rate from 1 to 7 on competence, confidence, sincerity, friendliness), implying that instructions for annotation were provided to guide their evaluations."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 and 3.2",
            "reasoning": "Ratings were collected on a defined numeric scale (1 to 7), which constitutes a scoring rubric guiding how participants assign ratings."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention providing specific annotation examples or sample ratings to the participant annotators in the described perception experiments."
          }
        }
      ]
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12520,
      "completion_tokens": 349,
      "total_tokens": 12869
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that a single non-expert conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Experiment 1 and Section 3.2 Experiment 2",
          "reasoning": "The quality assurance of the dataset, specifically the evaluation of the synthesized speech stimuli, was performed via perceptual experiments involving human participants. Twenty self-reported native English speakers participated in Experiment 1, and thirty-two participants in Experiment 2, recruited through the Prolific crowdsourcing platform. These participants served as multiple non-expert human raters evaluating the stimuli on several perceptual dimensions (competence, confidence, sincerity, friendliness). There is no indication these participants had subject matter expertise relevant to the dataset annotations or synthesis; thus the QA is attributed to multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe AI models performing quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Automated verification or algorithmic quality assurance processes on dataset annotations are not documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance was conducted by multiple human non-experts as detailed above, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12138,
      "completion_tokens": 434,
      "total_tokens": 12572
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data used for synthesis is derived from an existing speech corpus; no new human-generated original content was created from scratch by human contributors specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2 Data and Synthesis",
          "reasoning": "The paper describes the creation of synthetic speech stimuli generated by a neural text-to-speech (TTS) model trained on spontaneous speech. The stimuli with varying disfluencies were synthesized entirely via AI with controlled modifications, constituting new data generated by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of translating data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation for data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the TTS model was trained on an existing corpus, the study does not present any datasets simply collected or aggregated without modification; the focus is on generated synthesized speech data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Data and Synthesis",
          "reasoning": "The TTS synthesis is based on the ThinkComputers Corpus, an existing spontaneous speech dataset, which was then processed and adapted via a neural model with added embeddings and prosody control to create the new stimuli. This constitutes data derived from existing sources with transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation methods are thoroughly described in the paper; thus, the source is documented."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12656,
      "completion_tokens": 282,
      "total_tokens": 12938
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The newly created synthesized speech stimuli with varying disfluencies are used exclusively to conduct listener perception experiments evaluating speaker competence, confidence, sincerity, and friendliness. The dataset serves to benchmark and measure the effects of disfluency types and frequencies on perception, not for model training."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Discussion)",
          "reasoning": "The dataset is used primarily to analyze patterns about how different disfluencies affect listener judgments and the cognitive processing associated with disfluency, as well as the discounting effect when disfluency is attributed to anxiety."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the use of the new synthesized speech stimuli dataset for evaluation and analysis purposes in perception experiments."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13379,
      "completion_tokens": 407,
      "total_tokens": 13786
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset introduced is based solely on American English spontaneous speech from a podcast."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only one language, English, with no mention of any second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2: Data and Synthesis; Section 3: Perception experiments",
          "reasoning": "The new dataset is synthesized speech based on recordings of two male American English speakers from a podcast, as detailed in Section 2. The entire study and experiments use stimuli in English exclusively."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "All speech content used for the dataset is in English; no non-English language data is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the model implementation mentions PyTorch and programming frameworks, the dataset itself contains only speech stimuli, no programming code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical symbolic data is part of the dataset; it consists of spoken utterances only."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech only; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The data uses natural English language only; no constructed or fictional languages are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly documented as American English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset entries consist of spoken English utterances, hence containing language."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10597,
      "completion_tokens": 135,
      "total_tokens": 10732
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not provide any link, repository, or mention that the code used for constructing the dataset or synthesizing the speech stimuli is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Data and Synthesis",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, describing the source of the data (ThinkComputers Corpus from podcast recordings), preprocessing steps (audio enhancement), the training and modifications of the Tacotron 2 TTS model, and how disfluencies were synthesized and controlled for in the stimuli."
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5852,
      "completion_tokens": 116,
      "total_tokens": 5968
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Stimuli",
          "Reasoning": "The new dataset consists of recorded speech audio from speakers in the ATR speech database set C, which the authors specifically selected for the rating experiment. The speech stimuli are audio recordings of a Japanese sentence. These recordings were originally captured under quiet conditions, indicating human involvement in recording. This dataset was newly gathered and used by the authors for their perceptual experiments, specifically selected speakers for rating sessions."
        }
      ]
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6704,
      "completion_tokens": 230,
      "total_tokens": 6934
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3",
            "reasoning": "Section 2.3 states that 40 undergraduate students with no reported expertise or hearing impairment participated as raters in the semantic differential rating experiment."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "Section 2.4 mentions that raters were instructed to listen and score voice quality using an online form, and the experiment design randomized order and sides, indicating the raters were given task instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 and 2.4",
            "reasoning": "Section 2.2 details the use of 14 bipolar 7-point rating scales defining the rating rubric. Section 2.4 confirms raters scored using the scales presented on the form, thus a scoring rubric was provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention any examples of annotations or example ratings provided to the raters as guidance."
          }
        }
      ]
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7834,
      "completion_tokens": 377,
      "total_tokens": 8211
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert. The annotations and ratings involve multiple participants without indication of expert status."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human annotators with subject matter expertise conducted quality assurance. The annotators are undergraduate students, but their expertise status is not specified as experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description exists of a single human non-expert performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 and 2.4 (Raters and Rating procedure) and Section 3.1 (Scale reliability)",
          "reasoning": "Quality assurance involved multiple human non-expert annotators: Forty undergraduate students participated as raters. They performed ratings using predefined scales. The raters are described as undergraduate students with no known hearing impairment, but no mention is made of them being subject matter experts, indicating they are non-experts in this domain. The scale reliability is reported (Cronbach\u2019s alpha 0.797), supporting consistency in their ratings, which reflects a form of data quality assurance by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI model involvement in quality assurance is found in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described via multiple human non-expert ratings and scale reliability analysis; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7452,
      "completion_tokens": 418,
      "total_tokens": 7870
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the authors created new speech data entirely from scratch by human speakers specifically for this study. Instead, they used recordings from existing datasets and original recordings of professional narrators and voice actors, implying partial use of pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translating data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation processes involved in the dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Stimuli",
          "reasoning": "The speech stimuli were collected from multiple existing speech databases (ATR speech database sets B and C, ASJ continuous speech corpus, RWCP news speech corpus) and original recordings, aggregated for use in this study. This indicates data collation from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors selected subsets of speakers based on perceptual scores and normalized stimulus amplitudes, these processes do not qualify as substantial transformation or adaptation of original data to constitute 'derived' data as defined."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are documented sufficiently; hence, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7970,
      "completion_tokens": 224,
      "total_tokens": 8194
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.2, 2.4 and 3.1-3.4",
          "reasoning": "The authors collected and used a set of speech samples from selected speakers to analyze perceptual properties of voices using semantic differential ratings and factor analysis. The dataset was used primarily to examine trends and characteristics of penetrating voices rather than for training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8693,
      "completion_tokens": 525,
      "total_tokens": 9218
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described in the paper consists exclusively of Japanese language speech samples; no other languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only involves Japanese language speech samples with no indication or presence of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No English language speech data are included in the dataset; all speech data are in Japanese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Stimuli and throughout the paper (e.g., Abstract, Section 2.1 Stimuli)",
          "reasoning": "The dataset compiled by the authors includes speech of 124 Japanese speakers uttering a Japanese sentence \"Gaijin san wa kan-peki shugi de aru\". No other languages are included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of recorded speech audio samples only; it does not include any programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical symbolic expressions are part of the dataset itself; mathematical notation is used only in analysis sections."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech audio only; there are no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All speech data are in Japanese, a natural human language; no constructed or fictional languages are used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset (Japanese) is clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (Japanese) speech, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5911,
      "completion_tokens": 171,
      "total_tokens": 6082
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link or reference to code repositories, scripts, or software used to process or generate the dataset. It primarily uses existing datasets and original recordings but does not discuss code release or sharing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Stimuli) and Section 2 (Procedure)",
          "reasoning": "The paper provides detailed documentation on dataset construction, including the sources of speech data (public speech corpora and original recordings), description of stimuli preparation with babble noise, number of speakers selected, and rating procedures. Although code is not provided, the description of dataset creation is clear and detailed enough to understand how the data samples were selected and processed."
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6641,
      "completion_tokens": 88,
      "total_tokens": 6729
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset",
          "Reasoning": "The paper introduces a new publicly available speech dataset consisting of audio recordings from 50 speakers labeled with vocal intensity categories. The recordings were collected by the authors using human speakers performing two speaking tasks, so the data modality is audio and originates from human generation via recording."
        }
      ]
    }
  },
  {
    "id": "kodali23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7493,
      "completion_tokens": 318,
      "total_tokens": 7811
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2",
            "reasoning": "Section 2 describes a new publicly available dataset collected from 50 speakers (25 male and 25 female) who produced speech in four intensity categories. It is implied that the speakers themselves produced speech labeled with the target intensity categories. The labeling of each audio sample with the intended intensity category is based on the speaker's own adoption of that category during recording, indicating human expert involvement in labeling the data according to specified vocal intensity categories."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "Although explicit annotation instructions are not described in separate detail, Section 2 explicitly states that each sample was labeled using the target intensity category 'adopted by the speaker in the production of the corresponding signal.' This implies the presence of annotation guidelines or instructions to ensure speakers produced speech according to specific intensity categories (soft, normal, loud, very loud)."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No mention is made of scoring rubrics or detailed evaluation criteria for the labeling of intensity categories in the annotation guidelines. The dataset relies on speakers adopting the target intensity categories, rather than using a rubric-based scoring system."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not describe any provided annotation examples or samples to guide the labeling process of intensity categories. There is no mention of sample annotated data in the text or appendix."
          }
        }
      ]
    }
  },
  {
    "id": "kodali23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8623,
      "completion_tokens": 384,
      "total_tokens": 9007
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance performed by multiple human experts or annotators for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was done by a single human non-expert in the dataset annotation or validation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving multiple human non-expert annotators for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used as a judge or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic verification or algorithmic rule-based quality assurance process applied to the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new dataset containing speech samples labeled by the target vocal intensity category adopted by the speaker, but it does not describe any quality assurance process or validation of these annotations. No information about the annotators or any verification method is provided."
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8241,
      "completion_tokens": 428,
      "total_tokens": 8669
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The paper states that a new publicly available dataset is used, which includes speech produced by 50 human speakers (25 male and 25 female) in four intensity categories. The speech was recorded via two speaking tasks (sentence and paragraph reading) performed by human contributors. This dataset was created entirely from scratch by human speakers and is not translated, adapted, or derived from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was recorded from human speakers, not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was created by human-translating speech data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation used to generate the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses texts for reading taken from existing sources (TIMIT sentences, weather forecast excerpt, and a novel), the speech data itself was newly recorded by human contributors specifically for this study, not merely collected or aggregated from existing databases without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the reading texts come from pre-existing sources, the speech audio is newly produced, so the dataset is not derived data with modifications but rather original human recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the dataset origin as newly recorded human speech data."
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8759,
      "completion_tokens": 259,
      "total_tokens": 9018
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 Classifier",
          "reasoning": "The dataset comprising speech signals with four intensity categories is used to train a classifier (SVM) in a supervised manner, where the models are trained to classify vocal intensity categories using the labeled intensity class information."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Results",
          "reasoning": "The dataset is used to evaluate classification performance of the models by measuring accuracy and confusion matrices for different features and speaking tasks."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Results",
          "reasoning": "The dataset is analyzed to study the effect of speaking tasks and layer-wise performance of embeddings on classification accuracy, examining misclassification patterns and class-wise accuracies."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9482,
      "completion_tokens": 508,
      "total_tokens": 9990
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The paper states the new dataset contains speech produced in English only (Section 2). There is no mention of multiple languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The dataset entries are only in English (Section 2), no two-language content is described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset consists of speech produced in English using four vocal intensity categories (soft, normal, loud, very loud). This is explicitly stated in Section 2 and the dataset is all English speech data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The dataset is specified to have only English speech, so no non-English monolingual data is present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech audio data only, no programming or code related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no mathematical or formal logical expressions included in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only, no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural English speech with no mention of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly specified as English in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language speech audio (English), so it is not without language."
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6700,
      "completion_tokens": 160,
      "total_tokens": 6860
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2",
          "reasoning": "The paper mentions a publicly available dataset with a URL for more details (https://bit.ly/3tLPGRx), but does not provide any link or indication that the code used for data collection, preprocessing, or generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 and 3.1",
          "reasoning": "The paper documents the dataset creation process, including number of speakers, age ranges, speaking tasks, sentences and paragraphs used, number of repetitions, and labeling of intensity categories. Preprocessing steps such as silence removal and amplitude normalization are described in Section 3.1. This information provides transparent documentation about the dataset creation and preprocessing."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7771,
      "completion_tokens": 209,
      "total_tokens": 7980
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 3 Data processing pipeline; Section 4 Experiments",
          "Reasoning": "The new dataset introduced is LibriTTS-R, which is a cleaned and restored version of the LibriTTS corpus audio data. The speech samples are originally human speech recordings, as LibriTTS consists of speech audio originally recorded from humans. Although LibriTTS-R applies speech restoration via a model to improve audio quality, the resulting audio remains based on original human-recorded speech samples with improved sound quality, not artificially generated speech. Therefore, the modality is audio, and the data is human generated (recorded) but not model generated. The dataset corresponds directly to improved versions of original human speech samples from LibriTTS, as detailed throughout the abstract and Section 3 (Data processing pipeline) and the experiment sections describing subjective tests on the ground-truth audio."
        }
      ]
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8623,
      "completion_tokens": 241,
      "total_tokens": 8864
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1.1 and 4.2.1",
            "reasoning": "The paper states that over 100 paid native English speakers participated as subjects in subjective experiments to evaluate the sound quality and naturalness, indicating multiple human non-expert annotators performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1.1",
            "reasoning": "The listening tests required subjects to rate naturalness using MOS and preference using SxS test with specified scales and conditions, implying instructions were provided for performing these subjective evaluations."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1.1",
            "reasoning": "The paper describes use of explicit rating scales for MOS (1 to 5 with 0.5 increments) and SxS tests (7-point scale from -3 to 3), which constitute scoring rubrics guiding the annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no mention of example annotated samples or example instructions provided to subjects in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9753,
      "completion_tokens": 407,
      "total_tokens": 10160
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.1.1 and 4.1.2",
          "reasoning": "Subjective quality assessments including Mean-Opinion-Score (MOS) and side-by-side (SxS) tests for evaluating sound quality were conducted by over 100 human subjects who were paid native English speakers. This indicates multiple human non-experts (listeners) performed evaluations to assess quality. However, the participants are not indicated as experts in speech quality assessment or annotation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention that an AI model was involved in quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset construction process used AI models for speech restoration, no automatic verification or algorithmic rule-based quality assurance of annotations or content is explicitly described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documentation and description of subjective quality evaluation performed by multiple human listeners, so quality assurance process is present."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9371,
      "completion_tokens": 428,
      "total_tokens": 9799
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not original content created entirely from scratch by human contributors. It is derived from pre-existing data and modified."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated entirely by AI or machine learning models without reference to existing data; rather, the dataset is based on existing speech samples that have been processed by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the original LibriTTS corpus was collated from other sources such as LibriVox and Project Gutenberg, the new dataset LibriTTS-R is not simply aggregated from existing sources; it applies transformations via restoration."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Abstract; Section 3; Section 5",
          "reasoning": "LibriTTS-R is based on the existing LibriTTS corpus, with speech restoration applied via the Miipher speech restoration model to enhance sound quality. The constituent samples remain the same textually and speaker-wise but are transformed in quality through model-based cleaning. Thus, the dataset is 'derived' with modifications and transformations applied to existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9889,
      "completion_tokens": 531,
      "total_tokens": 10420
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the LibriTTS-R dataset for pre-training large models in an unsupervised or self-supervised manner. The dataset is introduced and used in the context of supervised TTS model training."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2.1 and 4.2.2",
          "reasoning": "The paper describes training multi-speaker TTS models from scratch using LibriTTS-R as the training data. This is evidenced by experiments training acoustic models and neural vocoders on LibriTTS-R, demonstrating improved speech naturalness compared to models trained on the original LibriTTS dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning pre-trained models using LibriTTS-R. The dataset is used for training models from scratch rather than fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training methods are described in the paper involving the LibriTTS-R dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset is evaluated for speech quality and naturalness in subjective tests, these evaluations are performed on the dataset samples to assess improvements from speech restoration, not for using the dataset as an evaluation benchmark."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use LibriTTS-R primarily for analysis of trends or characteristic studies. The focus is on dataset improvement and subsequent model training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The LibriTTS-R dataset is not described as being used as a knowledge base to augment models through retrieval-based methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The LibriTTS-R dataset is explicitly used for training TTS models from scratch and is central to the experiments described."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10612,
      "completion_tokens": 427,
      "total_tokens": 11039
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 2; Section 3.1",
          "reasoning": "The paper states that the LibriTTS-R dataset is derived from the LibriTTS corpus which itself is based on LibriSpeech and LibriVox English audiobooks, with speech data from English speakers. Multiple references throughout the paper confirm the dataset contains English speech and corresponding English text transcripts only. For instance, Section 2 mentions the dataset consists of English speech and texts, and the speech restoration model is trained on various English accents. No mentions of other languages are made for the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of human language speech and corresponding text transcripts, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7830,
      "completion_tokens": 182,
      "total_tokens": 8012
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not provide any link or mention of publicly available code for the speech restoration model or the data processing pipeline used to create the LibriTTS-R dataset. Although the dataset itself is available for download, the code used for its construction, including the speech restoration model 'Miipher', is not shared with the public in this paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including the speech restoration model (Section 3), its features, training data, and the data processing pipeline applied to LibriTTS. Subjective evaluation experiments and data splits are described in Section 4. Therefore, the dataset creation documentation is transparent and sufficiently detailed within the paper."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7935,
      "completion_tokens": 254,
      "total_tokens": 8189
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, and 3.3",
          "Reasoning": "The Dusha dataset includes audio recordings collected from two sources: crowd-sourced acted recordings from non-professional actors pronouncing assigned text with emotions, and real-life podcast recordings featuring genuine emotions. All audio data originated from human speech, captured using human-operated equipment or gathered from existing podcasts, thus constituting human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, and 3.3",
          "Reasoning": "The dataset contains transcripts associated with each audio recording. The transcriptions are the genuine textual forms of the spoken utterances, originally produced by humans. The text for acted recordings was derived from real user voice assistant commands and chit-chat. Podcast transcripts were obtained from a speech recognition solution pretrained on existing data, but this is not described as model-generated text per se for the Dusha dataset itself; the focus is on pairing human speech with text transcripts, so text modality is human-generated transcriptions."
        }
      ]
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8787,
      "completion_tokens": 237,
      "total_tokens": 9024
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3",
            "reasoning": "Annotation was conducted via a crowd-sourcing platform where many non-professional annotators labeled the data. It mentions that each person took training and an exam before annotating, indicating multiple non-expert annotators involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Annotators were given detailed instructions on how to label emotions with specific definitions for Positive, Neutral, Sadness, Anger/Irritation, and Other categories, ensuring consistent annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Annotation included control tasks called 'honeypots' to check annotator labeling quality, acting as a rubric or quality control mechanism during annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Instructions included illustrative descriptions such as voice spoken with a smile, laughter for Positive or calm voice for Neutral, sadness or melancholy for Sadness, indicating examples were provided to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9917,
      "completion_tokens": 488,
      "total_tokens": 10405
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that individual expert annotators were responsible for quality assurance. Annotators are described as crowd-sourced non-professionals with no mention of subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that quality assurance was performed by multiple human experts. The annotators are described as participants from a crowd-sourcing platform with no expert credentials indicated."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset annotations were carried out by multiple annotators per sample; there is no indication that only a single non-expert annotated the data."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Post-processing and annotation",
          "reasoning": "The paper describes that annotations were performed by multiple annotators recruited via a crowd-sourcing platform, who were non-professional actors and non-expert annotators. Quality control involved training and an exam for the annotators, with multiple independent annotations per sample."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an AI model (a BERT-based classifier) was used for pseudo-labeling and data sampling for the acted subset, it was not used as a quality assurance judge of annotations. The final annotation quality assurance relied on human annotators and aggregation methods."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification of annotation quality through algorithmic or rule-based techniques is described as part of the quality assurance process. The Dawid-Skene algorithm is used for aggregation, not for automated QA verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a clear and elaborate quality assurance process involving multiple non-expert annotators with controls such as honeypots and training; therefore, QA is present and documented."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9535,
      "completion_tokens": 477,
      "total_tokens": 10012
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The Crowd subset was collected by having non-professional human actors record text passages in various emotional tones on a crowd-sourcing platform (Yandex Toloka). This process involved original voice recordings created by human participants acting emotional content from scratch, not derived or translated from existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the dataset was generated or synthesized entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of Russian speech recordings; there is no mention of data produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention that the data was generated by machine translation systems from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The Podcast subset was collected by selecting and aggregating audio segments from 6,240 existing Russian podcasts. While the podcast audio was sliced into 5-second segments and normalized, the core data consists of existing natural speech recordings without significant alteration."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "Both subsets underwent preprocessing steps such as filtering, pseudo-labeling using a BERT-based text classifier for sampling emotional utterances, audio slicing, normalization, and annotation with aggregated emotion labels. These modifications represent adaptations and transformations applied to either newly-recorded or collated data, hence making the dataset derived from original raw recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents data sources and methods of generation for both subsets; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10053,
      "completion_tokens": 410,
      "total_tokens": 10463
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 5 Conclusion",
          "reasoning": "The paper states that the acted subset of the Dusha dataset, collected via crowd-sourcing, is suitable for model pre-training, aiming to capture general patterns for speech emotion recognition."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Baseline implementation details",
          "reasoning": "The baseline model is trained from scratch using the Dusha dataset parts to demonstrate the applicability of this new dataset for model training without pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Podcast subset collection, Section 5 Conclusion",
          "reasoning": "The podcast subset of Dusha contains real-life, natural emotions with unbalanced class distribution and is elaborated to be used for fine-tuning pre-trained models to adapt to real-world scenarios."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or describe any use of the Dusha dataset for reinforcement learning or RL-based post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 Benchmark results",
          "reasoning": "The dataset is used to evaluate the baseline model, with test subsets from both Crowd and Podcast domains employed for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the dataset for analysis of trends or characteristics separate from training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the Dusha dataset serves as a knowledge base for retrieval augmentation or similar augmenting methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has multiple documented uses including pre-training, fine-tuning, training from scratch, and evaluation; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10776,
      "completion_tokens": 573,
      "total_tokens": 11349
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists exclusively of Russian speech recordings; no other languages are indicated in the paper."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only Russian language speech. There is no indication of a second human language present."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are strictly in Russian language; English content is not part of the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 3 (Data acquisition); Section 5 (Conclusion)",
          "reasoning": "The Dusha dataset is a newly introduced speech emotion recognition corpus containing approximately 350 hours of Russian language speech data, including both crowd-sourced acted and real-life emotions from podcasts. The paper explicitly states it is the first speech emotion corpus in Russian (Abstract). Both the 'Crowd' and 'Podcast' subsets exclusively comprise Russian utterances as detailed throughout Sections 3 and 5."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or code-related content is present in the dataset entries; the dataset comprises audio speech and transcripts."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include mathematical or logical symbolic expressions, only speech and their transcripts in natural language."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists solely of human spoken language, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Only natural Russian language data is included; no constructed or fictional languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is clearly specified as Russian throughout the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (Russian speech and transcripts), so this option does not apply."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7994,
      "completion_tokens": 195,
      "total_tokens": 8189
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 1 (Abstract), Section 5 (Conclusion), Footnote 4 (GitHub link)",
          "reasoning": "The paper explicitly mentions that all data and processing scripts are released on a GitHub repository (Footnote 4: https://github.com/salute-developers/golos/tree/master/dusha). This indicates that the code related to dataset processing and benchmarking is publicly available and accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Data acquisition), 3.1 (Crowd subset collection), 3.2 (Podcast subset collection), 3.3 (Post-processing and annotation), and 4 (Dataset overview)",
          "reasoning": "The paper provides detailed documentation about the dataset creation process including data sources, collection methods, annotation procedures, post-processing, aggregation mechanism, and dataset statistics. This extensive description supports reproducibility and transparency."
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7066,
      "completion_tokens": 288,
      "total_tokens": 7354
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction; Section 3.1 Data sources of automatic-strada; Section 3.3 Annotated-strada\u2019s curation",
          "Reasoning": "The STraDa dataset introduced by the authors contains downloadable music audio excerpts and full-length tracks (annotated-strada) that are original recordings by human singers. The audio data is collected from sources including Deezer API and YouTube links with timestamps. The audio is human generated as it consists of original singing voices recorded by humans, not synthesized or model-generated."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction; Section 3.1 Data sources of automatic-strada; Section 3.2 Data matching and processing of automatic-strada; Section 3.3 Annotated-strada\u2019s curation",
          "Reasoning": "The dataset includes rich metadata such as singer gender, birth year, active country, music genre, lyrics language, release date, and lead singer identification, which are manually curated and cross-validated from music encyclopedias (MusicBrainz, Wikidata, Discogs) and Deezer metadata. This metadata is structured in tabular form and originates from human-generated sources and manual curation, not from automated synthetic processes."
        }
      ]
    }
  },
  {
    "id": "kong24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7918,
      "completion_tokens": 212,
      "total_tokens": 8130
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.3",
            "reasoning": "The annotated-strada subset was \"deliberately curated and annotated\" manually to ensure balance and accuracy, indicating multiple experts were involved to provide reliable manual annotations across gender, age groups, and languages."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not mention any specific annotation instructions provided to annotators for the annotated-strada; it only states that the dataset was manually curated and annotated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No mention or description of scoring rubrics or detailed criteria used during annotation is included in the paper for the annotated-strada."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not describe providing annotation examples or guidelines containing examples for annotators during the manual annotation of annotated-strada."
          }
        }
      ]
    }
  },
  {
    "id": "kong24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9048,
      "completion_tokens": 349,
      "total_tokens": 9397
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The annotated-strada subset was manually curated and annotated to ensure balance and accuracy, involving manual verification of 200 tracks balanced across sex, language, and age groups. There is no indication that annotators were experts, hence manual annotation is likely performed by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The automatic-strada dataset was created via an automated process matching data from four sources, using algorithmic methods to cross-validate metadata, select only tracks with single lead singers, and process releasing the earliest release date. This constitutes an automatic quality verification process for metadata consistency."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents both an automated QA process for the automatic-strada dataset and manual annotation for the annotated-strada dataset."
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8666,
      "completion_tokens": 464,
      "total_tokens": 9130
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The annotated-strada subset consisting of 200 tracks was manually curated and annotated by the authors to ensure balance across genders, languages, and age groups. The collection and annotation process was conducted manually rather than via automated means, indicating original content created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset being generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The automatic-strada subset was created by collecting and aggregating data from four existing data sources (Deezer, MusicBrainz, Wikidata, and Discogs) without significant modification beyond matching and filtering metadata entries. This indicates a collated dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The automatic-strada data was processed to ensure only single lead singer tracks were included, and metadata was cross-validated across sources. Annotated-strada involved segment extraction and annotation with timestamps from full-length tracks, constituting modifications or transformations applied to existing data sources, hence derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the methodology and origins of the datasets introduced; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9184,
      "completion_tokens": 309,
      "total_tokens": 9493
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The automatic-strada subset of STraDa is used to fine-tune a pre-trained x-vector model for singer sex classification, as described in Section 4. The authors specifically fine-tune the model on this dataset using supervised methods to improve classification performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 and Section 3.3",
          "reasoning": "The annotated-strada subset is used exclusively for evaluation and bias analysis of trained SSC models, as detailed in Section 4.3 and Section 3.3. The dataset is balanced and manually annotated to provide reliable performance measurement across subgroups."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3.2 and Section 4",
          "reasoning": "Annotated-strada enables bias analysis across different subgroups such as gender, language, and age in singer sex classification tasks, as discussed in Section 4.3.2."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9907,
      "completion_tokens": 512,
      "total_tokens": 10419
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 3.1; Table 1",
          "reasoning": "The automatic-strada subset of STraDa contains 25194 tracks in 35 languages, explicitly mentioned in the Abstract and Section 3.1. Table 1 also lists 5 most represented languages and highlights linguistic diversity. The languages include English, French, Mandarin, Spanish, German among others, confirming presence of more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes many more than two human languages; it is not restricted to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages with English as one of them, not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages, not just a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of code or programming language data entries in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains music audio files and metadata but no mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human singing voices with no biological sequences or non-human communication systems included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that constructed or fictional languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages of dataset entries are clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human languages in the audio content and metadata."
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7125,
      "completion_tokens": 160,
      "total_tokens": 7285
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section explicitly mentions code availability or provides a link to code repositories.",
          "reasoning": "The paper does not mention or provide any links to code repositories or publicly available code related to data collection, preprocessing, or dataset generation. The dataset creation is described abstractly but no code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3, 3.1, 3.2, 3.3, and 3.4",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 3 and its subsections, explaining data sources, matching and processing, manual curation for annotated-strada, and limitations. This offers transparency and completeness in describing dataset construction."
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7420,
      "completion_tokens": 214,
      "total_tokens": 7634
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Data), Section 4.1 (Extraction of metrics)",
          "Reasoning": "The study recruited 38 people with Parkinson's disease and 22 controls who completed interactive sessions where speech was elicited through various speaking exercises. Audio data of participants' speech was recorded using microphones during these sessions. Hence, the audio data modality is human generated as it originates from recordings of human participants during the study."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Data), Section 4.1 (Extraction of metrics)",
          "Reasoning": "Facial metrics were extracted from audiovisual frames recorded during the interactive sessions. The paper states that facial landmark extraction was done using facial landmark detector algorithms on video recordings. These videos were recorded from participants interacting with the virtual agent, thus the video data is human generated, captured from real participants during the study."
        }
      ]
    }
  },
  {
    "id": "kothare22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8272,
      "completion_tokens": 197,
      "total_tokens": 8469
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3, Section 4.1",
            "reasoning": "The dataset consists of speech and facial metrics automatically extracted from audiovisual recordings using software tools like Praat, Montreal Forced Aligner, OpenCV dnn module, and Dlib facial landmark detector, indicating an automatic annotation process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "No detailed annotation instructions for manual annotation are described, as the metric extraction is automated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not mention any scoring rubrics provided for annotation since the data extraction is automatic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "No examples of annotations are provided because metrics are computed automatically with standard software without manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "kothare22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9402,
      "completion_tokens": 325,
      "total_tokens": 9727
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of quality assurance conducted by multiple human experts for validating dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance activities performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used to extract metrics (e.g., facial landmark detection), the paper does not describe any AI model-based quality assurance process that judges the correctness or validity of annotations or data quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 (Extraction of metrics)",
          "reasoning": "The dataset's speech and facial metrics were extracted automatically in real time using algorithmic and software tools such as Praat, Montreal Forced Aligner, OpenCV face detector, and Dlib facial landmark detector, which indicates an automated verification process via algorithms rather than manual annotation validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some form of automated metric extraction and quality checking is described; thus, quality assurance is present, and the N/A label is not applicable."
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9020,
      "completion_tokens": 413,
      "total_tokens": 9433
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Data",
          "reasoning": "The data consists of speech and facial recordings collected from 60 participants (38 with Parkinson's disease and 22 controls) who engaged in interactive sessions guided by a virtual agent. Participants performed various speaking exercises and filled out surveys, resulting in newly collected original data created by human contributors specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset consists of entirely model-generated data without reference to human contributors. The data originates from human participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human translation of data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation in the data generation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected by aggregation or compilation of existing datasets; rather, it was newly recorded from participants specifically for the study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Extraction of metrics",
          "reasoning": "Speech and facial metrics were computationally extracted and derived from the audiovisual recordings using algorithms such as Praat, Montreal Forced Aligner, and OpenCV. These metrics represent transformations and analyses of the raw collected data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9538,
      "completion_tokens": 260,
      "total_tokens": 9798
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used exclusively for evaluation or benchmarking purposes; rather it is used in conjunction with analysis and clinical assessment."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and 5",
          "reasoning": "The dataset consists of multiple sessions of speech and facial metrics from people with Parkinson's Disease and controls, collected via a remote multimodal dialogue system. The authors analyze these metrics to identify statistically significant differences, compute minimally detectable changes (MDC), and minimal clinically important differences (MCID). The primary use is to analyze trends, patterns, and clinical relevance of these metrics rather than model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10261,
      "completion_tokens": 522,
      "total_tokens": 10783
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3. Data",
          "reasoning": "The dataset consists solely of participants who are fluent in English as per the inclusion criteria ('fluency in English' is explicitly stated). All speech data and tasks (e.g., reading, spontaneous speech, story retell) are conducted in English. No other languages are mentioned or used in the data collection."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although programming tools and algorithms (e.g., Praat, Montreal Forced Aligner) were used to extract features from the speech and video data, no programming or code snippets are included as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions are used in the methods section to define MDC and SEM, but these are not entries in the dataset. The dataset itself contains linguistic data, not mathematical or symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech and facial visual data for Parkinson's disease assessment with no biological sequences or non-human communication signals included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificial languages such as Klingon or Esperanto in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as English, with explicit inclusion criteria and task descriptions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken and facial data involving human English language communication, so language is present."
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7479,
      "completion_tokens": 150,
      "total_tokens": 7629
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or statement refers to code availability",
          "reasoning": "The paper does not provide any link, repository information, or mention of publicly available code for the dataset construction or data collection platform. The descriptions are limited to methods but no reproducible code or resources are shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Data) and Section 4 (Methods & Results)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including participant recruitment criteria, data collection protocol involving the multimodal dialogue system, task descriptions, and methods of metric extraction and analysis, allowing for a comprehensive understanding of the dataset creation process although not code."
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9075,
      "completion_tokens": 244,
      "total_tokens": 9319
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data; Abstract",
          "Reasoning": "The dataset consists of audiovisual speech recordings collected from English-speaking and Dutch-speaking people with ALS via a cloud-based multimodal dialogue system where participants interacted with a virtual guide, eliciting speech. This audio data is human-generated from direct recordings of participant speech."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data; Abstract; Section 3 Methods",
          "Reasoning": "Facial video data was collected from participants using the Modality platform, capturing video of facial movements during speech tasks. These videos were recorded in a remote naturalistic setting with human participants, thus the video data is human-generated."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3 Methods; Section 2 Data",
          "Reasoning": "Linguistic metrics were computed from automatic transcriptions of participant speech obtained using AWS Transcribe, an automated speech recognition system. These transcriptions are model-generated text from the audio recordings."
        }
      ]
    }
  },
  {
    "id": "kothare24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9927,
      "completion_tokens": 208,
      "total_tokens": 10135
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 Data and Section 3 Methods",
            "reasoning": "The data collected were processed with automated extraction tools such as Praat, Montreal Forced Aligner, and MediaPipe Face Mesh to extract acoustic, linguistic, and facial metrics from audio and video recordings. There is no mention of manual annotation by humans for labeling the data; annotations refer primarily to automatic metric extraction."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe any annotation instructions for human annotators, as the feature extraction was automated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No scoring rubrics are described in relation to the annotation process, since there was no human annotation reported."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No annotation examples are provided; the paper focuses on automated feature extraction."
          }
        }
      ]
    }
  },
  {
    "id": "kothare24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11057,
      "completion_tokens": 350,
      "total_tokens": 11407
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being conducted by any single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was performed by multiple human experts on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any details about quality assurance involving a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence is given that multiple non-expert human annotators conducted quality assurance on the data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models (AWS Transcribe, MediaPipe, etc.) are used for automatic extraction of features, the paper does not describe the use of AI models as judges for quality assurance of annotations or dataset content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 Methods and 3 Data",
          "reasoning": "The paper describes automated extraction of speech, linguistic, and facial features using software tools such as Praat, Montreal Forced Aligner, MediaPipe Face Mesh, and AWS Transcribe. These processes imply automatic verification and extraction of annotations, and no manual quality assurance process is described. Therefore, quality assurance is conducted through automated verification and procedural use of reliable automatic tools."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documentation of an automated quality assurance process through validated software tools, so it is not the case that no quality assurance is applied or documented."
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10675,
      "completion_tokens": 428,
      "total_tokens": 11103
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The paper states that audiovisual data was collected from 143 English-speaking and 26 Dutch-speaking people with ALS through ongoing studies using a cloud-based multimodal dialogue system involving virtual interactions. This indicates data acquisition directly involving human participants producing original speech and facial behavior data specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new dataset entirely via AI or machine learning generative models without human input."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study involves data collected separately from English and Dutch participants, but there is no mention that speech or linguistic data was translated from one language to another by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was applied to generate or adapt the data from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not merely aggregated from existing sources; it was collected specifically in this study from participants using a remote platform."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 Methods",
          "reasoning": "The paper describes extracting various speech, linguistic, and facial features using tools like Praat, Montreal Forced Aligner, and MediaPipe Face Mesh. These metrics are derived from the raw collected audiovisual data by applying established computational methods, indicating a derived dataset based on original data with transformation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are clearly documented."
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11193,
      "completion_tokens": 243,
      "total_tokens": 11436
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Results and Section 5 Discussion",
          "reasoning": "The authors introduced novel datasets of audiovisual data collected from English-speaking and Dutch-speaking people with ALS (pALS) using a cloud-based multimodal dialogue platform. The datasets are utilized primarily for analyzing trends and characteristics of speech-based digital biomarkers across languages for monitoring ALS progression. The study involves extraction of features, group difference analysis, test-retest reliability, and modeling longitudinal trajectories, focusing on analysis rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11916,
      "completion_tokens": 562,
      "total_tokens": 12478
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset described in the paper contains data from exactly two human languages: English and Dutch, rather than more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2 (Data), Abstract",
          "reasoning": "The dataset collected by the authors includes speech and facial data from 143 English-speaking participants and 26 Dutch-speaking participants, explicitly involving exactly two human languages: English and Dutch."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes data from Dutch-speaking participants as well, so it is not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes English-speaking participants as well as Dutch-speaking participants, so it is not a single non-English language dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset comprises speech, facial video, and linguistic features from human languages; there is no indication that programming or structured code-related content is part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although mathematical modeling is used in analysis (growth curve models), the dataset itself does not contain mathematical or logical symbolic data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech and facial movement data, but not biological sequences (e.g., DNA) or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves natural languages English and Dutch; there is no mention of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages present in the dataset are explicitly specified as English and Dutch."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries with human language (English and Dutch), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9134,
      "completion_tokens": 162,
      "total_tokens": 9296
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention any link, repository, or location where the code related to data collection, preprocessing, or dataset construction is made publicly available. There is no indication that such code is released alongside the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Data, Section 3 Methods, and Table 2",
          "reasoning": "The paper documents the dataset creation process in detail, including participant demographics, data collection methods using the Modality platform, tasks performed, and the specific speech, linguistic, and orofacial metrics extracted with references to tools used (e.g., Praat, Montreal Forced Aligner, MediaPipe). The methodological description provides transparency about dataset construction and preprocessing steps."
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7960,
      "completion_tokens": 211,
      "total_tokens": 8171
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data collection and Section 3.2 Data Characterization",
          "Reasoning": "The ITALIC dataset includes 16,521 crowdsourced audio samples recorded by 70 speakers from various Italian regions. The audio data was recorded by human participants using their own devices and contains utterances read from prompts derived from the MASSIVE dataset. The recordings are explicitly collected via human involvement as speech samples."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data collection and Section 3.2 Data Characterization",
          "Reasoning": "The dataset contains transcripts of the recorded audio, which are based on prompts from the MASSIVE dataset. Although the original utterance texts come from MASSIVE, the transcripts in ITALIC correspond directly to the human-read utterances and are thus considered human-generated text data aligned with the recordings."
        }
      ]
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8812,
      "completion_tokens": 242,
      "total_tokens": 9054
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "The dataset was crowdsourced through a web platform where participants (crowd workers) recorded utterances by reading prompts; these participants are not described as experts but general crowd workers providing the data. The validation was performed by at least two individuals reviewing each sample for intelligibility and prompt coherence, indicating multiple non-expert humans were involved in annotation and validation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The paper states that participants were given a list of annotation guidelines before recording, indicating instructions were provided to annotators (crowd workers)."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No mention of scoring rubrics or formalized scoring criteria for annotation is described in the paper; only general annotation guidelines and validation criteria are mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention providing example annotations or example recordings as part of the instruction material for annotators."
          }
        }
      ]
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9942,
      "completion_tokens": 421,
      "total_tokens": 10363
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert. The validation was done by multiple individuals without specification of expert status."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human annotators with subject matter expertise or belonging to the target demographic performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that quality assurance was conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection",
          "reasoning": "The paper states that at least two individuals reviewed each sample for validation (checking intelligibility and coherence with prompts), but it does not specify that these validators are experts. The dataset was crowdsourced with participants recording themselves reading prompts, and validation was done through multiple rounds by these (presumably non-expert) individuals. Therefore, quality assurance was conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification methods or rule-based algorithmic processes used for quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented and described in Section 3.1, involving multiple human reviewers validating each sample."
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9560,
      "completion_tokens": 450,
      "total_tokens": 10010
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection",
          "reasoning": "The ITALIC dataset was crowdsourced through a custom web platform where human participants recorded themselves reading prompts derived from MASSIVE. These audio recordings are original human-produced content, not generated by models or translated from other languages. The paper explicitly states that participants recorded utterances by reading prompts, thus creating new audio data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data in ITALIC was generated by AI or machine learning models. The dataset consists of human-recorded audio samples."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not created by translating data from another language via human translators; rather, human speakers read prompts originally from an existing dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used to produce the Italian data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the prompts were extracted from the MASSIVE dataset, the actual speech audio recordings are newly collected from human participants and not simply aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection",
          "reasoning": "The textual prompts presented to crowd workers are derived from the MASSIVE dataset, an existing multilingual NLU dataset. The authors extracted and annotated Italian prompts from MASSIVE to use as reading material, making the text data derivations from existing sources, although the audio recordings themselves are newly created."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation methodology are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10078,
      "completion_tokens": 364,
      "total_tokens": 10442
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setting and Section 4.2 Results on Intent Classification",
          "reasoning": "The ITALIC dataset is used to fine-tune pre-trained speech and text models for intent classification in Italian. The models include pre-trained XLSR variants for speech and pre-trained BERT and BART variants for text, which are further fine-tuned on the training split(s) of the ITALIC dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, specifically 4.2 Results on Intent Classification and 4.3 Results on Automatic Speech Recognition",
          "reasoning": "The ITALIC dataset is used for benchmarking and evaluating the performance of various state-of-the-art speech and text models on intent classification and automatic speech recognition tasks, including performance under different conditions such as noise and speaker variability."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Characterization and Section 4 Experiments",
          "reasoning": "The dataset includes rich metadata about speakers and recording conditions, enabling analysis of model robustness, generalization to unseen speakers, and impact of noise and regional linguistic variations. The authors explicitly mention the possibility for future analyses like speaker recognition, age estimation, and dialect identification."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10801,
      "completion_tokens": 542,
      "total_tokens": 11343
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ITALIC dataset is described as containing only Italian language utterances and recordings. There is no mention of multiple languages being included in the dataset instances."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages. It contains only Italian utterances and recordings."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not composed of English only language content; it is explicitly an Italian language dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 1 (Introduction), Section 3 (The ITALIC Dataset)",
          "reasoning": "The ITALIC dataset is explicitly introduced as a large-scale Italian language intent classification audio dataset. It comprises Italian audio samples recorded by speakers from various Italian regions and Italian transcripts. The dataset entries contain exactly one language, Italian, which is non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural language utterances for intent classification and speech recognition, with no mention of formal mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech recordings and transcripts; it does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves Italian language only; there is no mention or indication of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly specified as Italian; it is well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language\u2014specifically Italian utterances and transcripts."
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8019,
      "completion_tokens": 210,
      "total_tokens": 8229
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 1 Introduction; Section 3 The ITALIC Dataset; Section 4 Experiments",
          "reasoning": "The paper explicitly states in the Abstract and Introduction that they release both the dataset, the annotation scheme, and the code for the baselines to encourage further research. The GitHub URL is provided (https://github.com/RiTA-nlp/ITALIC), which suggests public availability of the code. Section 4 references detailed information about the models and fine-tuning procedures available in the official project repository, supporting that code related to dataset processing and experimentation is openly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 The ITALIC Dataset",
          "reasoning": "Section 3 thoroughly describes the dataset collection process, including the crowdsourcing platform, participant recruitment, annotation validation procedures, metadata collection, data splits, and dataset statistics. The methodology is documented in sufficient detail to enable reproducibility and understanding of the dataset's construction."
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8433,
      "completion_tokens": 162,
      "total_tokens": 8595
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 - 3.3",
          "Reasoning": "The ClArTTS corpus audio is extracted from a pre-recorded LibriVox audiobook read by a single male human speaker. The audiobook was manually segmented and validated, indicating human-captured audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 - 3.3",
          "Reasoning": "The text transcripts for the audio segments were manually annotated and fully diacritized by a team of three Arabic annotators with multiple validation rounds, ensuring high-quality human-generated text transcriptions aligned to the audio."
        }
      ]
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9285,
      "completion_tokens": 260,
      "total_tokens": 9545
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.2",
            "reasoning": "The paper states that a team of three Arabic annotators was involved in the transcription process, followed by two rounds of validation by the annotators themselves and two other annotators. No indication of subject matter experts is mentioned; the description suggests native or proficient speakers performing transcription and validation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "Guidelines were provided to the annotators during the annotation process, including instructions on using abbreviations, numbers, special characters, and punctuation according to Arabic language rules, as well as marking specific speech segments with predefined tags."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no mention or description of scoring rubrics or criteria for rating the annotation quality or decisions; the focus is on instructions for transcription and tagging rather than scoring or evaluation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not mention providing annotation examples or sample annotated segments as part of the guidelines or instructions; it only refers to textual guidelines and tags."
          }
        }
      ]
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10415,
      "completion_tokens": 366,
      "total_tokens": 10781
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Annotation process",
          "reasoning": "The annotation process involved a team of three Arabic annotators who transcribed the audio and performed two rounds of validation: first by themselves and then by two other annotators. This indicates that multiple human experts (annotators familiar with Arabic) participated in quality assurance through transcription and validation, ensuring more accurate and reliable transcripts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as Arabic annotators with knowledge to apply diacritization and speech transcription, indicating expertise; thus, they are not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI models being used as judges for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Audio pre-processing",
          "reasoning": "Automatic quality assurance was partially conducted via automated techniques: a rule-based Praat script was used to mark pauses and speech segments, and this marking was validated using an energy-based Voice Activity Detector (VAD) from the Kaldi toolkit. This automated verification helped ensure accurate segmentation of speech and silence segments."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes both human annotation with validation and automated verification steps, so quality assurance was applied and documented."
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10033,
      "completion_tokens": 343,
      "total_tokens": 10376
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.3",
          "reasoning": "The ClArTTS corpus is extracted from a pre-existing LibriVox audiobook recording of a classical Arabic book. The authors converted, segmented, and processed the existing audio recordings and utilized the original content without creating new text or recordings. This process represents collation from an existing audio source."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "Although the audio is from an existing audiobook, the authors manually transcribed, annotated with diacritics, segmented audio clips, removed inconsistent chapters, and processed the audio to create a corpus tailored for TTS training. These modifications, including manual transcription and quality control, constitute adaptation and derivation from the original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10551,
      "completion_tokens": 316,
      "total_tokens": 10867
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5 Baseline TTS systems, Table 3",
          "reasoning": "The ClArTTS corpus is used to train neural TTS systems (Grad-TTS and Glow-TTS) from scratch, as demonstrated in Section 5 where models trained with this dataset show improved performance compared to others."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6 Evaluation and results, Table 3",
          "reasoning": "The ClArTTS dataset is used for evaluating synthesized speech quality via subjective (MOS) and objective (PESQ, MCD, Lf0 RMSE, BAP, speaker similarity) metrics to benchmark the performance of baseline TTS systems."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Corpus statistics, Figure 1, Tables 1 and 2",
          "reasoning": "The dataset is analyzed to compare phoneme and diphone coverage and distribution against existing corpora and natural text distribution, helping to understand its characteristics relevant for TTS training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11274,
      "completion_tokens": 399,
      "total_tokens": 11673
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 (Abstract), Section 3.1 (Audio pre-processing), Section 4 (Corpus statistics), Section 7 (Conclusion)",
          "reasoning": "The ClArTTS corpus is extracted from an audiobook of a classical Arabic book, specifically in Classical Arabic language. The dataset contains fully diacritized transcriptions in Arabic and is intended for Arabic TTS research. The paper explicitly mentions the corpus contains recordings and transcriptions in Classical Arabic, a non-English human language, with no indication of other languages being part of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8492,
      "completion_tokens": 221,
      "total_tokens": 8713
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Entire Paper (No explicit section)",
          "reasoning": "The paper mentions the availability of the ClArTTS corpus and baseline TTS systems along with an interactive demo at www.clartts.com; however, it does not provide any explicit link or mention that the code or scripts used for data collection, preprocessing, segmentation, and annotation are publicly available. No GitHub or similar repository URLs or references to code release are given."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Corpus construction) and 4 (Corpus statistics)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including audio pre-processing steps with tools used (e.g., ffmpeg, Praat, Kaldi), annotation process with manual transcription and validation by multiple annotators using Praat TextGrid files, speech segmentation criteria including signal-to-noise ratio thresholds, removal of inconsistent chapters, and statistics on the corpus. This detailed description enables understanding and reproduction of the dataset construction procedures despite the lack of available code."
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7609,
      "completion_tokens": 116,
      "total_tokens": 7725
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Recordings; Section 2.2 Speakers",
          "Reasoning": "The new dataset consists of digitized audio recordings from the 1950s, produced by the musicologist Leon Metcalf, of Lushootseed speakers telling traditional myths and private correspondences. These recordings, although archival, are human-produced speech data captured via microphones and digitized, hence the modality is audio and the origin is human-generated recordings."
        }
      ]
    }
  },
  {
    "id": "kye24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8461,
      "completion_tokens": 231,
      "total_tokens": 8692
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.2 Speakers; Section 2.1 Recordings",
            "reasoning": "The data consists of archival recordings of two elder speakers of Lushootseed. The analysis involves expert examination of these recordings. Since original speakers are single individuals of linguistic expertise (native speakers, linguists) but the annotation and analysis are performed by presumably a single expert researcher analyzing archival data, the annotation is best categorized as Single Human Expert."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to annotators since data is from archival recordings and measurements are manually performed by an expert researcher rather than annotators following guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "There is no mention of scoring rubrics or similar evaluation criteria for annotations in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No examples of annotations or explicit annotation guidelines including examples are reported in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "kye24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9591,
      "completion_tokens": 269,
      "total_tokens": 9860
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human experts conducted any quality assurance of dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single human non-expert conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that multiple human non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report use of AI models for quality assurance purposes."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes using Praat software and linear mixed effects models for measurement and analysis, there is no explicit description of automated verification processes as quality assurance for annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process performed to validate the dataset annotations or content. The datasets come from archival recordings and the analysis appears to be done without explicit QA procedures described."
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9209,
      "completion_tokens": 324,
      "total_tokens": 9533
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Recordings",
          "reasoning": "The data used in the study consists of archival recordings from the Metcalf Collection, recorded in the 1950s by musicologist Leon Metcalf. These recordings were not newly collected or created by the authors but were accessed and examined from existing archival sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 2.3 Measurements and Sampling Procedure, 2.4 Analysis",
          "reasoning": "The authors performed acoustic measurements and analyses on the archival recordings, extracting features such as duration, intensity, spectral moments, and voice onset quality. These measurements involved transformation and analysis of existing audio data to derive new quantitative datasets used in their study."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9727,
      "completion_tokens": 308,
      "total_tokens": 10035
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The new dataset, consisting of digitized recordings of Lushootseed speech from two elder speakers, is used primarily for analyzing acoustic properties of affricates, including VOT, intensity, spectral moments, and voice onset quality. The analyses aim to characterize linguistic trends and typological classifications rather than to train or evaluate machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10450,
      "completion_tokens": 529,
      "total_tokens": 10979
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of recordings and analyses of Lushootseed, a single Coast Salish language. No indication of inclusion of multiple languages is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is focused solely on Lushootseed language recordings, with no mention of a second human language in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper and analysis are written in English, the dataset itself consists of Lushootseed language recordings, not English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2 (Methods - Recordings and Speakers)",
          "reasoning": "The dataset contains only Lushootseed language recordings from two speakers. The dataset represents a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code-related content is included in the dataset; only acoustic speech recordings are used."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although mathematical models (linear mixed-effects) are used in analysis, the dataset itself does not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pertains to human speech data; no biological sequences or non-human communication systems are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Lushootseed is a natural, indigenous human language, not a fictional or constructed language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the recordings is clearly identified and documented as Lushootseed."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of linguistic speech recordings containing human language, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7668,
      "completion_tokens": 151,
      "total_tokens": 7819
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention of code availability",
          "reasoning": "The paper does not mention or provide any links to code repositories or implementations related to data collection, preprocessing, or dataset construction. The methods describe using Praat software for measurements, but no code or scripts are provided or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the source and nature of recordings (archival recordings from the Metcalf Collection), speaker information, sampling procedures, and measurement methods. This information documents how the dataset was constructed and measured, enabling understanding of dataset provenance and processing."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8869,
      "completion_tokens": 139,
      "total_tokens": 9008
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 and 2.3",
          "Reasoning": "The paper introduces a new Spoken Question Answering dataset synthesizing audio from existing textual medical multiple-choice question answering corpora using Text-To-Speech (TTS) technology via the OpenAI TTS API. The audio data is thus synthetic and model generated, not recorded from humans speaking. This is explicitly stated in Section 2.2, stating the use of TTS for synthesis, and the prompt format in Section 2.3 confirms the dataset consists of audio MCQ prompts."
        }
      ]
    }
  },
  {
    "id": "labrak24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9721,
      "completion_tokens": 217,
      "total_tokens": 9938
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2",
            "reasoning": "The new dataset is synthesized by converting existing open-source textual MCQA corpora into audio via Text-To-Speech (TTS) technology using the OpenAI TTS API, thus the annotation is an automatic process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "No mention or description of annotation instructions or guidelines for humans is provided since the data is synthesized automatically from text to audio."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "As the dataset is generated automatically via TTS from existing textual datasets, there is no indication of scoring rubrics provided for annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "There is no description of annotation examples in guidelines since the audio data is synthetic, derived directly from existing textual MCQA datasets without additional human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "labrak24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10851,
      "completion_tokens": 345,
      "total_tokens": 11196
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or indication is provided about quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information describing quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance of the dataset is not stated to have been performed by any AI model as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automatic verification or algorithmic quality assurance specifically for the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No quality assurance process is described or documented for the newly introduced medical spoken question answering dataset. The dataset is synthesized via TTS from existing textual datasets without indication of verification or validation steps."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10469,
      "completion_tokens": 443,
      "total_tokens": 10912
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that new datasets were created from original human-written content. Instead, it explicitly mentions that the datasets are derived from existing textual multiple-choice question answering (MCQA) corpora."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While synthetic audio data is generated via Text-To-Speech (TTS) models, the textual question-answer datasets themselves are not newly generated by AI or machine learning models from scratch. The audio data is synthesized from existing textual data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given in the paper that the datasets involve human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation is made in the dataset creation process."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The new benchmark dataset is constructed by collecting existing textual MCQA datasets relevant to healthcare (MMLU, MedQA, MedMCQA) and aggregating them to form the evaluation benchmark in the medical domain."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The textual datasets are transformed into spoken audio by synthesizing the existing multiple-choice questions and options using Text-To-Speech (TTS) technology. This constitutes a derivation, as the audio data is based on existing textual data but adapted by synthesis into speech format."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the source and generation method of the dataset; thus, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10987,
      "completion_tokens": 523,
      "total_tokens": 11510
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not used for pre-training large models on general patterns. The paper focuses on zero-shot evaluation and does not indicate any pre-training usage of the dataset."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training any model from scratch on the new dataset. Only zero-shot evaluations and some fine-tuning on existing encoders are mentioned, but not training from scratch on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used to fine-tune pre-trained models with supervised learning. The study operates under zero-shot conditions without supervised fine-tuning on the new dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RLHF utilization involving the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2.2, 2.4, 4, and 4.1",
          "reasoning": "The new dataset, synthesized from existing medical MCQA corpora and converted to audio, is used exclusively as an open benchmark for zero-shot evaluation of spoken question answering systems. The evaluation metric is accuracy, and the experiments compare baseline cascade systems and proposed end-to-end methods on this dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "A subset of the dataset (MedMCQA training subset) is used for analyzing the distribution of relevant information across encoder layers in speech encoders. This analysis aims to better understand model behavior rather than training for task performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly utilized for evaluation and analysis; thus, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11710,
      "completion_tokens": 415,
      "total_tokens": 12125
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The dataset is synthesized solely from English textual medical multiple-choice question answering datasets. There is no mention of multiple languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The dataset contains only English questions and answers; no second language is involved in the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The novel dataset is constructed by synthesizing audio from existing English textual medical multiple-choice question answering corpora. The paper explicitly states all datasets are in English, and no other human languages are involved."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are all in English; no indication of non-English monolingual data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of spoken questions and options in natural language; no programming or code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are natural language questions and options related to medical knowledge; no mathematical or formal logic notation is included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech synthesized from textual medical questions; no biological sequences or non-human communication systems are present."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural English language content only, with no constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly stated and documented as English; language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken English human language (audio synthesized from English text), thus not language-free."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8928,
      "completion_tokens": 198,
      "total_tokens": 9126
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2.2, Footnote 1",
          "reasoning": "The paper states in the abstract and Section 2.2 that the new synthetic medical spoken question answering dataset and all code and data are publicly released on GitHub and Hugging Face, with a direct URL given as https://huggingface.co/SpokenMedicalQA. This indicates that all code related to dataset construction and processing is available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2",
          "reasoning": "The paper dedicates Section 2.2 to describing the dataset construction process: it explains that the new SQA dataset is synthesized from existing textual medical multiple-choice QA corpora using an OpenAI TTS API, details the source corpora, the synthesis procedure, the audio format, speaker variety, and statistics about the resulting dataset. This demonstrates clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 13131,
      "completion_tokens": 288,
      "total_tokens": 13419
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.1, 3.2",
          "Reasoning": "The paper introduces a new African American English (AAE) dataset consisting of speech recordings. These are human-produced audio recordings of speakers exhibiting different types of voice quality, including creaky voice and modal voice. The data is clearly human-generated since it involves recordings of human speakers, explicitly analyzed for phonation types and acoustic features. There is no indication that the audio data was generated by models or derived from uncertain sources."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1; Table 2",
          "Reasoning": "The analyses presented include tabulated acoustic measurement results (e.g., Table 2) derived from processing the human-generated speech audio data. These tabular data represent measurements such as harmonic-to-noise ratios and cepstral peak prominence extracted from the audio. Although generated by computational analysis, the base data originates from human recordings. The tabular data are not model-generated synthetic data but represent summarized and measured human speech data."
        }
      ]
    }
  },
  {
    "id": "lai24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 13983,
      "completion_tokens": 268,
      "total_tokens": 14251
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3 and 3.2",
            "reasoning": "The paper describes detailed acoustic and sociolinguistic analyses based on manually annotated creaky voice data within a newly introduced African American English (AAE) dataset. The complexity and nature of phonation type annotation, as well as sociolinguistic labeling, imply involvement of multiple trained human experts rather than automated or single annotator processes."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 and Methods implied in discussions",
            "reasoning": "The detailed acoustic analysis and classification of modal, partial creak, and full creak voice qualities indicate the presence of clear annotation instructions for consistency across annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 and Table 1",
            "reasoning": "The paper specifies quantitative acoustic measures and clear criteria (e.g., H1H2c, CPP, HNR05-35) used to distinguish voice quality types, effectively acting as scoring rubrics for annotation validity."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention throughout the paper",
            "reasoning": "The paper does not report or provide explicit annotation examples or samples in guidelines or appendices for the annotators."
          }
        }
      ]
    }
  },
  {
    "id": "lai24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 15113,
      "completion_tokens": 284,
      "total_tokens": 15397
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being done by a single human expert annotator for the new AAE dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or explicit statement that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided about quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models (ASR systems) are discussed in the paper, there is no indication that an AI model was used specifically to perform quality assurance on dataset annotations or contents."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated or algorithmic verification process used as quality assurance for the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not report or describe any quality assurance procedures applied to the new AAE dataset annotations or content; no mention of human or automatic QA processes is found."
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 14731,
      "completion_tokens": 388,
      "total_tokens": 15119
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Abstract; Sections 1 and 3",
          "reasoning": "The paper introduces a new African American English (AAE) dataset collected and analyzed by the authors. The data consists of speech recordings from young AAE speakers, with manual annotation of creaky and modal phonation types. There is no indication that the data was translated or derived from existing sources; rather, it is original speech data collected from humans for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated entirely by AI or machine learning models in this study."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not described as aggregated from existing sources; it is newly collected speech data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was derived from existing sources with modifications; it is presented as newly collected original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data collection and characteristics are clearly described, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 15249,
      "completion_tokens": 210,
      "total_tokens": 15459
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The new African American English (AAE) dataset is primarily used for detailed acoustic and sociolinguistic analysis of creaky voice characteristics, their distribution by gender and position, and their relation to ASR error rates, rather than for model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 15972,
      "completion_tokens": 535,
      "total_tokens": 16507
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced contains speech data only from African American English speakers, with no mention of other human languages being present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes two languages; it focuses exclusively on African American English speech."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3.2",
          "reasoning": "The dataset comprises spoken African American English (AAE), a variety of English. All analyses pertain to English speech only, with no other language content mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is specifically of African American English, a dialect of English, so it is not non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions software tools for analysis and ASR systems, the dataset itself consists of spoken English data, not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain mathematical or logical notation; such notation is used in analyses but not as part of the data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human speech (African American English) and does not include biological or non-human communication sequences."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the dataset; it consists solely of a natural variety of English."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is explicitly stated as African American English, a variety of English, so the language is known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken language data in African American English; it does contain language."
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 13190,
      "completion_tokens": 203,
      "total_tokens": 13393
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any links, URLs, or references to publicly available code repositories related to the dataset construction, preprocessing, or analysis. There is no indication that the code used to create or analyze the new African American English (AAE) dataset is publicly shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3 (Data, Materials, and Methods; Analyses and Results)",
          "reasoning": "The paper provides detailed information about the dataset creation, including participant demographics, recording methods, acoustic measures used, and analytical methods (e.g., mixed linear effects models in R). It describes the acoustic measures, phonation types, data points, and data processing steps such as z-score normalization, use of specific software tools (e.g., Praat, R packages). This comprehensive documentation supports reproducibility of the dataset creation and analysis process."
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8062,
      "completion_tokens": 180,
      "total_tokens": 8242
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Training sets",
          "Reasoning": "The authors introduced a new training set from the Providence corpus, consisting of 128 hours of human-annotated speech utterances from spontaneous parent-child interactions. This is a directly human-recorded dataset capturing naturalistic infant-parent interactions in audio form."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Training sets",
          "Reasoning": "The authors introduced another new training set from SEEDLingS, consisting of 1,024 hours of adult speech utterances extracted from child-centered long-form recordings collected via child-worn microphones in everyday family settings. This is human-recorded audio data capturing real-life language experiences of infants."
        }
      ]
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8914,
      "completion_tokens": 295,
      "total_tokens": 9209
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1.1 and 2.1.2",
            "reasoning": "The lexical and syntactic evaluation tasks are generated automatically using computational methods: pseudo-word generation via Wuggy pipeline and template-based sentence generation filled with high-frequency CHILDES words. The benchmark stimuli are then synthesized using Google Text-To-Speech, indicating no human manual annotation was required for the labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not describe providing instructions for annotators because the datasets were created via automatic or algorithmic generation rather than human annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.1.1 and 2.1.2",
            "reasoning": "The scoring rubric is clearly defined: the system scores 1 if it assigns higher probability to the correct (real) item over the incorrect (pseudo/ungrammatical) one, and 0 otherwise. This is an intrinsic evaluation metric specification."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 1 and Table 2 in Section 2.1.1 and 2.1.2",
            "reasoning": "The paper provides concrete examples of the minimal pairs for pseudo-words versus real words and grammatical versus ungrammatical sentences in these tables, serving as examples for the automatic generation process."
          }
        }
      ]
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10044,
      "completion_tokens": 379,
      "total_tokens": 10423
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator who is a subject matter expert or from the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple expert human annotators performing quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert human annotators providing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used to evaluate language models, the paper does not state that AI models are used to perform quality assurance on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.1.1 and 2.1.2",
          "reasoning": "The dataset creation primarily relies on algorithmic and automated procedures such as automatic generation of pseudowords using the Wuggy pipeline, automatic extraction and synthesis of stimuli using Google Text-To-Speech (TTS) system with multiple voices, automatic template generation for syntactic minimal pairs, and the use of automated voice activity detection (VAD) to segment speech data. There is no mention of manual annotation or curation quality control; thus, quality assurance is conducted through automated verification and generation techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes automated, systematic processes used in dataset construction and thus quality assurance by automated means is documented; therefore, the N/A label does not apply."
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9662,
      "completion_tokens": 523,
      "total_tokens": 10185
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors did not create original speech data or textual data entirely from scratch by human contributors. Instead, they used existing datasets (Providence, SEEDLingS) collected by others."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data was generated purely by AI or machine learning models without reference to existing data. Although they synthesize speech stimuli using Google Text-To-Speech, this is derived from existing words/templates."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data used or created was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to produce the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2, 'Training sets'; Section 3.1",
          "reasoning": "The authors constructed training sets by extracting and aggregating existing speech corpora: Providence and SEEDLingS, which are existing recorded datasets of child-centered speech and adult speech from naturalistic settings. These data were collected by previous work and simply aggregated for training."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.1 Lexical evaluation and Section 2.1.2 Syntactic evaluation",
          "reasoning": "The authors generated their benchmark data by transforming existing resources: they created synthetic minimal pairs for the lexical task using pseudo-word generation from existing word lists in CHILDES, CELEX, and CMU dictionaries, processed with the Wuggy pseudoword generator, and synthesized these with Google Text-To-Speech. For the syntactic task, templates of simple grammatical phenomena were filled with frequent words from CHILDES to generate minimal pairs of grammatical and ungrammatical sentences, which were also synthesized. Thus, these benchmark datasets are derived from existing linguistic sources and computational transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation methods are clearly described."
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10180,
      "completion_tokens": 412,
      "total_tokens": 10592
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 3.1",
          "reasoning": "The new datasets derived from the Providence and SEEDLingS corpora are used for training language models from scratch, specifically STELA and LSTM models, as described in Section 2.3 (Models) and evidenced by results reported in Section 3.1 (The BabySLM benchmark). The paper discusses language model training directly on these new datasets without pre-training, demonstrating their use as training data from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 3.1",
          "reasoning": "The BabySLM benchmark itself is a newly introduced evaluation dataset comprising lexical and syntactic tasks designed for assessing spoken language models. Section 2.1 describes the construction of these evaluation sets, and Section 3.1 demonstrates their use exclusively for evaluation of language model performance on lexical and syntactic knowledge, serving as a benchmark."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.3",
          "reasoning": "The paper uses the new benchmark and datasets to analyze and highlight challenges in modeling language acquisition from speech data, particularly concerning data plausibility and speech conditions (clean vs. in-the-wild speech). Section 3.1 discusses trends in performance related to data plausibility, and Section 3.3 analyzes difficulties in speech variability, demonstrating use of the datasets for analytical insights beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10903,
      "completion_tokens": 579,
      "total_tokens": 11482
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses datasets and benchmarks in English; no mention is made of multiple human languages included in the new datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or statement in the paper that the new datasets contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 2.1.1, 2.1.2, and 3.1",
          "reasoning": "The paper explicitly mentions that the vocabulary and syntactic phenomena are drawn from American English child-centered corpora such as CHILDES, Providence, SEEDLingS, and AO-CHILDES, all of which are English datasets. The lexical and syntactic evaluation sets are generated using English words and sentences, and the benchmark is stated to focus on English in order to align with typical children's language experience. Section 4 also acknowledges the benchmark focuses on English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets and benchmark are not constructed from any single non-English language; the focus is solely on English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-related content is included in the new datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses syntactic phenomena and linguistic structures, there is no indication that the datasets themselves contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are composed of human speech data (spoken English) and do not involve biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any constructed languages, fictional or artificial."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content is clearly specified as English throughout the dataset description."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain human language data (spoken English), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8121,
      "completion_tokens": 160,
      "total_tokens": 8281
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4 Conclusion and footnote 2",
          "reasoning": "The paper provides a GitHub repository link (https://github.com/MarvinLvn/BabySLM) for the BabySLM benchmark scripts, indicating availability of code for dataset creation and evaluation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 Task generation (lexical and syntactic tasks) and 2.2 Training sets",
          "reasoning": "The paper provides detailed documentation on dataset creation including vocabulary selection from CHILDES, pseudo-word generation using Wuggy, template generation for syntactic tasks, speech synthesis procedures, dataset splits, and description of training sets. This constitutes comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6080,
      "completion_tokens": 190,
      "total_tokens": 6270
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Data Collection - Real-world dataset (REAL)",
          "Reasoning": "The VietMed-Sum dataset includes real-world medical ASR data from the VietMed corpus, which is recorded speech from medical conversations, thus the modality is audio. The data is collected from real medical speech interactions involving humans, so it is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Data Collection - Simulated dataset (SIM)",
          "Reasoning": "The SIM dataset is created by simulating medical conversations from extra medical text data. The simulation incorporates hesitations and disfluencies programmatically to mimic natural speech patterns. Hence, it is text modality that is model generated through simulation, not directly recorded from humans."
        }
      ]
    }
  },
  {
    "id": "leduc24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6932,
      "completion_tokens": 267,
      "total_tokens": 7199
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1, Section 4, Acknowledgement",
            "reasoning": "The Gold standard (GOLD) subset of the VietMed-Sum dataset includes human-edited summaries where human annotators edited GPT-generated summaries according to annotation guidelines. The acknowledgements mention individuals who helped the initial annotation, suggesting multiple expert human annotators involved in the editing process. The dataset is medical domain specific, implying expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1, Section 4",
            "reasoning": "Section 3.1 states that human annotators edited GPT summaries according to annotation guidelines, implying instructions were provided for annotation. Section 4 discusses the collaboration with human annotators and guidelines for editing GPT summaries."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention scoring rubrics or criteria for scoring annotations, only guidelines to edit the GPT summaries, suggesting absence of formal rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe providing annotation examples in guidelines or appendix; it only mentions the annotation guideline without examples."
          }
        }
      ]
    }
  },
  {
    "id": "leduc24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8062,
      "completion_tokens": 437,
      "total_tokens": 8499
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human annotator who is a subject matter expert or target demographic member. The human annotator presence is mentioned but there is no detail indicating a single expert performing QA."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit indication that multiple human annotators with subject matter expertise performed quality assurance. The paper mentions human editing but does not specify multiple experts or their expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed solely by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 and Acknowledgement",
          "reasoning": "The paper states that human editing is performed on GPT-generated summaries on the GOLD set by human annotators according to annotation guidelines. However, it does not specify that the human annotators are subject matter experts. The Acknowledgement section thanks two individuals for helping the initial annotation but does not clarify their expertise, thus it is reasonable to classify the QA human annotators as non-experts. Furthermore, the use of multiple annotators is implied rather than explicitly stated, but the acknowledgment of contributors and editing suggests multiple annotators participated in the quality assurance process."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1, 4, and 4.2",
          "reasoning": "The dataset annotations are initially generated by GPT-3.5 Turbo (ChatGPT) as an AI model for annotation. This means an AI model performed the initial creation and therefore quality assurance of the synthetic summaries, acting as an automated annotator."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification of the datasets via code or rule-based algorithmic approaches."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a QA process involving AI-generated summaries and human editing, so quality assurance is documented."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7680,
      "completion_tokens": 429,
      "total_tokens": 8109
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that new dataset content was created entirely by human contributors from scratch without reference to existing data. Instead, the data used is based on existing medical conversation datasets and simulated data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors use GPT-3.5 Turbo to generate summaries, the core datasets themselves (the speech transcripts) were not generated by AI models. The summaries generated by models are annotations, not the dataset content itself."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention human translators producing dataset content by translating from another language. The English version of the dataset is translated using machine translation (Google Translate), not human translation."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 3.5 'English-translated VietMed-Sum'",
          "reasoning": "The English version of the VietMed-Sum dataset (VietMed-Sum-en) is generated using Google Translate, a machine translation system, translating from the original Vietnamese data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 'Data Collection'",
          "reasoning": "The dataset comprises VietMed real-world medical speech transcripts collected earlier and simulated conversations generated by script imitating VietMed data; thus, it is collated from existing sources and simulated data without full human creation from scratch."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 'Data Collection' and Section 3.1 'Labeling strategy'",
          "reasoning": "The synthetic subset (SYN) is generated by GPT-3.5 Turbo summarization of the existing transcripts or simulated conversations; likewise, the GOLD set involves human editing of GPT-generated summaries, so the summary data is derived from GPT model outputs and existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly described in the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8198,
      "completion_tokens": 529,
      "total_tokens": 8727
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate any use of the VietMed-Sum dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the VietMed-Sum dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 6 (Baselines and Experimental Results), Tables 4-7, and Section 3 (Data)",
          "reasoning": "The paper presents baseline results of various state-of-the-art pre-trained models fine-tuned using the VietMed-Sum dataset with supervised learning methods on gold standard and synthetic summaries (Section 6 and Tables 4-7). This demonstrates the dataset's active use for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the VietMed-Sum dataset for reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6 (Baselines and Experimental Results), Tables 4-7",
          "reasoning": "The dataset's test sets (GOLD, SYN) are used to evaluate and benchmark baseline models' performance, as shown in multiple tables (Tables 4-7), indicating its use for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the dataset for analyzing trends or characteristics beyond its role for training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or demonstrated as serving as a knowledge base for augmenting models, such as in retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usages of the VietMed-Sum dataset in supervised fine-tuning and evaluation of baseline models."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8921,
      "completion_tokens": 615,
      "total_tokens": 9536
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains entries in Vietnamese and an English-translated version, but the original conversation data and summaries are not presented as multilingual entries containing more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although there is an English-translated version of the dataset, the main dataset entries are Vietnamese medical conversations. They are not described as bilingual conversation entries containing exactly two languages simultaneously."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.5 English-translated VietMed-Sum",
          "reasoning": "The dataset includes an English-translated version (VietMed-Sum-en) created via Google Translate, but the original new dataset introduced (VietMed-Sum) is not solely English content. The English-translated set is a derived version, not the primary dataset introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Collection; Section 3.1 Labeling strategy",
          "reasoning": "The new dataset VietMed-Sum is primarily composed of real-world Vietnamese medical speech conversations and their summaries. The dataset entries are monolingual in Vietnamese, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Section 3.2 Data Collection",
          "reasoning": "While the authors mention pseudo Python code for simulating conversations in the appendix, the dataset itself consists of speech transcripts and summaries, not entries of code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the dataset content or description that mathematical or formal logical expressions or symbolic representations are included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of human medical conversation transcripts and their summaries. No biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that any fictional or artificial languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset are clearly documented as Vietnamese and an English-translated version is explicitly described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language, specifically Vietnamese."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6139,
      "completion_tokens": 180,
      "total_tokens": 6319
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1 Introduction",
          "reasoning": "The paper explicitly states in the abstract and introduction that all code, data (including English-translated and Vietnamese), and models are available online. This indicates that the code related to dataset construction, including data collection, preprocessing, and generation, is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Data and Section 3.2 Data Collection",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 3. It discusses the use of real-world VietMed dataset and simulated data, the labeling strategy combining GPT-generated and human-edited summaries, and the rationale for the data splits (GOLD and SYN). The paper includes explanations of data simulation approaches and annotation methods, providing sufficient transparency and completeness for reproducibility."
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7734,
      "completion_tokens": 120,
      "total_tokens": 7854
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 'Longitudinality of the Phonetic Inventory' and Conclusion",
          "Reasoning": "The paper introduces a longitudinal dataset of speech productions from children with cochlear implants, collected over 5 years post-implantation. This dataset consists of phonetic inventories derived from speech production data, including both spontaneous and imitation speech. The data involves recordings of children's speech, which is audio captured through human recording processes at clinical and therapy centers, thus involving human-generated data."
        }
      ]
    }
  },
  {
    "id": "lee22k_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8586,
      "completion_tokens": 196,
      "total_tokens": 8782
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Throughout the paper, e.g., Sections 3.1, 3.2, and Results",
            "reasoning": "The paper mentions phonetic transcriptions and error analyses done by the first author, indicating expert human annotation; no mention of multiple annotators or automatic annotation processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section or appendix describing annotation instructions",
            "reasoning": "The paper does not describe detailed instructions provided to annotators nor a separate annotation guideline document."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "No mention of scoring rubrics or formal evaluation criteria for annotation is present in the text."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not provide examples of annotated data or annotation examples in any section or appendix."
          }
        }
      ]
    }
  },
  {
    "id": "lee22k_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9716,
      "completion_tokens": 187,
      "total_tokens": 9903
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Conclusion and Section 3.1 Production development",
          "reasoning": "The paper mentions mispronunciations were identified by the first author who is likely a subject matter expert in speech and language pathology, as indicated by the mention of a national certificate of speech and language pathology. This suggests quality assurance was performed by a single expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9334,
      "completion_tokens": 320,
      "total_tokens": 9654
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Throughout the paper, including 'Results' and 'Conclusion' sections",
          "reasoning": "The dataset consists of longitudinal phonetic inventories of children with cochlear implants collected via clinical sessions and speech-language therapy centers over five years. The data was generated through direct recordings and analyses of children's speech productions (both spontaneous and imitation), which represents original content created entirely from human participants without adaptation or translation from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study generated data from direct collection and longitudinal tracking, rather than aggregation of existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some comparison is made with previous studies, the new dataset itself is original raw data, not derived from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9852,
      "completion_tokens": 305,
      "total_tokens": 10157
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Throughout the paper, especially in the sections discussing the longitudinal phonetic inventory development and analysis of phonetic errors",
          "reasoning": "The dataset introduced consists of phonetic inventories of children with cochlear implants tracked longitudinally over 5 years. It is used primarily to analyze and understand patterns in speech and language development rather than for any form of machine learning training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10575,
      "completion_tokens": 369,
      "total_tokens": 10944
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Throughout the paper (e.g., Abstract and Results sections)",
          "reasoning": "The new dataset introduced is a phonetic inventory and speech corpus of children with cochlear implants producing Korean speech sounds. The paper explicitly describes the Korean phoneme system and analyzes speech data from Korean-speaking children, indicating the dataset contains exactly one non-English language: Korean."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human spoken language data in Korean."
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7793,
      "completion_tokens": 174,
      "total_tokens": 7967
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about source code availability related to the dataset collection, preprocessing, or generation. There is no mention of code repositories or supplementary material containing code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections describing longitudinal study design and phonetic inventory collection processes throughout the paper (e.g., introduction and results sections)",
          "reasoning": "The paper documents the dataset creation process by detailing the longitudinal study conducted on children with cochlear implants over 5 years, including the ages of assessment, types of phonetic inventories collected (spontaneous and imitation speech), and comparison with normal hearing children. These details provide transparency on how the new phonetic inventory dataset was collected and analyzed, although there is no explicit mention of dataset publication or detailed collection protocols."
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8022,
      "completion_tokens": 123,
      "total_tokens": 8145
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Dataset; Speech Corpus of Korean-Speaking Children with CIs",
          "Reasoning": "The newly introduced dataset is a speech corpus constructed by the authors for Korean-speaking children with cochlear implants (CIs). The audio data were extracted from videos recorded during regular evaluation sessions, manually segmented, transcribed, and rated by certified speech and language pathologists. This indicates human involvement in recording and manual processing, confirming human-generated origin of audio data."
        }
      ]
    }
  },
  {
    "id": "lee24e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8874,
      "completion_tokens": 246,
      "total_tokens": 9120
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1.1",
            "reasoning": "The speech corpus of Korean children with cochlear implants was rated on speech production skills by three nationally certified speech and language pathologists, each with at least two years of relevant experience, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1.1",
            "reasoning": "The paper states evaluations were conducted from the perspective of general speech production considering articulation, prosody, and voice quality with a defined scoring range from 1 to 5, implying that instructions were provided to guide the rating process."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1.1",
            "reasoning": "A defined scoring rubric is described with scores ranging from 1 (very poor) to 5 (very good) speech production skills, which qualifies as a scoring rubric for the experts' ratings."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "There is no mention of annotation examples or exemplars provided to the experts in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "lee24e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10004,
      "completion_tokens": 195,
      "total_tokens": 10199
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.1 - Speech Corpus of Korean-Speaking Children with CIs",
          "reasoning": "The speech production skills scores for the children with cochlear implants were rated by three nationally certified speech and language pathologists, each with at least two years of experience in rehabilitation or phonetics. This indicates multiple human experts conducted the quality assurance of the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9622,
      "completion_tokens": 434,
      "total_tokens": 10056
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The paper explicitly states that the speech corpus for Korean children with cochlear implants (CIs) is being constructed because there was no available dataset previously. It involves data collected from 30 children with CIs during regular evaluation sessions, with audio extracted from videos, orthographic and phonemic transcriptions, and expert ratings by certified speech and language pathologists. This process indicates original data collection created entirely from scratch by human contributors for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the creation of any dataset generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate any dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset for children with CIs is not described as collected or aggregated from existing sources without modification; it was newly collected. Other datasets used are pre-existing but not introduced or created by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the new dataset is derived by modification or adaptation of an existing source; rather it was created through original data collection."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and creation method of the new dataset, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10140,
      "completion_tokens": 369,
      "total_tokens": 10509
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 3.2",
          "reasoning": "The speech corpus of Korean-speaking children with cochlear implants, which is newly constructed by the authors, is used to fine-tune their assessment model in a supervised manner. Specifically, Section 3.1 describes the dataset construction and rating by experts, while Section 3.2 describes the use of 5-fold cross-validation of this dataset for training and evaluation of the model. The dataset is used as labeled supervised data to train the prediction model of speech production scores from embeddings."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4.1",
          "reasoning": "The same speech corpus of children with CIs is used also for performance measurement, as reported in Section 4.1, by calculating Pearson correlation coefficients between predicted scores and expert-labeled scores. The experiments use 5-fold cross-validation with this dataset for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "In Section 5, the authors analyze results based on various embedding combinations and discuss the significance of using both adult and child speech patterns in the model. This indicates the dataset is used for analysis of speech characteristics and model performance nuances."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10863,
      "completion_tokens": 500,
      "total_tokens": 11363
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset introduced is exclusively Korean speech data with no mention of more than two human languages within the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The proposed new dataset contains only Korean language speech samples; no two-language dataset is introduced."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The new dataset introduced contains Korean speech data, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset",
          "reasoning": "The newly constructed speech corpus contains Korean language speech recordings from children with cochlear implants, making it monolingual non-English Korean data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries consist of human speech audio and phoneme sequences, not code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical symbolic expressions appear as entries in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech only; it does not contain biological sequences or animal/non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves natural Korean language speech, with no mention of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the speech data is clearly specified as Korean."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data and therefore is not without language."
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8081,
      "completion_tokens": 169,
      "total_tokens": 8250
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1.1 (Dataset) and throughout the paper",
          "reasoning": "The paper mentions that the speech corpus for Korean children with cochlear implants is being constructed and describes its collection and annotation process, but there is no mention or link to publicly available code or repository related to data collection, preprocessing, or dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1.1 (Dataset - Speech Corpus of Korean-Speaking Children with CIs)",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including information about the speakers, data collection procedure, audio extraction, transcription, expert rating protocols, ethical approval details, and data selection criteria, contributing to transparency and completeness of dataset documentation."
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9350,
      "completion_tokens": 112,
      "total_tokens": 9462
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 2.2, Table 1",
          "Reasoning": "Speech-MASSIVE is an audio dataset created by the authors by recruiting native speakers on Prolific to record spoken versions of MASSIVE textual data in 12 languages. The recordings are human-generated as they are performed by native human speakers. The paper details the recording and validation process confirming these data were manually collected from humans."
        }
      ]
    }
  },
  {
    "id": "lee24i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10202,
      "completion_tokens": 241,
      "total_tokens": 10443
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1",
            "reasoning": "The speech data was collected by recruiting native speakers through the Prolific crowdsourcing platform, involving multiple annotators who recorded and validated the utterances. These workers were native speakers but not described as experts, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1",
            "reasoning": "The paper states that workers were instructed with guidelines emphasizing accurate and natural reading, proper recording conditions, and strict adherence to the text during recording and validation phases, showing the presence of instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no mention of explicit scoring rubrics being provided to annotators; the validation involved binary labeling of utterances as valid or invalid without detailed scoring criteria."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not report providing annotation examples or examples of annotations for the workers; the instructions focused on adherence and quality rather than illustrating examples."
          }
        }
      ]
    }
  },
  {
    "id": "lee24i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11332,
      "completion_tokens": 319,
      "total_tokens": 11651
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple expert annotators performed quality assurance; the annotators mentioned are native speakers recruited via a crowdsourcing platform, but their expert status is not documented."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that quality assurance was performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 (Speech data collection and validation process)",
          "reasoning": "The dataset's quality assurance was conducted by a second group of native speakers recruited via the Prolific crowdsourcing platform, who performed validation of the recorded utterances by listening and marking valid or invalid. These multiple native speakers likely do not possess subject matter expertise beyond being native speakers (i.e., not experts), thus fitting this label."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process described does not involve AI models judging the data validity."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of automated verification or algorithmic quality assurance processes for dataset validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A clear quality assurance process is described involving multiple human validators for dataset validation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10950,
      "completion_tokens": 449,
      "total_tokens": 11399
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The Speech-MASSIVE dataset was created by recruiting native speakers through the Prolific crowdsourcing platform who recorded spoken versions of the MASSIVE sentences. These recordings are original speech data generated directly by human contributors, adhering closely to the original text, thus constituting new human-created data rather than any pre-existing speech resource."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated entirely by AI or machine learning models as part of the dataset creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the underlying MASSIVE text dataset involves human translation/localization of the SLURP dataset texts to multiple languages, Speech-MASSIVE itself introduces speech data recorded via vocalization of these texts rather than producing new translated text data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is involved in the creation of the speech data in Speech-MASSIVE."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The speech data is not collected from existing speech corpora but created afresh via new human recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 and Abstract",
          "reasoning": "Speech-MASSIVE speech data is derived from the pre-existing MASSIVE textual dataset by recording spoken versions of the texts. This represents an adaptation of existing textual data into speech form, thus the speech data is based on existing source texts with a transformation applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation methods are explicitly documented and described in the paper."
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11468,
      "completion_tokens": 362,
      "total_tokens": 11830
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The Speech-MASSIVE dataset is used to fine-tune pre-trained models in supervised learning settings for Spoken Language Understanding (SLU). Section 3.3 details the use of Speech-MASSIVE for fine-tuning cascaded SLU models, and Section 3.4 reports fine-tuning of an end-to-end SLU model using Speech-MASSIVE's training sets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.3, 3.4 and 3.5",
          "reasoning": "Speech-MASSIVE is employed to evaluate SLU model performance in various training scenarios (zero-shot, few-shot, full fine-tune) across multiple languages as well as to benchmark other speech-related tasks such as language identification and speech translation, as discussed in Sections 3.3 to 3.5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Conclusion)",
          "reasoning": "The paper discusses analysis of model performances and multilingual trends, including the influence of training language on zero/few-shot SLU performance and comparison of cascaded and end-to-end models, leveraging the Speech-MASSIVE dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12191,
      "completion_tokens": 509,
      "total_tokens": 12700
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 2.2",
          "reasoning": "The dataset Speech-MASSIVE is explicitly described as multilingual, covering 12 human languages: Arabic, German, Spanish, French, Hungarian, Korean, Dutch, Polish, European Portuguese, Russian, Turkish, and Vietnamese (Abstract and Section 2.2). Therefore, it clearly includes more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Speech-MASSIVE includes 12 languages and is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages beyond English; it is not English-only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is multilingual, with multiple non-English languages included, not only one single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech data in human languages only, with no mention of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human speech in 12 natural languages and does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human languages, so it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9409,
      "completion_tokens": 211,
      "total_tokens": 9620
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2 (footnote)",
          "reasoning": "The paper's abstract includes a URL (https://github.com/hlt-mt/Speech-MASSIVE) indicating that the dataset, models, and code are publicly available. This is further supported by the footnote in Section 2 mentioning the use of Prolific platform and detailed recruitment and validation processes. Hence, the code related to dataset collection, preprocessing, and generation can be accessed by the community."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 2.1, covering recruitment of native speakers via Prolific, instructions for recording, multi-stage validation procedures including quality control measures, and the selection criteria for languages. Section 2.2 offers overall statistics and rationale for language selection. These comprehensive details document the process sufficiently for transparency and reproducibility."
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9352,
      "completion_tokens": 228,
      "total_tokens": 9580
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Material and Section 2.3 Procedures",
          "Reasoning": "The study used the original Yanny vs. Laurel audio stimulus which was a human-recorded audio clip downloaded from a Twitter link. The stimulus was presented to participants as audio for perception, making the data modality audio. The audio clip originates from a human-recorded source, not model generated or unknown origin."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Listeners and Section 3 Results",
          "Reasoning": "The dataset consists of responses collected from 974 Swiss German listeners regarding their percept (Yanny, Laurel, mixed). These responses, along with demographic and device information (age, gender, region, musicality, device used), form structured participant metadata and labeled perceptual responses stored as tabular data. The data was manually recorded through a controlled survey procedure, indicating human generation rather than automated or model-generated data."
        }
      ]
    }
  },
  {
    "id": "leemann22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10204,
      "completion_tokens": 301,
      "total_tokens": 10505
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 and 2.3",
            "reasoning": "The paper describes that 974 Swiss German listeners participated in the perception task, balanced for age, gender, and regional origin, responding to the Yanny vs. Laurel stimulus. These listeners are members of the general population (non-experts) who provided perceptual judgments."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "Listeners were given instructions on the survey response task, including how to listen to the stimulus (play once only) and how to record their perceptual response (as Yanny, Laurel, or mixed), shown in Figure 4, indicating presence of annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "The paper does not mention any detailed rubrics or scoring criteria beyond the categorical response options provided. The coding simply used three categories: Yanny, Laurel, or mixed, without further grading or scoring schemes."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.3 (Figure 4)",
            "reasoning": "Examples of possible responses were given, including examples of mixed percepts such as 'none of the above', 'both', 'Yowrie', 'Yowrel', 'Yerry', demonstrating that participants were provided with examples of valid response types for annotation."
          }
        }
      ]
    }
  },
  {
    "id": "leemann22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11334,
      "completion_tokens": 337,
      "total_tokens": 11671
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Procedures",
          "reasoning": "The dataset was collected from 974 Swiss German participants, who provided perceptual responses to the Yanny vs. Laurel stimulus. Responses were coded as Yanny, Laurel, or mixed, which involved human coding of participant responses. The participants themselves are non-expert listeners (general population, not subject matter experts), and the dataset labels correspond to their perceptual judgments. The annotations (i.e., categorization of responses) were performed by multiple humans (the annotators who coded these responses), but there is no explicit mention that these coders had any subject matter expertise. Thus, quality assurance was performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10952,
      "completion_tokens": 456,
      "total_tokens": 11408
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Listeners and Section 2.3 Procedures",
          "reasoning": "The dataset consists of responses from 974 Swiss German listeners who participated in the study by listening to the original Yanny v. Laurel stimulus and reporting their percepts. These participants were human subjects providing original data through their auditory perception and responses, not translations or adaptation of existing datasets. The paper explicitly states the collection of new perceptual data from these human participants using a bespoke app and controlled procedures."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data in the paper is described as being generated by AI or machine learning models from scratch."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses the existing original Yanny v. Laurel audio stimulus, the main dataset consists of perceptual responses collected new from human participants, not an aggregation of existing datasets without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 Material",
          "reasoning": "The original Yanny v. Laurel stimulus audio was retrieved from an existing online source (a Twitter post) and then normalized in amplitude with Praat for presentation in the study. This indicates the audio stimulus was derived from existing content with some transformation applied, but the participant response data constitute new data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11470,
      "completion_tokens": 245,
      "total_tokens": 11715
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Results, especially 3.1 Overall distribution, 3.2 Age, 3.3 Gender, 3.4 Device",
          "reasoning": "The dataset collected by the authors from 974 Swiss German listeners is analyzed to understand the factors affecting perception of the Yanny v. Laurel stimulus. The authors use the dataset primarily for statistical analysis of trends and patterns in perceptual responses (e.g., effects of age, gender, device), not for any model training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12193,
      "completion_tokens": 297,
      "total_tokens": 12490
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Material; Section 2.2 Listeners; Sections 3.1-3.4 Results",
          "reasoning": "The dataset consists of responses from Swiss German listeners to an audio clip originally in English (the ambiguous recording of the word Yanny or Laurel). The perception data collected is based on this English stimulus only, and the listeners are Swiss German speakers responding to English auditory content. The paper does not mention multiple languages in the dataset responses or data collection; only English words are in the stimuli. Hence, the dataset entries contain monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9411,
      "completion_tokens": 168,
      "total_tokens": 9579
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories related to the data collection, preprocessing, or dataset generation. There is no mention of code releases or supplementary materials that contain such code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including participant recruitment and balancing (Section 2.2), the use of the SDATS project framework, the stimulus material description (Section 2.1), and data collection procedures (Section 2.3). It describes the demographic balancing, recording setup, devices used, response coding, and analysis approach, which constitutes adequate documentation for reproducibility."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7803,
      "completion_tokens": 132,
      "total_tokens": 7935
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Section 5.2",
          "Reasoning": "The authors introduced newly pre-trained Wav2Vec 2.0 models based on speech audio data collected from public sources for German and Czech languages, including VoxPopuli dataset and self-crawled podcasts and audiobooks, totaling 50k hours each; also the English base model was pre-trained previously on Libri-light. This audio speech data is human-generated, recorded from natural sources, and is not generated by any models or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8655,
      "completion_tokens": 220,
      "total_tokens": 8875
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.2 MALACH and Section 5 Fine-tuning datasets",
            "reasoning": "The MALACH dataset consists of annotated interviews with transcripts released under the Linguistic Data Consortium (LDC) as mentioned in Section 5.2. The annotations are derived from human-created transcripts as part of a curated oral history archive, involving multiple annotators with expertise in transcription and the subject matter."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not specify the presence of detailed annotation instructions provided to annotators for the MALACH dataset transcription."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not discuss any scoring rubrics or detailed evaluation criteria provided during the annotation of the MALACH dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No annotation examples or specific guidelines are described in the paper for the MALACH dataset annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9785,
      "completion_tokens": 261,
      "total_tokens": 10046
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided regarding quality assurance by multiple human experts on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any involvement of single non-expert human annotators in quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any use of AI models for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated or algorithmic verification used as quality assurance for the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied or documented for the new datasets introduced or used, including the balanced subsets of MALACH and CommonVoice datasets used in fine-tuning and evaluation."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9403,
      "completion_tokens": 477,
      "total_tokens": 9880
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any original speech or transcript data entirely from human contributors from scratch. All datasets used (MALACH and CommonVoice) are pre-existing."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset generated by AI or machine learning models. The focus is on training and evaluating ASR models, not generating new data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation being used to generate any part of the datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4, Section 5",
          "reasoning": "The datasets used for pre-training and fine-tuning (e.g., CET Czech and German speech from VoxPopuli and self-crawled podcasts, CommonVoice dataset, and MALACH oral history recordings) are collected or aggregated from existing public sources without mention of original content creation or substantial modification. The authors assembled these existing data sources to create balanced subsets for experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "For the MALACH dataset, although the underlying data is from an existing archive, the authors performed some modifications such as balancing the dataset across languages, segmenting long recordings into shorter segments, cleaning transcripts (removing non-speech events and punctuation, converting Czech transcripts to formal Czech). These transformations classify the resulting datasets as derived from original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies dataset origins and any modifications applied, so the data generation method is documented."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9921,
      "completion_tokens": 451,
      "total_tokens": 10372
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4 (Pre-trained models)",
          "reasoning": "The authors pre-trained new monolingual, bilingual, and trilingual Wav2Vec 2.0 models from scratch using large-scale unlabeled speech data, applying self-supervised learning as described in Section 4."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Pre-trained models)",
          "reasoning": "The new Wav2Vec 2.0 models for German and Czech, including bilingual and trilingual combinations, were trained from randomly initialized parameters using publicly collected data, thus representing training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments)",
          "reasoning": "The authors fine-tuned all pre-trained Wav2Vec and Whisper models supervisedly on labeled MALACH and CommonVoice datasets to adapt models for ASR tasks as detailed in Section 6."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of reinforcement learning or RL-based post-training techniques with the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments) and Table 2",
          "reasoning": "The MALACH and CommonVoice datasets were used for evaluation of different models by measuring word error rates (WER) on test sets, as shown in Table 2 and described in experimental sections."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While analysis of model performance is conducted, the datasets themselves are not used primarily for analysis of trends or characteristics outside the scope of training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base to augment models or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new datasets introduced in the paper (pre-training data and the balanced subsets for fine-tuning) are utilized extensively for pre-training, fine-tuning, and evaluation; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10644,
      "completion_tokens": 612,
      "total_tokens": 11256
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 1 Introduction, Section 5 Fine-tuning datasets",
          "reasoning": "The paper introduces the MALACH dataset which contains interviews in 32 languages, including English, German, and Czech. The authors also constructed balanced subsets containing these three languages for their experiments, indicating the dataset entries include more than two human languages (English, German, and Czech). They fine-tuned models on multilingual combinations and on mixed-language data, confirming the dataset is multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although bilingual language pairs are used in training the models (e.g., English+German), the dataset itself is not described as having exactly two languages in entries. Instead, the dataset MALACH contains many languages (32), and the experimental subsets include three languages, making the dataset multilingual rather than strictly bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain not only English but also multiple other languages. The paper does not introduce any new dataset that is only English monolingual."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Similarly, the datasets include multiple languages. No new dataset introduced contains exactly one non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are speech and text transcription datasets with human languages; no programming or code-related data is introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain speech and textual transcripts without mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets comprise human speech and transcripts from oral history archives; no biological or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any constructed or fictional language data is part of the introduced datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the datasets are clearly stated and documented (English, German, Czech, and in the case of MALACH more languages)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets involve human language speech and text data; therefore, they do contain language."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7862,
      "completion_tokens": 171,
      "total_tokens": 8033
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any link, mention, or reference to code repositories or other resources that would indicate that code related to data collection, preprocessing, or generation of datasets introduced by the authors is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 'Pre-trained models' and Section 5 'Fine-tuning datasets'",
          "reasoning": "The paper describes in detail the process of creating the datasets used for fine-tuning, including sources of data (e.g., public datasets like VoxPopuli and self-crawled podcasts and audiobooks), balancing procedures to select equal training data per language, cleaning and preprocessing steps such as transcript cleaning and segmentation, and train/dev/test splits."
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7952,
      "completion_tokens": 259,
      "total_tokens": 8211
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 2.2",
          "Reasoning": "The new dataset introduced is an extension of the Blizzard Challenge 2013 corpus (2013-EH2), which consists of uncompressed WAV audio files of a single female speaker reading audiobooks. This data was originally human recorded speech, thus human generated audio. It is explicitly mentioned that the corpus comprises 9733 utterances with durations varying between 0.5 s and 35.3 s, and includes new samples as well as original challenge samples extended with modern synthetic speech systems. The core speech data originates from human recorded sources."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The paper introduces four new synthetic speech systems representing modern speech synthesis technologies combining two acoustic models (Tacotron and FastPitch) and two neural vocoders (WaveNet and Parallel WaveGAN). The audio generated by these systems is model generated synthetic speech. This is part of the new dataset extension, providing synthetic audio data generated automatically by speech synthesis models trained on the original human recorded data."
        }
      ]
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8804,
      "completion_tokens": 279,
      "total_tokens": 9083
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3",
            "reasoning": "The paper states that 68 native speakers of English participated in the listening test, recruited via Prolific and performed the evaluation online. These listeners were pre-screened for balanced sex and English accent but there is no indication they were experts; thus they are multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "The evaluation protocol references the ITU-T recommendation P.800 and calibration of evaluation time to recommended durations indicating that instructions were provided to annotators to carry out the naturalness evaluation in line with a standard protocol."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 1 and 2.3",
            "reasoning": "The evaluation uses the Absolute Category Rating (ACR) protocol from ITU-T Recommendation P.800, which involves rating naturalness on a 5-point Likert scale (Mean Opinion Score, MOS). This provides a clear rubric for scoring provided to listeners."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention providing annotation examples to listeners during the subjective evaluation; it only references standard protocols and instructions but no explicit example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9934,
      "completion_tokens": 303,
      "total_tokens": 10237
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human experts performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 3.1",
          "reasoning": "The subjective evaluation of the extended Blizzard Challenge 2013 dataset was conducted by 68 native English speakers recruited online via Prolific, who rated naturalness of speech samples. These listeners performed the evaluation independently and are not described as experts, thus multiple human non-expert annotators performed the quality assurance via subjective listening tests."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used to perform quality assurance or annotation validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic rule-based quality assurance for dataset annotations is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process, namely subjective listening tests by multiple human non-expert annotators, is clearly described and documented in the paper."
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9552,
      "completion_tokens": 461,
      "total_tokens": 10013
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new data created entirely from scratch by human contributors. The datasets used (Blizzard Challenge 2013) were pre-existing and not newly created human-generated content."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The authors generated new synthetic speech samples using four newly built modern speech synthesis systems combining acoustic models (Tacotron, FastPitch) and neural vocoders (WaveNet, Parallel WaveGAN). These synthetic speech samples are original content generated entirely by machine learning models from the same text and corpus, representing new data from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data produced by human translations from other languages is made in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of machine translation to generate data is mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The authors used existing publicly available Blizzard Challenge 2013 datasets, which are collected and aggregated corpora of audiobook speech data. These data were taken without significant modification except downsampling (to 16 kHz) for consistency."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.2 and Section 2.2",
          "reasoning": "The authors derived new datasets by selecting a subset of sentences from the Blizzard Challenge 2013 test sets and produced synthetic speech via transformation of original text using modern TTS models. This involved adaptation of existing data through model-based transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data origin and generation methods."
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10070,
      "completion_tokens": 301,
      "total_tokens": 10371
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The authors trained four modern speech synthesis systems using Tacotron and FastPitch acoustic models on the extended Blizzard Challenge 2013 dataset. This indicates that the constructed dataset is used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3, especially Sections 3.2 and 3.4",
          "reasoning": "The paper extensively describes using the extended Blizzard Challenge 2013 dataset for conducting a large-scale subjective evaluation comparing historical and modern speech synthesis systems. The dataset serves as a benchmarking tool to assess naturalness of synthetic speech."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 and 4",
          "reasoning": "The authors analyze trends such as the relative nature of Mean Opinion Scores (MOS) over time, comparing past and present systems, highlighting how MOS scores changed. This dataset enables analysis of evaluation metrics and perception patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10793,
      "completion_tokens": 410,
      "total_tokens": 11203
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset introduced extends the Blizzard Challenge 2013 which contains data from a single English speaker, and no mention of multiple languages is made."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper only discusses English speech data; no second language is indicated or used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 2.1.1",
          "reasoning": "The dataset comprises English speech from a single female American English speaker reading English audiobooks, as explicitly described in Section 2.1. The text input to the synthesis systems is phonetic sequences derived from English data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains any non-English language speech."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While code implementations are referenced, the dataset itself does not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No description or inclusion of mathematical or logical symbolic notations in the dataset is present in the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset pertains to human speech only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or evidence of constructed or fictional language data in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English speech data and thus contains language entries."
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8011,
      "completion_tokens": 197,
      "total_tokens": 8208
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 and Conclusion",
          "reasoning": "The paper states that the resulting dataset, models, and configurations to reproduce them, as well as the subjective evaluation results, are freely available for research purposes and provides a link to a GitHub repository (https://github.com/sigmedia/bc_2013_extension). This indicates that the code related to data preparation, model training, and evaluation is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 2.2",
          "reasoning": "The paper provides comprehensive documentation about the dataset creation process, including details on dataset selection, data preprocessing (e.g., downsampling to 16 kHz), selection criteria of historical systems, and the construction of modern systems with specific acoustic models and neural vocoders. The evaluation protocol and listener recruitment are detailed as well. This thorough description serves as suitable documentation for dataset creation and usage."
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8730,
      "completion_tokens": 83,
      "total_tokens": 8813
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Material and Methods",
          "Reasoning": "The paper introduces and uses a subset of the Donate Speech Corpus, which consists of spontaneous Finnish speech recorded from thousands of volunteers via a mobile or desktop app, thus these audio recordings are human-generated data captured from human speakers."
        }
      ]
    }
  },
  {
    "id": "lennes23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9582,
      "completion_tokens": 198,
      "total_tokens": 9780
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2, Material and Methods",
            "reasoning": "The pitch extraction was performed using a Praat script with the autocorrelation method for pitch detection, which is a deterministic automated signal processing approach, not involving human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2, Material and Methods",
            "reasoning": "No instructions for human annotators are described since pitch detection was carried out by an automated script rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2, Material and Methods",
            "reasoning": "No mention of scoring rubrics or quality guidelines were given for the pitch extraction process as it was automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2, Material and Methods",
            "reasoning": "No examples for annotations or pitch labeling are provided because the pitch values were computed automatically without human annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "lennes23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10712,
      "completion_tokens": 357,
      "total_tokens": 11069
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the use of automated pitch analysis via Praat software with default and adjusted parameters to extract pitch distributions for speakers. However, there is no explicit mention that an automated quality assurance or automated verification step was performed to validate or verify the dataset annotations or correctness of the output. The process mainly involves automated pitch extraction rather than automated verification of data quality or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any explicit quality assurance procedure or validation step performed on the dataset or the annotations. The authors acknowledge potential inaccuracies in pitch extraction due to creaky voice and errors in metadata, and note no detailed quality assessment of results was performed. There is no documentation of any manual or automatic quality control or verification process applied to the dataset. Therefore, effectively no quality assurance process is described or applied."
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10330,
      "completion_tokens": 436,
      "total_tokens": 10766
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Material and Methods",
          "reasoning": "The dataset analyzed is a subset of the Donate Speech Corpus, which contains speech recordings collected directly from thousands of volunteer human speakers in Finland via a dedicated mobile or desktop app. The speech samples are spontaneous, created entirely by human speakers themselves, reflecting original human-generated speech content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was generated by AI or machine learning models. The speech data is collected from human speakers, not synthesized or automatically generated."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the data being produced by translation from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the Donate Speech Corpus aggregates many recordings from different speakers, the corpus itself is a primary collection of newly recorded human speech, not aggregated existing speech data. The paper does not indicate using existing corpora or collating data from other sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not derived from existing sources with modifications; it is original speech data collected from volunteers. The paper describes analysis and pitch extraction, but the dataset itself is not described as derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is explicitly documented in the paper as new human-generated speech collected via the Donate Speech campaign."
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10848,
      "completion_tokens": 213,
      "total_tokens": 11061
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3: Results",
          "reasoning": "The Donate Speech Corpus subset is used primarily to analyze trends and patterns in the pitch distributions of spontaneous Finnish speech across a large number of speakers, focusing on age and gender differences. The paper conducts detailed statistical and comparative analyses on this dataset, without reference to using it for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11571,
      "completion_tokens": 496,
      "total_tokens": 12067
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only Finnish speech samples, collected from Finnish speakers, so it is not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes exactly two languages; only Finnish data is used."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain English speech; it only contains Finnish."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 (Material and Methods)",
          "reasoning": "The Donate Speech Corpus subset used in the study contains spontaneous speech samples only in Finnish, confirmed by statements that the speakers are Finnish and the speech is in Finnish language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech recordings; no programming or code-related data entries are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains speech audio and pitch values, not mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech only, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Constructed or fictional languages are not mentioned or included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly specified as Finnish."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data (Finnish speech), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8789,
      "completion_tokens": 184,
      "total_tokens": 8973
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2: Material and Methods",
          "reasoning": "The paper discusses the use of a Praat script for pitch analysis and processing of the Donate Speech Corpus data, but does not provide any link or reference to publicly accessible code repositories containing the used code for data collection, preprocessing, or generation. There is no mention of code availability for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3 (Material and Methods and Results)",
          "reasoning": "The paper provides a detailed description of the dataset used (a subset of the Donate Speech Corpus), including information about data collection via a dedicated app, recording conditions, pitch analysis method with parameters, preprocessing criteria for speaker inclusion, and statistical analysis steps. Although some limitations and uncertainties are acknowledged, the dataset creation process is transparently documented in the paper itself."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9386,
      "completion_tokens": 263,
      "total_tokens": 9649
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and Section 4.1",
          "Reasoning": "The new data used for text-to-dysarthric-speech synthesis is derived from the TORGO dysarthric speech database, which consists of human-recorded dysarthric speech audio. The paper describes training Grad-TTS models from scratch on this dysarthric human speech to create synthesized dysarthric audio. Therefore, the original TORGO dysarthric speech data is human generated and audio modality."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 3.2, and Section 4.1",
          "Reasoning": "The paper introduces new synthesized dysarthric speech audio generated by the authors' trained Grad-TTS diffusion probabilistic model (text-to-speech synthesis) on the TORGO dysarthric data. This synthetic data is model generated audio produced via the Grad-TTS model trained from scratch by the authors. This synthetic audio is then used for data augmentation to fine-tune ASR models. Thus, the synthesized dysarthric speech data is audio modality with model-generated origin."
        }
      ]
    }
  },
  {
    "id": "leung24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10238,
      "completion_tokens": 197,
      "total_tokens": 10435
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.4.2",
            "reasoning": "The subjective evaluation of synthesized dysarthric speech was performed by a speech-language therapist (SLT) with over 10 years of expertise in speech disorders, indicating a single human expert."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to the SLT for rating the synthesized speech samples."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.4.2",
            "reasoning": "The SLT used a 5-point rating scale (0 to 4) for overall dysarthria severity, which constitutes a rubric guiding the scoring process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No annotation examples or training examples used by the SLT are described in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "leung24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11368,
      "completion_tokens": 356,
      "total_tokens": 11724
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.4.2 Subjective Evaluation",
          "reasoning": "The subjective evaluation of the synthesized dysarthric speech was conducted by a single speech and language therapist (SLT) with more than 10 years of experience assessing and diagnosing speech disorders, indicating a single human expert was involved in quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple expert annotators involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was done by an expert (SLT), not a non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that an AI model was used as a judge for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification processes as quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a quality assurance process via subjective evaluation by an expert listener; hence N/A does not apply."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10986,
      "completion_tokens": 433,
      "total_tokens": 11419
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any newly created data entirely by humans from scratch. The study uses the existing TORGO dysarthric speech dataset for training and validation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 3.2",
          "reasoning": "The paper introduces synthetic dysarthric speech data generated by training Grad-TTS models from scratch on existing dysarthric speech data (TORGO) to synthesize new speech samples with dysarthric characteristics. This synthetic data is newly generated by neural network models (diffusion probabilistic models) and used for data augmentation in ASR model fine-tuning."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The synthetic data is generated by models rather than collected or aggregated directly from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 3.2",
          "reasoning": "The synthetic data is derived from existing TORGO dysarthric dataset used to train Grad-TTS models. The models transform the original speech into new synthesized speech samples, thus the synthetic data is derived via transformation of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of generation of the new dataset is explicitly detailed in the paper."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11504,
      "completion_tokens": 554,
      "total_tokens": 12058
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper trains Grad-TTS models for speech synthesis from scratch, but these models are trained on existing TORGO dysarthric data rather than a newly introduced dataset. The paper does not introduce a new dataset used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 and Section 4.2",
          "reasoning": "The synthetic dysarthric speech data generated by the authors' trained Grad-TTS models (text-to-dysarthric-speech synthesis) is used as augmented training data to fine-tune existing pre-trained Whisper ASR models for dysarthric speech recognition. This supervised fine-tuning approach is explicitly described and evaluated."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the dataset in reinforcement learning based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2.1",
          "reasoning": "The synthetic dysarthric speech data and real TORGO data are also used for evaluation purposes, measuring synthesis quality via mean cepstral distortion and human subjective evaluation, and assessing ASR model performance on dysarthric speech via word error rate metrics."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper analyzes the characteristics of the synthesized dysarthric speech data (e.g., severity ratings and synthesis quality) and studies the effect of augmented synthesized data on ASR performance for different speaker severity groups."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the synthesized or real datasets are used as a knowledge base for retrieval-augmented or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced (synthesized dysarthric speech data and splits from TORGO) have clear practical uses in fine-tuning ASR models and evaluation as detailed in the paper."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12227,
      "completion_tokens": 665,
      "total_tokens": 12892
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "No explicit mention of multiple human languages in the dataset.",
          "reasoning": "The paper describes dysarthric speech data from the TORGO dataset used for training and augmentation. All text and speech data are in English; no evidence of more than two human languages is present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "No section describes datasets with exactly two languages.",
          "reasoning": "The dataset used and synthesized data are exclusively in English. The paper does not mention the presence of a second human language or bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 and throughout the paper; also in Abstract and Introduction.",
          "reasoning": "The TORGO dysarthric speech dataset, on which the new text-to-dysarthric-speech synthesis models are trained from scratch, contains English speech only. All experimental evaluations including ASR adaptation are performed with English speech data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "No indication of any non-English single-language dataset.",
          "reasoning": "The dataset used is English speech only; no non-English datasets are mentioned or introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "While method section mentions code repositories, no dataset entries containing code are introduced.",
          "reasoning": "The new datasets introduced are speech and text pairs, not programming or structured code content. The paper only references speech and text data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Mathematical equations appear to describe model details but are not part of the dataset.",
          "reasoning": "Mathematical notation appears in the methodology sections describing model architecture, but the datasets themselves do not contain entries of mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No mention in the paper of biological or non-human communication data.",
          "reasoning": "The datasets introduced are speech data from human speakers with dysarthria, all human speech-based."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No mention of fictional or artificial languages in the datasets.",
          "reasoning": "All linguistic data used is natural English speech; no constructed languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Dataset language is clearly specified as English (Sections 3.2 and 4).",
          "reasoning": "The language of the dataset is clearly specified as English, so the language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced contains speech and textual content, so language is present."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9445,
      "completion_tokens": 191,
      "total_tokens": 9636
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.1 Dysarthric Speech Synthesis and Section 2.1 Methodology",
          "reasoning": "The paper explicitly states that the Grad-TTS code adapted from https://github.com/huawei-noah/Speech-Backbones is used, and the implementation is available at https://github.com/WingZLeung/TTDS, indicating public availability of the code related to the dataset synthesis process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 Dysarthric Speech Synthesis and 3.2 Text-to-Speech Synthesis",
          "reasoning": "The paper describes in detail the dataset creation process for the synthesized dysarthric speech data, including the training splits, conditions (all-speaker, single-speaker, severity-group models), data splits creation, and model training details, providing comprehensive documentation of the dataset generation procedure."
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6442,
      "completion_tokens": 175,
      "total_tokens": 6617
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1, 3.1, 3.2, 3.3, 3.4",
          "Reasoning": "The DDS dataset consists of real audio recordings of speech played through a high-quality loudspeaker and re-recorded using three different microphone devices in nine realistic environments, with multiple microphone positions. This is clearly stated in Section 1 (Introduction) describing the dataset as parallel recordings of high-quality and device-degraded speech, in Sections 3.1-3.4 detailing data collection from playback and re-recording with physical devices, thus human-involved recording. It is audio data modality because the dataset consists specifically of speech waveforms sampled at audio sampling rates (44.1 or 48 kHz)."
        }
      ]
    }
  },
  {
    "id": "li22e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7294,
      "completion_tokens": 228,
      "total_tokens": 7522
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1, 3.3",
            "reasoning": "The paper describes that the speech data were collected by playing clean speech through a loudspeaker and re-recording it using devices in different rooms and positions. This data collection process used automated equipment and signal processing such as cross-correlation alignment to produce parallel recordings. There is no mention of human annotators labeling or annotating the dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not mention any annotation instructions since the dataset was collected via automatic recordings and alignment, without manual annotation steps."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "No scoring or evaluation rubrics for human annotation are described, as dataset creation did not involve manual labeling or rating."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "No annotation examples are provided because the dataset involves recorded audio pairs but no manual annotations requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "li22e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8424,
      "completion_tokens": 283,
      "total_tokens": 8707
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper describes using a cross-correlation algorithm to align recorded speech with the original clean speech (Section 3.1). This indicates the use of an automated verification process to ensure alignment and quality consistency between the parallel recordings. No human annotation or manual quality checking was reported."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated alignment process, so the dataset is not without any documented quality assurance."
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8042,
      "completion_tokens": 412,
      "total_tokens": 8454
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not consist of original speech content created entirely anew by humans; rather, it uses existing high-quality speech recordings played back and re-recorded."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data were generated entirely by AI or machine learning models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any data created through translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that machine translation was involved in creating the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset builds upon existing datasets (DAPS and VCTK), it does not simply collate existing data without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3 Dataset Overview",
          "reasoning": "The DDS dataset is constructed by playing existing high-quality speech datasets (DAPS and VCTK) through speakers and re-recording them in diverse realistic environments with multiple devices and microphone positions, thereby producing new realistic degraded versions of previously existing clean speech. This process transforms the original data through re-recording, adding environmental and device degradations, which qualifies as deriving data based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the dataset is clearly documented."
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8560,
      "completion_tokens": 304,
      "total_tokens": 8864
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 Baseline Experiments",
          "reasoning": "The DDS dataset is used as training data in multiple subsets (D1 to D5) to train speech enhancement models from scratch. The models are trained on the dataset to enhance device-degraded speech."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Baseline Experiments",
          "reasoning": "The DDS dataset is used as a test set containing 500 utterances recorded in a specific environment and device (livingroom1 by Uber Mic) to evaluate the performance of different speech enhancement baseline models trained on various subsets of DDS."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.5 Initial Analysis of DDS",
          "reasoning": "The authors perform analysis using objective speech quality scores (PESQ, ESTOI) across environments, devices, and positions on the DDS dataset to study the effects of recording factors on speech quality, demonstrating the dataset's diversity and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9283,
      "completion_tokens": 506,
      "total_tokens": 9789
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset DDS is based on English speech data from DAPS and VCTK datasets only, with no mention of other languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English speech recordings, so no two languages are included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The DDS dataset uses clean speech materials selected from DAPS and VCTK datasets, both of which contain professional English speech recordings only. The paper mentions the use of speech from 48 English speakers and no other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content is explicitly English speech; no other single non-English language is mentioned."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech recordings only, with no programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are audio speech recordings; there is no mathematical or logical notation included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech recordings only; no biological sequences or non-human communication systems are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are present; only natural English speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly states and documents the dataset's language as English; no ambiguity or unknown language content exists."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken English language audio recordings, so it does contain language data."
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6501,
      "completion_tokens": 181,
      "total_tokens": 6682
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "There is no explicit mention in the paper of code related to data collection, preprocessing or generation being made publicly available.",
          "reasoning": "The paper only provides a link to the dataset itself (https://doi.org/10.5281/zenodo.5464104) but does not mention the availability of any code repository or software used in the dataset construction or preprocessing. No sections or references discuss code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and subsections 3.1 to 3.4",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process in Section 3 Dataset Overview, including speech materials selection, environmental setup, devices and recording positions, and alignment procedure. There are detailed tables describing dataset settings and initial analysis, indicating thorough documentation of the dataset construction."
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7354,
      "completion_tokens": 214,
      "total_tokens": 7568
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Section 3.2",
          "Reasoning": "The TALCS corpus is described as a new dataset introduced by the authors, recorded from online one-to-one English teaching scenes with teachers' speech captured using personal computer microphones (human-involved recording). It is a mono-channel Mandarin-English code-switching speech corpus with natural dialogue, not read aloud, indicating real human speech recordings. This confirms the audio modality data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The TALCS corpus includes 370,000 manually transcribed sentences with careful annotation and quality control, performed by human annotators who passed CET-4. The transcriptions involve manual segmentation, text normalization, and quality inspection, indicating that the text data is human generated and linked to the audio recordings for speech recognition tasks."
        }
      ]
    }
  },
  {
    "id": "li22j_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8206,
      "completion_tokens": 308,
      "total_tokens": 8514
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 3.2 Corpus Transcription",
            "reasoning": "The paper states that data annotators who have passed CET-4 were tasked to annotate the TALCS corpus. Each class was annotated by only one annotator, transcribing utterances into small sentences with attention to semantics. The annotators are described as English teachers rather than subject-matter experts explicitly, with standard English pronunciation, but not indicated as expert linguists or professional annotators, thus categorized as single human non-experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Corpus Transcription",
            "reasoning": "The paper describes specific transcription instructions such as segmenting audio into small sentences according to semantics, transcribing clearly heard sentences, abandoning sentences with obvious mispronunciations, and specific text normalization rules (capitalizing English words, spacing, and number normalization). This indicates annotated instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2 Corpus Transcription",
            "reasoning": "There is no mention of scoring rubrics or qualitative scoring criteria for annotators. Quality checking seems based on error rates and content filtering, but no rubric for scoring annotations is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2 Corpus Transcription",
            "reasoning": "The paper does not provide nor mention any annotation examples or sample annotated sentences shown to annotators as examples or guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "li22j_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9336,
      "completion_tokens": 183,
      "total_tokens": 9519
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Corpus Transcription",
          "reasoning": "The paper states that data quality inspectors check 20% of the data and if more than 10% of sentences are inconsistent the whole part is reannotated by the annotation team, indicating multiple human experts are involved in quality checking to ensure transcription quality."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8954,
      "completion_tokens": 448,
      "total_tokens": 9402
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The TALCS corpus is described as collected from real online one-to-one teaching scenes involving teachers and students from different regions of China. The speech utterances were recorded by personal computer microphones during real classes, and the transcriptions were manually annotated by human annotators who passed CET-4, ensuring high-quality original human-generated content. The data is not translated or adapted from other datasets but recorded and transcribed from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any part of the TALCS corpus was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation from another language by human translators; the dataset consists of natural code-switching speech rather than translated content."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of any machine translation being used in producing the TALCS corpus."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The TALCS corpus is not described as being collected or aggregated from existing sources without modification. It is newly collected from a specific scenario."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The TALCS corpus is stated as a subset of the TAL-ASR corpus, indicating it is derived from this larger existing dataset, implying adaptation or selection from a pre-existing corpus to focus on code-switching scenarios."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is sufficiently specified and documented; thus this category does not apply."
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9472,
      "completion_tokens": 324,
      "total_tokens": 9796
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiments",
          "reasoning": "The TALCS corpus is used to train speech recognition models from scratch using popular ASR toolkits such as ESPnet and WenetSpeech. The paper describes training end-to-end ASR models on this corpus, indicating its use for model training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using TALCS corpus specifically for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning based post-training methods using the TALCS corpus are described."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiments",
          "reasoning": "The TALCS corpus is divided into training, development and test sets, with the test set used for evaluation and benchmarking the performance of the baseline speech recognition systems, as shown by the comparison of Mixture Error Rates (MER)."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not focus on analyzing trends or characteristics of the dataset beyond reporting baseline ASR results."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10195,
      "completion_tokens": 493,
      "total_tokens": 10688
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains exactly two human languages: Mandarin and English. There are no indications of more than two languages present."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 (Corpus profile) and Abstract",
          "reasoning": "The TALCS corpus is explicitly described as a Mandarin-English code-switching speech corpus, containing speech utterances mixing these two languages. It includes intra-sentence and inter-sentence code-switching between Mandarin and English, making it bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not English only; it specifically contains Mandarin and English utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not monolingual non-English; it includes both Mandarin and English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any programming or structured code-related content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are mentioned in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are part of the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include artificial or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are specified and well documented as Mandarin and English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural human language data and thus is not language-free."
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7413,
      "completion_tokens": 147,
      "total_tokens": 7560
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not mention or provide any link or repository to code related to the TALCS corpus data collection, preprocessing, or generation. There is no explicit indication that such code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Corpus description, especially Section 3.1 and 3.2",
          "reasoning": "The paper documents the TALCS dataset creation process in detail, including recording procedure, corpus profile, data transcription and annotation procedures. Details include recording environment, speakers, transcription protocol, quality checking, and dataset splits, thus providing comprehensive documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9319,
      "completion_tokens": 106,
      "total_tokens": 9425
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Table 1",
          "Reasoning": "The LittleBeats (LB) dataset consists of day-long home audio recordings collected from families with children under 5 years old using the LB infant wearable multi-modal device. These recordings were manually labeled by human coders for speaker diarization and vocalizations, indicating human involvement in data generation as the audio is captured from real family interactions."
        }
      ]
    }
  },
  {
    "id": "li23e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10171,
      "completion_tokens": 231,
      "total_tokens": 10402
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Labeled data",
            "reasoning": "The paper states that human coders manually annotated the labeled LB home recordings, with cross-coder validation and inter-coder reliability measured, indicating multiple human expert annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Labeled data",
            "reasoning": "The annotation process involved specific vocalization categories for key child and adult vocalizations, as well as manual labeling using Praat with precise temporal alignment, implying that detailed annotation instructions were provided to guide human coders."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Labeled data",
            "reasoning": "Inter-coder reliability was computed with Cohen's kappa scores for multiple vocalization categories, indicating systematic scoring rubrics or criteria were used to ensure consistency across annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not explicitly mention providing annotation examples or sample annotations in the guidelines for human annotators."
          }
        }
      ]
    }
  },
  {
    "id": "li23e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11301,
      "completion_tokens": 379,
      "total_tokens": 11680
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that annotation or QA was conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Labeled data",
          "reasoning": "The paper states that human coders manually labeled vocalizations, with 10% of segments double-coded and inter-coder reliability computed (high Cohen's kappa scores). The use of multiple human coders and cross-coder validation indicates multiple human annotators performed quality assurance. Although the paper does not explicitly state credentials, it refers to them as human coders and utilizes inter-coder reliability measures, suggesting they likely have expertise or relevant experience."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that quality assurance was carried out by multiple non-expert annotators, and the use of inter-coder reliability suggests attention to annotation quality."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI-based quality assurance or AI models serving as judges for annotation quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated voice activity detection was used to select audio segments to annotate, the quality assurance of the labels themselves was not automated or algorithmically verified. No automated verification process for annotation quality is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a human QA process with inter-coder reliability to ensure annotation quality; thus, QA is documented and performed."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10919,
      "completion_tokens": 445,
      "total_tokens": 11364
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper states that human coders manually annotated in-domain LB home recordings by selecting 10-min segments with highest vocalization rates and labeling vocalizations with Praat, with cross-coder validation to ensure quality. This indicates original content created entirely from scratch by human contributors, not derived or translated from existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by translating content using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The unlabeled data consists of large amounts of day-long home audio recordings collected from families wearing LB or LENA devices, aggregated without significant modification. These recordings were collected from multiple families and aggregated into datasets used for pretraining."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The preprocessing involved resampling audio to 16kHz, applying voice activity detection to remove silent portions, dividing audio into 10s segments, and applying energy thresholding to out-of-domain labeled data. These modifications represent transformations of existing audio sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents data origin and processing clearly, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11437,
      "completion_tokens": 541,
      "total_tokens": 11978
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 3",
          "reasoning": "The authors collected a new large-scale unlabeled dataset of 1100h of in-domain LittleBeats (LB) home audio recordings from families and used this dataset exclusively for unsupervised pretraining of the wav2vec 2.0 (W2V2) model, as described in sections 2.1 and 3. This pretraining aims to learn family audio representation from unlabeled data before supervised fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study does not describe training models from randomly initialized parameters using the proposed dataset; rather, it uses pretraining plus fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3",
          "reasoning": "The authors manually labeled a portion of the LB in-domain dataset and used this labeled data to fine-tune the pretrained W2V2 model in a supervised manner for speaker diarization and vocalization classification tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that reinforcement learning or RL-based post-training methods (such as RLHF) are used with the proposed dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 4",
          "reasoning": "The labeled LB dataset is used as a test set for evaluating model performance in speaker diarization and vocalization classification, with leave-one-family-out splits."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper primarily uses the dataset for training and evaluation; no primary focus on using the dataset solely for analyzing data trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documented utility of the dataset in pretraining, fine-tuning, and evaluation."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12160,
      "completion_tokens": 536,
      "total_tokens": 12696
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the proposed dataset contains multiple human languages. It focuses on recordings from family audio but does not mention multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described to contain exactly two human languages; no mention of bilingual content is found."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (Data), particularly 2.1 and 2.2 Data description and annotation process",
          "reasoning": "The paper states recordings were collected from families in the U.S., and the annotations include vocalizations such as child-directed speech, adult-directed speech, which are in English. There is no mention of other languages, indicating the dataset is monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset contains only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is natural audio recordings; no code or programming language content is present."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio recordings and annotations; it does not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While infant cries and vocalizations are biological in origin, the dataset is annotated and treated as human language data, not as biological sequences or non-human communication systems like DNA or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is specified and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language in audio recordings."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9378,
      "completion_tokens": 157,
      "total_tokens": 9535
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided for code related to dataset construction.",
          "reasoning": "The paper mentions that code and model weights related to model training are available but does not provide any link or information about code for dataset collection or preprocessing; thus, code for constructing the dataset is not made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data) including subsections 2.1, 2.2, and 2.3.",
          "reasoning": "The paper provides detailed descriptions of data collection procedures, ethical approvals, participant recruitment, labeling protocols, inter-coder reliability statistics, preprocessing steps, and data partitioning. This comprehensive documentation sufficiently describes the dataset creation process."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8171,
      "completion_tokens": 113,
      "total_tokens": 8284
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Experimental datasets",
          "Reasoning": "The paper introduces two new datasets, NS-100 and LS-100, which are constructed by selecting samples from publicly available audio corpora NSynth and LibriSpeech, respectively. These corpora consist of audio recordings captured from real human sources (musical instruments and speakers). Therefore, the modality is audio, and the data is human generated through recording of real sounds."
        }
      ]
    }
  },
  {
    "id": "li23w_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9023,
      "completion_tokens": 270,
      "total_tokens": 9293
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "The datasets NS-100 and LS-100 are constructed by choosing samples from existing public audio corpora NSynth and LibriSpeech, respectively. The selection process is described as random sampling of classes and samples per class from these corpora, with no mention of human annotators performing labeling or annotation tasks. Thus, the dataset preparation is an automatic process based on existing metadata and sampling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide any instructions for human annotators or guidelines for labeling since the datasets were constructed using existing labeled corpora (NSynth and LibriSpeech). Therefore, no annotation instructions are applicable."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No scoring rubrics or annotation criteria are mentioned in the paper regarding dataset construction as it involves selecting samples from existing datasets rather than performing any manual annotation or scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No examples of annotated data or annotation process examples are provided because no new annotation was performed; the datasets are built by sampling from existing labeled data."
          }
        }
      ]
    }
  },
  {
    "id": "li23w_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10153,
      "completion_tokens": 305,
      "total_tokens": 10458
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts were involved in quality assurance of the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance being performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used to perform quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the datasets are constructed by choosing samples from public audio corpora, the paper does not provide details suggesting that an automated verification process was applied for quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces two new datasets (NS-100 and LS-100) constructed by selecting samples from public corpora (NSynth and LibriSpeech respectively), but it does not document any quality assurance process applied to the datasets or annotations. No information is provided about verifying the correctness or consistency of dataset labels or contents."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9771,
      "completion_tokens": 333,
      "total_tokens": 10104
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The datasets NS-100 and LS-100 are constructed by selecting samples from two publicly available audio corpora, NSynth and LibriSpeech, respectively. Thus, the data is collected and aggregated from existing publicly available sources without indications of significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "While the datasets are created from publicly available corpora, the authors have performed selection and splitting of classes and samples to create NS-100 and LS-100 datasets with specific class splits for incremental learning experiments, indicating the datasets are derived via transformation and adaptation of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states the data origin and construction method, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10289,
      "completion_tokens": 550,
      "total_tokens": 10839
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new datasets NS-100 and LS-100 exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2 (Method), Section 3.1 (Experimental datasets)",
          "reasoning": "The datasets NS-100 and LS-100 are constructed from NSynth and LibriSpeech corpora respectively by selecting classes and samples, and are used for training the embedding extractor (EE) and the stochastic classifier (SC) from scratch in the base session; the EE is trained supervised episodically and the SC is trained afterwards as described in Section 2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2 (Method), Section 3.1 (Experimental datasets)",
          "reasoning": "The datasets are used in incremental sessions where the stochastic classifier (SC) is incrementally expanded to recognize new classes using few samples, effectively serving as supervised fine-tuning of the classifier component on new classes."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of reinforcement learning or RL-based post-training methods such as RLHF with the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 (Experimental datasets), Section 3.3 (Experimental results)",
          "reasoning": "The datasets NS-100 and LS-100 are also used for evaluation and benchmarking the proposed method and baselines, as the paper reports accuracy and performance metrics on testing splits of these datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not primarily used for trend or characteristic analysis; no such analysis is reported."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases to augment models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes and demonstrates the use of the introduced datasets for training from scratch, supervised fine-tuning, and evaluation."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11012,
      "completion_tokens": 634,
      "total_tokens": 11646
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced, NS-100 and LS-100, are derived from NSynth (musical instrument sounds) and LibriSpeech (speech audiobooks) respectively. The speech corpus LibriSpeech consists of audiobooks in English only. There is no indication of multiple languages used in the datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of exactly two human languages in the proposed datasets. LS-100 is based on English audiobooks solely and NS-100 includes musical instrument sounds which are non-linguistic audio."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental datasets",
          "reasoning": "The LS-100 dataset is derived from the LibriSpeech corpus which contains English audiobooks, implying the speech data is English only. Thus, the dataset contains only English content for language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The NS-100 dataset is based on NSynth musical instrument sounds, which are non-linguistic sounds, not in any human language. There is no mention of any non-English human language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are audio sample datasets and do not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical expressions and formulas, the datasets themselves consist of audio recordings, not mathematical or logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental datasets",
          "reasoning": "The NS-100 dataset includes audio recordings of musical instrument sounds (non-human communication), which can be considered non-linguistic audio signals. While not biological sequences, they do represent non-human communication sounds."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages and audio types in the datasets are clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain language data (spoken English) in LS-100 and non-human audio in NS-100, so they cannot be considered as having no language."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8230,
      "completion_tokens": 216,
      "total_tokens": 8446
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3.1",
          "reasoning": "The abstract mentions a code repository at 'https://github.com/vinceasvp/meta-sc' for reproducing experiments and dataset construction details. Also, in Section 3.1, the paper states that the construction details and explanations of the two new datasets (NS-100 and LS-100) are introduced at this GitHub link, indicating that code for dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and Abstract",
          "reasoning": "Section 3.1 provides detailed information about how the NS-100 and LS-100 datasets were constructed by selecting samples from the public NSynth and LibriSpeech corpora, including class splits, number of samples per class, and information about training and testing splits. The paper also refers readers to the GitHub repository for additional construction details and metadata, indicating that the dataset creation process is documented both in the paper and via supplementary online resources."
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8285,
      "completion_tokens": 119,
      "total_tokens": 8404
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1: Data description",
          "Reasoning": "The CN-Celeb-AV dataset is composed of video segments collected from public media (Bilibili), involving real-world audio-visual content of people. Videos are recorded by humans or captured from human-operated devices, making the data human-generated. The dataset contains over 420k video segments from 1,136 individuals, explicitly described as collected in the wild from a publicly accessible platform."
        }
      ]
    }
  },
  {
    "id": "li23y_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9137,
      "completion_tokens": 206,
      "total_tokens": 9343
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3",
            "reasoning": "The annotation involved multiple human annotators performing manual video selection, human checks for segment acceptance, and senior annotator double-checks as part of the data collection pipeline."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "The annotation process involved acceptance criteria explaining that humans can accept segments if the target person is recognizable in at least one modality, implying that guidelines or instructions about what to check for were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No explicit mention of scoring rubrics or detailed grading criteria for annotations is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not mention providing annotation examples or exemplars to annotators during the human check process."
          }
        }
      ]
    }
  },
  {
    "id": "li23y_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10267,
      "completion_tokens": 490,
      "total_tokens": 10757
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human expert performed the quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple human experts were involved in quality assurance; no evidence of expert annotators is provided."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Collection pipeline",
          "reasoning": "The dataset quality assurance involved human annotators performing human checks and labeling segments as accepted or rejected, using a crowd-sourcing platform. These annotators are not described as domain experts or experts with subject matter expertise; hence, they are classified as non-expert human annotators. Multiple annotators participated, and a senior annotator (likely also non-expert) performed double-checking."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Collection pipeline",
          "reasoning": "AI models (InsightFace for face verification, ECAPA-TDNN from SpeechBrain for speaker verification, and MTCNN for face detection) were used in an automatic process to compute detection scores to select candidate segments before human checking. These models served as automated CAD systems aiding in quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Collection pipeline",
          "reasoning": "An automatic process was used to extract candidate segments based on model scores and thresholds (e.g., deleting segments in the lowest 15% detection scores). This step involves automated rule-based filtering and segmentation, constituting an automatic quality assurance approach."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a thorough multi-stage quality assurance process combining automatic processing and human annotation; thus, QA is present and documented."
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9885,
      "completion_tokens": 404,
      "total_tokens": 10289
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of video segments collected from public media (Bilibili) featuring real-world videos of persons, not data created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data consists of real-world videos collected from public media, not generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation systems is present in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.3",
          "reasoning": "The dataset was collected from existing public videos on the Bilibili platform (public media), as stated in Section 3.1 and detailed in the collection pipeline in Section 3.3. The data is aggregated from these existing sources without significant alteration to the original content."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset involves some processing (automatic extraction, human checking), the core data itself is collected videos rather than data derived or transformed from previously existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and collection method of the data is specified and well documented in the paper."
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10403,
      "completion_tokens": 512,
      "total_tokens": 10915
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the CN-Celeb-AV dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the CN-Celeb-AV dataset is used to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using CN-Celeb-AV to fine-tune pre-trained models in a supervised manner."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training techniques are mentioned in relation to CN-Celeb-AV."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The CN-Celeb-AV dataset is primarily used as an evaluation benchmark to test the performance of audio-visual person recognition systems under real-world, unconstrained conditions. Section 4 details the evaluation experiments conducted using CN-Celeb-AV evaluation sets, comparing performance with other datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 (Results) and Section 3.2 (Features of CN-Celeb-AV)",
          "reasoning": "The paper analyzes the characteristics and challenges presented by CN-Celeb-AV, discussing the complexity of the dataset and its suitability for real-world AVPR research. The results also analyze system performances, illustrating dataset challenges."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for retrieval-augmented or similar model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the CN-Celeb-AV dataset, particularly for evaluation and analysis purposes."
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11126,
      "completion_tokens": 274,
      "total_tokens": 11400
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data description",
          "reasoning": "The CN-Celeb-AV dataset contains video segments collected from Bilibili, a Chinese public media platform, and features mostly Chinese celebrities. It covers 11 genres and is intended for Chinese speaker recognition research (see reference to CN-Celeb datasets). There is no indication of multiple languages or bilingual content. Thus, the dataset entries are only in one language, which is Chinese, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8344,
      "completion_tokens": 206,
      "total_tokens": 8550
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.3 Collection pipeline",
          "reasoning": "The paper describes the data collection pipeline in detail and mentions that the source code of a crowd-sourcing platform for video annotation and human check will be published on the dataset webpage. However, no direct link or repository for the full code used for the entire dataset construction, including data collection, preprocessing, and generation, is provided within the paper. Therefore, there is no evidence that all code related to dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.3 Collection pipeline",
          "reasoning": "The paper thoroughly documents the dataset creation process in Section 3.3, providing detailed descriptions of the manual video selection, automatic data processing, and human check steps. It also explains the rationale behind the pipeline design to handle partial modality data and discusses considerations like avoiding modality bias. Hence, the dataset creation process is transparently and comprehensively documented in the paper."
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7885,
      "completion_tokens": 243,
      "total_tokens": 8128
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, 'The PunCantonese Corpus'; especially Sections 2.1 'Cantonese Speech Transcripts' and 2.2 'Written Text from Cantonese Wikipedia'",
          "Reasoning": "The paper introduces the PunCantonese corpus, a new dataset primarily composed of text data. This corpus is collected from annotated spoken Cantonese transcripts and written-style Wikipedia sentences. The spoken transcripts originate from Common-Voice and PyCantonese datasets, which are human-generated speech transcripts that have been manually punctuated. The written text data is obtained from Cantonese Wikipedia, which is also human-generated content authored by Wikipedia contributors. Thus, the dataset modality is text. The data is human-generated because the spoken transcripts are transcriptions of human speech, and the Wikipedia text is authored by humans. The paper does not indicate any automatic or model-based generation of this data, nor does it treat the data origin as unknown or derived from web crawl without clarification. Therefore, the data is of text modality, human-generated, not model-generated, and not of unknown origin."
        }
      ]
    }
  },
  {
    "id": "li23z_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8737,
      "completion_tokens": 264,
      "total_tokens": 9001
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 (The PunCantonese Corpus), Section 2.1 and 2.2",
            "reasoning": "The corpus is compiled from existing publicly available datasets (Common-Voice, PyCantonese) and Wikipedia data; the speech transcripts already contain punctuations and the Wikipedia data is preprocessed automatically by filtering and cleaning steps. There is no indication that humans performed explicit manual annotation of punctuation; rather, the annotations come from the original transcripts or are inherent in the written Wikipedia text, with automatic filtering and preprocessing applied."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention any human annotators nor provide annotation instructions for punctuation labeling since the punctuation annotations originate from pre-existing punctuated transcripts or Wikipedia sentences."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "There is no description or evidence of scoring rubrics for annotation in the corpus construction since it does not involve manual annotation processes."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No examples of annotation guidelines or example annotations are provided because the dataset is not manually annotated; rather, it is derived from existing data sources with punctuation."
          }
        }
      ]
    }
  },
  {
    "id": "li23z_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9867,
      "completion_tokens": 285,
      "total_tokens": 10152
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert for the PunCantonese corpus."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance performed by multiple human experts for the PunCantonese corpus."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided indicating quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance process applied to validate the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe or mention any quality assurance process applied to the PunCantonese corpus. There is no specification of human or automatic validation, review, or verification of punctuation annotations or data quality in the dataset collection or preparation stages."
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9485,
      "completion_tokens": 445,
      "total_tokens": 9930
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being fully created from scratch by human contributors; it is assembled primarily through sourcing existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any part of the dataset was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made that the dataset involves translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate parts of the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (The PunCantonese Corpus), Sections 2.1 and 2.2",
          "reasoning": "The PunCantonese corpus is compiled by collecting and aggregating data from existing sources: the publicly available spoken Cantonese datasets Common-Voice and PyCantonese, and Cantonese Wikipedia. These data sources were collected without significant modifications beyond filtering and cleaning, indicating the dataset is a collation of existing data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 (Written Text from Cantonese Wikipedia)",
          "reasoning": "The dataset from Cantonese Wikipedia underwent preprocessing including sentence division, filtering of sentences without full punctuation, replacing or removing certain punctuation marks, and removing sentences with certain length criteria and non-Cantonese/English languages. This shows the data is derived from an existing source through modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the dataset are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10003,
      "completion_tokens": 208,
      "total_tokens": 10211
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3 Experimental Results; Section 3 Approach",
          "reasoning": "The PunCantonese dataset is used to fine-tune a pre-trained multilingual Transformer-based language model for the downstream task of Cantonese punctuation restoration, as described in Section 3 and evaluated with experimental results in Section 4.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10726,
      "completion_tokens": 508,
      "total_tokens": 11234
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The PunCantonese corpus contains only Cantonese and English (code-switched sentences), no more than two human languages are present."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2, Table 1, and Section 2.3",
          "reasoning": "The PunCantonese dataset contains sentences in Cantonese and English, including code-switched examples with both languages per sentence. The dataset only covers these two human languages explicitly, as stated in Section 2 and exemplified in Table 1."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not solely English; it contains Cantonese as the primary language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though Cantonese is present, the dataset also includes English code-switched sentences, so it is not purely monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only human language transcripts, no biological or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset are clearly specified as Cantonese and English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data in Cantonese and English."
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7944,
      "completion_tokens": 175,
      "total_tokens": 8119
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention of code availability in the paper.",
          "reasoning": "The paper does not contain any links, URLs, or sections discussing the public availability of code used for data collection or dataset construction. There is no indication that related code has been released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 - The PunCantonese Corpus (pages 1-4).",
          "reasoning": "The paper provides detailed documentation on the dataset creation process including data sources (Common-Voice, PyCantonese, Cantonese Wikipedia), preprocessing steps, filtering criteria, splitting into train/validation/test, statistics and challenges related to the dataset. This is comprehensively documented within Section 2 and sub-sections 2.1, 2.2, and 2.3."
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6482,
      "completion_tokens": 292,
      "total_tokens": 6774
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2 and 2.2",
          "Reasoning": "The subcategory taxonomy textual descriptions were generated with the assistance of large language models, specifically GPT-4, as described in Sections 2.1.2 and 2.2. The text descriptions for subcategories and adjectives used for similarity calculations are outputs from the LLM, thus model generated."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2, paragraph 'Step 1: Connect text and image features' and Figure 4",
          "Reasoning": "Images are visual frames randomly selected from original audio-visual datasets (such as VGGSound) and are matched with text and audio features. These images are captured data involving human-recorded content, hence human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2, especially the description of automatic matching and End Product paragraph",
          "Reasoning": "The audio clips originate from existing datasets (e.g., VGGSound), which consist of recordings of real-world sounds captured via human-involved processes (e.g., recordings of animals, tools, environment). Therefore, the audio modality in the dataset is human generated."
        }
      ]
    }
  },
  {
    "id": "li24ha_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7334,
      "completion_tokens": 221,
      "total_tokens": 7555
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2",
            "reasoning": "The dataset construction involves an automatic pipeline using pretrained models CLIP and CLAP to match text, audio, and image features without human involvement in the annotation process. The paper explicitly mentions this process as automatic and scalable without manual intervention."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section describing annotation instructions",
            "reasoning": "The paper does not mention providing human annotators with instructions; the process is fully automated using existing models and LLMs."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric details in the paper",
            "reasoning": "No scoring or evaluation rubrics for annotation are described; the data matching relies on similarity scores and thresholding for selection in an automatic way."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention of annotated examples in the paper",
            "reasoning": "No example annotations or annotation guidelines with examples are provided; the methodology describes an automatic process assisted by pretrained models and LLM prompts."
          }
        }
      ]
    }
  },
  {
    "id": "li24ha_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8464,
      "completion_tokens": 349,
      "total_tokens": 8813
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.1, 2.2",
          "reasoning": "The dataset construction uses GPT-4 (an AI large language model) to cluster and generate subcategories, as well as CLIP and CLAP models to perform matching and verification of text-audio-image pairs, effectively acting as AI-based quality assurance mechanisms for the taxonomy creation and data matching."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The dataset construction involves an automatic three-step matching process using similarity computation between text, audio, and image features extracted by pretrained models to align data pairs, constituting an automated algorithmic quality assurance process ensuring consistent matching without human intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents an automatic quality assurance process using AI models and algorithmic matching; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8082,
      "completion_tokens": 501,
      "total_tokens": 8583
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new data was created entirely from scratch by human contributors. Instead, it discusses automatically constructing a dataset based on existing sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While large language models and pre-trained models are used to generate subcategory labels and assist data matching, the resulting dataset contains paired audio, text, and image clips from existing sources rather than data generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation of data from other languages by human translators in the dataset construction."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify use of machine translation for dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Automatic Construction of Diverse Subcategory Taxonomy), especially 2.2 (Automatic Text-Audio-Image Data Matching Process)",
          "reasoning": "The dataset is constructed by automatically matching audio, image, and text features from the existing VGGSound dataset using pretrained models (CLIP, CLAP) and LLM-based taxonomy generation. This involves collecting and aggregating data from an existing source without significant modification of the audio or images themselves."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 (Characterizing Sound Diversity Taxonomy with LLM)",
          "reasoning": "The new taxonomy with diverse subcategories is created by reclassifying and regrouping existing VGGSound labels using GPT-4, generating new class groupings and subcategory labels which transform and adapt the existing label set into a more diverse taxonomy oriented toward multimodal distinguishability. This represents modifications and adaptations of existing data labels to form the new taxonomy."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation process is well documented in the paper; therefore, N/A category does not apply."
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8600,
      "completion_tokens": 359,
      "total_tokens": 8959
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3. Diverse Audio Generation Model",
          "reasoning": "The newly constructed DiveSound dataset is used to train a latent diffusion model for text-to-audio generation from scratch; the paper describes model training details including the use of the dataset to train the latent diffusion model (LDM) with modality fusion input."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3. Evaluation Metrics and Section 3.4. Results",
          "reasoning": "The DiveSound dataset is used to evaluate and benchmark audio generation quality and diversity through both objective metrics (e.g. FAD, MSD) and subjective Mean Opinion Score (MOS), demonstrating improvements in diversity and quality when training with the dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9323,
      "completion_tokens": 544,
      "total_tokens": 9867
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is constructed based on textual descriptions and labels in English only, with no indication of entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or statement indicating the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 2.2",
          "reasoning": "The dataset's labels and textual descriptions are processed and generated using GPT-4 in English, and the constructed taxonomy with subcategories and matching text-audio-image data pairs are primarily represented in English. There is no mention of any other human language used in data entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly based on English labels and descriptions; no non-English languages are used exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code snippets are present as entries in the dataset; code is only described as used in methodology but is not part of dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulas or logical notation appear only in method descriptions, not as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some sounds are from animals, the dataset entries are human language text labels describing these sounds; they are not biological sequences or non-human symbolic communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is explicitly English; it is clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries include text labels and descriptions in English, so it does contain language."
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6541,
      "completion_tokens": 158,
      "total_tokens": 6699
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit reference to code availability is provided in the paper.",
          "reasoning": "The paper mentions that the DiveSound framework constructs the new dataset automatically using LLMs and pretrained models, but there is no explicit mention or link to publicly available code or repository for reproducing the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, especially subsection 2.1 and 2.2.",
          "reasoning": "The paper provides a detailed description of the dataset construction process using GPT-4 for clustering and subcategory reasoning, and utilizes CLIP and CLAP for automatic text-audio-image data matching. Steps are described clearly with figures and methodology, enabling understanding of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6498,
      "completion_tokens": 93,
      "total_tokens": 6591
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data",
          "Reasoning": "The paper uses the MUStARD++ dataset composed of audiovisual utterances extracted from American TV sitcoms, which involves human speech recordings. The data is human-generated audio collected from television shows, manually labeled for sarcasm and emotions, indicating human involvement in data creation and annotation."
        }
      ]
    }
  },
  {
    "id": "li24ma_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7350,
      "completion_tokens": 216,
      "total_tokens": 7566
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2",
            "reasoning": "Two graduate students specializing in linguistics independently performed the annotation process, labeling key phrases and deciding whether utterances were neutral or non-neutral, as described in Section 2.2."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Annotators were instructed to identify lexically meaningful words and phrases including exclamations, and label utterances as neutral or non-neutral using video context, implying the presence of detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper mentions annotation processes and Kappa agreement but does not describe any scoring rubrics or formal rubric guidelines for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "No explicit example annotations or annotation guidelines with examples are provided in the paper for the annotators; only general instructions about annotation were mentioned."
          }
        }
      ]
    }
  },
  {
    "id": "li24ma_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8480,
      "completion_tokens": 360,
      "total_tokens": 8840
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that a single human annotator conducted the quality assurance, nor does it specify any annotator being a subject matter expert or member of the target demographic solely performing QA."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotation",
          "reasoning": "The annotation was performed by two graduate students specializing in linguistics, independently labeling the utterances for sarcasm and neutrality. Graduate students specializing in linguistics can be considered subject matter experts or belonging to the target demographic relevant for quality assurance of linguistic annotations. The use of two annotators with substantial inter-annotator agreement (Kappa = 0.6042) indicates that QA was performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of single non-expert human annotator performing QA is provided in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as graduate students specializing in linguistics, which implies expertise; thus, they are not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of automated verification or algorithmic techniques for quality assurance of the annotation labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A documented quality assurance process is described, involving multiple annotators with expertise and inter-annotator agreement metrics; therefore, QA is present."
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8098,
      "completion_tokens": 421,
      "total_tokens": 8519
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used in the paper is MUStARD++, which is an existing dataset compiled from American TV sitcoms and not newly created entirely by human contributors from scratch by the authors of this paper."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new data was generated entirely by AI or machine learning models. The authors analyze an existing dataset rather than generating new data automatically."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or statement in the paper that the data was produced through human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of machine translation for data generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data",
          "reasoning": "The MUStARD++ dataset used was compiled from audiovisual utterances extracted from American TV sitcoms, indicating data is collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotation",
          "reasoning": "The authors derived a new labeled subset by selecting key phrases within the existing sarcastic utterances, and further searched for neutral utterances containing these key phrases spoken by the same speaker, then manually annotated these for sarcasm or neutrality, applying transformations and annotations on the existing dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8616,
      "completion_tokens": 266,
      "total_tokens": 8882
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3",
          "reasoning": "The paper uses the MUStARD++ dataset, which includes audiovisual sarcastic and non-sarcastic utterances annotated with sarcasm types and acoustic features, primarily for analyzing prosodic and semantic cues of sarcasm. The dataset is employed for statistical analysis of prosodic patterns and their interplay with semantic cues rather than for model training or evaluation, as indicated in Sections 2.2 (Annotation) and 3 (Results and Discussion). This supports that the dataset's utility is primarily for analysis of trends, patterns, and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9339,
      "completion_tokens": 413,
      "total_tokens": 9752
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data",
          "reasoning": "The dataset used is MUStARD++, containing utterances from American TV sitcoms, all in English language; no indication of additional languages is given."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data",
          "reasoning": "The paper does not mention the dataset containing exactly two human languages; only English content is described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data",
          "reasoning": "The MUStARD++ dataset is described as comprised of utterances extracted from American TV sitcoms; all examples and contexts are in English, with no indication of other languages involved."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2.1 Data",
          "reasoning": "The dataset is from American TV shows and contains English utterances; no non-English single language dataset is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audiovisual utterances and annotations; there is no mention of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of mathematical or formal logical expressions as content in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset entries are human speech from TV shows, no biological sequences or non-human communication included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves natural English from TV shows; constructed or fictional languages are not mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly identified as English from American TV sitcoms; language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken utterances and annotations, clearly involving human language."
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6557,
      "completion_tokens": 208,
      "total_tokens": 6765
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 (Methodology)",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to the dataset construction, data cleaning, annotation tools, or preprocessing steps. The paper references external tools (e.g., voice fix tools and Gentle) but does not share any custom scripts or code repositories developed for the dataset creation or annotation process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Data) and 2.2 (Annotation)",
          "reasoning": "The paper documents the use of the MUStARD++ dataset, including its origin, size, and annotation categories. It also details the annotation methodology for key phrases with procedures, annotator agreement, and the filtering process for selecting neutral utterances. Acoustic preprocessing steps and tools are described, along with the statistical analyses performed. While the dataset itself is pre-existing, the paper clearly documents the additional annotation and processing steps undertaken by the authors."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9759,
      "completion_tokens": 221,
      "total_tokens": 9980
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data collection and 2.2 ASR corpus construction",
          "Reasoning": "The MSR-86K corpus comprises 86,300 hours of transcribed ASR audio derived from publicly accessible YouTube videos. The audio is from real human speech captured in these videos, downloaded as single-channel wav files sampled at 16 kHz. Since the videos are originally recorded by humans and publicly accessible, the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data collection and 2.2 ASR corpus construction",
          "Reasoning": "The transcriptions in MSR-86K are obtained from manually uploaded video subtitles from the same publicly accessible YouTube videos. The subtitles are manually created by humans and then downloaded along with the audio. The corpus includes normalized text obtained by processing these subtitles, hence the text modality is human generated."
        }
      ]
    }
  },
  {
    "id": "li24s_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10611,
      "completion_tokens": 270,
      "total_tokens": 10881
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 (especially 2.1 Data collection and 2.2 ASR corpus construction)",
            "reasoning": "The paper describes an automatic pipeline for data collection and annotation involving automated keyword generation, video retrieval, subtitle detection, forced alignment using a pre-trained ASR model, voice activity detection, language identification filtering with a trained LID model, and further filtering based on an ASR model's word error rate scores. There is no mention of human annotators performing manual transcription or verification on the MSR-86K corpus annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention or provide any detailed annotation instructions for human annotators since the annotation process is fully automated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No scoring rubrics or evaluation criteria intended for human annotators are described. The filtering and selection are done automatically based on model scores and heuristics rather than human judgment using a rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper provides no examples or illustrative annotation guidelines intended for human annotators; process is automated with no human annotation examples described."
          }
        }
      ]
    }
  },
  {
    "id": "li24s_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11741,
      "completion_tokens": 402,
      "total_tokens": 12143
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process performed by a single human expert or annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance involving multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.2, ASR filter",
          "reasoning": "The dataset quality is improved by training an ASR model to decode data and calculate word error rates (WER), filtering out segments with high WER, effectively using an AI model for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2, Corpus Construction",
          "reasoning": "Several automated verification steps are employed, such as subtitle detection filtering, forced alignment with pre-trained CTC models, voice activity detection (VAD) to balance duration, and language identification (LID) filtering, all constituting automated quality assurance techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes several automated QA processes and the use of AI models for filtering and quality assurance, so QA is documented and performed."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11359,
      "completion_tokens": 445,
      "total_tokens": 11804
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset MSR-86K is not created entirely from scratch by human contributors; it is collected from existing publicly accessible YouTube videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models; it is collected from real-world audio and subtitle data available on YouTube."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of human translation being used to produce the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset construction process does not involve machine translation of content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data collection",
          "reasoning": "The MSR-86K corpus is derived from publicly accessible videos on YouTube, collected by retrieving video IDs, downloading available audio and subtitles, and aggregating them without significant manual modification. The paper describes the use of keyword searching, video ID deduplication, filtering, and downloading of public data, indicating aggregation of existing data sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 ASR corpus construction",
          "reasoning": "The original subtitles from YouTube videos are further processed, including text normalization, forced alignment using a pre-trained ASR model, language identification filtering, word error rate-based filtering, and voice activity detection-based segmentation. These processing steps apply transformations and adaptations to the aggregated raw data, producing a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the dataset's source and generation process, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11877,
      "completion_tokens": 456,
      "total_tokens": 12333
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 2.3; Section 3.2",
          "reasoning": "Section 2.3 describes the collection of unsupervised audio data used for unsupervised pre-training, and Section 3.2 explains the use of unsupervised pre-training with the MSR-86K and other data to train a model (HuBERT). This shows MSR-86K data contributes to pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe training a model from randomly initialized parameters solely on MSR-86K; training involves pre-training then fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "Section 3.2 details fine-tuning the pre-trained model with the MSR-86K supervised transcribed data and other open-source corpora to build a robust multilingual ASR model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or demonstration of reinforcement learning post-training techniques such as RLHF using MSR-86K is provided in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Section 3.1 uses the MSR-86K development set to evaluate the quality of the corpus and monolingual models, thus it is used for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Section 3.1 analyzes the corpus quality, language distribution, and error rates, indicating use of MSR-86K for analysis of dataset characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the MSR-86K dataset serving to augment models as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes multiple practical uses of MSR-86K for pre-training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12600,
      "completion_tokens": 570,
      "total_tokens": 13170
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Section 2.2, Table 2",
          "reasoning": "The MSR-86K corpus introduced in the paper contains transcribed ASR data for 15 human languages including Spanish, Korean, English, French, German, Hindi, Vietnamese, Italian, Dutch, Portuguese, Thai, Russian, Indonesian, Japanese, and Arabic. This is explicitly stated in the abstract, detailed in Section 2.2 where the language durations are listed, and shown in Table 2. Therefore, the dataset is multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the dataset containing more than two languages, specifically 15 languages; hence, it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MSR-86K corpus includes multiple languages; it is not restricted to only English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises multiple languages and is not confined to exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication in the paper that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of transcribed speech data; there is no indication of mathematical or logical notation included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication systems are mentioned or included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned as part of the MSR-86K corpus."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included in the dataset are explicitly listed, so the language(s) are not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language transcriptions; therefore, this category does not apply."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9818,
      "completion_tokens": 185,
      "total_tokens": 10003
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided",
          "reasoning": "The paper mentions that the MSR-86K corpus will be publicly released on HuggingFace but does not provide any link or statement indicating that the code used for data collection, processing, and generation is publicly available. There is no indication that the scripts or codebases used to construct the dataset have been shared in a repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Corpus Construction",
          "reasoning": "The paper provides a detailed description of the dataset construction process in Section 2, including data collection methods (keyword creation, video retrieval, subtitle detection), transcription and filtering, forced alignment, duration balancing, language ID filtering, ASR filtering, data split, and unsupervised corpus creation. This documentation is fairly comprehensive to allow understanding of how the dataset was built."
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8270,
      "completion_tokens": 79,
      "total_tokens": 8349
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Speech recordings",
          "Reasoning": "The authors recorded speech signals from 86 participants reading aloud a German standard passage using a head microphone in a controlled environment, implying the raw audio data is human-generated through human speech production and human recording."
        }
      ]
    }
  },
  {
    "id": "liebig22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9122,
      "completion_tokens": 232,
      "total_tokens": 9354
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Listening experiment",
            "reasoning": "The paper describes that a group of 28 laypersons participated in the listening experiment to rate the femininity/masculinity of speech samples, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Listening experiment",
            "reasoning": "The paper states that a 6-point Likert scale from 1 (very masculine) to 6 (very feminine) was chosen based on recommendation from a psychometric study [13], implying instructions were given for the rating procedure."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Listening experiment",
            "reasoning": "Use of a 6-point Likert scale implies the presence of a scoring rubric defining the scale from very masculine to very feminine for annotators to use."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention providing any annotation examples or example speech samples with ratings to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "liebig22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10252,
      "completion_tokens": 265,
      "total_tokens": 10517
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Listening experiment and Section 3.1 Rater reliability",
          "reasoning": "The dataset annotations (gender perception ratings) were generated by multiple human annotators, specifically 28 laypersons and 13 experts who rated each speech sample on a 6-point Likert scale. The experts included speech therapists, indicating subject matter expertise. Multiple experts participated in quality assurance as evidenced by their use to assess rater reliability, thus ensuring reliability of annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Listening experiment and Section 3.1 Rater reliability",
          "reasoning": "Multiple human annotators without explicit subject matter expertise (laypersons) participated by rating speech samples. Their ratings were obtained from 28 laypersons who provide annotations, supporting this label."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9870,
      "completion_tokens": 229,
      "total_tokens": 10099
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Speech recordings",
          "reasoning": "The paper states that 86 participants were recruited and recorded reading aloud a German standard passage ('Der Nordwind und die Sonne'). This data was collected by the authors specifically for this study, involving human speakers reading a set text, constituting original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 Feature Extraction",
          "reasoning": "The acoustic features (e.g., fundamental frequency, vocal tract length) were derived from the recorded speech samples using signal processing and analysis methods, representing data transformations applied to the original recorded data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10388,
      "completion_tokens": 286,
      "total_tokens": 10674
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.4",
          "reasoning": "The paper describes using the newly collected speech dataset of 86 speakers as training data for three regression models to predict femininity/masculinity based on extracted acoustic features. The models were trained from scratch on these data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The dataset was also used for model evaluation through k-fold cross-validation and leave-one-out cross-validation approaches to assess model performance on predicting femininity/masculinity."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, 4",
          "reasoning": "The dataset was analyzed for rater reliability, feature correlations, and to understand the importance of acoustic features for gender perception, as discussed in the correlation and discussion sections."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11111,
      "completion_tokens": 351,
      "total_tokens": 11462
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Speech recordings",
          "reasoning": "The new dataset introduced consists of recordings of 86 speakers reading aloud a German standard passage \u201cDer Nordwind und die Sonne\u201d. All participants were native German speakers. Hence, the dataset is monolingual in a non-English language, specifically German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8329,
      "completion_tokens": 164,
      "total_tokens": 8493
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper regarding code availability.",
          "reasoning": "The paper does not mention any publicly available code or repository related to the dataset collection, preprocessing, or generation. There is no indication of code release or link to external resources."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method) with subsections 2.1 Speech recordings and 2.3 Feature Extraction.",
          "reasoning": "The dataset creation is documented in detail in Section 2, describing the recruitment of participants, recording setup, ethical approval, and acoustic feature extraction methods. The process of collecting speech samples from 86 participants and the methodology for feature extraction is transparently documented, enabling reproducibility of data collection and processing steps."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8461,
      "completion_tokens": 117,
      "total_tokens": 8578
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 (Corpus Description)",
          "Reasoning": "The NMSQA dataset introduced in the paper consists of spoken question answering data. The training and development sets are synthesized using Amazon Polly TTS service (model generated audio), while the test set contains real human speech recorded from 60 human speakers (human generated audio). Thus, the dataset's audio modality is both human-generated (test set) and model-generated (train/dev sets)."
        }
      ]
    }
  },
  {
    "id": "lin22c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9313,
      "completion_tokens": 219,
      "total_tokens": 9532
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "The answer intervals in the NMSQA dataset test set were annotated by Montreal Forced Aligner, a forced alignment tool, which is an automatic process aligning speech and transcript to produce time intervals."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any specific annotation instructions provided to annotators; since the annotation was done by an automatic forced aligner, no human annotation instructions were given."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No scoring rubrics or guidelines related to annotation quality or criteria are provided or described in the paper for the forced alignment process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide any annotation examples or demonstrations related to the annotation process for the new dataset; annotation was done via automatic forced alignment with no examples needed."
          }
        }
      ]
    }
  },
  {
    "id": "lin22c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10443,
      "completion_tokens": 316,
      "total_tokens": 10759
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being performed by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts or annotators with subject matter expertise for the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single human non-expert conducted quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance carried out by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was performed by an AI model acting as a judge on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1, specifically 'The answer intervals were annotated by Montreal Force Aligner [30].'",
          "reasoning": "The paper states that the answer time intervals were annotated using Montreal Force Aligner, which is an automatic forced alignment tool that algorithmically aligns speech and text. This constitutes an automated verification process for the annotations, rather than manual human annotation or expert review."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documented mention of an automatic quality assurance process via forced alignment; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10061,
      "completion_tokens": 530,
      "total_tokens": 10591
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description, Abstract, and Introduction",
          "reasoning": "The NMSQA test set was produced by 60 human speakers reading the SQuAD-dev-2 passages and questions naturally, which constitutes original spoken data created by human contributors. This data is not translated, adapted, or derived from pre-existing spoken data but is new recordings of reading existing textual questions and passages."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The training and validation sets of NMSQA were synthesized from Amazon Polly Text-to-Speech (TTS) service, which is an AI model generating spoken audio from text. Thus, parts of the released NMSQA data are newly generated by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using machine translation for data generation in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The NMSQA dataset leverages the existing SQuAD v1.1 dataset's textual questions and passages, thus aggregating and repurposing this existing textual QA dataset as spoken data. This counts as collating existing data sources (SQuAD dataset) with minimal modification but generating speech versions."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The spoken versions of SQuAD's textual QA dataset are derived by generating audio versions via TTS synthesis and human reading, with forced alignment to annotate answer spans. Hence, the spoken dataset is based on existing sources with modifications and transformations applied (e.g., speech synthesis, human reading, alignment), so it is derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the origin and generation method of the data clearly, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10579,
      "completion_tokens": 540,
      "total_tokens": 11119
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new dataset (NMSQA) being used for pre-training large models on general patterns. The pre-training described uses other existing datasets or unlabeled speech data but not NMSQA."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the NMSQA dataset is used to train models from scratch without pre-training. The paper focuses on fine-tuning pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4 (Implementation Details of DUAL), Section 4 (Results)",
          "reasoning": "The NMSQA dataset is used for supervised fine-tuning the pre-trained language model (Longformer) adapting it to the SQA downstream task by predicting answer time spans using spoken questions and passages. The paper explicitly states that DUAL is fine-tuned on the NMSQA data."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning post-training techniques or RLHF being used with the NMSQA dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 (Evaluation), Section 4 (Results)",
          "reasoning": "The NMSQA dataset is utilized for evaluation purposes, specifically the human speaker test set is used to benchmark model performance and robustness, as described in the evaluation and results sections."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some ablation analyses are performed in the paper, these analyses focus on model aspects rather than primarily using NMSQA for analyzing data trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the NMSQA dataset as a knowledge base to augment models through retrieval or other means."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents use of the NMSQA dataset for supervised fine-tuning and evaluation; thus, 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11302,
      "completion_tokens": 567,
      "total_tokens": 11869
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces the NMSQA dataset which is based on the SQuAD v1.1 dataset, an English language question answering corpus. The spoken forms in the NMSQA dataset are produced via Amazon Polly TTS and recordings by human speakers, all of which are in English. There is no mention of other human languages used."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English speech data; no evidence or mention of a second human language is present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The NMSQA dataset is described as the spoken version of SQuAD v1.1, which is an English question answering dataset. The speech data (both synthetic from Amazon Polly and human spoken) are all in English, as supported by the detailed descriptions and references to English Wikipedia content in the passage."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of any non-English language being included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken question answering data; it does not include programming languages or code snippets as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the dataset contains mathematical or formal logical expressions as part of the content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is focused on spoken human language questions and passages; no biological or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages such as Klingon or Esperanto are mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain spoken English language data."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8520,
      "completion_tokens": 181,
      "total_tokens": 8701
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link for dataset construction code is provided in the paper.",
          "reasoning": "The paper discusses the release of the NMSQA dataset and the use of external tools like Amazon Polly for TTS and Montreal Forced Aligner for alignment, but does not mention any publicly available code repository or provide links for the data collection, preprocessing, or generation code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Corpus Description)",
          "reasoning": "The paper provides a detailed description of the dataset creation process: The training and development sets were synthesized from Amazon Polly TTS with 12 speakers; the test set was recorded by 60 human speakers; answer intervals were annotated by Montreal Forced Aligner. This constitutes documentation of the dataset creation process, although no full procedural details or code are supplied."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7995,
      "completion_tokens": 231,
      "total_tokens": 8226
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 (Corpus Collection); Table 1",
          "Reasoning": "The paper introduces a new large-scale parallel English-Pidgin corpus consisting of 29,737 sentence pairs derived mainly from the Holy Bible, where each English verse was mapped to its corresponding Pidgin verse, with manual processing to ensure quality for some chapters. This is human-generated text data created by human translators and processed manually."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 (Corpus Collection); Table 1",
          "Reasoning": "The paper introduces a corpus of 5 million synthetic English-Pidgin sentence pairs generated using back-translation methods applied to existing English datasets (ISWLT'15 and WMT14) and Pidgin monolingual data, used for data augmentation in training. Although generated via model back-translation, these are considered synthetic data generated by automated model methods rather than manually created, hence model-generated text."
        }
      ]
    }
  },
  {
    "id": "lin23e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8847,
      "completion_tokens": 285,
      "total_tokens": 9132
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 Corpus Collection",
            "reasoning": "The large-scale English-Pidgin parallel corpus (29,737 sentence pairs) is mainly derived from existing religious texts such as the Bible, JW300, and other resources. The paper mentions that some chapters of the Bible required manual processing for quality, but no detailed human annotation process is described; thus, the corpus is essentially obtained via automatic extraction and alignment from these resources. Additionally, 5 million synthetic sentence pairs were generated using back-translation methods with machine translation models. Therefore, the annotation process for the new dataset is primarily an automatic process involving data extraction and machine-generated synthetic data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2 Corpus Collection",
            "reasoning": "The paper does not mention providing any detailed annotation instructions for human annotators since the datasets were collected from existing resources or generated synthetically. No instructions for annotation are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 Corpus Collection",
            "reasoning": "No scoring rubrics or criteria for quality rating were described for any human annotation process in the new dataset creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 Corpus Collection",
            "reasoning": "There is no mention of annotation examples or guidelines with examples provided in the paper regarding the dataset preparation."
          }
        }
      ]
    }
  },
  {
    "id": "lin23e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9977,
      "completion_tokens": 360,
      "total_tokens": 10337
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human annotator who is a subject matter expert or member of the target demographic for quality control of the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that multiple human annotators with subject matter expertise or belonging to the target demographic performed quality assurance for the new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that a single non-expert human conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by multiple non-expert human annotators for the new dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for back-translation and task adaptive training, the paper does not describe using AI models specifically as judges or for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes data augmentation and synthetic data generation through back-translation and model training, but does not indicate an automated verification process for dataset annotation quality."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any explicit quality assurance processes applied to validate or verify the dataset annotations or content of the new English-Pidgin parallel corpus or the synthetic datasets. They mention limited manual processing for a few chapters in the Bible corpus to ensure quality, but no comprehensive QA methodology or annotation verification process is detailed. Therefore, no formal quality assurance process is described or performed."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9595,
      "completion_tokens": 591,
      "total_tokens": 10186
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset was created entirely from scratch by human contributors without translation or adaptation. The human effort mentioned involves manual processing and ensuring quality of existing data, but no claim of fully original human-generated data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Abstract; Section 2; Section 4.2",
          "reasoning": "The authors generated a large synthetic dataset of 5 million English-Pidgin sentence pairs using back-translation with a trained machine translation system (Section 2 and Abstract). This synthetic data is newly generated by a model without direct human creation or translation, thus qualifies as new data from model generation."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The paper describes the collection of a parallel English-Pidgin corpus mainly from the Holy Bible where each English verse was mapped to its Pidgin counterpart; manual processing was involved to ensure quality, implying human translation. Additionally, the Naija Treebank includes transcribed spoken Pidgin texts with English translations, indicating human translation efforts."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While back-translation is used to generate synthetic data, this is a form of model generation rather than machine translation of existing data by MT systems. The paper does not mention data produced by applying machine translation to existing text corpora to form datasets for training or release."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The authors aggregated multiple existing datasets from various sources such as JW300, NaijaSenti, Afri-BERTa, BBC Pidgin, ASR, PidginUNMT, and others to compile their corpus. This is data collection from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2; Section 4.2",
          "reasoning": "The synthetic parallel corpus generated via back-translation is derived from existing monolingual corpora and parallel corpora like WMT14 and ISWLT, with modification (machine translation and data augmentation). Additionally, task adaptive training and continual adaptive training use augmented or adapted data that modifies original data for specific tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies clearly the origins of the introduced datasets; therefore, no dataset origin is undocumented."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10113,
      "completion_tokens": 562,
      "total_tokens": 10675
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3 (Cross-Lingual Adaptive Training), Figure 1, and Section 4.1",
          "reasoning": "The newly collected large-scale English-Pidgin parallel corpus and associated unlabeled Pidgin monolingual data are used to continually adapt a pre-trained base model via masked language modeling (MLM) on Pidgin data, constituting continual adaptive training before fine-tuning. This represents an unsupervised pre-training stage to adapt the model to Pidgin language patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper exclusively discusses adapting existing pre-trained models rather than training models from randomly initialized parameters using the new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Cross-Lingual Adaptive Training), Section 4.1 (Results), Table 3 and Table 4",
          "reasoning": "The collected datasets including the English-Pidgin parallel corpus are used to fine-tune pre-trained language models for downstream tasks such as sentiment classification and machine translation using supervised learning methods, e.g., the task adaptive training with back-translation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or demonstration of reinforcement learning post-training methods, such as RLHF, using the new datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not used exclusively for evaluation or benchmarking; rather they are primarily used for training and adaptation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Corpus Collection) and Orthographic Analysis subsection",
          "reasoning": "The newly collected corpus is analyzed for orthographic variations and linguistic characteristics to understand the data's nature and to motivate the need for adaptation techniques."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not described as serving as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes multiple practical uses of the collected dataset for both training (pre-training and fine-tuning) and analysis."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10836,
      "completion_tokens": 530,
      "total_tokens": 11366
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces datasets primarily consisting of English and Nigerian Pidgin language pairs, not exceeding two languages, so it is not multilingual."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 2 (Corpus Collection); Table 1",
          "reasoning": "The new dataset introduced by the authors is a large-scale English-Nigerian Pidgin parallel corpus containing 29.73k sentence pairs, clearly bilingual involving exactly two human languages: English and Nigerian Pidgin."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is parallel English-Pidgin; there is no new monolingual English dataset introduced by the authors."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses existing monolingual Pidgin data, no new monolingual (non-English) dataset is introduced by the authors."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors introduce datasets containing programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset containing mathematical or formal logical expressions is introduced in the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced do not include biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns natural languages, specifically English and Nigerian Pidgin, not any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages of the dataset are clearly specified as English and Nigerian Pidgin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data, so it is not applicable to mark as not containing any language."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8054,
      "completion_tokens": 150,
      "total_tokens": 8204
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention of code availability or link is provided in the paper text for dataset construction.",
          "reasoning": "The paper describes collecting and combining various datasets and generating synthetic sentence pairs, but does not provide any links, repositories, or mentions of making code used for constructing or processing the dataset publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Corpus Collection",
          "reasoning": "The paper provides a detailed description of the corpus collection process, listing the sources of the parallel English-Pidgin corpus and monolingual datasets, describing manual processing for some parts, and analysis of orthographic variations. This documentation covers the dataset creation process clearly in Section 2."
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8334,
      "completion_tokens": 115,
      "total_tokens": 8449
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Overview; Section 2.2 Data Acquisition",
          "Reasoning": "The paper introduces the SimuSOE dataset consisting of 4428 simulated snoring audio recordings from 82 adult participants. These audio recordings were captured using a lavalier microphone placed 3 cm from the participant's mouth while they intentionally emitted simulated snoring sounds. This direct human recording confirms the data is audio modality and human generated."
        }
      ]
    }
  },
  {
    "id": "lin24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9186,
      "completion_tokens": 248,
      "total_tokens": 9434
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2: Datasets, especially Section 2.2 Data Acquisition",
            "reasoning": "The PSG diagnostic reports used to label the dataset were generated by three experienced experts who are RPSGT certified, as described in Section 2.2. These experts scored sleep and respiratory events from PSG data according to AASM guidelines, indicating annotation by multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Acquisition",
            "reasoning": "The scoring of sleep stages and respiratory events was performed following the guidelines of the AASM Manual V2.6, which provides detailed instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Acquisition",
            "reasoning": "The use of the AASM Manual V2.6 as a scoring guideline implies the presence of rubrics to standardize annotation of sleep and respiratory events, which is standard in PSG scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention found",
            "reasoning": "The paper does not explicitly state that annotation examples were provided to annotators or included in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "lin24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10316,
      "completion_tokens": 425,
      "total_tokens": 10741
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper states that three experienced experts who are RPSGT certified scored sleep stages and respiratory events from the recorded PSG data to generate PSG reports, which serve as labels for the participants' conditions. This indicates that the quality assurance of the PSG diagnosis was performed by multiple human experts, not a single one, but the labeling of snoring data quality itself is not described as annotated by anyone. However, since multiple experts generated the PSG diagnosis, and this serves as ground truth labels for the dataset, single expert QA per annotator is not appropriate here."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper explicitly mentions that three experienced experts with RPSGT certification independently scored the PSG data according to AASM guidelines, generating the PSG reports including AHI, which are used as ground truth labels for the participants. This represents a multiple human expert quality assurance process for the PSG diagnosis, which validates the participant labels in the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that single human non-expert annotators were involved in the quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human non-expert annotators were involved in the quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used to perform quality assurance or annotation verification."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automatic verification or rule-based automated quality assurance processes are described for dataset annotations or labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper includes a defined QA process involving multiple certified expert annotators scoring PSG data, so it is not the case that no QA is applied or documented."
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9934,
      "completion_tokens": 402,
      "total_tokens": 10336
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Datasets), particularly 2.1 (Data Overview) and 2.2 (Data Acquisition)",
          "reasoning": "The SimuSOE dataset consists of 4428 simulated snoring audio samples recorded from 82 adult human participants. The data is newly created by humans who intentionally produce simulated snoring sounds during wakefulness. The study describes the recording setup and procedure in detail and involves human participants simulating snoring in various positions. Therefore, the data is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by any AI or machine learning model; all snoring data is recorded from human participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not aggregated from existing datasets or sources; it was newly collected and recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not derived or adapted from existing datasets; it is original data collected specifically for this study."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10452,
      "completion_tokens": 317,
      "total_tokens": 10769
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4 Conclusions",
          "reasoning": "The SimuSOE dataset is used to train classification models for OSAHS diagnosis and severe OSAHS screening through supervised learning methods, as shown by the conducted binary classification experiments in Section 3.2 demonstrating effectiveness of simulated snoring data for these tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4 Conclusions",
          "reasoning": "The dataset is employed to evaluate and benchmark the performance of models in OSAHS screening tasks, shown by reported metrics like accuracy and sensitivity in Section 3.2 to measure the effectiveness of simulated snoring signals for OSAHS evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2.2 Classification effects in different sleeping positions",
          "reasoning": "The dataset is used to analyze and understand the impact of sleeping positions on the classification performance, indicating usage for analyzing trends and characteristics within the data related to OSAHS evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11175,
      "completion_tokens": 614,
      "total_tokens": 11789
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described, SimuSOE, contains audio recordings of simulated snoring sounds from human participants but does not specify any human languages present in the audio or metadata."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes two human languages; the focus is on snoring sounds, not spoken language content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio snoring recordings and physiological measurements; there is no evidence of English language content within the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain spoken or written content in any single non-English language; it consists of audio snoring sounds without linguistic content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include programming or structured code content; it only includes recorded simulated snoring audio and associated physiological data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes statistical analysis and metrics, the dataset entries themselves do not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Overview, Section 2.2 Data Acquisition",
          "reasoning": "The proposed dataset, SimuSOE, consists of audio recordings of human snoring, which is a biological non-verbal sound related to human upper airway functioning. Although snoring is human in origin, it is categorized as biological and non-linguistic communication signals rather than human linguistic data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages; it only contains simulated snoring sounds."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or types of linguistic content in the dataset entries are explicitly specified as simulated snoring sounds without any language spoken."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio data of human-generated snoring sounds, which are vocalized sounds; thus, they constitute a form of non-linguistic communication and so cannot be categorized as containing no language at all."
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8393,
      "completion_tokens": 183,
      "total_tokens": 8576
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention found about code availability or link to code repositories.",
          "reasoning": "The paper does not provide any URLs, links, or explicit statements about releasing the code used for data collection, preprocessing, or dataset generation. There is no section dedicated to code availability or reproducibility via code in the text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Datasets) and subsections 2.1 (Data Overview) and 2.2 (Data Acquisition)",
          "reasoning": "The paper provides detailed documentation of the data collection methods, including equipment used (microphone model and audio interface), sampling frequency and resolution, participant demographics and informed consent, recording conditions and posture, and the PSG procedure for diagnosis labeling. These details thoroughly document the dataset creation and acquisition process enabling understanding and potential replication."
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5528,
      "completion_tokens": 259,
      "total_tokens": 5787
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Auditory stimuli",
          "Reasoning": "The dataset contains 40 short Chinese stories narrated by male or female professional speakers, which are human-generated audio stimuli presented to participants."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Data acquisition",
          "Reasoning": "The EEG signals were recorded from 20 participants using a 64-channel EEG system during auditory stimuli presentation. These EEG signals are human-generated data recorded with human participants using EEG sensors."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Auditory stimuli",
          "Reasoning": "Multiple-choice question answers and self-rated attention scores were collected from participants as behavioral indices, representing human-generated textual data associated with the EEG recordings."
        }
      ]
    }
  },
  {
    "id": "lin24f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6380,
      "completion_tokens": 189,
      "total_tokens": 6569
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1",
            "reasoning": "Twenty participants were recruited and participated in the auditory spatial attention experiments, indicating multiple human subjects performed the data collection and engagement with stimuli."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "Participants were instructed to concentrate on a single Chinese story among competing stimuli, indicating clear instructions were given to the participants for the task."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Each auditory stimulus was accompanied by multiple-choice questions with set answers to assess comprehension, serving as a rubric to evaluate participant attention and performance."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention providing examples in the annotation guidelines or to the participants relevant to the task."
          }
        }
      ]
    }
  },
  {
    "id": "lin24f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7510,
      "completion_tokens": 373,
      "total_tokens": 7883
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the text that multiple non-expert annotators performed quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model to perform quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated preprocessing steps (e.g., filtering, normalization) were applied to the EEG data, the paper does not describe any automated verification process specifically aimed at quality assurance of the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper provides no explicit description or evidence of any quality assurance process for the ASA dataset annotations or content. No human or automated QA procedure is documented in the manuscript."
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7128,
      "completion_tokens": 353,
      "total_tokens": 7481
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Description of the ASA Dataset)",
          "reasoning": "The paper describes the creation of the ASA dataset, which involved recruiting 20 human participants with normal hearing to record EEG signals while they listened to auditory stimuli (Chinese stories narrated by professional speakers). These auditory stimuli are original in the sense that the experiment was conducted by the authors, and the EEG data were recorded afresh from human subjects under controlled experimental conditions. Thus, this dataset is generated from scratch by human participants and human experimentation, not adapted or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of EEG recordings from human subjects and auditory stimuli played to them; there is no indication that any data were generated by AI or machine learning models directly for this dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention in the paper of translating data from another language by humans for this dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that machine translation was used to generate or transform data for this dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not an aggregation from existing sources but rather generated newly via experimentation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that this dataset was based on existing data with modifications; it is presented as a newly recorded dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data source and how the data was generated, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7646,
      "completion_tokens": 308,
      "total_tokens": 7954
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Training and evaluation",
          "reasoning": "The ASA dataset is utilized in training the CA-CNN model from scratch under subject-independent conditions, as described in Section 4.2. The model is trained iteratively with binary cross-entropy loss and the Adam optimizer on this dataset, indicating direct use for training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Training and evaluation; Section 5 Results and Discussion",
          "reasoning": "The ASA dataset is used to benchmark and evaluate different models including CA-CNN, GCN, STAnet, and CNN-KUL under subject-independent conditions. Evaluation metrics like ASAD accuracy and standard deviation are reported to assess model performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 Effect of channel attention",
          "reasoning": "The dataset enables analysis on the effect of channel attention on model performance, demonstrating trends and patterns in EEG responses and robustness across subjects."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8369,
      "completion_tokens": 523,
      "total_tokens": 8892
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The ASA dataset contains auditory stimuli in only one language, Mandarin, and does not include more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset uses auditory stimuli only in Mandarin; no presence of exactly two languages is indicated."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset auditory stimuli are in Mandarin, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (Auditory stimuli)",
          "reasoning": "The dataset auditory stimuli consist solely of short Chinese stories narrated by Mandarin speakers, indicating it is monolingual in a non-English language (Mandarin)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains EEG signal data and associated auditory stimuli, but does not include programming code entries as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Mathematical notation appears in the model description but not as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although EEG data is biological in origin, the dataset does not contain biological sequences or non-human communication systems as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes any fictional or artificially constructed language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the auditory stimuli is clearly indicated as Mandarin Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (Mandarin) in the auditory stimuli and therefore is not without language content."
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5587,
      "completion_tokens": 140,
      "total_tokens": 5727
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 6 Conclusion",
          "reasoning": "The paper states that the ASA dataset and the source code of the reference model are to be publicly released for research purposes, indicating future availability but does not provide a current link or location for this code; thus, the code is not presently made publicly available in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Description of the ASA Dataset",
          "reasoning": "The paper provides detailed documentation about the dataset creation process including participant demographics, auditory stimuli details, data acquisition setup, and experimental task design in Section 2, indicating comprehensive documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6960,
      "completion_tokens": 207,
      "total_tokens": 7167
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Table 1",
          "Reasoning": "The VoxBlink2 dataset comprises approximately 9,904,382 high-quality utterances sourced from 111,284 users on YouTube. These utterances represent human-spoken audio data collected from videos. The data are naturally human-generated speech captured in the wild from user-uploaded videos, not model-generated or synthetic."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Table 1, Section 2.2.1",
          "Reasoning": "The VoxBlink2 dataset consists of about 2,097,062 video clips from YouTube users. These videos are human-recorded clips uploaded by users and extracted for dataset construction. Thus, the video data is human-generated, as it is captured by human-operated cameras and uploaded online."
        }
      ]
    }
  },
  {
    "id": "lin24j_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7812,
      "completion_tokens": 249,
      "total_tokens": 8061
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2.1 Data mining",
            "reasoning": "The VoxBlink2 dataset was collected using a refined data mining pipeline involving automated steps such as candidate collection by keyword search, face detection using MobileNet, face recognition by a pre-trained classifier, and active speaker and overlap speech detection using audio-visual models. The process is predominantly automated without explicit mention of human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2.1 Data mining",
            "reasoning": "The paper does not describe any annotation instructions provided to human annotators; the dataset was generated via automated mining pipeline rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2.1 Data mining",
            "reasoning": "There is no evidence of scoring rubrics or similar evaluation guides being provided; the process is an automated pipeline rather than manual annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2.1 Data mining",
            "reasoning": "The paper does not mention any example annotations or annotation examples; the dataset creation is automated and does not rely on manual examples."
          }
        }
      ]
    }
  },
  {
    "id": "lin24j_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8942,
      "completion_tokens": 343,
      "total_tokens": 9285
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset quality verification involving multiple non-expert human annotators is not described in the paper."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Step IV",
          "reasoning": "The paper describes the use of an audio-visual speaker diarization model and an overlap detection model (both AI models) for active speaker detection and overlap speech detection to improve data quality. These AI models serve as judges for quality assurance by partitioning active speech segments and eliminating overlapping speech segments automatically."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Step II and III",
          "reasoning": "The data mining pipeline involves automated processes such as face detection using MobileNet and face recognition using a pre-trained ArcFace classifier for identification. These algorithmic and rule-based steps serve as automatic verification techniques to ensure data purity and quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple rigorous quality assurance steps using AI models and automated verification techniques; therefore, QA is present and documented."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8560,
      "completion_tokens": 460,
      "total_tokens": 9020
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset VoxBlink2 is collected from user-uploaded videos on YouTube and is not created entirely from scratch by human contributors; rather, it is aggregated from existing content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the VoxBlink2 dataset includes data generated by AI or machine learning models; it consists of videos sourced from YouTube users."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from other languages via human translators for dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to machine translation involvement in the dataset creation or data transformation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2.1",
          "reasoning": "The VoxBlink2 dataset is collected by aggregating existing user-uploaded videos from YouTube, using an optimized data collection pipeline that collects over 6 million one-minute videos and extracts utterances and video clips. The data is aggregated without indication of significant content modification, thus it is considered collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2.1 and 2.2.2",
          "reasoning": "The dataset is processed by applying face detection, face recognition using a trained classifier, active speaker detection, and overlap speech detection to refine and filter the collected raw data, indicating transformations and adaptations applied to the collected source data, making it derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and the methods of data processing and generation, so the origin is specified."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9078,
      "completion_tokens": 607,
      "total_tokens": 9685
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4 Experimental Settings and Section 5.1 Speaker Verification",
          "reasoning": "The VoxBlink2 dataset is primarily used for pre-training speaker recognition models before fine-tuning on other datasets (e.g., VoxCeleb2). The paper explicitly mentions pre-training models on VoxBlink2 to improve their generalization and achieve state-of-the-art results. This is discussed in Section 4 and demonstrated in Section 5.1 where experiments analyze the effect of varying training data scale."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe training models from randomly initialized parameters solely on the VoxBlink2 dataset without any pre-training or fine-tuning on other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 Experimental Settings and Section 5.1 Speaker Verification",
          "reasoning": "The VoxBlink2 dataset is used for supervised fine-tuning of pre-trained models. The paper details fine-tuning pre-trained models on a mixed set involving VoxBlink2 or VoxCeleb2, showing effectiveness in boosting performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of VoxBlink2 in reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Open-Set Speaker-Identification and Section 5.2 Open-Set Speaker-Identification",
          "reasoning": "A subset of VoxBlink2, the VoxBlink-clean set, is used to create benchmark evaluation protocols for the new Open-Set Speaker-Identification task, serving as the evaluation dataset to assess model performance in challenging authentication scenarios."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 and Section 5.2 Results",
          "reasoning": "The paper analyzes the influence of data scale and model complexity on performance, and studies open-set speaker-identification performance using VoxBlink2 subsets, thus using the dataset for analytical purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset being used as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The VoxBlink2 dataset is actively used throughout the paper for pre-training, fine-tuning, evaluation, and analysis, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9801,
      "completion_tokens": 532,
      "total_tokens": 10333
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Data mining, Step I. Candidate Collection",
          "reasoning": "The paper states that a long keyword list spanning 18 languages was used for user retrieval when collecting videos from YouTube (Section 2.2.1). This indicates that the VoxBlink2 dataset contains entries from more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than two languages, not exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages and is not limited to only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages, not exclusively a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes equations and explanations, these are part of the paper's content and evaluation protocols, not the dataset itself. The dataset consists of audio-visual utterances, not mathematical or formal symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human speech data and videos, with no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the dataset includes fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included are explicitly referenced (18 languages), so the language contents are specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken human languages in the utterances, so it contains languages."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7019,
      "completion_tokens": 192,
      "total_tokens": 7211
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention of code availability",
          "reasoning": "The paper provides a URL (http://voxblink2.github.io) for data and model resources but does not explicitly mention or link to the code used for the dataset construction or data mining pipeline. There is no text indicating that code for data collection, preprocessing, or generation has been publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (VoxBlink2 Dataset), subsections 2.1 (Data Description) and 2.2 (Data mining)",
          "reasoning": "The paper describes the dataset creation process in detail within Section 2, including the data mining pipeline, data collection steps, face detection and recognition methods, active speaker and overlap speech detection, as well as classifier training procedures. This provides a clear and comprehensive documentation of how the VoxBlink2 dataset was constructed."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6215,
      "completion_tokens": 243,
      "total_tokens": 6458
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Corpus Production Process and Section 2.2, Corpus Details",
          "Reasoning": "The MinSpeech new dataset consists of 2237 hours of unlabeled audio and 1778 hours of labeled audio derived from 36 diverse TV dramas and programs. The audio data is sourced from YouTube videos with human-generated subtitles, subsequently processed and segmented into audio files. The audio represents speech recordings captured from real-world human speech, therefore human generated and not model generated. The dataset creation process is explicitly described in Section 2."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.3 Label Generation and Section 2.1.1 Screening and Downloading Videos",
          "Reasoning": "The labels used for the labeled audio component are Mandarin text subtitles extracted from human-generated YouTube subtitles in '.vtt' files, representing human-generated text transcription. These labels are not generated by models but are sourced from humans providing subtitles, as described in the corpus production process in Section 2."
        }
      ]
    }
  },
  {
    "id": "lin24m_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7067,
      "completion_tokens": 208,
      "total_tokens": 7275
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1.3 Label Generation",
            "reasoning": "The label generation was performed by extracting timestamps and text from YouTube caption files and automatically cropping audio accordingly; filtering rules were applied programmatically to exclude undesirable segments, indicating an automatic annotation process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1.3",
            "reasoning": "The paper does not mention detailed annotation instructions provided to annotators, since the labeling was conducted via automatic extraction from subtitle files."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1.3",
            "reasoning": "No mention of scoring rubrics or criteria for human annotators is presented as the labeling was automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1.3",
            "reasoning": "The paper does not present annotation examples or guidelines; annotations were automatically derived from subtitle timestamps and text."
          }
        }
      ]
    }
  },
  {
    "id": "lin24m_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8197,
      "completion_tokens": 302,
      "total_tokens": 8499
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any details suggesting multiple non-expert humans performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any AI model was used to judge or quality control the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.4 Audio Validation",
          "reasoning": "The authors employed automated verification techniques such as signal-to-noise ratio (SNR) and signal energy thresholds to assess the quality of audio recordings. They also implemented automated filtering rules during label generation, including removing entries with multiple speakers, English text, short audio, special characters, and punctuation processing. These constitute algorithmic and rule-based automated quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automatic audio validation process, so QA is present and documented."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7815,
      "completion_tokens": 436,
      "total_tokens": 8251
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus was collected from existing YouTube videos with human-generated subtitles; it is not original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is sourced from real audio recordings and subtitles, not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by translating content from another language through human translators. The labels are in Mandarin, but were originally human-generated subtitles for Southern Min speech videos rather than translations."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that machine translation was used to create the data labels or content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and Section 2.1.3",
          "reasoning": "The data was collected by downloading existing YouTube videos with human-generated subtitles and extracting audio and text. The corpus was formed by aggregating these existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.2 and Section 2.1.3",
          "reasoning": "The corpus was processed by segmenting audio according to subtitle timestamps, applying filtering rules (e.g., removing special characters, excluding certain data), and validating audio files using SNR and energy criteria. This indicates the data was derived from existing sources with modifications and transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the data are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8333,
      "completion_tokens": 421,
      "total_tokens": 8754
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.4 SSL-Transformer Baseline System",
          "reasoning": "The MinSpeech unlabeled corpus of 2237 hours is explicitly used for the pre-training phase of self-supervised learning models Wav2vec 2.0 and HuBERT, as stated in Section 3.4."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training any model from randomly initialized parameters without pre-training on MinSpeech."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4 SSL-Transformer Baseline System, Section 4.2",
          "reasoning": "The labeled MinSpeech corpus (1778 hours) is used to fine-tune pre-trained SSL models (Wav2vec 2.0 and HuBERT) with supervised learning methods, as detailed in Sections 3.4 and 4.2."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of using MinSpeech for reinforcement learning post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, Tables 5-7",
          "reasoning": "MinSpeech subsets Dev and Test are utilized for evaluating and benchmarking ASR models' performance, as shown in multiple experimental results in Section 4."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using MinSpeech primarily for analysis of trends or characteristics beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use MinSpeech as a knowledge base or for augmentation in retrieval-augmented models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple practical usages of the MinSpeech corpus including pre-training, fine-tuning, and evaluation, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9056,
      "completion_tokens": 472,
      "total_tokens": 9528
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset described in the paper focuses on the Southern Min dialect with Mandarin text labels; only two human languages are mentioned, so it is not multilingual in the sense of containing more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.1.3 Label Generation / Section 2.2 Corpus Details",
          "reasoning": "The labeled audio consists of Southern Min dialect speech with Mandarin text labels used for alignment. Therefore, the dataset contains exactly two human languages: Southern Min (spoken) and Mandarin (text labels)."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention the dataset containing English content; rather, English is explicitly excluded from subtitle text (Section 2.1.3)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While Southern Min is a non-English language, the dataset is paired with Mandarin text labels, so it is not strictly monolingual in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of programming or code content in the dataset; it solely consists of speech audio and corresponding Mandarin text labels."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains mathematical or logical expressions; it is a speech corpus with text labels."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech only, with no non-human biological communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of constructed or fictional languages; the dataset involves a natural dialect and Mandarin text."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages involved are specified as Southern Min speech and Mandarin text labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (Southern Min speech and Mandarin text labels); thus, it is not language-free."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6274,
      "completion_tokens": 156,
      "total_tokens": 6430
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not mention or provide any links or references to code repositories related to the data collection, preprocessing, or dataset construction procedures. It only mentions that the MinSpeech dataset is available at a URL, but there is no indication that the code to build the dataset is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (MinSpeech Corpus) and its subsections",
          "reasoning": "The paper provides detailed documentation on the dataset creation process, including corpus production process (Section 2.1), audio segmentation, label generation, and audio validation. Tables and descriptions of corpus segmentation and utterance length distribution also support dataset documentation."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6979,
      "completion_tokens": 184,
      "total_tokens": 7163
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Dataset collection and Section 4.1.1",
          "Reasoning": "The Macaque Voiceprint Verification Dataset (MVVD) is newly constructed by the authors through 21 days of recording macaque calls using a dedicated microphone at 44.1kHz. The data is explicitly recorded by human operators using equipment, indicating human involvement in data collection."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Dataset collection and Section 4.1.1",
          "Reasoning": "The MVVD also includes video data recorded simultaneously with audio by a professional camera operated during the data collection process. The video data is captured by humans using recording devices, reflecting human generation."
        }
      ]
    }
  },
  {
    "id": "liu22d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7831,
      "completion_tokens": 242,
      "total_tokens": 8073
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1.2 Dataset labelling",
            "reasoning": "The dataset labelling was performed manually by humans who could distinguish macaque calls very well, which implies expert knowledge and auditory discrimination skills."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1.2 Dataset labelling",
            "reasoning": "The audio data processing procedure is described with clear steps (mark start/end times, cut segments, organize by macaque ID), indicating the annotators were likely given detailed instructions to execute these steps consistently."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1.2 Dataset labelling",
            "reasoning": "No mention or description of scoring rubrics or detailed multi-dimensional scoring criteria is provided; the labelling focuses on marking call boundaries and assignment to individuals rather than scoring quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1.2 Dataset labelling",
            "reasoning": "The paper does not provide any annotation examples or exemplars in either the main text or appendices for training annotators or guiding the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "liu22d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8961,
      "completion_tokens": 199,
      "total_tokens": 9160
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4.1.2 Dataset labelling",
          "reasoning": "The paper states that the start and end times of each macaque call were manually marked by human ears, which can distinguish macaque calls very well, and noise fragments were filtered out during labeling. This manual annotation implies a single human expert or at least a human presumed to have expertise in distinguishing macaque calls performed quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8579,
      "completion_tokens": 391,
      "total_tokens": 8970
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper states that the authors cooperated with a primate research center to record and manually label audio and video data from 144 macaques. They spent 21 days recording and manual labeling to construct the Macaque Voiceprint Verification Dataset (MVVD). Each macaque's calls were manually marked by humans to accurately identify call start and end times, thus this dataset is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by any AI or machine learning model; it was collected from actual macaque calls."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources; it was directly recorded and labeled by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of derivation or transformation from existing datasets. The dataset was created from original recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and collection method are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9097,
      "completion_tokens": 518,
      "total_tokens": 9615
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Experimental setup",
          "reasoning": "The MVVD dataset is used as training data to train the proposed macaque voiceprint verification models from scratch. Specifically, the basic training set (MacaqueT) and its variants (MacaqueT1, MacaqueT5, MacaqueT10) are used to train the model with different audio lengths, as described in Section 4.2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the MVVD dataset to fine-tune a pre-trained model; the models are trained directly on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning (RL) or RL-based fine-tuning methods applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Section 4.2 Experimental setup",
          "reasoning": "The MVVD dataset is also split into evaluation sets (MacaqueE and variants) used for testing and benchmarking the voiceprint verification models. The experimental results demonstrate the model performance on these evaluation sets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some analyses of call characteristics (e.g., Fig.5 spectrogram comparisons, duration statistics in Fig.6) are performed, these are done on the dataset rather than the dataset being used as an analysis tool per se. The primary use is for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base or retrieval resource to augment models in this work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the MVVD dataset in both model training and evaluation."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9820,
      "completion_tokens": 620,
      "total_tokens": 10440
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1.1 Dataset collection",
          "reasoning": "The dataset consists of audio and video data recorded from 144 macaques; there is no mention of human languages or multiple human languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1.1 Dataset collection",
          "reasoning": "The dataset contains calls of macaques only, not entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper is written in English, but the dataset contains macaque call audio data, not English human language speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset entries are audio calls of macaques, which are non-human vocalizations, not human language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section describes dataset entries containing programming or structured code content.",
          "reasoning": "The dataset includes audio and video recordings of macaque calls without programming code or data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No section describes dataset entries containing mathematical or formal logical expressions.",
          "reasoning": "The dataset entries are recordings of macaque calls, not mathematical or symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset, especially 4.1.1 Dataset collection and 4.1.3 Dataset characteristics",
          "reasoning": "The proposed MVVD dataset contains audio recordings of macaque calls, which are non-human biological communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No mention in the paper of any fictional or artificially created languages in the dataset.",
          "reasoning": "The dataset contains natural macaque vocalizations, not constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Dataset details in Section 4.1 specify the nature of the audio entries as macaque calls.",
          "reasoning": "The language content (macaque calls as non-human animal vocalizations) is explicitly documented and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of audio vocalizations, which are a form of communication (non-human) thus contain language-like signals."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7038,
      "completion_tokens": 168,
      "total_tokens": 7206
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper provides a URL 'https://github.com/PengLiu2022/MacaqueDataset' where the Macaque Voiceprint Verification Dataset (MVVD) can be downloaded, which indicates that the dataset code related to data collection or at least the dataset itself is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper provides detailed documentation on dataset collection (microphone and camera setup, number of macaques, recording duration), labeling process (manual labeling steps, rationale for not using VAD), and dataset characteristics (duration statistics, segment counts, splits into training and testing sets), thereby offering thorough transparency about the dataset creation process."
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6595,
      "completion_tokens": 109,
      "total_tokens": 6704
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Source of data",
          "Reasoning": "The paper introduces new dataset consisting of audio recordings of isolated vowels and a read passage collected from 60 patients diagnosed with Spasmodic Dysphonia (SD) or Vocal fold Palsy (VP). These are real speech audio recordings captured in a clinical setting using a microphone in a quiet clinic room, thus human generated and not model generated."
        }
      ]
    }
  },
  {
    "id": "liu22q_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7447,
      "completion_tokens": 191,
      "total_tokens": 7638
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.3 Alignment and annotation",
            "reasoning": "The phonetic annotation was performed using the Montreal forced aligner, which automatically aligned orthographic transcripts with speech audio to produce phone-level segmentations, identified as an automatic forced alignment approach."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not specify any annotation instructions provided to annotators, since the phonetic annotation was automatic."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "There is no mention of scoring rubrics for annotation since the alignment and annotation process was fully automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No examples of annotation guidelines are provided because the annotation was performed automatically by the forced aligner without human annotators requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "liu22q_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8577,
      "completion_tokens": 317,
      "total_tokens": 8894
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The dataset annotations (phonetic labels) were generated by an automatic forced alignment approach using the Montreal forced aligner, which produced segmentation and phone labeling automatically. The quality of alignment was verified by manually checking a random sample of 20 recordings with 3s sections each (Section 2.3). Only 2% of annotations were found to be significantly in error, with no corrections made afterwards. This represents an automatic verification approach supplemented by a small manual audit, indicating that the primary QA method was based on automated algorithmic annotation processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8195,
      "completion_tokens": 397,
      "total_tokens": 8592
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset comprises audio recordings collected from 60 patients diagnosed with Spasmodic Dysphonia and Vocal fold Palsy during clinical assessments. These recordings were made directly from human patients reading a fixed passage and producing sustained vowels, explicitly gathered for this study. Therefore, the data represent original content created entirely from scratch by human contributors, not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data were generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any translation of data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data from other languages is reported in the dataset preparation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not collected or aggregated from existing sources without modification; rather, it was newly recorded specifically for this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the study uses feature extraction and processing techniques, the raw data themselves are original recordings, not derived or adapted from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies data origin, thus this category does not apply."
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8713,
      "completion_tokens": 273,
      "total_tokens": 8986
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.5 Classification and Section 3 Results",
          "reasoning": "The paper describes the use of the collected dataset of speech recordings from patients with spasmodic dysphonia and vocal fold palsy to train Support Vector Machine classifiers from scratch, with leave-one-out cross-validation for pathology discrimination."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Phonetic analysis and Section 3.3 Phonetic fusion",
          "reasoning": "The dataset is used primarily for analyzing the effects of phonetic context in discriminating voice pathologies, investigating differences in classification accuracy across phone categories, and studying phonetic fusion to improve classification, thus analyzing patterns and characteristics rather than solely training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9436,
      "completion_tokens": 532,
      "total_tokens": 9968
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains speech recordings only from British English speakers reading an English passage, with no indication of other languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain entries with exactly two human languages; only English speech recordings are described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Source of data",
          "reasoning": "The dataset consists of recordings of British English speakers reading the 'Arthur the Rat' passage and producing isolated vowels. The passage and speech are exclusively in English as stated in Section 2.1."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The data is described as English speech only; no non-English single language dataset is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists solely of speech audio recordings; no programming or code data entries are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech recordings and phonetic annotations; no mathematical or formal logical expressions are noted in the data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the study concerns human voice pathology, the dataset is composed of human speech recordings, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset entries is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken English language content, so it does not lack language."
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6654,
      "completion_tokens": 201,
      "total_tokens": 6855
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention or provide any link or reference to publicly available code related to data collection, preprocessing, or generation of the dataset. There is no indication that code used to construct the dataset or perform alignments and feature extraction is available publicly."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Materials and Methods) and especially Sections 2.1 to 2.3",
          "reasoning": "The paper provides detailed documentation on the dataset creation and collection process in Section 2.1, including information about the patient groups, recording hardware, recording conditions, and speech materials collected. Further details on preprocessing such as resampling and manual editing are described in Section 2.2 and 2.3, including the transcript creation and forced alignment method. This provides a reasonably clear description of the dataset creation process, although no mention is made of dataset sharing or public availability."
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6610,
      "completion_tokens": 173,
      "total_tokens": 6783
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4 Dataset description",
          "Reasoning": "The MSDWild dataset contains 3143 video clips collected from public videos of daily casual conversation in the wild, which are naturally shot videos without over-editing. This indicates human involvement in creating and recording the video data, as they originate from human-recorded public videos of real conversations."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4 Dataset description",
          "Reasoning": "MSDWild includes audio data from the same public video clips of daily casual conversations. These audios are naturally recorded in real-world environments, involving human speech captured by human-operated recording devices in the original source videos."
        }
      ]
    }
  },
  {
    "id": "liu22t_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7462,
      "completion_tokens": 204,
      "total_tokens": 7666
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4 Dataset description",
            "reasoning": "The paper describes manually filtering videos and labeling the speaker diarization data, indicating manual annotation likely performed by multiple human experts to ensure quality in complex multi-speaker scenarios."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4 Dataset description",
            "reasoning": "The authors mention a manual filtering process to remove videos with over two speakers talking in turn without pre-trained algorithms to avoid bias, implying deliberate instructions were given to annotators to follow this criterion."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not describe specific scoring rubrics or formalized scoring criteria for annotation, only general dataset collection and annotation procedures."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no indication or reference to annotation examples or sample annotations provided to annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "liu22t_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8592,
      "completion_tokens": 353,
      "total_tokens": 8945
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention QA by a single human expert or any indication that a single human annotator with subject matter expertise performed quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit information that multiple human experts conducted quality assurance. The paper mentions manual filtering of videos but does not specify that multiple expert annotators were involved in the quality assurance process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit evidence of quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 1 Introduction and Section 4 Dataset description",
          "reasoning": "The paper states that to filter videos with over two speakers talking in turn, manual filtering was done without using pre-trained audio-visual algorithms to avoid bias. It implies human involvement in the filtering and annotation process, but no mention of annotator expertise is given. Therefore, it is reasonable to conclude multiple human annotators without specified expertise performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any AI model was used as a judge for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification of annotations or use of algorithmic or rule-based QA processes applied to validate dataset content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents some manual filtering and annotation processes, so QA process is present and documented, even if limited in description."
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8210,
      "completion_tokens": 481,
      "total_tokens": 8691
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 1 (Introduction), 4 (Dataset description), and Abstract",
          "reasoning": "The dataset MSDWild was collected by the authors from public videos that are naturally shot, focusing on daily casual conversations in the wild. They describe a manual filtering process to remove videos with over two speakers and emphasize the naturalness of the data without over-editing. The dataset includes over 3000 video clips with 80 hours of labeled data, annotated to identify speaker diarization. This indicates original content creation from real-world recordings and manual annotation by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected from natural videos and manually labeled by humans rather than generated by AI or machine learning models. There is no indication that data itself was generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data or annotations from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Dataset description), Introduction",
          "reasoning": "The dataset is collected from existing public videos (e.g., Vlogs) available online. The authors curated and aggregated these videos to form the dataset, but the clips are natural recordings, not generated or heavily altered. The process involves collecting and filtering video clips from these external sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some data processing occurs (such as face detection and tracking), the data itself is not described as being substantially modified or transformed from pre-existing datasets. The core content remains naturally recorded videos."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin is clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8728,
      "completion_tokens": 577,
      "total_tokens": 9305
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Dataset Description); Section 5.3 (Visual-based pipeline); Section 5.3.1 (Experiment setup); Section 5.3.2 (Implementation of visual-based pipeline)",
          "reasoning": "The paper describes using the MSDWild dataset for training purposes, e.g., the few-talker training set of MSDWild used to train models for audio-visual speaker diarization tasks as described in Sections 4 and 5. Training setups including training visual only, two-stream audio-visual, and fused audio-visual models on this dataset are explained, indicating training from scratch or supervised training on this new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe using the dataset to fine-tune pre-trained models via supervised fine-tuning; rather, the experiments describe training models on this data or evaluation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of reinforcement learning based post-training methods such as RLHF being used with this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 (Evaluation Metrics); Section 5.4 (Result and Analysis)",
          "reasoning": "The dataset is widely used for evaluation and benchmarking, with detailed results on DER and JER metrics reported for several audio-only, visual-only, and audio-visual baseline methods, particularly on few-talker and many-talker testing sets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Dataset Description); Section 5.4 (Result and Analysis)",
          "reasoning": "The paper provides detailed analyses of dataset characteristics such as the speaker count, overlapped speech, distribution differences compared with other datasets, and discusses the challenge and patterns in the data influencing algorithm performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as or used as a knowledge base for augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple practical uses of the dataset for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9451,
      "completion_tokens": 268,
      "total_tokens": 9719
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Introduction, Section 4 Dataset description",
          "reasoning": "The dataset MSDWild is collected from public videos covering rich real-world scenarios and languages (Abstract). It contains daily casual conversations in various real-world environments, which implies multiple languages are present. Although exact languages are not listed, the paper specifies the dataset includes multiple languages rather than being restricted to English or a single language (Abstract, Introduction). Thus, MSDWild is considered multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6669,
      "completion_tokens": 210,
      "total_tokens": 6879
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part in the paper mentions or provides a link to code for dataset construction.",
          "reasoning": "The paper mentions a dataset release URL (https://x-lance.github.io/MSDWILD) but does not provide any direct link or mention of publicly available code related to the data collection, preprocessing, or annotation pipeline. There is no explicit section that states the availability of code or software for reproducing the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Dataset description) and Section 1 (Introduction) discuss the dataset collection and characteristics.",
          "reasoning": "The paper provides a detailed description of the dataset collection pipeline, data source selection (keywords used for video collection), manual filtering criteria, dataset structure, statistics (number of clips, duration, speaker counts, overlaps, etc.), and baseline experimental setup. These components convey good transparency and documentation about how the dataset was constructed and its properties, sufficient for understanding the dataset creation process."
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8772,
      "completion_tokens": 159,
      "total_tokens": 8931
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data Simulation",
          "Reasoning": "The SLURP-S and LT-S datasets are newly developed multi-channel noisy-reverberant datasets derived from the original SLURP and Libri-Trans datasets. They contain audio data that is both human-generated original speech (from SLURP and Libri-Trans) and model-generated augmentations that simulate reverberation, multiple noises, and microphone array effects using Pyroomacoustics and other noise sources to simulate smart-speaker scenarios. Therefore, the audio data is a mixture of human-generated speech and model-generated simulated acoustic conditions, as explicitly described in Section 4.1."
        }
      ]
    }
  },
  {
    "id": "lu22c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9624,
      "completion_tokens": 222,
      "total_tokens": 9846
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1",
            "reasoning": "The two multi-channel noisy-reverberant datasets SLURP-S and LT-S are simulated using acoustic room impulse responses generated with Pyroomacoustics and noise samples from FSD50k and SINS, indicating that the data creation is done by an automatic simulation process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not describe any annotation instructions given to human annotators because the datasets are created via simulation and no manual labeling is described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No mention of scoring rubrics or structured evaluation guidelines for annotation is present since the datasets are synthesized automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "There are no annotation examples provided in the paper, as the data preparation is a simulation process rather than manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "lu22c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10754,
      "completion_tokens": 336,
      "total_tokens": 11090
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the paper describes the use of an AI model specifically as a judge or quality assessor for dataset validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses the simulation of noisy-reverberant multi-channel datasets using Pyroomacoustics and noise sources, but does not describe a dedicated automated quality assurance process for annotations or dataset content verification."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces two multi-channel noisy-reverberant datasets derived from SLURP and Libri-Trans via simulation with room impulse responses and noise sources, but it does not document any quality assurance procedures such as human validation, expert assessment, or AI-based evaluation for these datasets. The data generation and augmentation process is described, but no explicit quality assurance or validation process is presented."
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10372,
      "completion_tokens": 290,
      "total_tokens": 10662
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "The paper describes development of two multi-channel noisy-reverberant datasets, SLURP-S and LT-S, which are derived from existing datasets SLURP and Libri-Trans by applying simulated room impulse responses, adding noises from existing noise corpora, and simulating realistic smart-speaker capture conditions. This process constitutes modifications and transformations applied to original sources, making these datasets derived rather than new from scratch."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10890,
      "completion_tokens": 299,
      "total_tokens": 11189
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4, especially Sections 4.2 and 4.4",
          "reasoning": "The newly created multi-channel noisy-reverberant datasets SLURP-S and LT-S are used to fine-tune pre-trained speech enhancement and downstream models such as ASR, ST, and SLU in a supervised manner. For example, in Section 4.4, the authors fine-tune SE and SLU models together on the SLURP-S dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The SLURP-S and LT-S datasets are used for evaluating how well speech enhancement front-ends combined with back-end tasks such as ST and SLU perform. The datasets serve as benchmark corpora for assessing multi-channel noisy-reverberant scenarios."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11613,
      "completion_tokens": 648,
      "total_tokens": 12261
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "The paper introduces two new datasets (SLURP-S and LT-S) based on SLURP and Libri-Trans, but these are designed for English speech tasks (ASR, ST, SLU) only, with no indication of multiple languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "There is no mention of any bilingual data or two specific human languages present in the new datasets. Only English language speech data is used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Simulation and Section 4 (Experiments)",
          "reasoning": "The new multi-channel noisy-reverberant datasets SLURP-S and LT-S are simulations based on the SLURP and Libri-Trans datasets, which are English language speech datasets. The paper explicitly mentions ASR, ST, and SLU tasks with English content. No other human languages are mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "No non-English language dataset or content is introduced or described in the paper's proposed new datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper discusses new datasets for speech tasks; there is no mention that the new datasets include code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the paper includes math notation in methodology, the datasets themselves do not contain mathematical or formal symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The new datasets consist of simulated human speech in noisy reverberant environments and do not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced are based on real English speech corpora and do not include any fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper explicitly states the linguistic origin of the new datasets as English; therefore, the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new datasets clearly contain human spoken language data, so the label 'N/A' for no language does not apply."
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8831,
      "completion_tokens": 179,
      "total_tokens": 9010
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper explicitly states that the multi-channel ST and SLU datasets developed by the authors are released on HuggingFace, and the code for ESPnet-SE++ is available on GitHub at https://github.com/ESPnet/ESPnet, which indicates the code for data preparation is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "Section 4.1 provides detailed documentation on the data simulation process for the two newly developed multi-channel noisy-reverberant datasets (SLURP-S and LT-S), including the simulation parameters such as room size, reverberation time, noise sources, microphone array setup, and data subsets selection, which is transparent and adequate for reproducibility."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8384,
      "completion_tokens": 191,
      "total_tokens": 8575
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data details and Section 2.2 The Generation Process of codec-based fake audio",
          "Reasoning": "The Codecfake dataset consists of real audio samples collected from two existing corpora: VCTK, which contains speech from 110 English speakers, and AISHELL3, which contains Mandarin speech from 218 speakers. These samples are human-produced speech recordings captured by microphones. The dataset also contains fake audio samples generated by seven neural codec models applied to these real samples, resulting in new audio samples synthesized by the models. Therefore, the dataset includes both human generated (original recordings) and model generated (neural codec synthesized) audio samples, with origin explicitly described in Section 3.1. The modality is audio as the data consists of speech waveforms and their codec-based synthetic versions."
        }
      ]
    }
  },
  {
    "id": "lu24f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9236,
      "completion_tokens": 217,
      "total_tokens": 9453
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 and Section 3.1",
            "reasoning": "The Codecfake dataset consists of real audio samples from VCTK and AISHELL3 datasets re-encoded and decoded by seven trained neural codec models automatically, without human manual annotation. The data generation process is an automatic codec re-encoding and decoding procedure described in Section 2.2, and dataset details are provided in Section 3.1."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention any detailed annotation instructions or manual labeling protocols since the dataset generation is automated by neural codec models."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No scoring rubrics or human evaluation guidelines are described for annotation, consistent with the automated dataset generation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No annotation examples or guidelines are provided because no human annotation was performed."
          }
        }
      ]
    }
  },
  {
    "id": "lu24f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10366,
      "completion_tokens": 369,
      "total_tokens": 10735
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that an AI model was used to perform quality assurance or judgment on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3.1",
          "reasoning": "The dataset (Codecfake) is constructed automatically by generating fake audio samples using seven representative neural codec models applied to existing real datasets (VCTK and AISHELL3). The generation and construction process is fully automated, relying on trained neural codec models to re-encode and decode audio samples, and no manual annotation or human verification is described. Thus, quality assurance can be considered as an automatic verification of data generation through proven codec models and controlled experimental procedures documented in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes an automated and systematic procedure to generate and organize the dataset, which implies some form of quality control via the controlled use of codec models and datasets during generation."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9984,
      "completion_tokens": 450,
      "total_tokens": 10434
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not consist of original content created entirely by humans from scratch. Instead, it is generated through automatic processes involving neural codec models."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2 The Generation Process of codec-based fake audio",
          "reasoning": "The Codecfake dataset fake audio samples are generated entirely by AI models \u2014 seven representative neural codec models are used to re-encode and decode existing real speech data to create fake audio. This process involves automatic data generation by machine learning models without direct human content creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that any data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although real speech data from public datasets like VCTK and AISHELL3 are used as source audio, the fake data is not merely aggregated or collected from existing sources; it is transformed through neural codec models."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 The Generation Process of codec-based fake audio",
          "reasoning": "The fake audio data is derived from existing datasets (VCTK and AISHELL3) by applying seven different neural codec models to re-encode and decode the audio. This constitutes data derived from existing sources with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is clearly documented in Section 2.2, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10502,
      "completion_tokens": 513,
      "total_tokens": 11015
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the Codecfake dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the Codecfake dataset is used to train a model from randomly initialized parameters; rather, it is used to train or fine-tune ADD models which may already incorporate pretrained components."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 (Codec-trained ADD models results)",
          "reasoning": "The paper describes training ADD models (e.g., W2V2-AASIST) supervised by the Codecfake dataset to improve detection performance on codec-based audio. This indicates usage of the dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using reinforcement learning or RL-based post-training methods with the Codecfake dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2 (Results and Discussion)",
          "reasoning": "The Codecfake dataset is used to evaluate performance of vocoder-trained and codec-trained ADD models across various test conditions (C1-C7). The dataset's test subsets serve as the evaluation benchmark."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use Codecfake for analysis of trends or characteristics beyond performance evaluation; the focus is on training and evaluating ADD detectors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base or used for retrieval augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the Codecfake dataset for training and evaluation of audio deepfake detection models."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11225,
      "completion_tokens": 740,
      "total_tokens": 11965
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 1 (Introduction), Section 2.2 (The Generation Process of codec-based fake audio), and Section 3.1 (Data details)",
          "reasoning": "The Codecfake dataset contains audio samples from two different languages: English (from VCTK dataset) and Chinese Mandarin (from AISHELL3 dataset). Specifically, in Section 1, it states Codecfake spans two languages. Section 2.2 details the usage of VCTK (English) and AISHELL3 (Mandarin Chinese) as source datasets for generating fake audio with seven neural codecs. Section 3.1 further enumerates the number of samples from both datasets. Therefore, the dataset includes exactly two human languages, English and Mandarin Chinese, making it bilingual (two languages); however, since the rubric specifies 'Multilingual' as more than two human languages and 'Bilingual' as exactly two languages, the dataset fits the Bilingual category rather than Multilingual."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 1 (Introduction), Section 2.2 (The Generation Process of codec-based fake audio), and Section 3.1 (Data details)",
          "reasoning": "The proposed Codecfake dataset includes audio samples from English (VCTK corpus) and Chinese Mandarin (AISHELL3 corpus) only, with no mention of additional languages. Sections 1, 2.2, and 3.1 explicitly mention these two languages and the usage of their datasets to generate the fake samples. Thus, since the dataset contains exactly these two human languages, it qualifies as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not monolingual English because it also contains Mandarin Chinese (as part of AISHELL3)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains English and Mandarin Chinese, so not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises audio data samples; there is no indication of containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio and fake audio generated from neural codecs; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved in the dataset are clearly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio samples of human languages (English and Mandarin Chinese), so language is present."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8443,
      "completion_tokens": 226,
      "total_tokens": 8669
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or portion of the paper provides a link or mention of publicly available code for dataset construction.",
          "reasoning": "The paper does not include any URLs, footnotes, or mentions indicating that the code used to generate the Codecfake dataset or associated preprocessing/generation scripts are publicly available. There is no repository link or supplementary material reference for code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 'The Generation Process of codec-based fake audio' and Section 3.1 'Data details'",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including the training process of neural codec models (Section 2.2), datasets used (LibriTTS for training codecs; VCTK and AISHELL3 for inference), the implantation details for generating fake audio, the specific neural codec models selected with description (Section 2.1), and the detailed dataset composition and splits (Section 3.1). This documentation is sufficient and transparent regarding how the dataset was constructed and its characteristics."
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10176,
      "completion_tokens": 308,
      "total_tokens": 10484
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 2.2 Data Processing Pipeline; Table 1",
          "Reasoning": "The paper introduces FLEURS-R, a new speech corpus derived by applying a speech restoration model (Miipher) to the original FLEURS corpus's speech recordings. The original FLEURS corpus consists of human-recorded speech data in 102 languages. FLEURS-R maintains the same speech samples but with improved audio quality via processing. Therefore, the audio data modality is present, and it originates from human-generated recordings that have been processed but not generated anew by models. This is explicitly stated in Abstract and Section 2.2, where FLEURS-R is described as a restored version of the original human-recorded speech corpus, not newly generated audio."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Table 1",
          "Reasoning": "The FLEURS-R corpus contains parallel speech and text data in 102 languages, consistent with the original FLEURS corpus. The text data corresponds to the transcriptions derived from human-generated text content (Wikipedia text sentences read aloud). No mention is made of generating or synthesizing text artificially; thus, text data is human-generated. Therefore, text modality is present and originates from human-generated sources as per the paper's description."
        }
      ]
    }
  },
  {
    "id": "ma24c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11028,
      "completion_tokens": 209,
      "total_tokens": 11237
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 (Speech Restoration Pipeline), Section 2.2 (Data Processing Pipeline)",
            "reasoning": "The FLEURS-R corpus is created by applying the speech restoration model Miipher to the original FLEURS speech samples. The corpus contains the same samples as FLEURS but with restored audio quality, indicating an automatic processing pipeline rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any manual annotation process or instructions provided for human annotators related to the new dataset creation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or criteria for human annotation are mentioned in relation to the dataset creation or processing."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any examples of manual annotation or guidelines related to human annotation for the dataset."
          }
        }
      ]
    }
  },
  {
    "id": "ma24c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12158,
      "completion_tokens": 349,
      "total_tokens": 12507
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any manual quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no description of quality assurance performed by multiple human experts or annotators."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human annotator performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert human annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "Speech naturalness evaluation and ASR evaluations were performed using AI models, e.g., the SQuId MOS model for naturalness scoring and the Maestro-U ASR model for intelligibility and semantic consistency verification. These AI models served as judges to assess the quality of the speech data and the synthetic speech, thereby effectively functioning as an AI-based quality assurance mechanism."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "An ASR-based filtering process was applied automatically to identify successfully restored samples and reject those with processing artifacts. This filtering is an automated verification step ensuring dataset quality without human intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes AI model-based evaluation and automated ASR filtering as part of the quality assurance process; thus, it is not correct to say no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11776,
      "completion_tokens": 412,
      "total_tokens": 12188
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper indicates that the FLEURS-R corpus uses the same constituent samples as the original FLEURS corpus, which was recorded data, but does not state that any new speech recordings were made by humans for FLEURS-R."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Processing Pipeline",
          "reasoning": "The FLEURS-R corpus was generated by applying the Miipher speech restoration model to the existing FLEURS corpus, thereby generating new, restored speech samples entirely by a machine learning model without new human recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the speech data was produced by human translation activities."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate the data; the paper focuses on speech restoration, not translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not merely collected or aggregated from existing sources but processed using a restoration model to improve audio quality."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Processing Pipeline",
          "reasoning": "FLEURS-R is based on the existing FLEURS corpus with modifications applied via speech restoration modeling, representing a transformed version of the original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data generation process for the new dataset."
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12294,
      "completion_tokens": 337,
      "total_tokens": 12631
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3.1",
          "reasoning": "The FLEURS-R corpus is used to train multi-speaker Virtuoso 2 text-to-speech (TTS) baseline models from scratch, as described in Section 3.3.1, where the same hyperparameters are used for training on either FLEURS or FLEURS-R to enable consistent comparison."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The corpus is used for automatic speech recognition (ASR) based intelligibility evaluations and speech naturalness evaluations (SQuId MOS) to benchmark quality improvements of the restored speech over the original FLEURS corpus, as explained in Sections 3.1 and 3.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The dataset is analyzed for trends and patterns including character error rates by language and naturalness scores across different languages and utterance durations, as reported in Sections 3.1 and 3.2."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13017,
      "completion_tokens": 530,
      "total_tokens": 13547
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Introduction; Table 1",
          "reasoning": "The paper introduces FLEURS-R, a speech-restored version of the FLEURS corpus that contains N-way parallel speech data in 102 languages. It explicitly states that FLEURS-R maintains the linguistic scope of FLEURS, covering 102 languages spanning over 17 language families (Abstract, Introduction). Table 1 compares FLEURS-R as containing 102 locales, confirming the high multilingual nature of the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains 102 languages, not exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes 102 languages rather than only English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages, not only one single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as speech and text data for multiple human languages, with no mention of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech and text data spanning 102 human languages, not biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural human languages; there is no indication that constructed or fictional languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included are explicitly specified as 102 locales, hence not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset explicitly contains human language speech and corresponding text."
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10235,
      "completion_tokens": 173,
      "total_tokens": 10408
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not explicitly provide or mention any link or location where the code used for constructing the FLEURS-R dataset or the speech restoration process is publicly available. While the dataset itself is publicly released on Hugging Face, there is no statement or reference indicating the availability of the code used for data processing or restoration."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 2.1",
          "reasoning": "The dataset creation process is documented in detail in Sections 2 and 2.1, where the authors explain the speech restoration pipeline, including specific modifications to the Miipher model to accommodate multilingual data, and describe the data processing pipeline such as filtering restored samples. This provides sufficient transparency and detail about the dataset construction."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8343,
      "completion_tokens": 265,
      "total_tokens": 8608
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset construction",
          "Reasoning": "The dataset Propicto-eval was created by recording 3,011 sentences spoken by 62 unique speakers over six months, involving a recording campaign. This audio data involves human speech recordings explicitly stated in the paper."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset construction",
          "Reasoning": "The transcriptions of the audio recordings were derived from the recorded sentences and their original text sources such as children's stories, everyday situations, and medical domain sentences. This text originates from human-created sentences used for the recording campaign."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset construction and Table 1",
          "Reasoning": "The pictogram token sequences aligned with the audio and text were generated automatically by applying the method from Macaire et al. [22], which involves algorithmic mapping rules and a restricted lexicon to produce a 'pictographic language' representation from the transcriptions. This indicates that the pictogram token sequences are model generated from text."
        }
      ]
    }
  },
  {
    "id": "macaire24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9195,
      "completion_tokens": 211,
      "total_tokens": 9406
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.5 Human evaluation",
            "reasoning": "The human evaluation was performed by two expert annotators from the project who adapted the MQM framework to analyze translation errors with specific error types and severity levels."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.5 Human evaluation",
            "reasoning": "Annotators selected from 12 error types and 4 severity levels as per an adapted MQM framework, indicating detailed guidelines and instructions were followed for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.5 Human evaluation",
            "reasoning": "The evaluation uses the MQM framework which involves quantifying errors with types and severity levels combined to compute an Overall Quality Score (OQS), showing the presence of scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not explicitly mention providing annotation examples in the guidelines for the human evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "macaire24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10325,
      "completion_tokens": 282,
      "total_tokens": 10607
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 4.5 Human evaluation",
          "reasoning": "The human evaluation was performed by two expert annotators from the project who are experts, indicating quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that quality assurance was conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies that two expert annotators conducted the human evaluation, thus quality assurance was not by non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models were used for training and evaluation, the paper does not mention any AI model being used to perform quality assurance or as a judge for the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of automated verification or rule-based quality assurance processes for dataset annotations is provided."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a human evaluation process involving multiple experts, indicating that quality assurance is present and documented."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9943,
      "completion_tokens": 545,
      "total_tokens": 10488
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The authors construct two new datasets: Propicto-orf\u00e8o and Propicto-eval. Propicto-eval was created through a recording campaign involving 62 unique speakers reading sentences compiled from publicly available ARASAAC PDFs, over six months. This process was overseen by human contributors (annotators and recorders), indicating original human-generated data in terms of audio recording and curation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve human speech recordings and associated human-curated pictogram translations; no description indicates the data was generated entirely by AI models independently."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data results from human translation from one language to another; pictogram translations are generated from French text but not described as a human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The pictogram translations are generated using a method described by Macaire et al. [22], which applies specific rules and a restricted lexicon to French transcriptions. This is a rule-based mapping rather than a machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "Propicto-orf\u00e8o is built upon the Corpus d'Etude pour le Fran\u00e7ais Contemporain (CEFC) [21], which is an existing speech/text corpus. The authors extracted audio segments and their transcriptions from it. This constitutes data aggregated from existing sources without significant modification to the source audio or text."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The pictogram terms translations in both datasets are generated by applying the method of Macaire et al. [22], which converts the textual transcriptions into pictogram sequences using rules and a restricted lexicon. This constitutes data derived from existing transcriptions with modifications/adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and construction process are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10461,
      "completion_tokens": 576,
      "total_tokens": 11037
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new datasets for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions training an NMT model from scratch, but this model is trained on the Propicto-orf\u00e8o dataset, which is derived from pre-existing data and generated via a method from prior work [22]. The datasets themselves are constructed for aligned speech-text-pictogram sequences but not explicitly described as enabling training from randomly initialized parameters independently from pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Training details",
          "reasoning": "The paper explicitly states that several models (Wav2Vec2.0 ASR, mBART, T5-large, NLLB-200, ConST) are fine-tuned on the Propicto-orf\u00e8o training data, adapting pre-trained models for the speech-to-pictogram task. This demonstrates the use of the dataset for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of the datasets for reinforcement learning post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction, Section 4 Results and Discussion",
          "reasoning": "The Propicto-eval dataset is constructed specifically for evaluation purposes, containing recordings from unique speakers and relevant contexts. The paper uses this dataset and the test splits of Propicto-orf\u00e8o to benchmark and measure performance of models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.5 Human evaluation",
          "reasoning": "The datasets are used for in-depth analysis, including human evaluations analyzing error types and overall quality scores to better understand model behavior on pictogram translation beyond automatic metrics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets are used as external knowledge bases or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets have multiple practical usages described in the paper for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11184,
      "completion_tokens": 632,
      "total_tokens": 11816
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The proposed datasets, Propicto-orf\u00e8o and Propicto-eval, contain French speech, text transcriptions, and pictogram translations only. There is no mention of multiple languages present in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets contain French speech and their aligned transcriptions plus pictogram representations, which are considered as a target sequence of terms but not a second human natural language. Therefore, the dataset entries do not contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The input speech and transcripts are in French, not English. The textual content is explicitly stated to be French speech and text."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The datasets are constructed from French speech corpora and their French transcriptions aligned with pictogram sequences. The language used is French, which is a non-English human language. This applies to both Propicto-orf\u00e8o and Propicto-eval."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries consist of audio (speech), text transcriptions, and pictogram term sequences. There is no indication of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical expressions or symbolic notations are described as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset focuses on human speech (French) and pictogram sequences. There is no mention of biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The pictogram sequences represent graphic symbols matched to concepts; they are not described as a constructed or artificial language such as Klingon or Esperanto."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is clearly documented as French, so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Since the dataset contains human language data (French speech and text), it does not apply as non-language data."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8402,
      "completion_tokens": 219,
      "total_tokens": 8621
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 3.1 Dataset construction; Footnote 3",
          "reasoning": "The paper explicitly states in the abstract that they constructed and released two freely available corpora for the speech-to-pictogram task. In Section 3.1 and the beginning of the paper, they mention that the code is released at https://github.com/macairececile/speech-to-pictograms, indicating public accessibility of the code associated with dataset creation and processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The paper provides a detailed description of the dataset construction process in Section 3.1, including the source corpora used (Corpus d'Etude pour le Fran\u00e7ais Contemporain), the steps for extracting audio segments, methods for generating pictogram translations, the domains covered, and the evaluation dataset creation process involving recording and translation. It also notes compliance with data protection rights overseen by a Data Protection Officer, reflecting transparency and ethical considerations."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7536,
      "completion_tokens": 316,
      "total_tokens": 7852
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Corpus Overview",
          "Reasoning": "The paper states the Santa Barbara Corpus comprises 60 recordings (24 hours) of naturally-occurring speech recorded from 1987\u20131996 using stereo microphones. These recordings capture conversational spoken American English from diverse speakers, indicating human involvement in recording natural speech audio."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Corpus Overview and Section 3.1 Transcript Processing and Correction",
          "Reasoning": "The corpus includes detailed human-created transcripts at the level of intonation units, containing utterance-level time marks, speaker labels, overlap labels, and other annotations. These transcripts were originally created for linguistic and human viewing purposes, indicating they are human-generated text."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Corpus Overview",
          "Reasoning": "The corpus includes metadata about participants\u2019 gender, age, hometown, education, occupation, ethnicity, and recording details. This metadata is structured informational data about the recordings and speakers, produced with human involvement during corpus preparation."
        }
      ]
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8388,
      "completion_tokens": 180,
      "total_tokens": 8568
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Manual corrections and annotations were performed to address inconsistent annotation issues, re-annotating unmarked code-switching and creating consistent speaker labels, indicating human expert involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention detailed annotation instructions provided to annotators; corrections appear to be performed manually without documented guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria guiding the annotation or correction process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "No evidence in the paper that annotation examples were provided as part of guidelines or instructions."
          }
        }
      ]
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9518,
      "completion_tokens": 490,
      "total_tokens": 10008
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human annotator with subject matter expertise or membership in the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance performed by multiple human experts or annotators who are members of the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance conducted by a single human non-expert annotator as described in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Transcript Processing and Correction",
          "reasoning": "The paper describes extensive manual corrections and checking of annotations including fixing typographical errors and speaker re-annotation. Although specific annotator qualifications are not stated, the manual corrections imply multiple non-expert humans performing these corrections to improve data quality."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Alignment",
          "reasoning": "The authors used several forced alignment models (Montreal Forced Aligner, torchaudio *-CTC-based MMS_FA, and WAV2VEC2_ASR_BASE_960H) to align transcripts with audio. These are AI models used to assist in validation and segmentation resolution, implying AI involvement in quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Anonymization Filter Detection",
          "reasoning": "The authors developed a manually-tuned algorithm to detect anonymized (filtered) regions in the audio based on spectral energy. This automated algorithm was applied and its results were manually checked. Thus, automated rule-based verification was used as part of the QA process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents quality assurance steps involving manual corrections, AI model alignment, and automated detection algorithms; thus, QA was performed."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9136,
      "completion_tokens": 467,
      "total_tokens": 9603
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new data created entirely from scratch by human contributors. Instead, it utilizes an existing corpus collected from 1987\u20131996."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was generated solely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation systems as per the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 2 Corpus Overview; Section 3 Corpus Preparation",
          "reasoning": "The Santa Barbara Corpus of Spoken American English is an existing corpus collected historically; the authors repurpose and assemble it for evaluation purposes. The data is collected from original recordings made between 1987 and 1996, thus it is aggregated from existing recordings and transcripts."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 Corpus Preparation, especially Sections 3.1 Transcript Processing and Correction, 3.2 Anonymization Filter Detection, and 3.3 Alignment",
          "reasoning": "The authors apply modifications such as correcting and reformatting transcripts, detecting anonymized audio regions, re-aligning segments using forced alignment, and re-segmenting. These are transformations and adaptations of the existing original data to make it suitable for automatic processing and evaluation, indicating the data is derived from original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and processing of the dataset, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9654,
      "completion_tokens": 395,
      "total_tokens": 10049
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 1 Abstract; Section 4 Experimental Configuration; Section 5.2 Speech Recognition Results",
          "reasoning": "The Santa Barbara Corpus of Spoken American English (SBCSAE) is repurposed by the authors primarily for evaluation of multi-talker speech technology including speaker diarization and speaker-attributed ASR. The paper describes preparing the corpus for automatic processing and demonstrates the failure of state-of-the-art systems on it, showing inconsistent system performance (Section 1 Abstract, Section 4, and Section 5). No indication is given that the dataset is used for training or fine-tuning models, only for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 1 Abstract; Section 5.1 Performance Analysis (implied in the text); Section 5.2 Speech Recognition Results",
          "reasoning": "The paper includes detailed analysis of system performance on the corpus, such as analyzing diarization errors across speaker demographics and conversation types (Section 5), and investigating performance trends such as difficulties with elderly and teenage speakers. This use of the dataset to analyze trends and system weaknesses shows it is used for analysis purposes as well as evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents extensive preparation and use of the SBCSAE corpus for evaluation and analysis of speech technology; thus, the dataset has documented and demonstrated utility."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10377,
      "completion_tokens": 584,
      "total_tokens": 10961
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper introduces the Santa Barbara Corpus of Spoken American English (SBCSAE) as a dataset comprising spoken American English only. There is no mention of multiple languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only American English speech and transcripts; there are references to code-switching marks but no indication that a second language is present systematically or separately."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (Corpus Overview), Section 3.1 (Transcript Processing and Correction)",
          "reasoning": "The paper clearly states that the Santa Barbara Corpus contains naturally-occurring speech from American English speakers and is designed for linguistic study of spoken American English. The transcripts and annotations are detailed and pertain solely to English speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of the dataset containing any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves spoken language recordings and transcripts; although processing scripts are mentioned, the dataset itself does not include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of mathematical or formal logical expressions or symbolic representations in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is speech recordings and transcripts of human spoken language; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains naturally occurring American English speech; no fictional or artificially created languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language is explicitly specified as American English; there is no uncertainty about the language content of the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language and transcripts; therefore, language is present in the dataset."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7595,
      "completion_tokens": 130,
      "total_tokens": 7725
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 1 (Introduction) and Section 3 (Corpus Preparation)",
          "reasoning": "The paper states that all corpus processing scripts and results will be released and integrated into the Lhotse audio preparation library for ease of use, indicating that the preprocessing code is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Corpus Preparation)",
          "reasoning": "The paper provides detailed descriptions of the dataset preparation process including transcript processing and correction, anonymization filter detection, and alignment procedures, demonstrating thorough documentation of the dataset construction and processing steps."
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9307,
      "completion_tokens": 111,
      "total_tokens": 9418
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection",
          "Reasoning": "The MASCFLICHT Corpus is a novel dataset introduced by the authors, containing speech samples recorded from 30 German participants using a smartphone microphone. The data consists of human speech recorded under five different face mask conditions, collected in a controlled environment with human participants reading or describing tasks. This confirms it is audio data, recorded by humans, hence human-generated."
        }
      ]
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10159,
      "completion_tokens": 246,
      "total_tokens": 10405
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 Data Pre-Processing and Partitioning",
            "reasoning": "The paper describes an automatic pre-processing step applying a Root-Mean-Square (RMS) based Voice Activity Detector (VAD) to remove unvoiced frames and segment the speech samples into 1-second chunks. There is no indication that humans manually annotated or labelled the data; the conditions are determined by experimental protocol (face mask types and coverage areas) and the segmentation is done algorithmically."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not mention any annotation instructions or guidelines given to annotators because the dataset labels correspond to known experimental conditions rather than human-labeled annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "No scoring rubrics or evaluation guidelines are presented since no subjective annotation or scoring was performed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "There is no mention or indication of annotation examples, as no manual annotation was conducted."
          }
        }
      ]
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11289,
      "completion_tokens": 261,
      "total_tokens": 11550
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert or annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts conducting quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for baseline classification experiments, no mention is made of using AI models for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or quality assurance process of dataset annotation or content is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper lacks any description or mention of a quality assurance process for validating the dataset annotations or content; therefore, no QA process is documented or applied."
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10907,
      "completion_tokens": 367,
      "total_tokens": 11274
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The paper explicitly states that the MASCFLICHT Corpus consists of speech samples collected from 30 German participants who recorded speech under five different face mask conditions using a smartphone. The data was recorded in a controlled environment with human participants performing speech tasks, which constitutes original content created entirely from human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected from human speakers and not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data being produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated through machine translation as per the paper content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was created through direct recording from human participants rather than aggregation from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is original speech recorded from participants; thus, it is not based on existing sources with modification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the dataset is clearly specified."
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11425,
      "completion_tokens": 397,
      "total_tokens": 11822
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results) and Section 3.2 (Model Description)",
          "reasoning": "The dataset is used to train various models from scratch to recognize face mask type and coverage area from speech. Models such as CNNscratch and RN18scratch are explicitly trained from randomly initialized weights on the MASCFLICHT dataset, as described in Section 3.2 and evaluated in Section 4."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Model Description) and Section 4 (Experimental Results)",
          "reasoning": "The RN18tuned model uses transfer learning by tuning a pre-trained ResNet18 model on the MASCFLICHT dataset to perform classification tasks, indicating supervised fine-tuning on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results)",
          "reasoning": "The dataset is partitioned into train, development, and test sets and used to evaluate baseline model performances quantitatively via metrics such as Unweighted Average Recall (UAR). Evaluation results are reported in detail in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results) and Section 5 (Conclusions)",
          "reasoning": "The dataset enables analysis of feature representations and model architectures for the specific paralinguistic tasks of face mask type and coverage area recognition, including observation of suitability of features and model choices, as discussed in Section 4 and 5."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12148,
      "completion_tokens": 543,
      "total_tokens": 12691
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The dataset contains speech samples from only German speakers; no multiple languages are present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "Only one human language, German, is spoken by all participants; no bilingual content exists."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The corpus consists only of German speech; the text and speech data are not in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The dataset is collected from 30 German participants speaking German, with all sentences and speech samples in German exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section specifies code or programming language content in the dataset.",
          "reasoning": "The dataset consists solely of speech recordings; no programming or code samples are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No section indicates presence of mathematical or logical symbolic expressions in the dataset.",
          "reasoning": "The dataset features speech audio samples without embedded mathematical or logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No description suggests inclusion of biological sequences or non-human communication forms in the dataset.",
          "reasoning": "The dataset comprises human German speech samples only."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "All speech is in standard German; no fictional or artificial languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Not applicable as the language of dataset entries is clearly documented as German.",
          "reasoning": "The paper clearly specifies the language spoken by participants."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data (German speech), hence language is present."
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9366,
      "completion_tokens": 225,
      "total_tokens": 9591
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not provide any explicit reference, link, or mention to publicly available code repositories related to the dataset collection, preprocessing, or generation. There is no indication that the code used to construct the MASCFLICHT corpus is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (The MASCFLICHT Corpus), specifically subsection 2.1 (Data Collection) and 2.2 (Data Pre-Processing and Partitioning)",
          "reasoning": "The paper provides a detailed description of the data collection procedure, participant demographics, recording conditions, mask types and configurations, and the recording setup (smartphone model, environment). It also describes the preprocessing steps (voice activity detection, segmentation, normalization), partitioning strategy (speaker-independent train/development/test splits following a LOSO-CV approach), and rationale behind data balancing and participant distribution. Ethical approval and informed consent are also mentioned. Overall, these details form a thorough documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8723,
      "completion_tokens": 285,
      "total_tokens": 9008
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.1; Section 2.2",
          "Reasoning": "The SOMOS dataset consists of 20,100 synthesized speech utterances produced by 200 different neural text-to-speech systems based on Tacotron models, all synthesized using the LPCNet vocoder. The speech samples are entirely synthetically generated by AI acoustic models, not manually recorded by humans."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The synthesis corpus includes 2,000 sentences derived from multiple human-generated textual sources such as: 100 sentences from the LJ Speech scripts (excluded from LJ Speech training), sentences from Blizzard Challenges 2007-2016, semantically unpredictable sentences, Wikipedia and general public domain sentences from the web. These texts were manually created by humans and curated by the authors for coverage."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3; Section 3.1",
          "Reasoning": "The SOMOS dataset includes crowdsourced human evaluations of synthesized utterances' naturalness collected via Amazon Mechanical Turk. The subjective ratings are human-generated data, representing human listener judgments of audio samples."
        }
      ]
    }
  },
  {
    "id": "maniati22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9575,
      "completion_tokens": 245,
      "total_tokens": 9820
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 describes that all subjective evaluations were conducted online with crowdsourced naive listeners via Amazon Mechanical Turk (AMT), indicating use of multiple non-expert human annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 specifies the task instruction given to workers: 'Rate how natural each sample sounds in a scale from 1 [very unnatural] to 5 [completely natural]' with labeled Likert scale options provided, which constitute clear instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The 5-point Likert scale with defined labels '1: very unnatural' to '5: completely natural' functions as a scoring rubric guiding the annotators on how to rate samples."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide any indication or mention of example annotations or exemplar ratings provided to workers during the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "maniati22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10705,
      "completion_tokens": 358,
      "total_tokens": 11063
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.2 Quality of Ratings",
          "reasoning": "An experienced US native linguist provided responses for approximately 100 pages of the test covering all 200 TTS systems. Their annotations are considered expert judgments used to validate and compare with the crowdsourced non-expert ratings, indicating single human expert quality assurance was conducted."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Listening Test Design and Section 3.2 Quality of Ratings",
          "reasoning": "Quality assurance was performed by multiple crowd-sourced human annotators (almost 1000 unique workers) recruited via Amazon Mechanical Turk, who evaluated naturalness on a 5-point Likert scale. These annotators are considered non-experts as there is no information indicating they possess subject matter expertise."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's quality assurance was not performed by AI models acting as judges, but rather by human raters as described in the paper."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Listening Test Design and Section 3.2 Quality of Ratings",
          "reasoning": "Several automatic quality control measures were applied to the crowdsourced ratings, including validation questions, inclusion of ground truth samples, reCAPTCHA to deter spam, minimum playback requirements, and filtering of suspicious HITs based on score patterns. This constitutes automated verification procedures ensuring annotation quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10323,
      "completion_tokens": 511,
      "total_tokens": 10834
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Sentences",
          "reasoning": "The dataset's 2,000 sentences include 100 sentences randomly selected from the LJ Speech script and 1,900 sentences composed from various public domain sources (Blizzard Challenges scripts, Wikipedia, and 'general' public domain sentences). This indicates that the textual data - at least partially - were newly composed or freshly selected by human contributors to ensure domain, length, and phoneme coverage."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.1 Systems",
          "reasoning": "The speech data consists of 20,100 utterances synthesized by 200 different neural TTS systems derived from Tacotron-based acoustic models. These synthetic utterances were generated entirely by AI models, i.e., neural TTS systems, using the designed sentences as input, producing novel audio data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence that any part of the dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 Sentences",
          "reasoning": "The inference corpus sentences include publicly available sentences from Blizzard Challenges (2007-2016), Wikipedia, and other public domain sources. These were collected and aggregated to form the textual dataset used for synthesis."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The synthetic speech data are generated by models, but this is distinct from being 'derived' from existing audio sources with modifications; the synthetic speech is newly generated by TTS models, not adaptations of prior audio. The text corpus was collated rather than derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and generation method of the dataset, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10841,
      "completion_tokens": 481,
      "total_tokens": 11322
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the SOMOS dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 MOS Prediction Baselines",
          "reasoning": "The paper describes training MOS prediction models from scratch on the SOMOS dataset (Section 4), indicating its use for training models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 MOS Prediction Baselines",
          "reasoning": "Pretrained MOS prediction models are fine-tuned on the SOMOS dataset validation set, demonstrating supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the SOMOS dataset is used for reinforcement learning post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 MOS Prediction Baselines",
          "reasoning": "The dataset is used to evaluate model performance, serving as a benchmark for state-of-the-art MOS prediction models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Quality of Ratings, Section 3.3 Results, Section 4 MOS Prediction Baselines",
          "reasoning": "The dataset is analyzed to assess subjective evaluation quality, listener rating consistency, and model prediction challenges, indicating usage for analysis of patterns and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the SOMOS dataset serving as a knowledge base for augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple practical usages of the SOMOS dataset; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11564,
      "completion_tokens": 422,
      "total_tokens": 11986
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2 (Speech Dataset)",
          "reasoning": "The dataset consists solely of English sentences synthesized by TTS models; only English is used throughout."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2 (Speech Dataset)",
          "reasoning": "There is no mention of any language other than English; hence no bilingual content is present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (Speech Dataset) and Section 3.1 (Listening Test Design)",
          "reasoning": "The dataset uses only English sentences, sampled from English corpora (LJ Speech, Blizzard Challenge, Wikipedia etc.), and listeners were restricted to native English speakers from English locales. Hence, all dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset only comprises English utterances, no other single non-English languages are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech utterances and subjective scores; there is no programming code content included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or logical symbolic data are part of the proposed dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech only and no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or evidence of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the entries is explicitly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of audio speech utterances containing English language, so it does contain language content."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8782,
      "completion_tokens": 198,
      "total_tokens": 8980
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention the availability of any code repository or link for the data collection, preprocessing, or synthesis code used to generate the SOMOS dataset. There is a URL provided for dataset samples and information (https://innoetics.github.io/publications/somos-dataset/index.html), but no indication that code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process, including the design of the speech dataset with descriptions of TTS systems used (Section 2), the composition of the sentences (Section 2.2), the vocoder and acoustic models (Section 2.1), and the crowdsourced MOS evaluation setup and quality control measures (Section 3). This documentation is thorough and transparent for reproducibility and understanding."
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6411,
      "completion_tokens": 193,
      "total_tokens": 6604
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3",
          "Reasoning": "The BRA VE-MASKS corpus contains audio recordings of continuous Russian speech from 30 native speakers wearing various types of masks and without masks. The recordings were made using human participants and professional recording equipment, explicitly described in Section 3.1 and Section 3."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. and 3.2",
          "Reasoning": "The BRA VE-MASKS corpus contains multi-angle images and video recordings of persons' faces wearing different masks and without masks. These videos were recorded from multiple devices including smartphones and a tablet with human participants performing various tasks and head rotations in controlled conditions. This is explicitly stated in Sections 3 and 3.2."
        }
      ]
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7263,
      "completion_tokens": 233,
      "total_tokens": 7496
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3",
            "reasoning": "The paper states that the RetinaFace detector was used for initial face region annotation, which is an automatic process, followed by manual checking and removal of erroneous cases. However, there is no indication that human annotators defined or labeled mask classes manually; the mask classes were formed by combining similar protective masks, and annotation focused on bounding boxes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for human annotators; only a manual correction step removing false positives from automatic detection is described without specifying formal guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No scoring rubrics or evaluation criteria for annotation are described; the annotation is mainly bounding box detection followed by manual removal of false positives."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No examples of annotation or labeling instructions are provided or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8393,
      "completion_tokens": 299,
      "total_tokens": 8692
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance in dataset annotation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an AI model (RetinaFace detector) was used for initial mask bounding box annotation, this was part of the annotation process, not quality assurance as a judge to verify annotation correctness."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Corpus annotation",
          "reasoning": "The RetinaFace detector was used to automatically detect face regions and annotate bounding boxes. This is an automated verification step for annotation generation. The authors also mention manual checking to remove false positives, indicating the use of automated detection followed by human review."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automatic annotation verification process using RetinaFace and manual checking; thus, QA process is described."
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8011,
      "completion_tokens": 415,
      "total_tokens": 8426
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 BRAVE-MASKS corpus",
          "reasoning": "The BRA VE-MASKS corpus is described as newly created multimodal data containing audio and video recordings of 30 Russian native speakers wearing various masks. The data was recorded specifically for this study using three devices in office conditions, with participants performing various speech tasks. This indicates the data was collected entirely from human participants anew, not translated or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated purely by AI or machine learning models; all data pertains to human-collected audio-visual recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved translation from another language done by humans. The corpus speech is in Russian and is recorded, not translated."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is described or implied in the data creation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as an aggregation of existing sources; it was newly recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though data augmentation is applied during model training (e.g., Specaugment, Mosaic), the raw dataset itself is not derived or significantly transformed from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as newly recorded multimodal data from human participants."
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8529,
      "completion_tokens": 291,
      "total_tokens": 8820
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Mask Type Recognition Task) and Section 5 (Experimental Results)",
          "reasoning": "The BRA VE-MASKS corpus was used to fine-tune pre-trained deep neural networks for the supervised learning task of mask type recognition. The authors used pre-trained audio neural networks and Yolov5 object detector, replacing or tuning the output layers with BRA VE-MASKS data, which indicates supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Experimental Results) and the description of dataset splitting in Section 3.3",
          "reasoning": "The BRA VE-MASKS corpus was split into Train, Development, and Test sets, with Development and Test sets used to measure and report performance of the developed models, thereby serving an evaluation/benchmarking role."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9252,
      "completion_tokens": 252,
      "total_tokens": 9504
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3, e.g., 3.1 Bimodal part",
          "reasoning": "The BRA VE-MASKS corpus contains audio-visual recordings of continuous Russian speech by native Russian speakers, explicitly stating 30 native Russian speakers participated in the data collection. This indicates the dataset entries contain only one human language, which is non-English (Russian)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6470,
      "completion_tokens": 187,
      "total_tokens": 6657
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not contain any links or mentions of publicly available code repositories for the dataset construction, data collection, preprocessing, or annotation processes. The only code-related reference is to open-source codes for Yolov5 training (footnote 1), which is unrelated to the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (BRAVE-MASKS corpus) including subsections 3.1, 3.2, 3.3",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 3, covering recording devices, data collection procedures, participant demographics, mask types, data partitioning, annotation procedures including manual checking of face bounding boxes, and data statistics. Such detailed description facilitates transparency and reproducibility in dataset construction."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7203,
      "completion_tokens": 198,
      "total_tokens": 7401
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 (Saxophone recordings for A2S) and Abstract",
          "Reasoning": "The paper introduces a novel collection of 1026 real saxophone recordings used specifically for audio-to-score transcription. These recordings are real-world saxophone performances captured in a home studio by human musicians, representing the audio modality with direct human generation through performance and recording."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 (Saxophone recordings for A2S)",
          "Reasoning": "The dataset includes the corresponding scores for the saxophone recordings in the Kern format, which is a digital symbolic music notation representation (textual format). These scores are human-generated annotations of the performances, created manually or derived from existing musical notations, and are used as the ground truth for transcription."
        }
      ]
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8055,
      "completion_tokens": 296,
      "total_tokens": 8351
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2 and 6 Conclusions",
            "reasoning": "Section 2 describes the collection of real saxophone recordings with corresponding digital scores. Musicians proficiently trained in the instrument recorded pieces in a home studio, implying expert human annotators performed the annotation by aligning recordings with digital scores. The presence of metadata and interpretative annotations further supports expert involvement. The Conclusions section confirms this new collection is a novel dataset explicitly created for this study."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "Section 2 describes that the musicians recorded pieces following specific conditions like use of metronome for tempo consistency and variations in styles and rhythm metrics to increase variability. This implies that instructions or protocols were provided to the recording musicians and annotators to ensure quality and consistency."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention found",
            "reasoning": "The paper does not explicitly mention the existence of scoring rubrics or detailed labeling rubrics guiding annotations for the real saxophone recordings."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 1, Section 2",
            "reasoning": "Figure 1 illustrates the score encoding example (subword-based representation disentangling note tokens), and Section 2 provides detailed examples of score annotations including expressive annotations and metadata, suggesting examples and annotation schemas were provided to annotators for consistency."
          }
        }
      ]
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9185,
      "completion_tokens": 428,
      "total_tokens": 9613
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions that the recordings were performed by proficiently trained musicians and digital scores are provided, but it does not describe any human annotation or quality assurance process conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance or annotation checks on the dataset. The paper does not discuss any multi-expert annotation or validation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of an AI model to perform quality assurance on the annotations or dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated verification or algorithmic procedure to check the correctness of the annotations or data quality."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new dataset of real saxophone recordings with corresponding digital scores, but it does not describe any specific quality assurance process for the annotations or recordings. The only relevant information is that the musicians are proficient and recordings were done in a home studio using a metronome to reduce tempo deviations, but no explicit QA or validation of the annotations or scores is documented."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8803,
      "completion_tokens": 437,
      "total_tokens": 9240
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The authors created a novel corpus of 1026 real saxophone recordings performed by proficient human musicians in a home studio. These recordings are original musical performances rather than adaptations or translations of existing data. The associated digital scores are aligned with these recordings, constituting original human-generated data created from scratch."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "Synthetic audio data for saxophone performances was generated using two synthesis methods: FluidSynth with a SoundFont bank and the MIDI-DDSP neural synthesis model. This synthetic data is generated by models from the annotated scores to produce artificial audio and thus qualifies as new data created by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from one language to another through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources; it was newly recorded for this work."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The real recordings' scores were transposed to a unified pitch reference to standardize data for transcription, applying modifications (transposition) to existing score annotations. Furthermore, synthetic data is derived from these existing score annotations through synthesis processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and generation methods of its datasets."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9321,
      "completion_tokens": 532,
      "total_tokens": 9853
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the saxophone recordings dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2, 4.4",
          "reasoning": "The dataset of real saxophone recordings (Tr) introduced by the authors is used to train a model from scratch, i.e., training the neural network using the recordings and their score annotations as supervised training data in an end-to-end audio-to-score transcription task."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2, 4.4",
          "reasoning": "The combined dataset (Tc) scenario includes a fine-tuning policy where the model is first trained on synthetic data and then fine-tuned on the real saxophone recordings dataset introduced, constituting supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of reinforcement learning based post-training techniques such as RLHF applied to this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The newly introduced real saxophone recordings dataset is also used as test data for evaluation of transcription models' performance, serving as a benchmark for evaluating end-to-end audio-to-score transcription systems."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4, 6",
          "reasoning": "The dataset is used to analyze patterns in transcription performance, the benefits of synthetic data, and to provide insights into challenges of transcribing real recordings, which is primarily analytical."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base to augment models via retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced is demonstrably used in multiple practical ways including training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10044,
      "completion_tokens": 622,
      "total_tokens": 10666
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced in the paper comprises recorded saxophone performances and their corresponding music scores, without any mention of linguistic content in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries in exactly two human languages; there is no indication of such linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset primarily consists of musical audio recordings and their corresponding symbolic music scores; no English textual content is described as part of the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any entries in a single non-English human language; the focus is on music recordings and symbolic music notation."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper references software tools and programming for analysis, the dataset itself does not contain programming code or structured programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 2 and 3.1",
          "reasoning": "The dataset includes digital music scores encoded in Kern format, a symbolic representation including musical notation, pitch, rhythm, and other annotations, which can be considered as a formal symbolic language. Additionally, the representation involves subword encodings of music tokens (duration, pitch, accidentals), which are formal symbolic notations. These symbolic music annotations are akin to a formal logical or mathematical notation system used to encode music."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human music recordings and symbolic music scores; there are no biological sequences or non-human communication systems included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any fictional or artificially created languages such as Klingon or Esperanto; it contains musical recordings and scores only."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages or symbolic systems used in the dataset are explicitly described (e.g., Kern format music notation); thus, the linguistic nature is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains symbolic representations of music scores (Kern format), which is a formal symbolic system, thus language or notation is present."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7262,
      "completion_tokens": 152,
      "total_tokens": 7414
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or explicit mention",
          "reasoning": "The paper does not mention or provide any link or reference to source code related to the dataset creation, collection, preprocessing, or generation. There is no indication that the code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Saxophone recordings for A2S",
          "reasoning": "The paper documents the dataset creation process in Section 2 by describing the collection of 1026 real saxophone recordings with digital scores in Kern format. It details the instrument types, recording conditions, music styles, annotation formats, transposition process, and metadata included. This provides transparency about the dataset creation and characteristics."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9240,
      "completion_tokens": 182,
      "total_tokens": 9422
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 and 4.3",
          "Reasoning": "The new 60-hour and extended up to 300-hour artificial training dataset was created from clean Czech TV/R broadcast recordings. The authors artificially created overlapped speech by summing two recordings with a random shift, adding noise via augmentation from CHiME-4 dataset, and appending non-speech frames. Thus, the base audio is human-generated broadcast recordings, while the overlapped speech training data is model generated through mixing and augmentation as described in Section 4.2. Additionally, the new evaluation dataset of about 10 hours of Czech and Slovak broadcast shows is human generated, recorded from multiple TV, radio, and internet sources, manually annotated by humans (Section 4.3)."
        }
      ]
    }
  },
  {
    "id": "mateju22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10092,
      "completion_tokens": 272,
      "total_tokens": 10364
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.3",
            "reasoning": "Section 4.3 states that the authors prepared a new dataset of Czech and Slovak broadcast shows with over 10 hours of data, which they annotated by hand, labeling acoustic events such as speech, non-speech, overlapped speech, and cross-noise. The phrase 'annotated the data by hand' implies that human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "There is no mention in Section 4.3 or elsewhere of detailed annotation instructions provided to annotators during dataset creation. The paper states only that the data were hand annotated, without describing any explicit guidelines or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "The paper does not mention any scoring rubrics or criteria used for annotation decisions in the creation of the new dataset. This absence of details suggests no rubrics were provided or documented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "No examples or sample annotations are described or referenced in the paper regarding the annotation of the new dataset. Hence, no annotation examples are explicitly provided."
          }
        }
      ]
    }
  },
  {
    "id": "mateju22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11222,
      "completion_tokens": 309,
      "total_tokens": 11531
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single human expert conducted quality assurance of the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts validated the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single non-expert annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The paper states that over 10 hours of Czech and Slovak broadcast recordings were manually annotated by hand for evaluation purposes. However, no explicit mention is made about the expertise of these annotators, implying they are likely non-experts. The term 'annotated by hand' suggests human annotators and the use of plural recordings (16 in dev and 13 in test) implies multiple annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic QA process for the dataset annotations is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states that the new dataset was manually annotated by hand, so quality assurance was performed and documented."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10840,
      "completion_tokens": 451,
      "total_tokens": 11291
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The authors created a new evaluation dataset comprised of over 10 hours of Czech and Slovak TV, radio, and internet shows which were manually annotated by hand with four acoustic event labels (speech, non-speech, overlapped speech, and cross-noise). This data was created entirely from original broadcast recordings and manually labeled by humans, indicating original content not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new evaluation dataset consists of original broadcast recordings manually annotated rather than being a simple collection or aggregation of existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The authors prepared an artificial training dataset for x-vector classifier training by mixing clean Czech recordings from existing TV/radio broadcasts to create overlapped speech examples. This dataset is derived from existing clean recordings with modifications such as time shifts, mixing of speakers, noise augmentation and addition of non-speech frames, indicating a derived data source."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and creation process of the new datasets."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11358,
      "completion_tokens": 343,
      "total_tokens": 11701
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 and Section 5",
          "reasoning": "The authors introduce a new Czech/Slovak broadcast dataset with hand-annotated overlapped speech segments which they use exclusively for evaluation and benchmarking of their proposed overlapped speech detection approach. This is evidenced in Section 4.3 where they describe the preparation and annotation of the dataset, and in Section 5 where the dataset is used to evaluate the method's performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The newly introduced Czech/Slovak broadcast dataset is clearly used for evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12081,
      "completion_tokens": 523,
      "total_tokens": 12604
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4.3 Data and metrics",
          "reasoning": "The new evaluation dataset introduced by the authors consists of Czech and Slovak TV, radio, and internet shows, which implies the use of two different, but closely related, human languages. Hence, the dataset is bilingual but also can be considered multilingual as it contains data in more than one language."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 4.3 Data and metrics",
          "reasoning": "The authors explicitly mention the evaluation dataset contains recordings in Czech and Slovak languages only, exactly two languages, making it bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset containing English-only material introduced by the authors."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset has entries from two languages (Czech and Slovak), so it is not monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described consist of audio recordings with speech, no programming or structured code-related content is involved."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of datasets containing mathematical or logical expressions is provided."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is introduced as a new dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or artificial languages in the new datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the new dataset are explicitly stated and clearly described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language speech, so it cannot be non-linguistic."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9299,
      "completion_tokens": 180,
      "total_tokens": 9479
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.3",
          "reasoning": "The paper states that the authors have prepared and manually annotated a new evaluation dataset of Czech and Slovak broadcast recordings and that they decided to release these annotated data to the speech research community, but there is no mention or link to source code or scripts related to the data collection, preprocessing, annotation, or generation being publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.3",
          "reasoning": "The paper documents the dataset creation process in Section 4.3, describing in detail how the dataset was gathered (from Czech and Slovak TV, radio, internet shows), its total duration, the hand-annotation process with defined labels, and the format of annotation files provided. This documentation provides transparent and complete information regarding dataset creation for downstream use."
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8856,
      "completion_tokens": 169,
      "total_tokens": 9025
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Automated data-harvesting process",
          "Reasoning": "The new dataset is automatically harvested by processing Swedish public sources with audio recordings (broadcasts, parliament sessions, YouTube, audiobooks) involving human-spoken speech audio, which is recorded and provided originally by humans."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Automated data-harvesting process",
          "Reasoning": "The harvesting uses associated textual data from subtitles, transcripts, summaries, and e-books, which are human-generated texts manually created for these media; thus, the text data are human created and not model generated or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "mateju23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9708,
      "completion_tokens": 235,
      "total_tokens": 9943
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 and Section 4.3",
            "reasoning": "The paper states that each test file was automatically transcribed and then edited by a native speaker, indicating expert human annotation for the new test sets collected and prepared for evaluation. This points to the involvement of single human experts (native speakers) performing or verifying the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section describing annotation instructions",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to the annotators or editors of the test sets, so we cannot confirm the presence of instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of rubrics in annotation process",
            "reasoning": "There is no indication in the paper of scoring rubrics or formal criteria used for annotation or transcription correction; hence, rubrics appear to be absent."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided",
            "reasoning": "The paper does not present examples of annotation or transcription corrections, nor refer to any examples given to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "mateju23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10838,
      "completion_tokens": 304,
      "total_tokens": 11142
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert for the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit information indicating that multiple human experts performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human annotator who is a non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert human annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was employed as a judge for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.3 Automated data-harvesting process",
          "reasoning": "The paper describes an automated iterative data harvesting scheme where ASR outputs are aligned with reference texts using Levenshtein distance, and inclusion criteria based on low character error rate (below 2%) are applied automatically using multiple models. This constitutes an algorithmic, rule-based verification method validating the data quality without explicit manual checks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented and performed via automated verification methods during data harvesting."
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10456,
      "completion_tokens": 456,
      "total_tokens": 10912
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that original content was created entirely from scratch by human contributors for the training datasets. Instead, publicly available datasets were used and additional data were harvested automatically using ASR systems."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the new datasets were generated entirely by AI or machine learning models without referencing or transforming existing data. The harvested data comes from publicly available recordings and associated text, not model-generated content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that any data was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The new training data was collected from existing public sources such as broadcast, parliament, YouTube, and audiobook archives. These sources provided audio and associated text, which were harvested by processing and alignment but the content was collected as-is from existing archives without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The data harvesting process involved aligning ASR output with reference texts, splitting into chunks, and only selecting segments with low character error rate. This processing and selection represents modifications and transformation of the original data sources to create a usable training corpus, thus the data can be considered derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and the method of generation are well documented in the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10974,
      "completion_tokens": 406,
      "total_tokens": 11380
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2, 4.4, 4.5",
          "reasoning": "The newly harvested Swedish dataset (over 1,000 hours collected via automatic harvesting combining Swedish and Norwegian data/models) is used to train E2E ASR models, including training from scratch or fine-tuning initialized from Norwegian models, demonstrating improved model performance with increasing amounts of this data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3, 4.4, 4.5",
          "reasoning": "The harvested Swedish datasets are used to fine-tune pre-trained models (initialization from Norwegian models) iteratively as part of the data harvesting and model improvement process to reduce word error rates in speech recognition."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning based post-training techniques such as RLHF applied to these datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.5",
          "reasoning": "The authors prepared and publicly released multiple Swedish test sets covering various applications and domains which are used exclusively for evaluation and benchmarking of the developed ASR models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The paper analyzes the impact of increasing amounts of harvested training data on ASR model performance, investigating trends in word error rate reduction relating to multilingual training and transfer learning to understand bootstrapping efficiency."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of the dataset serving as a knowledge base for augmenting models in retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11697,
      "completion_tokens": 630,
      "total_tokens": 12327
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets primarily focused on Swedish and Norwegian languages for training and evaluation purposes. Although there is mention of English models used for transfer learning, the proposed datasets introduced do not contain entries with more than two human languages simultaneously."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Sections 4.2, 4.3, and 4.4",
          "reasoning": "The new datasets introduced for training combine Swedish and Norwegian languages, as described extensively in Sections 4.2 and 4.3, and detailed in Table 4 (Section 4.4). The harvested datasets include Swedish audio-text pairs supported by Norwegian data/models to improve training and the final ASR model. The bilingual nature is explicitly leveraged for multilingual training and transfer learning, making these datasets bilingual (Swedish and Norwegian)."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets introduced in the paper contain only English-language content. The English model mentioned is pre-existing and used for transfer learning, not part of a new dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not solely monolingual in Swedish or any other single non-English language, as they incorporate bilingual (Swedish-Norwegian) data combinations for model training and harvesting."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are part of the introduced datasets."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets pertain solely to human speech and text alignments; there is no mention or inclusion of biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is clearly specified as Swedish and Norwegian; there is no ambiguity about the language(s) used."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain human language data (Swedish and Norwegian) and thus cannot be labeled as containing no language."
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8915,
      "completion_tokens": 170,
      "total_tokens": 9085
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.3 and 5",
          "reasoning": "The paper does not include any link or mention of publicly available code repositories for the data harvesting or dataset construction process. It only mentions that test sets or links to their sources and detailed logs are released on their cloud, but no code for dataset creation or data harvesting is provided or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 4.3 and 5",
          "reasoning": "The paper provides a detailed description of the dataset creation and automated data harvesting process, including splitting of audio files, aligning with text, criteria for including data chunks, and iterative retraining of models. This documentation is comprehensive and precise, enabling understanding of the dataset construction process, although no publicly shared code is mentioned."
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6487,
      "completion_tokens": 272,
      "total_tokens": 6759
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3: DocSpeech Dataset",
          "Reasoning": "DocSpeech dataset consists of text tokens extracted from semi-structured Word documents (from ReadingBank dataset) which are originally human-created documents. The text is used with their reading order and bounding boxes. The source documents are explicitly human-generated Word documents."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3: DocSpeech Dataset",
          "Reasoning": "DocSpeech contains 10K audio clips synthesized by concatenating mel-spectrograms of word-level audio mappings obtained from forced alignment on LJSpeech dataset. The audio clips are generated by a process involving forced alignment and concatenation with mel-spectrogram tokens to produce the speech audio clips, thus the audio modality is model generated synthetic audio, not originally human recorded speech."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 4.1: Problem Formulation",
          "Reasoning": "Document images used as input containing the layout and bounding boxes are mentioned but their origin is not specified or detailed in terms of being human-generated or model-generated in the paper; thus origin is unknown."
        }
      ]
    }
  },
  {
    "id": "mathur22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7339,
      "completion_tokens": 217,
      "total_tokens": 7556
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3, paragraph 1 and 3",
            "reasoning": "The DocSpeech dataset is a synthetic dataset created by re-purposing the open-source ReadingBank dataset and constructing audio clips via forced alignment and concatenation methods using tools like Gentle Forced Aligner on the LJSpeech dataset. The process uses existing automatic tools and algorithms rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No annotation instructions for human annotators are described; the dataset is synthesized automatically without mention of human instruction guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "There is no mention of scoring rubrics or annotation quality criteria as the data construction is automatic and synthetic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No annotation examples or annotated samples for human annotators are provided since the dataset is generated via automatic alignment and concatenation processes."
          }
        }
      ]
    }
  },
  {
    "id": "mathur22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8469,
      "completion_tokens": 437,
      "total_tokens": 8906
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was performed by a single human annotator who is an expert or a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of QA conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of an AI model to perform quality assurance on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3. DocSpeech Dataset",
          "reasoning": "The dataset is synthetic and created by re-purposing the open-source ReadingBank dataset. The authors use the Gentle Forced Aligner (Kaldi-based tool) to perform forced audio alignment with words on the LJSpeech dataset. This forced alignment and subsequent assembly of mel-spectrograms for word-level audio clips is an automatic verification process ensuring alignment between text and audio. The paper describes automated methods to construct the dataset from existing datasets and align audio, indicating an automated QA or verification process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated process used for forced alignment and dataset creation, indicating that some form of QA process is present."
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8087,
      "completion_tokens": 316,
      "total_tokens": 8403
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3, DocSpeech Dataset",
          "reasoning": "The DocSpeech dataset is created by re-purposing the existing open-source ReadingBank dataset which contains semi-structured Word documents with annotated reading sequences and bounding boxes. The authors then map audio clips from the LJSpeech dataset to each unique word using forced alignment. They combine mel-spectrograms of these word-level audio snippets in the correct reading order of the document to synthesize speech clips for documents. This process involves modifying, transforming, and adapting existing datasets (ReadingBank and LJSpeech) to construct the new DocSpeech dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8605,
      "completion_tokens": 506,
      "total_tokens": 9111
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.3 (Training Details), Section 4 (Our Approach), Section 6 (Results and Analysis)",
          "reasoning": "The DocSpeech dataset is used to train the proposed DocLayoutTTS model from scratch in a supervised manner, as described in Section 5.3 where the models are trained using the dataset. The dataset provides paired documents and speech, allowing the model to learn text reordering and speech synthesis end-to-end. This is evidenced by descriptions of curriculum learning with DocSpeech data and performance evaluation on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using DocSpeech for fine-tuning pre-trained models. Instead, DocSpeech is used primarily for training and evaluation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of reinforcement learning or RL-based post-training methods applied on the DocSpeech dataset is present in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 (Evaluation), Section 6 (Results and Analysis)",
          "reasoning": "DocSpeech is used for evaluation purposes where 100 samples are selected from DocSpeech test set to evaluate models and collect MOS scores comparing the proposed method with baselines. This shows the dataset serves as a benchmark for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or characteristics; rather, it is used for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not serve as a knowledge base or retrieval augmentation resource."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The DocSpeech dataset is actively used for both training and evaluation as documented in multiple sections."
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9328,
      "completion_tokens": 551,
      "total_tokens": 9879
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper only mentions English language data, with no indication of other human languages present in the DocSpeech dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only English text and audio; no mention is made of a second human language in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3, DocSpeech Dataset; Introduction, Abstract",
          "reasoning": "The DocSpeech dataset is created from Word documents with English text tokens and audio clips of a single English speaker reading the documents. The paper specifies the use of LJSpeech (an English speech dataset) and Gentle forced aligner for English word-level alignment, indicating that the content is in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention inclusion of any non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or structured code-related content is indicated within the dataset; the dataset consists of natural language text and speech."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is based on document text and audio recordings, with no indication of mathematical or logical symbolic content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only human speech data; no biological sequences or non-human communication data is involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper explicitly states the language content as English; language is well specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes natural language text and corresponding speech, hence language is present."
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6546,
      "completion_tokens": 191,
      "total_tokens": 6737
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper does not provide any section or link referring to the dataset construction code",
          "reasoning": "The paper mentions the creation of the DocSpeech dataset by re-purposing the open-source ReadingBank dataset and describes the data processing steps in Section 3, but it does not provide any link or mention making the code for dataset construction (such as audio concatenation, forced alignment, or preprocessing scripts) publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: DocSpeech Dataset",
          "reasoning": "The paper provides detailed documentation about the dataset creation process in Section 3, explaining how DocSpeech was created by combining ReadingBank documents with LJSpeech forced alignment, the method of concatenating mel-spectrograms with m-tokens to ensure natural pauses, and the dataset statistics such as number of files, average duration, and train/test splits."
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7354,
      "completion_tokens": 122,
      "total_tokens": 7476
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 2.3, Section 3",
          "Reasoning": "SiDi KWS is a dataset consisting of 24.3 million labeled audio recordings of single-spoken keywords. These audio samples were created by applying forced alignment to existing public transcribed speech datasets (LibriSpeech, Mozilla Common Voice, MLS) that contain human speech recordings. Therefore, the audio samples originate from human recordings captured in the original datasets, not generated by models or synthetic procedures."
        }
      ]
    }
  },
  {
    "id": "meneses22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8206,
      "completion_tokens": 208,
      "total_tokens": 8414
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2, Section 2.3",
            "reasoning": "The annotations, i.e., the labeled single-spoken keywords, were generated automatically by applying forced alignment using the Montreal Forced Aligner (MFA) tool within the KeywordMiner framework on existing transcribed speech datasets. There is no mention of human annotators performing manual labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no mention of annotation instructions for human annotators since the labeling was performed by an automatic forced alignment process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "Given the annotations were automatically generated with forced alignment, there is no indication of any scoring rubrics used during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No examples of manual annotation or annotation guidelines including examples were provided since the process was fully automated."
          }
        }
      ]
    }
  },
  {
    "id": "meneses22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9336,
      "completion_tokens": 335,
      "total_tokens": 9671
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are given about multiple human non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by an AI model as a judge."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1, 2.2, 2.3",
          "reasoning": "Quality assurance of the dataset labeling was achieved via automatic forced alignment using the Montreal Forced Aligner (MFA), an established algorithmic tool for aligning speech and text. The segmentation and labeling process was automated via the KeywordMiner framework, relying solely on this forced alignment output without human verification. Thus, the QA process is an automatic algorithmic verification of alignment and segmentation, as described in Sections 2.1 through 2.3."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes an automated alignment and segmentation process used to derive the dataset labels, so some form of quality assurance through algorithmic means is present."
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8954,
      "completion_tokens": 324,
      "total_tokens": 9278
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by human contributors; instead, it is derived from existing public transcribed speech datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated entirely by AI or machine learning models creating new content; it is created by segmenting existing speech recordings using forced alignment."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 2.3, Section 3",
          "reasoning": "While the dataset uses existing public transcribed speech datasets as inputs, the final single-word audio clips are not just aggregated unchanged; they are segmented based on forced alignment, which is a transformation process."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 2.2",
          "reasoning": "The SiDi KWS dataset is derived from existing public datasets (LibriSpeech, Common Voice, MLS) by applying forced alignment (using the Montreal Forced Aligner) to segment and label single spoken keywords. This process is a modification and adaptation of existing data to produce the new dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9472,
      "completion_tokens": 205,
      "total_tokens": 9677
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3, Results and Discussion",
          "reasoning": "The dataset is analyzed in Section 3 in terms of distribution of keywords, recording counts, length of keywords and audio, and language characteristics. These analyses demonstrate the use of the dataset primarily to understand characteristics and trends rather than specific training or evaluation tasks."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10195,
      "completion_tokens": 533,
      "total_tokens": 10728
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract and Section 3 Results and Discussion",
          "reasoning": "The paper explicitly states that the SiDi KWS dataset is a public large-scale multilingual dataset composed of audio recordings in four human languages: English, French, German, and Spanish. Section 3 details the dataset statistics per language and Figure 2 shows spectrograms of keywords in all four languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two languages (English, French, German, Spanish), thus it is not a bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages, not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than one language and includes English, so it is not monolingual in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of spoken keyword audio recordings in human languages; there is no mention of any programming code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains no mathematical or formal logical notations or symbolic representations; it is composed of speech audio samples."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech only, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fictional or artificially created languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are all explicitly identified and documented as English, French, German, and Spanish."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language speech data and thus does contain language content."
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7413,
      "completion_tokens": 170,
      "total_tokens": 7583
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.2 Keyword Miner and Section 4 Conclusion",
          "reasoning": "The paper explicitly states that KeywordMiner, the framework used to generate the SiDi KWS dataset, is open-source and publicly shared to benefit the research community (Section 2.2 and Conclusion). This indicates that the code for data preprocessing, forced alignment, segmentation, and export is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Sections 2 (Methodology) and 3 (Results and Discussion), explaining forced alignment, the KeywordMiner framework components, input datasets, lexicons, infrastructure used, and dataset statistics. This detailed documentation supports reproducibility."
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6394,
      "completion_tokens": 193,
      "total_tokens": 6587
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2, 2.3",
          "Reasoning": "The dataset includes microphone audio recordings of isolated phonemes uttered by two human participants during the voice-based cursor control task. These audio recordings were captured in real time using a human-operated microphone device as part of the experimental protocol designed by the authors."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, 2.3",
          "Reasoning": "The dataset contains intracranial neural recordings obtained via stereotactic electroencephalography (SEEG) depth electrode arrays implanted in two human patients. The neural data are human-generated in the sense that they are acquired from human brain activity using clinical recording equipment during the experimental task as introduced and collected by the authors."
        }
      ]
    }
  },
  {
    "id": "meng22d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7246,
      "completion_tokens": 186,
      "total_tokens": 7432
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.4, 2.5",
            "reasoning": "The annotation involved creating binary time series labeling phonemes and silence based on raw audio signals and extracting neural features automatically; no mention of human annotators performing labeling is present, indicating an automated annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not describe any detailed instructions provided to annotators, as annotation was automated from audio signals."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There are no scoring rubrics described related to annotation guidelines since the process was automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper provides no examples of annotation guidelines or examples due to the automated nature of annotation."
          }
        }
      ]
    }
  },
  {
    "id": "meng22d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8376,
      "completion_tokens": 270,
      "total_tokens": 8646
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single human non-expert conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of any AI model for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes automated processing of audio and neural signals and decoding models, it does not explicitly state that these processes serve as formal quality assurance or verification for the dataset annotations or labels."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The authors do not describe any explicit quality assurance process or validation for the dataset annotations or labels; no QA procedures are documented in the paper."
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7994,
      "completion_tokens": 383,
      "total_tokens": 8377
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.3 and 4 Discussion",
          "reasoning": "The dataset was collected from two patients who produced isolated phonemes in a voice-based cursor control task, generating original intracranial neural recordings and corresponding audio data. This data was created entirely from scratch by human participants during the described experiment and is not derived, translated, or adapted from any existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was generated purely by AI or machine learning models. The data collected represents human neural and audio signals."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of data being produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any machine translation was performed on the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from existing sources but generated newly in the experimental sessions."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper performs analyses derived from the raw data, the dataset itself is not described as derived or transformed from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of data generation are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8512,
      "completion_tokens": 260,
      "total_tokens": 8772
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 and Figure 1",
          "reasoning": "The dataset is used to train SVM decoders for voice activity tracking and phoneme classification, and the performance is measured with accuracy values compared to chance levels, indicating usage for evaluation of decoding accuracy."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, 4; Figures 2 and 3",
          "reasoning": "The dataset is primarily used to analyze correlations between intracranial neural signals and phoneme utterances to understand neural encoding of speech sounds, focusing on spatiotemporal and spectral characteristics for speech onset and sustained responses."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9235,
      "completion_tokens": 649,
      "total_tokens": 9884
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that the participants are native speakers of Australian English and produces isolated phonemes from English language sounds only. There is no mention of any other human language data present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains phoneme utterances exclusively in English, from native Australian English speakers only. No two languages are referenced or recorded."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (Study participants), Section 2.3 (Model calibration and data collection), Abstract",
          "reasoning": "The dataset consists of isolated phoneme utterances produced by native speakers of Australian English without any other language context. The phonemes used (/a/, /i/, /o/, /m/, /t\u0283/) correspond to English vowel and consonant sounds. The paper explicitly mentions participants as native Australian English speakers and analyzes English phonemes."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language utterances are recorded or presented in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes the implementation via Python code for data processing and modeling, the dataset itself consists of neural recordings and corresponding phoneme utterances, not entries of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data entries are neural signal time series and audio phoneme utterances. Mathematical descriptions are used in analysis, but the dataset itself does not contain mathematical or symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2.1 (Study participants), Section 3 (Results), Abstract",
          "reasoning": "The dataset includes intracranial neural recordings (SEEG) capturing brain activity related to speech production. These neural signals are biological data underlying human communication, thus qualifying the dataset as containing biological non-linguistic communication system data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are involved in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language context and identity are explicitly stated as English; thus the dataset language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset does contain linguistic content in the form of English phoneme utterances and corresponding neural recordings linked to speech."
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6453,
      "completion_tokens": 185,
      "total_tokens": 6638
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any link or reference to a public repository or website hosting the code related to data collection, preprocessing, or dataset generation. There is no mention of code availability anywhere in the text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Materials and Methods) specifically subsections 2.1 to 2.5",
          "reasoning": "The paper thoroughly documents the dataset creation process including participant recruitment (2.1), the voice-based cursor control task and audio processing (2.2), model calibration and data collection procedure (2.3), methods for neural feature identification (2.4), and classification approaches (2.5). The experimental setup, synchronization of data streams, tasks, and analyses are well described to enable reproducibility in principle."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11354,
      "completion_tokens": 80,
      "total_tokens": 11434
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Speech material",
          "Reasoning": "The dataset consists of speech recordings collected from 11 Polish preschool children via a picture naming test using an electret microphone. The recordings are of real children's speech, captured directly, thus human generated audio signals."
        }
      ]
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12206,
      "completion_tokens": 189,
      "total_tokens": 12395
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 (Speech material)",
            "reasoning": "The annotation was performed by an independent annotator who is described as a linguist and a speech-language pathologist (SLP), thus an expert in the subject matter."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "No explicit mention of detailed annotation instructions or guidelines provided for the annotator is stated in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not describe any formal scoring rubrics used during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Sections 2.1 and 2.2",
            "reasoning": "There is no indication or description of example annotations or annotated samples provided to annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13336,
      "completion_tokens": 181,
      "total_tokens": 13517
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 Speech material",
          "reasoning": "An independent annotator, described as a linguist and a speech-language pathologist (SLP), performed an additional auditory assessment to ensure that the speech diagnostic description matched the recorded signals. This indicates that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12954,
      "completion_tokens": 384,
      "total_tokens": 13338
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 (Speech material)",
          "reasoning": "The dataset consists of speech recordings collected from 79 Polish preschool children through a picture naming test conducted in kindergartens. The recordings were made specifically for this study as part of a larger project and involved human participants uttering words selected by the researchers. This data was not translated or adapted from existing datasets but created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated entirely by AI or machine learning models; all data are human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced through human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data were collected specifically for this study and not aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although acoustic features are extracted from the raw audio data, this is a standard signal processing step, not a transformation of pre-existing data sources. The underlying dataset itself is original."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection method are clearly stated in the paper."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13472,
      "completion_tokens": 355,
      "total_tokens": 13827
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 Materials and Method; Section 3 Results & Discussion",
          "reasoning": "The dataset consists of speech samples from 11 Polish children used to extract acoustic features and conduct statistical analysis to investigate covert contrasts in sibilant substitutions. The data is analyzed to detect acoustic differences and patterns rather than used to train, fine-tune, or evaluate machine learning models. The study explicitly describes usage of linear mixed-effects models for acoustic analysis, indicating the dataset serves primarily for linguistic and acoustic analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used for the analysis of speech patterns through acoustic feature extraction and statistical modeling, as detailed in the paper."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14195,
      "completion_tokens": 403,
      "total_tokens": 14598
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and Table 1",
          "reasoning": "The dataset consists of recordings of Polish children's speech and uses only Polish language stimuli. The words for the picture naming test are all Polish words including Polish phonemes /\u00f9/ and /s/. The paper explicitly describes the study as focused on Polish phonology and child speech acquisition in Polish. Therefore, the dataset contains entries with exactly one language which is non-English (Polish)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken language samples from Polish children and thus contains language data, so it is not N/A."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11413,
      "completion_tokens": 143,
      "total_tokens": 11556
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link or reference to code repositories or other sources where the data collection, preprocessing, or analysis code associated with the dataset can be accessed."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Materials and Method",
          "reasoning": "The paper provides detailed documentation on dataset creation, including participant selection criteria, recording equipment and settings, the picture naming test and word selection, annotation and preprocessing steps, and feature extraction methods within Section 2. This thorough description ensures a reasonable level of transparency and completeness about how the dataset was constructed."
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8577,
      "completion_tokens": 215,
      "total_tokens": 8792
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Materials",
          "Reasoning": "The paper describes a new speech corpus collected by the authors, recording audio data from 106 children pronouncing Polish dental voiceless sibilants. The recordings were conducted in kindergartens and a primary school, capturing natural speech audio. This is a newly introduced dataset, explicitly collected by the authors for the study, thus the modality is audio, and it is human generated (recorded from children)."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Materials",
          "Reasoning": "The paper mentions that the data were collected using a multimodal acquisition device that records multiple audio channels and video streams with a stereovision module. Although the current study uses mainly audio data, the newly collected dataset includes video modality. The video data were recorded from children during speech tasks, thus human generated and captured with human involvement."
        }
      ]
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9429,
      "completion_tokens": 210,
      "total_tokens": 9639
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 1.1, Section 2",
            "reasoning": "The speech corpus was collected and annotated by a team consisting of speech engineers and speech and language pathologists (SLP), indicating multiple human experts performed annotation based on clinical and engineering expertise."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 1.1, Section 2",
            "reasoning": "Speech diagnoses were prepared based on a dedicated questionnaire covering articulation features, implying the existence of instructions guiding annotation for place of articulation (dental/interdental) by experts."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not explicitly mention scoring rubrics or standardized scoring schemes used during annotation for the speech examinations or dataset annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No annotation examples or exemplar annotated data are described or presented in the paper to guide annotators."
          }
        }
      ]
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10559,
      "completion_tokens": 414,
      "total_tokens": 10973
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 1.1 and Section 2",
          "reasoning": "The paper states that the speech corpus was collected and annotated by a team consisting of speech engineers and speech and language pathologists (SLP), implying that the quality assurance involved at least one human expert annotator in the specific domain of speech and language pathology. This indicates quality assurance by a human expert. However, it does not specify multiple independent expert annotators or cross-validation among multiple experts."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the annotation team included speech and language pathologists and speech engineers, there is no explicit information indicating that multiple human experts independently performed quality assurance or annotation cross-checking."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any automated verification process for the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented involving experts (speech and language pathologists); thus, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10177,
      "completion_tokens": 417,
      "total_tokens": 10594
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Materials",
          "reasoning": "The paper clearly states that the speech data were collected from 106 children aged 4;11\u20138;0 in kindergartens and a primary school using audio recordings obtained via a multimodal device. The data collection involved human participants who produced original utterances in Polish, recorded specifically and solely for this study. The authors emphasize that the corpus includes active vocabulary words and was annotated by their team. There is no indication that the dataset was translated, adapted from existing corpora, or generated by models. Therefore, the dataset is original data created entirely by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by AI or machine learning models without human speech input."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was created by new audio recordings from children, not aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset was derived by modification or adaptation of existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the origin and collection method of the dataset, so origin is documented."
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10695,
      "completion_tokens": 221,
      "total_tokens": 10916
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results & Discussion)",
          "reasoning": "The data collected and analyzed from the speech corpus of 106 Polish children are used primarily to analyze acoustic differences in the frication noise of dental voiceless sibilants and the influence of interdental articulation. Statistical modeling and acoustic feature analysis were performed to investigate trends and characteristics rather than for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11418,
      "completion_tokens": 500,
      "total_tokens": 11918
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of Polish child speech data. There is no mention of more than two human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only Polish speech; there are no two human languages represented."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English content; it is based entirely on the Polish language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 2 Materials; Section 4 Results & Discussion",
          "reasoning": "The new dataset is composed of speech samples collected from Polish children speaking Polish, which is a non-English language. The paper explicitly states the speech materials and words used are in Polish."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-related content is included in the dataset; it is strictly speech recordings."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions appear only in the analysis section but do not form entries of the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech samples only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of inclusion of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as Polish."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset comprises spoken language data (Polish), so language is present."
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8636,
      "completion_tokens": 158,
      "total_tokens": 8794
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any links, references, or indications to publicly available code repositories for the dataset collection, preprocessing, annotation, or feature extraction. No code release or sharing information is provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 (Materials) and 3 (Methods)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including recruitment, recording setup (multimodal acquisition device), setting (kindergartens and school), ethical approval, speech examination and diagnosis by speech-language pathologists, language material details, the annotation procedure, and acoustic feature extraction methods. This documentation offers transparency and completeness about dataset construction supporting reproducibility."
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8317,
      "completion_tokens": 162,
      "total_tokens": 8479
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Spontaneous dialogue corpus",
          "Reasoning": "The authors recorded spontaneous audio dialogues between human speakers in controlled environments to create the dataset for training the dialogue TTS system. This data consists of recorded speech capturing natural conversations, indicating direct human involvement in data creation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Spontaneous dialogue corpus",
          "Reasoning": "The recorded speech was automatically transcribed using ASR, then manually corrected and annotated with time information by humans, producing text transcripts aligned with audio. This process confirms human-generated text data derived from human-produced speech recordings."
        }
      ]
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9169,
      "completion_tokens": 219,
      "total_tokens": 9388
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 (Spontaneous dialogue corpus)",
            "reasoning": "The speech recordings of spontaneous dialogues were automatically transcribed by ASR and then manually modified to correct transcription errors and provide time information for utterance-level segmentation. The manual transcription and time annotation imply involvement of multiple human annotators, likely non-experts employed for transcription tasks, since there is no indication they are subject-matter experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to the human annotators for transcription or time annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No scoring rubrics or formal guidelines for annotation quality or criteria are described for the transcription or segmentation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "There is no mention of annotation examples or sample annotations provided to annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10299,
      "completion_tokens": 429,
      "total_tokens": 10728
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance by a single human annotator who is a subject matter expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple expert annotators performing quality assurance of the extracted data or transcripts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2 (Spontaneous dialogue corpus)",
          "reasoning": "The paper states that after automatic speech recognition (ASR) transcription, the transcripts were manually modified and given time information by humans to produce the final transcripts with timing. No information about expertise of these annotators is provided, so they are presumed to be non-expert annotators performing manual correction and segmentation, indicating multiple human non-experts performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for speech recognition and style prediction, the paper does not describe quality assurance being performed by an AI model as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the use of ASR for transcription to reduce manual labor, but the quality assurance of final transcripts is done manually, not through fully automated or algorithmic verification procedures."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is described as involving manual correction of ASR-generated transcripts, so quality assurance processes were applied and documented."
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9917,
      "completion_tokens": 416,
      "total_tokens": 10333
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Spontaneous dialogue corpus",
          "reasoning": "The paper states that the authors recorded and transcribed actual spontaneous dialogues between human speakers in a controlled environment without predefined transcripts. The dialogues were freely conducted on a given topic, recorded on independent channels, and manually transcribed with time information added based on ASR outputs that were manually corrected. This indicates that the speech data is original content created entirely from scratch by human contributors and is not derived or translated from existing materials."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the generation of new datasets entirely by AI or machine learning models without references to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of data being produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation was made in the data preparation or dataset description."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from existing sources; it was explicitly recorded anew by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While certain derived data such as latent representations are produced by the model training, the original dataset (speech corpus) itself is not described as derived from existing sources via modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data creation process; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10435,
      "completion_tokens": 416,
      "total_tokens": 10851
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2",
          "reasoning": "The spontaneous dialogue corpus recorded by the authors is used directly to train the proposed VAE/GMV AE-VITS text-to-speech models from scratch, as described in Sections 4.1 and 4.2. The training uses 15,880 utterances from the recorded dataset to optimize the model parameters. Thus, the dataset serves as training data starting from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The spontaneous dialogue corpus is also used for evaluation purposes. Specifically, subsets of the recorded dialogues are utilized as evaluation sets to measure the quality of synthesized speech using objective metrics (e.g., MCD, MSD, DUR) in Section 4.2 and subjective mean opinion score (MOS) tests for utterance- and dialogue-level naturalness in Section 4.3. This shows the dataset\u2019s use for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2.4",
          "reasoning": "The paper uses the spontaneous dialogue dataset to analyze the learned latent space of the proposed GMVAE-VITS model. In Section 4.2.4, analyses such as latent distribution visualization and characterization of latent classes in terms of pitch, duration, and loudness are performed using the dataset. Hence, the dataset is used to analyze characteristics and trends relevant to speaking styles."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11158,
      "completion_tokens": 403,
      "total_tokens": 11561
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1.1",
          "reasoning": "The dataset described consists of Japanese dialogues only, as explicitly stated that the dialogues are in Japanese between two female speakers."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1.1",
          "reasoning": "There is no mention of any dataset containing exactly two human languages; the dialogues are in Japanese only."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 4.1.1",
          "reasoning": "The dataset is not in English but in Japanese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1.1",
          "reasoning": "The proposed spontaneous dialogue corpus consists of Japanese dialogues only, as indicated by the description of 'Japanese dialogues' between two female speakers in isolated soundproof chambers."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The dataset consists of speech recordings and transcripts; no programming or code segments are part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "While the paper contains mathematical expressions to explain the methods, the dataset itself does not contain mathematical or logical notation as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The dataset pertains to human spoken language dialogues; no biological or animal communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The dataset does not include any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The language used in the dataset is clearly specified as Japanese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language data (Japanese), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8376,
      "completion_tokens": 148,
      "total_tokens": 8524
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not mention or provide any link or reference to publicly available code related to data collection, preprocessing, or dataset generation. No repositories or URLs are cited for dataset construction code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "Section 2 'Spontaneous dialogue corpus' describes the dataset construction process, including recording method, transcription process, and characteristics of the data. It details how dialogues were recorded freely on given topics without predefined transcripts, ASR usage, manual transcription with time information, and processing to split audio into utterances. This provides substantial documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 12489,
      "completion_tokens": 167,
      "total_tokens": 12656
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Dataset Curation",
          "Reasoning": "The authors curated an audio-visual dataset by leveraging meta-data and open-source databases (FluencyBank) and conducted new annotations for audio segments labeling disfluencies. The audio data are sampled from human speech recordings, indicating human-generated origin."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Dataset Curation",
          "Reasoning": "The dataset includes video recordings of individuals speaking from FluencyBank, which contains video recordings captured from human subjects. The videos are segmented based on metadata aligned to audio annotations, indicating human-generated origin of the video modality."
        }
      ]
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 13341,
      "completion_tokens": 216,
      "total_tokens": 13557
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1 Dataset Curation",
            "reasoning": "The curated audio-visual dataset is based on annotations provided by 3 annotators who systematically labeled each audio segment for disfluency types, indicating multiple human experts were involved in the annotation process."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Dataset Curation",
            "reasoning": "The annotators labeled disfluencies according to a taxonomy with five classes and fluent speech, implying instructions and guidelines were used to standardize annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 Dataset Curation",
            "reasoning": "No explicit mention of scoring rubrics or detailed evaluation rubrics is provided in the paper's annotation description."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 Dataset Curation",
            "reasoning": "The paper does not mention providing annotation examples or sample annotations to annotators as part of the guidelines or dataset documentation."
          }
        }
      ]
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 14471,
      "completion_tokens": 233,
      "total_tokens": 14704
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The paper states that three annotators were recruited to systematically label each audio segment as one of the five disfluency types or fluent speech. The use of multiple annotators indicates that quality assurance was performed by multiple human experts, although the paper does not explicitly state their expert status but given the task complexity and systematic labeling, it is reasonable to classify them as experts involved in the QA process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of automated verification or rule-based techniques being used for quality assurance of the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 14089,
      "completion_tokens": 327,
      "total_tokens": 14416
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The authors curate their audio-visual disfluency dataset by leveraging meta-data and existing open-source databases from previous works, specifically from FluencyBank and SEP28k datasets. They construct a paired multimodal dataset by segmenting videos based on existing audio annotation meta-data without creating original content independently."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The authors apply modifications such as filtering samples via majority vote distillation for annotation consistency and segmenting video clips aligned with audio annotations to build a multimodal dataset. This involves transforming and adapting pre-existing datasets to suit their multimodal disfluency detection task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 14607,
      "completion_tokens": 288,
      "total_tokens": 14895
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation; Abstract; Section 2 Approach",
          "reasoning": "The curated audio-visual disfluency dataset is used to train and fine-tune a multimodal disfluency detection model. The paper describes using this paired and annotated dataset to supervise the learning of their proposed multimodal fusion model that improves detection performance compared to unimodal baselines, indicating supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Experiments; Table 2; Figure 2",
          "reasoning": "The curated audio-visual dataset is used for benchmarking the performance of their proposed multimodal disfluency detection approaches and comparison with baselines across multiple disfluency tasks. Results are reported quantitatively, demonstrating use for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 15330,
      "completion_tokens": 526,
      "total_tokens": 15856
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The curated dataset is based on FluencyBank which contains only English language speech recordings. No evidence of multiple human languages is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English speech data; no mention of exactly two human languages being included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The dataset is curated from the FluencyBank corpus, which contains video recordings from 32 individuals with disfluent speech in the English language. This is explicitly stated in Section 2.1."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any non-English language data; only English is indicated."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio and video data with annotations for disfluency. There is no indication of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to define loss functions and models, the dataset itself does not contain entries with mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech recordings only; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of constructed (fictional or artificial) languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language (English) and thus cannot be classified as not containing any language."
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 12548,
      "completion_tokens": 218,
      "total_tokens": 12766
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The paper mentions that they curate a custom audio-visual dataset by leveraging metadata and raw datasets from previous works, and that they release this curated dataset along with training and evaluation splits for reproducibility. However, there is no mention or provided link to the release of the code used for the dataset curation, including processing, annotation distillation, or segmentation. Thus, the paper does not provide the code for constructing the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The paper provides a detailed description of the dataset curation process, including leveraging meta-data and open-source databases from past works, adhering to taxonomic recommendations, recruiting annotators with majority vote distillation to address inter-annotator disagreement, segmenting videos based on meta-data to construct paired multimodal dataset, and sampling rates of audio and video. Therefore, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8449,
      "completion_tokens": 437,
      "total_tokens": 8886
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 FluencyBank",
          "Reasoning": "The FluencyBank dataset consists of video recordings (audio content) of people who stutter in reading and interview settings, recorded from real human subjects. The data was re-annotated by trained speech-language pathologists, indicating human involvement in both collection and annotation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 FluencyBank and 2.3 Data Augmentation for Disfluent Speech",
          "Reasoning": "Text transcripts in CHAT format accompany the audio data in FluencyBank, created by human annotators with expertise in stuttering. Additionally, ground truth transcripts are used as a base for synthetic disfluency insertion during data augmentation."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Data Augmentation for Disfluent Speech",
          "Reasoning": "Synthetic speech data is created using the OpenAI TTS API with inserted artificial disfluencies (word repetitions, phrase repetitions, interjections), generated during training to augment the small FluencyBank dataset, thus generated by a model."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Non-Disfluent Speech Datasets",
          "Reasoning": "FluencyBank-N is a synthetic dataset generated from FluencyBank transcripts using the SpeechT5 text-to-speech model combined with pre-trained speaker vectors, producing disfluency-free speech samples. This is model-generated audio data."
        }
      ]
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9301,
      "completion_tokens": 229,
      "total_tokens": 9530
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 FluencyBank",
            "reasoning": "The FluencyBank dataset was re-annotated in CHAT format by trained speech-language pathologists with expertise in stuttering, ensuring inter-annotator agreement, indicating multiple experts were involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 FluencyBank",
            "reasoning": "The dataset was re-annotated by experts likely following CHAT transcription conventions, implying guidelines and instructions were used to maintain consistency and ensure inter-annotator agreement."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 FluencyBank",
            "reasoning": "The paper does not mention explicit scoring rubrics for annotation, only that transcripts adhere to CHAT standard and inter-annotator agreement was ensured without specifying rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 FluencyBank",
            "reasoning": "The paper does not describe providing specific annotation examples or sample transcripts in guidelines for the re-annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10431,
      "completion_tokens": 426,
      "total_tokens": 10857
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.2 FluencyBank",
          "reasoning": "The paper states that the FluencyBank dataset was re-annotated in CHAT format by trained speech-language pathologists with expertise in stuttering, ensuring inter-annotator agreement. These pathologists are subject matter experts, indicating quality assurance was conducted by multiple human experts. However, since multiple experts were involved, this label is false. But to comply with rubric constraints and the provided text, the QA involves multiple experts."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 FluencyBank",
          "reasoning": "The re-annotation of FluencyBank transcripts was performed by trained speech-language pathologists with expertise in stuttering with inter-annotator agreement, indicating multiple human experts conducted the QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence that only a single non-expert conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that quality assurance was carried out by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an AI model being used as judge for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that algorithmic or rule-based automatic verification processes were used for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is explicitly documented and performed by multiple expert annotators."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10049,
      "completion_tokens": 492,
      "total_tokens": 10541
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 FluencyBank",
          "reasoning": "The FluencyBank dataset consists of video recordings of people who stutter and corresponding transcripts re-annotated in CHAT format by trained speech-language pathologists. This indicates original content created by human contributors specifically for research purposes, not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.3 Data Augmentation for Disfluent Speech",
          "reasoning": "The authors generated new speech data by applying a novel data augmentation method involving synthesized disfluencies through the OpenAI TTS API, creating synthetic speech samples with controlled disfluencies. This generation is model-based and creates original augmented data without direct derivation from original speech audio."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any machine translation used to produce the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 FluencyBank",
          "reasoning": "The FluencyBank dataset is a collection of video recordings and transcripts aggregated from existing sources of people who stutter, albeit with re-annotation. Thus, it represents collated data gathered for the study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Non-Disfluent Speech Datasets",
          "reasoning": "The FluencyBank-N dataset was derived by synthesizing a disfluency-free version of the FluencyBank dataset using SpeechT5 TTS model with pre-trained speaker vectors, indicating data transformation based on existing source transcripts."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10567,
      "completion_tokens": 531,
      "total_tokens": 11098
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper fine-tunes a pre-trained wav2vec 2.0 model but does not train any model from scratch using the new datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.5 Implementation, Section 3 Results & Discussion",
          "reasoning": "The FluencyBank dataset and the augmented disfluent speech datasets introduced by the authors are used to fine-tune a pre-trained wav2vec 2.0 ASR model using supervised learning, as explicitly outlined in the methodology and confirmed by experimental results showing performance improvements after fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the datasets are used in any reinforcement learning based post-training approaches such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2.2 FluencyBank, 3 Results & Discussion",
          "reasoning": "The FluencyBank dataset (re-annotated by the authors) is also used as an evaluation benchmark for assessing ASR performance on disfluent speech and its augmented version for non-disfluent speech, as detailed in their experiments and tables reporting word error rates and BERTScore."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Results & Discussion",
          "reasoning": "The authors analyze the impact of various fine-tuning and data augmentation strategies on ASR accuracy bias, speaker-level and disfluency-type specific results, providing a detailed characterization and understanding of ASR behavior on disfluent speech."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the datasets as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly demonstrates multiple practical usages including fine-tuning, evaluation, and analysis of the introduced datasets."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11290,
      "completion_tokens": 619,
      "total_tokens": 11909
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's introduced datasets, including FluencyBank and the augmented versions, contain only English content. There is no mention of data involving multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as containing English speech only, with no mention or evidence of exactly two languages being present in the data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (FluencyBank); Sections 2.3 and 2.5 (Data augmentation and dataset creation); throughout descriptions of the datasets.",
          "reasoning": "The FluencyBank dataset, which is the basis for the new data annotated and augmented by the authors, contains video and audio recordings of people who stutter speaking in English. The transcripts and audio samples are in English, and the synthetic speech generation (for FluencyBank-N) is also based on English transcripts. There is explicit reference to reading passages and interviews in English, and no mention of any other language. Hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or statement in the paper that the dataset contains any non-English language speech."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While mathematical notation is present in the paper for describing evaluation metrics and formulas, the dataset entries themselves are speech and transcripts, which do not contain mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech recordings and transcripts related to stuttering; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly stated and identified as English; not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language speech and transcripts, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8508,
      "completion_tokens": 168,
      "total_tokens": 8676
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any links or references to public code repositories for the data augmentation, dataset construction, or preprocessing code. No URLs or mentions of hosting platforms like GitHub were found for the dataset construction or augmentation code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.3 Data Augmentation for Disfluent Speech and Section 2.2 FluencyBank",
          "reasoning": "The paper documents the dataset creation process including the use and re-annotation of the FluencyBank dataset, details on data augmentation methodology including how disfluencies are introduced and the parameters used, and the overall dataset characteristics. This provides transparency on how the data was prepared and augmented for training."
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 12282,
      "completion_tokens": 252,
      "total_tokens": 12534
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Datasets, paragraph describing the new dataset starting with 'In order to evaluate our models on realistic unseen data in-the-wild...'",
          "Reasoning": "The new dataset consists of 37.9 hours of audio clips from English-speaking celebrities and politicians. The 'authentic' or 'real' data clips are manually collected from publicly available sources such as podcasts and speeches, which are human generated recordings. These are directly captured human speech audio files."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3 Datasets, paragraph describing the new dataset starting with '...The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes...'",
          "Reasoning": "The fake data clips in the new dataset are generated by audio deepfake algorithms as the segmentations come from publicly available video and audio files advertising audio deepfakes (e.g., synthesized clips of celebrities speaking absurd or out-of-character content). This implies they are model generated audio deepfake syntheses rather than authentic human recordings."
        }
      ]
    }
  },
  {
    "id": "muller22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 13134,
      "completion_tokens": 224,
      "total_tokens": 13358
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 Datasets",
            "reasoning": "The 'in-the-wild' dataset was created by the authors who manually collected audio deepfake and corresponding genuine material, verifying authenticity by content context. This process indicates annotation by expert humans involved in the study."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 Datasets",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for the 'in-the-wild' dataset; it describes manual collection and verification but not formal instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 Datasets",
            "reasoning": "No scoring rubrics or formal evaluation criteria for annotation are described in the paper regarding the 'in-the-wild' dataset annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 Datasets",
            "reasoning": "The paper does not provide or mention annotation examples in the guidelines for the 'in-the-wild' dataset; the description is limited to the methodology for dataset collection."
          }
        }
      ]
    }
  },
  {
    "id": "muller22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 14264,
      "completion_tokens": 351,
      "total_tokens": 14615
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance was conducted by a single human expert annotator for the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate involvement of multiple human expert annotators for quality assurance of the new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance being performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance carried out by multiple non-expert human annotators for the new dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models for quality assurance of the new dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided that quality assurance of the dataset annotations or labels involved an automated verification process such as code or formula verification or rule-based algorithmic checks."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes collection and manual selection of fake and real audio clips to form the new 'in-the-wild' dataset by segmenting publicly available videos explicitly advertising audio deepfakes and then manually collecting corresponding genuine instances. However, it does not document any formal quality assurance process, annotation protocol, multiple annotators, expert validation, or automated verification steps. The verification of fake clips is rather by contextual indicators (absurd or out-of-character speech) but this informal method is not presented as a systematic quality assurance process."
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 13882,
      "completion_tokens": 478,
      "total_tokens": 14360
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset consists of audio clips collected by segmenting publicly available video and audio files that explicitly advertise audio deepfakes, and manually collecting genuine instances from the same speakers using publicly available material. The data is not created from scratch by humans, but rather collected from existing sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset does not include data generated entirely by AI or machine learning models by the authors themselves. Although the fake clips are audio deepfakes created by TTS or voice synthesis models, these were sourced from publicly available files rather than generated by models the authors trained or created."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "There is no indication in the paper that data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "There is no indication in the paper that machine translation was used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The new dataset is composed by collecting and segmenting publicly available video and audio files advertising audio deepfakes for fake data, and collecting corresponding genuine audio from publicly available material such as podcasts and speeches. This constitutes data collation from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset is not described as a derived dataset with modifications or adaptations applied to existing sources. The authors segmented and selected clips but did not apply transformations to create new data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the origin and method of data collection for the new dataset; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 14400,
      "completion_tokens": 319,
      "total_tokens": 14719
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 'Datasets', Section 5.3 'Evaluation on in-the-wild data', and Table 1",
          "reasoning": "The new audio deepfake dataset is introduced explicitly to evaluate the generalization of existing models on real-world, in-the-wild deepfake audio data. The paper reports results of various models tested on this dataset to assess performance degradation compared to the ASVspoof benchmark. There is no indication that this dataset is used for model training or fine-tuning; it is solely used for benchmarking and performance measurement purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3 'Evaluation on in-the-wild data' and Conclusion",
          "reasoning": "The dataset is also used for analyzing model performance gaps and challenges in generalization from lab-based benchmarks to real-world scenarios. The authors analyze trends and discrepancies in model behavior on this dataset, highlighting its utility for understanding audio deepfake detection challenges."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 15123,
      "completion_tokens": 529,
      "total_tokens": 15652
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "The new dataset contains English-speaking celebrities and politicians only; there is no evidence of more than two human languages being present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "The dataset entries are explicitly described as English-speaking content only, with no mention of precisely two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 Datasets, first paragraph; Abstract",
          "reasoning": "The new dataset consists of English-speaking celebrities and politicians, both real and deepfake audio clips. The paper repeatedly states that the dataset features English-speaking content, e.g., clips of Barack Obama and Donald Trump, with no mention of other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "No indication that the dataset contains any non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "The dataset consists of audio files, no programming or code-related content is present."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "The dataset is audio data, no mathematical or logical symbolic expressions are included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "The dataset concerns human voice audio deepfakes, no biological sequences or animal communication involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "The dataset contains real and synthetic English speech, no constructed fictional languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "Language of dataset entries is clearly stated as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset entries contain spoken English language audio."
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 12341,
      "completion_tokens": 225,
      "total_tokens": 12566
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 Datasets",
          "reasoning": "The paper states that the new real-world audio deepfake dataset is collected and published, with a URL provided to access the data (deepfake-demo.aisec.fraunhofer.de/inthe-wild). However, the paper does not mention or provide any link to publicly available code related to the data collection, preprocessing, or construction of this dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Datasets",
          "reasoning": "The paper provides a detailed description in Section 3 about the creation of the new 'in-the-wild' audio deepfake dataset. It explains the process of collecting 17.2 hours of forged audio and 20.7 hours of authentic audio from 58 celebrities and politicians, including how fake clips are obtained from publicly available deepfake videos and how corresponding genuine instances are manually collected with matching style and conditions. The paper also mentions the average clip length and that the data is publicly accessible. This constitutes clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8355,
      "completion_tokens": 180,
      "total_tokens": 8535
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "KSC2 is an industrial-scale speech corpus comprising transcribed audio recordings collected from multiple sources including crowdsourced read speech, professional speakers, television news, televised talk shows, radio programs, parliament speeches, and podcasts. These are manually transcribed recordings of human speech, indicating human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The corpus includes manually created and verified transcriptions of the speech audio data, produced by native Kazakh and Russian speakers (human transcribers). These transcriptions are human-generated text data corresponding to the speech audio, confirming the existence of human-generated text modality."
        }
      ]
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9207,
      "completion_tokens": 180,
      "total_tokens": 9387
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that all recordings were manually transcribed by native Kazakh and Russian speakers, implying multiple human annotators with expertise in these languages performed the transcriptions."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper mentions manual transcription by native speakers but does not describe any detailed annotation instructions provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of annotation rubrics or scoring criteria in the annotation process described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide or mention any examples used during the annotation or transcription process."
          }
        }
      ]
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10337,
      "completion_tokens": 388,
      "total_tokens": 10725
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance was performed by a single expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Structure",
          "reasoning": "The paper states that all recordings were manually transcribed by native Kazakh and Russian speakers, who can be considered members of the target demographic and experts in the language, indicating quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance being performed by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that non-experts performed quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1.1 KSC",
          "reasoning": "For the KSC portions, the ASR system automatically accepted utterances recognized with 0% CER while human transcribers checked the remaining, indicating use of an AI model for partial quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that quality assurance involved algorithmic or rule-based verification beyond the AI model usage described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents quality assurance processes involving human annotators and partial AI model verification."
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9955,
      "completion_tokens": 480,
      "total_tokens": 10435
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.3 to 3.1.7",
          "reasoning": "The new datasets, including television news, television programs, radio programs, parliament speeches, and podcasts, were manually transcribed by native Kazakh and Russian speakers, created from scratch via original recordings of live or broadcasted speech, not derived from existing datasets or translations. The authors explicitly state manual transcription and new data collection (Sections 3.1.3-3.1.7)."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any of the new datasets were generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced via translation from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1.3 to 3.1.7",
          "reasoning": "The additional data consists of recordings extracted from existing broadcasting sources such as television news (from Khabar Agency), television and radio programs, parliament YouTube channels, and podcasts. This data was collected and aggregated from publicly available or permission-granted sources without significant modification, constituting collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The KSC portion includes crowdsourced data partially checked and verified by an ASR system (automatic acceptance of clear recognitions; others manually transcribed), representing derived data since it was modified and validated from initially raw volunteer submissions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides explicit information regarding the origin and generation of all dataset components."
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10473,
      "completion_tokens": 350,
      "total_tokens": 10823
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper describes training Transformer-based ASR models from scratch using the KSC2 corpus as the primary training data (Section 4.1). The experimental results in Section 4.2 demonstrate model training and evaluation on this newly introduced corpus, indicating use for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning of pre-trained models using this corpus. Instead, it discusses training models from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RLHF techniques applied to this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The paper performs speech recognition experiments where the KSC2 dataset is split into training, validation, and test sets; evaluation metrics such as WER and CER are reported on these splits (Section 4.2), demonstrating the dataset\u2019s use for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some data statistics and code-switching analysis are discussed, the primary purpose of the dataset described is not analysis but training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of the dataset as a knowledge base for retrieval or augmentation is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11196,
      "completion_tokens": 677,
      "total_tokens": 11873
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Sections 3.1.4-3.1.7, 3.2",
          "reasoning": "The KSC2 dataset contains utterances with Kazakh-Russian code-switching, a conversational practice common among Kazakh speakers. The sections describing television programs, radio programs, parliament speeches, and podcasts explicitly mention that the utterances include Kazakh and Russian languages, including intra-sentential and intra-word code-switching (Section 3.1.4 to 3.1.7). This means the dataset includes at least two human languages used mixed within utterances, fulfilling requirements for bilingual/multilingual content. Since entries contain both Kazakh and Russian languages, it is bilingual. There is no indication of additional human languages beyond Kazakh and Russian, so only these two are documented."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Abstract, Sections 3.1.4-3.1.7, 3.2",
          "reasoning": "KSC2 contains utterances from Kazakh and Russian languages, especially annotated code-switching between Kazakh and Russian. All descriptions and experimental setups mention these two languages; no other human language is referenced in the dataset. Therefore, the dataset is bilingual with Kazakh and Russian languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English-only entries; the paper focuses on Kazakh and Russian languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not monolingual but bilingual Kazakh-Russian; monolingual non-English only does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset contains programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No presence of mathematical or formal logical symbolic expressions is described in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data is included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain fictional or constructed languages; only natural Kazakh and Russian languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present are explicitly identified as Kazakh and Russian; there is no uncertainty about language annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken human languages (Kazakh and Russian), thus not applicable."
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8414,
      "completion_tokens": 214,
      "total_tokens": 8628
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 4.1; Conclusion",
          "reasoning": "The paper explicitly states that the KSC2 corpus, training recipes, and pretrained models have been made publicly available through their GitHub repository. While the exact URL is not given within the provided excerpt, the mention of shared code and resources indicates that the code related to dataset usage and model training is accessible to the public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (KSC2 Specifications); Section 4 (Speech Recognition Experiments)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the sources of data, data collection methods, manual transcription procedures, and dataset statistics in Section 3. Sections 3.1 and its subsections thoroughly describe the origin and nature of the dataset components. Furthermore, the paper details experimental setup and results in Section 4, facilitating understanding and use of the dataset. Ethical approval and transcription validation are also mentioned, contributing to transparency."
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7524,
      "completion_tokens": 82,
      "total_tokens": 7606
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Dataset), Section 3.2 (Data Collection)",
          "Reasoning": "The dataset RAOFFENSE consists of speech segments extracted from 28 movies and TV series in Arabic, which are scripted media recorded by human actors, thus human generated audio recordings."
        }
      ]
    }
  },
  {
    "id": "nafea24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8376,
      "completion_tokens": 221,
      "total_tokens": 8597
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Sections 3.3 and 3.4",
            "reasoning": "The paper states in Section 3.4 that three of the authors annotated the dataset independently and held review sessions to resolve disagreements, indicating multiple expert annotators involved in the labeling process."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 describes the annotation guidelines including the definition of offensiveness and instructions for dealing with ambiguous cases, showing that annotators were provided detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not mention any formal scoring rubrics or rating scales; it only mentions binary labels (offensive or not) and discusses guidelines for handling ambiguity and noise."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit section or appendix",
            "reasoning": "The paper does not present any annotation examples or samples from the dataset annotations in the paper or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "nafea24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9506,
      "completion_tokens": 289,
      "total_tokens": 9795
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human expert performed quality assurance; multiple annotators were involved."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.4 Annotation Procedure",
          "reasoning": "Quality assurance was conducted by three of the authors who served as annotators; they are fluent in Arabic and familiar with various dialects, implying subject matter expertise and membership in the target demographic. Annotations were done independently and disagreements were resolved via discussion and review sessions to ensure consistency."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert annotator performed or validated the annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators were authors fluent and familiar with the language and dialects, indicating they are not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model was used to perform quality assurance on the annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic verification of annotations or dataset quality was described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A described quality assurance process exists involving multiple expert annotators; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9124,
      "completion_tokens": 403,
      "total_tokens": 9527
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Data Collection and Annotation",
          "reasoning": "The dataset consists of more than 2000 spoken segments from 28 Arabic movies and TV series, manually selected, filtered, and annotated by the authors. These are original audio clips from scripted media, not translations or derivatives, annotated by the authors themselves with specific offensiveness labels."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models; it originates from actual media clips."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine translation being used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Collection",
          "reasoning": "The authors have collected publicly available audio clips from existing Arabic movies and TV series on YouTube. Thus, they aggregated data from existing media sources without modifying the original content."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of extracts from existing media, but it does not describe modifications or transformations to the source material beyond filtering and labeling. The data is not substantially transformed or adapted beyond annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and method of dataset creation."
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9642,
      "completion_tokens": 312,
      "total_tokens": 9954
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5, Experiments and Section 6, Results & Discussion",
          "reasoning": "The RAOFFENSE dataset is used to fine-tune pre-trained models such as Wav2vec-Arabic and AraBERTv2 for the binary classification task of offensive speech detection. The paper describes training and hyperparameter tuning of classifiers on this dataset for supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3 Evaluation Setup and Section 6 Results & Discussion",
          "reasoning": "The dataset is split into training, validation, and test sets, and model performance is evaluated and benchmarked using metrics like Matthews Correlation Coefficient (MCC). The dataset serves as a benchmark for offensive speech detection approaches."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.5 Dataset Analysis",
          "reasoning": "The dataset is analyzed in terms of offensive instance proportions across loudness levels, gender disparities, and length distributions, providing insights into data characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10365,
      "completion_tokens": 544,
      "total_tokens": 10909
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 Dataset",
          "reasoning": "The dataset contains entries from various Arabic dialects (Egyptian, Gulf, Levantine, Iraqi, Maghriby) but all are varieties of Arabic, a single language, so it is not considered multilingual with more than two distinct human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 Dataset",
          "reasoning": "The dataset does not contain exactly two human languages; it contains only Arabic dialects."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3 Dataset",
          "reasoning": "The dataset entries are not in English; the data is exclusively Arabic speech from media rated for adult audiences."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3 Dataset",
          "reasoning": "The dataset entries consist solely of Arabic speech across different dialects (Egyptian, Gulf, Levantine, Iraqi, Maghriby), which is a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no indication that the dataset contains programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset does not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset does not contain biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset only includes natural Arabic dialects, no constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3 Dataset",
          "reasoning": "The language of the dataset is clearly described as Arabic and its dialectal varieties."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language in the form of spoken Arabic; thus, it is not language-free."
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7583,
      "completion_tokens": 187,
      "total_tokens": 7770
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit reference to code availability or code repository for dataset construction in the paper.",
          "reasoning": "The paper does not mention any code release or provide a link to any repository containing code used for data collection, preprocessing, or dataset generation. There is a reference to model configurations hosted on Zenodo, but this pertains to model baselines and not to the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 3.1 to 3.5",
          "reasoning": "The paper includes detailed descriptions of the dataset creation process, including data collection (Section 3.2), defining offensiveness (Section 3.1), annotation guidelines (Section 3.3), annotation procedure (Section 3.4), and dataset analysis (Section 3.5). These sections provide transparency and completeness regarding the dataset development."
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10711,
      "completion_tokens": 163,
      "total_tokens": 10874
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Materials and Methods",
          "Reasoning": "The paper describes collecting acoustic data from five Japanese speakers reading sentences with English laterals. The audio was recorded using a microphone and processed for analysis, indicating human involvement in data generation and usage of real human speech recordings."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Materials and Methods",
          "Reasoning": "The articulatory data were obtained using high-speed ultrasound tongue imaging with the Telemed MicrUs system while participants spoke. The ultrasound probes are human-operated sensors capturing tongue movements, thus human generated data from sensor measurements."
        }
      ]
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11563,
      "completion_tokens": 247,
      "total_tokens": 11810
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1, Section 2.2",
            "reasoning": "The dataset consists of acoustic and articulatory data collected from five individual native Japanese speakers reading controlled stimulus sentences. The annotation and data collection involved specially trained researchers or experts performing acoustic and articulatory measurements, rather than crowd-sourced non-expert annotators or automated methods."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation guidelines or instructions described",
            "reasoning": "The paper does not describe any detailed instructions for annotators, as the data are primarily raw acoustic and ultrasound tongue imaging data collected under experimental protocols, not manual annotation tasks requiring separate instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric mention in the paper",
            "reasoning": "No scoring rubrics or criteria for annotating or rating data quality or categories are described, as the data involves direct acoustic measurements and articulatory imaging rather than subjective labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided",
            "reasoning": "The paper does not provide annotation examples or guidelines; the dataset consists of quantitative experimental recordings rather than annotated labels requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12693,
      "completion_tokens": 281,
      "total_tokens": 12974
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that multiple non-expert annotators were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using AI models for quality assurance of the dataset or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though some automated processing (e.g., linear mixed effects modelling, generalised additive mixed models) was used for data analysis, the paper does not describe an automated quality assurance process for verifying annotations or data correctness."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any specific quality assurance procedures or processes applied to the dataset annotations or data. There is no explicit mention of validation or QA of annotations by humans or automated methods."
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12311,
      "completion_tokens": 373,
      "total_tokens": 12684
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.",
          "reasoning": "The paper reports acoustic and articulatory data obtained directly from five Japanese speakers recorded under controlled experimental conditions (including simultaneous audio and high-speed ultrasound tongue imaging). This data was elicited using specifically designed stimuli and carrier phrases developed by the authors, indicating that the dataset was newly created from original human production, not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by AI or machine learning models without human production."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not based on machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from existing sources but was collected directly from human participants in a new study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data does not appear to be based on existing data with modifications but represents novel recordings collected specifically for this research."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection methods are described explicitly in the paper."
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12829,
      "completion_tokens": 248,
      "total_tokens": 13077
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2.2, 3.1, and 3.2",
          "reasoning": "The dataset constructed by the authors consists of acoustic and articulatory recordings from Japanese speakers producing English laterals in different syllabic positions and vowel contexts. This dataset is used primarily to analyze phonetic and articulatory patterns and variations in second language speech acquisition, not for training or evaluating machine learning models. The paper uses sophisticated statistical modeling (linear mixed effects and GAMMs) to analyze the data trends and articulatory characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13552,
      "completion_tokens": 547,
      "total_tokens": 14099
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study involves two human languages, Japanese and English, but does not have entries with more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Abstract; Sections 2.1 and 2.2",
          "reasoning": "The dataset includes speech productions by native speakers of Japanese (L1) producing English (L2) laterals. The dataset contains entries in both Japanese (participants' L1) and English (L2 speech productions being analyzed). Although the speech data for analysis focuses on English laterals, the participants' first language (Japanese) is also pertinent to the study, making the dataset bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While English laterals constitute the analyzed speech data, the participants are native Japanese speakers and the study discusses L1 influence, implying the dataset is not exclusively monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes English language material for analysis; thus, it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code content is involved in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though statistical methods (e.g., linear mixed-effects modeling) are used, the dataset entries themselves do not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are present."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are involved."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present (Japanese and English) are explicitly described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes linguistic data in human languages (Japanese and English)."
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10770,
      "completion_tokens": 129,
      "total_tokens": 10899
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention or provide a link to any code repository or scripts related to data collection, preprocessing, or analysis for the newly introduced dataset of articulatory and acoustic measurements from Japanese speakers."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Methods",
          "reasoning": "The paper provides detailed documentation of the data collection procedures including participant information, stimuli materials development, recording setup for simultaneous acoustic and ultrasound data, data preprocessing criteria, and analysis methods, as described primarily in the Methods section."
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5517,
      "completion_tokens": 146,
      "total_tokens": 5663
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, The WHiSER Corpus and its subsections",
          "Reasoning": "The WHiSER corpus is newly introduced by the authors and consists of speech files obtained from declassified Nixon's Oval Office recorded conversations (1971-1973). These are authentic, natural emotional speech recordings captured via distant microphones during real-life political meetings. The data modality is audio because the data consists of speech recordings. The origin is human generated since the recordings are directly captured from human conversations, not simulated or generated by models. The paper explicitly states the source as Nixon's tapes, confirming human origin."
        }
      ]
    }
  },
  {
    "id": "naini24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6369,
      "completion_tokens": 185,
      "total_tokens": 6554
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "The corpus was annotated by 33 student workers, which implies multiple non-expert human annotators performed the labeling."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Self-assessment manikins (SAMs) were provided as visual references to help annotators assess emotional attributes, indicating that detailed instructions were given to guide annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Annotations used a seven-point Likert scale for emotional dimensions (arousal, valence, dominance), which constitutes a scoring rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention providing annotation examples or example annotations to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "naini24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7499,
      "completion_tokens": 379,
      "total_tokens": 7878
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states the annotators were 33 student workers but does not indicate that they were subject matter experts or members of the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotations involved multiple annotators per utterance, not a single annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Pipeline and Data Annotation",
          "reasoning": "The WHiSER corpus annotations were done by 33 student workers, who are likely non-experts. Each utterance was annotated by at least five annotators, indicating multiple human non-experts conducted the quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models were used to select emotionally rich samples, they did not perform the final quality assurance via annotation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Pipeline and Data Annotation",
          "reasoning": "Automatic filtering was applied to remove samples with low speech quality using voice activity detection, music detection, and noise estimation algorithms before annotation, constituting an automated QA step."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes detailed annotation and filtering processes indicating quality assurance procedures."
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7117,
      "completion_tokens": 440,
      "total_tokens": 7557
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not original content created entirely from scratch by human contributors. Instead, the data consists of pre-existing recorded conversations from President Nixon's Oval Office recordings."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated by AI or machine learning models; it originates from real-world audio recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was produced by translation from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 Introduction and 3 The WHiSER Corpus",
          "reasoning": "The WHiSER corpus is collected from existing sources, specifically the declassified President Nixon\u2019s Oval Office recorded conversations from 1971 to 1973. The recordings were archived and then selected and annotated for emotional content by the authors, but the base audio data itself is aggregated from these existing archival recordings without creation from scratch."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Pipeline and Data Annotation",
          "reasoning": "The dataset is derived in that the authors applied extensive processing steps including filtering for quality, segmenting, and emotional content selection using machine learning models, followed by manual annotation for emotional labels. These represent modifications and transformations applied to the original recordings, producing a dataset derived from the raw data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of generation of the dataset is clearly documented and specified."
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7635,
      "completion_tokens": 293,
      "total_tokens": 7928
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 (The WHiSER Corpus), Section 4 (Experimental Results), and Section 5 (Conclusions)",
          "reasoning": "The WHiSER corpus is explicitly described as intended to be used as an independent test set for evaluating speech emotion recognition (SER) systems in complex and realistic acoustic environments. The paper reports experimental results evaluating SER models trained on other datasets and tested on WHiSER, illustrating its role for benchmarking and performance measurement rather than training or fine-tuning."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 (Emotional Content Analysis)",
          "reasoning": "The paper performs detailed analyses of the emotional content and statistical properties of the WHiSER corpus, including distributions of emotional attributes and categorical emotions, signifying its use primarily for analyzing emotion trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8358,
      "completion_tokens": 512,
      "total_tokens": 8870
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The WHiSER corpus is based on President Nixon's Oval Office recordings and no mention is made of multiple languages; only English is referenced."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The WHiSER corpus entries are not described as containing exactly two languages; there is no indication of bilingual content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (The WHiSER Corpus) and Abstract",
          "reasoning": "The paper describes the dataset as consisting of speech recordings from President Nixon's Oval Office conversations, which are in English. There is no mention of any other language in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains English speech only; there is no indication that the entries are in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio files and annotated emotional labels; there is no content related to programming or code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset contains mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech only; it does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken English language content, so it is not applicable to mark as having no language."
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5576,
      "completion_tokens": 172,
      "total_tokens": 5748
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 3",
          "reasoning": "The abstract states that all data and emotional labels are released on their GitHub repository (https://github.com/msplabresearch/WHiSER). This indicates that the associated code and data are publicly available. While the paper does not explicitly state the availability of all code for data collection and preprocessing, the link to the repository and mention of data and labels release strongly suggests that relevant code for dataset construction is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3",
          "reasoning": "Section 3 thoroughly documents the dataset creation process, including pipeline stages such as audio preparation, filtering, selection criteria, annotation methodology, and statistics on dataset size and emotional content. This detailed description constitutes comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11382,
      "completion_tokens": 123,
      "total_tokens": 11505
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1: Collecting Recognition Errors for a Keyword List with Synthetic Audio",
          "Reasoning": "The authors generate speech audio for a list of keywords using a Text-to-Speech (TTS) system, which is an AI model, to create synthetic audio data. This audio is then used with a recognition model to obtain pairs of correct labels and recognition errors. Since the audio data is generated via TTS without direct human recording, it is model generated synthetic audio."
        }
      ]
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12234,
      "completion_tokens": 207,
      "total_tokens": 12441
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1",
            "reasoning": "Section 4.1 describes manual removal of clear morphological analysis errors and selection of keywords based on morphological labels by human experts, indicating that a single group of experts performed annotation for keyword selection in the new dataset for evaluation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1",
            "reasoning": "Section 4.1 details the process of selecting misrecognized words including use of morphological analysis and manual filtering, implying instructions were given to annotators to identify proper nouns and remove errors."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No explicit mention of scoring rubrics or criteria beyond instructions for filtering words in the annotation process was found."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not provide explicit annotation examples or exemplars used in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13364,
      "completion_tokens": 462,
      "total_tokens": 13826
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about quality assurance performed by multiple human experts for the new datasets or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or description of quality assurance by a single human non-expert annotator is provided in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The paper states that after extracting misrecognized words using morphological analysis, proper nouns and personal names were retained, and then 'In the final stage, we manually removed clear morphological analysis errors from the extracted set of bias keywords.' This manual removal implies at least one person performed a manual review to clean the keyword list. While the paper does not explicitly describe the number or expertise of annotators, this indicates at least some manual QA by human annotators who were likely non-experts, as there is no specification of subject matter expertise or alignment to the target demographic."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any AI model was used as a judge for quality assurance of the dataset or annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "Automated verification is part of the dataset preparation process: the pairs of correct labels and recognition errors are collected by generating synthetic audio from keywords using TTS (Section 3.1), then feeding this audio through a recognition model to obtain intermediate predictions and error instances automatically. Moreover, morphological analysis via MeCab is used to extract misrecognized words. These automated, algorithmic processes contribute to quality assurance in dataset creation by automatically pairing correct labels with recognition errors without requiring manual intervention for these steps."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents several quality assurance steps involving manual filtering and automated processing for the keyword lists and synthetic data, so QA is present and described."
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12982,
      "completion_tokens": 471,
      "total_tokens": 13453
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper explicitly states that synthetic speech audio was generated for the keyword list using Text-to-Speech (TTS) synthesis, which is a model-based generation method. This synthetic speech data was then used to create pairs of correct labels and recognition errors by feeding it into the recognition model. Since this synthetic audio is generated entirely by a machine learning model and not derived from existing natural recordings, this content qualifies as new data from a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper describes using existing test sets such as CSJ eval1, eval2, eval3, JSUT, Common Voice, and TEDxJP-10K for evaluation purposes, where keywords were extracted by analyzing recognition errors from prior recognition runs. Specifically, keywords were extracted from evaluation sets through morphological analysis and manual filtering, thus these keyword lists are collated from existing sources without significant content creation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "The keyword lists themselves are extracted from recognition errors on existing evaluation sets, meaning they are derived from existing data with some filtering, morphological analysis, and manual corrections applied. Additionally, the pairs of correct labels and recognition error instances were created by processing synthesized speech through the recognition model, representing a transformation of original data for adaptation to their InterBiasing method."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Data sources and generation methods are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13500,
      "completion_tokens": 240,
      "total_tokens": 13740
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.3",
          "reasoning": "The datasets created by the authors consist of pairs of correct labels and recognition error instances generated using Text-to-Speech (TTS) and the recognition model, which are used to bias intermediate predictions in the acoustic encoder. These datasets are then utilized to evaluate and measure improvements in recognition accuracy of unknown words through experiments reported in Section 4.3. Thus, the dataset is used exclusively for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14223,
      "completion_tokens": 441,
      "total_tokens": 14664
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The dataset introduced is based on the Corpus of Spontaneous Japanese (CSJ) and other Japanese speech-related test sets such as JSUT-basic 5000 and TEDxJP-10K. The paper's experiments and evaluations are conducted strictly in Japanese, indicating the dataset entries are in one non-English language, Japanese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Equations and formulas in Sections 2.1, 3.1, and 3.2",
          "reasoning": "The paper includes multiple mathematical expressions and formulas used to describe model computations and algorithmic operations that pertain to the dataset construction and usage descriptions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data, specifically Japanese speech and text labels and associated synthetic speech, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11441,
      "completion_tokens": 181,
      "total_tokens": 11622
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No link or mention of code availability related to dataset construction.",
          "reasoning": "The paper does not provide any URLs, footnotes, or text indicating that code for constructing the dataset (such as generation of misrecognized keyword pairs or intermediate prediction correction) is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and Section 4.1",
          "reasoning": "The paper documents the dataset creation process by describing the generation of synthetic speech from keywords using Text-to-Speech (TTS), the collection of recognition error pairs through model predictions on TTS audio (Section 3.1), and details the identification and manual cleaning of misrecognized words from evaluation sets to generate bias keyword lists (Section 4.1). These descriptions provide transparency about dataset construction even though the code is not provided."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7443,
      "completion_tokens": 146,
      "total_tokens": 7589
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, 'Bilingual speaker recognition test set'; Section 2.2, 'VoxCeleb1-B Evaluation list'",
          "Reasoning": "The VoxCeleb1-B evaluation set is a newly introduced large-scale bilingual speaker recognition test protocol derived from the VoxCeleb1 dataset. It consists of audio data (utterances) annotated with speaker and language labels, manually checked by human annotators. The data originates from human speech recordings as part of the original VoxCeleb1 dataset and further manual annotation, as described in the paper, thus is human generated audio data."
        }
      ]
    }
  },
  {
    "id": "nam23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8295,
      "completion_tokens": 209,
      "total_tokens": 8504
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1",
            "reasoning": "Section 2.1 states that language labels for VoxCeleb1 utterances were manually checked by annotators of various nationalities based on pseudo-labels from a pre-trained spoken language recognition model, indicating multiple human non-expert annotators performed the verification."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not describe detailed annotation instructions provided to the annotators for language labeling, only stating manual checking was performed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no mention of scoring rubrics or specific evaluation criteria provided to annotators for consistency in language annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "No examples or sample annotations are discussed or included in the paper to guide annotators."
          }
        }
      ]
    }
  },
  {
    "id": "nam23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9425,
      "completion_tokens": 375,
      "total_tokens": 9800
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The language labels for the VoxCeleb1 dataset were manually checked by annotators of various nationalities, indicating multiple human annotators were involved in the quality assurance process for the language annotations. There is no explicit information regarding their subject matter expertise or domain knowledge, so they are considered non-experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "Initial language pseudo-labels were generated using a Spoken Language Recognition (SLR) model pretrained on the VoxLingua107 dataset. This automatic labeling by an AI model was part of the QA/preparation process for the language annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The process described involves AI-generated pseudo-labels and manual annotation validation rather than purely algorithmic or rule-based automated verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a quality assurance procedure involving both AI-generated pseudo-labels and manual validation by multiple human annotators, so QA is present and documented."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9043,
      "completion_tokens": 612,
      "total_tokens": 9655
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Bilingual speaker recognition test set), especially 2.2 VoxCeleb1-B Evaluation list",
          "reasoning": "The paper introduces VoxCeleb1-B, a large-scale bilingual evaluation set derived from the existing VoxCeleb1 dataset. The authors manually annotated language labels of VoxCeleb1 utterances (Section 2.1), involving human annotators from various nationalities who manually checked and excluded some utterances with unrecognizable language. The evaluation protocol VoxCeleb1-B was constructed based on these manual annotations and careful trial design to simulate bilingual scenarios. Thus, the language labels and evaluation protocol are original content created by human contributors for the purposes of this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was generated purely by AI or machine learning models. While pseudo-labels from a spoken language recognition model were used to assist annotations, these are not presented as new datasets generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was produced by translating content from one language to another via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generation by machine translation is mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Bilingual speaker recognition test set), especially 2.2 VoxCeleb1-B Evaluation list",
          "reasoning": "VoxCeleb1-B evaluation set is derived from VoxCeleb1 dataset, which is an existing dataset. The authors aggregated and filtered the existing speech samples, added newly annotated language labels, and constructed a new evaluation trial list to simulate bilingual scenarios. This involves data collection and aggregation from an existing source with limited modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 Obtaining language labels and Section 2.2 VoxCeleb1-B Evaluation list",
          "reasoning": "The VoxCeleb1-B test protocol is based on existing VoxCeleb1 data but with significant adaptations such as manual language annotation, exclusion of ambiguous utterances, and the creation of a large number of cross-lingual trial pairs. These constitute transformations and adaptations of existing data, making it a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are documented in the paper, specifically in Section 2 and subsections."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9561,
      "completion_tokens": 444,
      "total_tokens": 10005
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly introduced dataset for pre-training large models on general patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset VoxCeleb1-B is not used for training models from scratch; it is described as an evaluation benchmark rather than a training resource."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using VoxCeleb1-B to fine-tune pre-trained models with supervised methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using the dataset in reinforcement learning based post-training such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "VoxCeleb1-B is introduced explicitly as a large-scale bilingual speaker recognition evaluation set to benchmark speaker recognition models under bilingual scenarios. It is used exclusively to evaluate and analyze model performance, not for training."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses the dataset for analyzing bilingual speaker recognition performance, this is a use of the benchmark for evaluation rather than primary data analysis in isolation. The dataset's main purpose is evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base for augmentation or retrieval-augmented generation in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used in evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10284,
      "completion_tokens": 564,
      "total_tokens": 10848
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The newly proposed test protocol VoxCeleb1-B is constructed from the VoxCeleb1 dataset and explicitly includes utterances labeled with 15 different human languages including English, French, Hindi, German, Spanish, Italian, Afrikaans, Portuguese, Dutch, Korean, Urdu, Swedish, Russian, Chinese, and Arabic. This dataset contains cross-lingual trials involving multiple languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains entries from more than two languages (15 languages are used), so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to only English; multiple other languages are included."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not restricted to a single non-English language; it contains multiple languages including English and others."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are audio utterances labeled with language identities; there is no mention of mathematical or logical notations in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech recordings without biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language labels have been manually annotated for the dataset, and 883 utterances without recognized language were excluded, so the dataset languages are specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of speech utterances in multiple human languages, so it contains language entries."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7502,
      "completion_tokens": 145,
      "total_tokens": 7647
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2",
          "reasoning": "The paper does not provide any link or mention of publicly available code or scripts for constructing or generating the VoxCeleb1-B evaluation dataset or the language label annotations."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (2.1 Obtaining language labels and 2.2 VoxCeleb1-B Evaluation list)",
          "reasoning": "The paper documents the dataset creation process including how language labels were obtained using a Spoken Language Recognition model and manual checking, the selection criteria for languages and samples, and how evaluation trials were constructed, detailing the scale and composition of VoxCeleb1-B."
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10755,
      "completion_tokens": 123,
      "total_tokens": 10878
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. Versatile Voice Dataset",
          "Reasoning": "The Versatile Voice Dataset (VVD) is introduced as a new dataset collected by the authors comprising audio recordings from 3 trans-feminine voice teachers. The dataset consists of these speakers reading the 6 CAPE-V sentences in various pitch, resonance, and weight configurations. Audio clips were recorded by humans (voice teachers) and provided directly by the authors, with no other preprocessing besides sampling rate standardization."
        }
      ]
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11607,
      "completion_tokens": 255,
      "total_tokens": 11862
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "The Versatile Voice Dataset (VVD) was created by three trans-feminine gender-affirming voice teachers who are experts within the Informal Trans Voice Training Community, producing voices in various configurations. The annotation involves their interpretation and production of voices, reflecting expert knowledge in this domain."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "Speakers produced voices along self-interpreted configurations of low, medium, and high for pitch, resonance, and weight. Although instructions were ambiguous and allowed for personal interpretation, there was a guiding framework instructing the speakers on how to produce these variations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no mention of specific scoring rubrics or formal scoring guidelines used in the annotation or data production process. The instructions were qualitative and interpretive without rubric-based evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No annotation examples or exemplar voices are described as provided to speakers or annotators for guidance in producing or evaluating voice configurations."
          }
        }
      ]
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12737,
      "completion_tokens": 318,
      "total_tokens": 13055
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Collection and Section 4.2.2 Error Analysis and Difficulty",
          "reasoning": "The dataset was collected from three trans-feminine gender-affirming voice teachers (experts in the target demographic) who produced various voice configurations. Expert listeners, who are these voice teachers themselves, participated in perceptual tasks verifying speaker identity, indicating their expert judgment was used to evaluate the data."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.2.2 Error Analysis and Difficulty",
          "reasoning": "Six non-expert listeners with a Master's Qualification on Amazon Mechanical Turk performed speaker verification tasks on the dataset. Their involvement constitutes quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2 Speaker Verification and Section 4.2.2 Error Analysis and Difficulty",
          "reasoning": "State-of-the-art ECAPA-TDNN speaker verification models from SpeechBrain and NeMo were used to perform speaker verification on the dataset, effectively serving as AI models performing quality assurance by evaluating how well the dataset's voice variations could be correctly identified."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12355,
      "completion_tokens": 403,
      "total_tokens": 12758
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Versatile Voice Dataset), especially Section 3.2 (Data Collection)",
          "reasoning": "The Versatile Voice Dataset (VVD) was created by three trans-feminine gender-affirming voice teachers who produced recordings of themselves reading six CAPE-V sentences in multiple voice configurations along pitch, resonance, and weight. The speakers deliberately modified their voices following instructions and their own interpretations, producing original audio content. The dataset was collected anew and is not derived, translated, or aggregated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any part of the dataset was generated solely by AI or machine learning models. All audio data originated from human speakers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation was used as the data consists of original English speech recordings."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The VVD was not collated or aggregated from existing datasets; it is newly recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not derived or transformed from existing recorded materials; it is newly produced."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are specified clearly."
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12873,
      "completion_tokens": 450,
      "total_tokens": 13323
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The Versatile Voice Dataset (VVD) is explicitly used to evaluate current speaker embeddings on gender classification and speaker verification tasks. The paper presents experiments measuring equal error rate (EER) and classification accuracy of state-of-the-art models on the VVD, demonstrating limitations due to vocal variability. These evaluations illustrate how existing models fail to reliably identify speaker identity or gender across diverse voice modifications, hence the VVD serves a benchmarking and performance measurement role."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The dataset is used primarily for analyzing trends and characteristics related to vocal modifications, gender perception, and speaker identity modeling. The paper uses the VVD to analyze sensitivity of gender classifiers to intra-speaker vocal changes, speaker verification errors, human vs model performance comparisons, and the relationship between vocal texture and categorical gender labels. Additionally, Section 5 discusses using the dataset for the future modeling of vocal texture, emphasizing the analysis of perceptual voice qualities."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the VVD as a knowledge base for retrieval-augmented generation or similar augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13596,
      "completion_tokens": 551,
      "total_tokens": 14147
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The dataset consists of three trans-feminine voice teachers each reading sentences in English only; no mention is made of multiple languages being present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The dataset entries are in English only; there is no indication of a second language being used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The Versatile Voice Dataset (VVD) contains recordings of three speakers reading the CAPE-V sentences, which are English sentences. The paper consistently refers to the data as English speech without mention of other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The dataset consists only of English speech samples; no non-English language is described or included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset consists of spoken audio recordings, not code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset entries are speech audio samples; mathematical or logical notation is not part of the dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset consists of human speech; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no indication that constructed or fictional languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The language of the dataset entries is clearly specified as English and detailed; thus, the language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains spoken language (English), so the N/A category does not apply."
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10814,
      "completion_tokens": 185,
      "total_tokens": 10999
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention found",
          "reasoning": "The paper does not include any links or mentions of publicly available code or repositories related to the Versatile Voice Dataset or its construction. There is no indication that code used for data collection, preprocessing, or generation has been made publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Versatile Voice Dataset), specifically Sections 3.1 and 3.2",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including descriptions of pitch, resonance, and weight as defined by the voice teachers, the data collection procedure with the three trans-feminine voice teachers producing voices in different configurations, and information on preprocessing (only sample rate standardization). The dataset characteristics, speaker information, and recording conditions are clearly described, making the dataset creation transparent."
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6945,
      "completion_tokens": 279,
      "total_tokens": 7224
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data collection; Section 3 Feature Extraction and Data Preprocessing",
          "Reasoning": "The audio data was recorded from human participants during a structured conversation with a virtual agent, using their end devices, captured at 44.1 kHz sampling rate as stated in Section 2. This constitutes human-generated audio data."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data collection; Section 3 Feature Extraction and Data Preprocessing",
          "Reasoning": "Video data consists of facial video recorded from participants during the conversation with the virtual agent, as facial landmarks and orofacial features were extracted. The video was captured from human participants using their devices, making it human-generated video data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data collection; Section 3 Feature Extraction and Data Preprocessing",
          "Reasoning": "Text data arises from automatic speech recognition transcriptions of human speech during the conversational protocol. Though transcription is machine-produced, the speech content is created by human participants. The linguistic and cognitive features, such as word recall and digit span, are based on these human-generated spoken utterances."
        }
      ]
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7797,
      "completion_tokens": 184,
      "total_tokens": 7981
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3",
            "reasoning": "The paper describes that cognitive task scores (e.g., word recall and digit spans) were scored automatically based on automatic speech recognition output, indicating an automated annotation process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no mention of detailed annotation instructions provided for this automatic scoring process in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The annotation process is automated scoring based on ASR output with a defined scoring scheme, but explicit annotation rubrics are not described as provided to annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No examples of annotation or scoring guidelines are provided in the paper for this automatic annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8927,
      "completion_tokens": 331,
      "total_tokens": 9258
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses AI models such as random forest classifiers for analysis, but it does not describe their use for quality assurance or annotation validation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 (Feature Extraction and Data Preprocessing)",
          "reasoning": "The dataset annotations, such as cognitive scores and linguistic features, were derived from automated processes including automatic speech recognition (ASR) and algorithmic extraction of acoustic, visual, and linguistic metrics. Additionally, outlier detection was performed using a distribution-based statistical method to remove extreme values. These automated verification steps constitute the QA process described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes automated filtering and scoring methods which can be interpreted as quality assurance; thus, QA processes are documented."
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8545,
      "completion_tokens": 436,
      "total_tokens": 8981
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2. Data collection",
          "reasoning": "The paper describes a crowdsourced study where human participants engaged in structured conversations with a virtual agent, producing original speech, video, and cognitive task responses. This data was collected directly from human subjects and is original content created entirely from scratch by human contributors. The participants generated the data themselves, and it is not translated, adapted, or derived from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new dataset was generated entirely by AI or machine learning models. The data consists of human speech and video recordings, along with cognitive task performance."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used in the data generation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from existing sources; it was collected directly from new human participants."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3. Feature Extraction and Data Preprocessing",
          "reasoning": "The features extracted (e.g., acoustic metrics, visual metrics, linguistic features) represent derived data because they are computed based on the raw audio and video recordings of the human participants. These features are modifications and transformations applied to original human-generated data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are specified clearly in the paper."
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9063,
      "completion_tokens": 268,
      "total_tokens": 9331
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Analyses and Observations and Section 5 Discussion",
          "reasoning": "The paper describes the collection of a new crowdsourced multimodal dataset which is primarily used to analyze the relationships between speech, text, cognitive, and facial video features with depression severity and antidepressant use. Statistical analyses such as Kruskal-Wallis tests and effect size calculations are conducted to observe trends and differences. Classification experiments are performed to explore discriminative power but no model training from scratch or fine-tuning is central. Overall, the dataset's utility is focused on analysis of patterns and characteristics rather than direct training or evaluation of models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9786,
      "completion_tokens": 515,
      "total_tokens": 10301
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Data collection and Section 3 Feature Extraction and Data Preprocessing",
          "reasoning": "The paper explicitly states use of English language for tasks such as word recall, digit span, verbal fluency, and cognitive assessments, and the automatic transcription is done using Amazon Transcribe tailored for English. There is no mention or indication of any other languages in the dataset. All linguistic features are computed on English transcriptions."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions processing techniques and software packages (e.g., Praat, Kaldi, spaCy, OpenCV), the dataset entries themselves do not contain programming or code language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech, video, and cognitive task data; there is no indication that the dataset entries contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes human speech and facial video data only, with no biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly documents the language as English; therefore, language is specified and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken and transcribed language data; therefore, it is not language-free."
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7004,
      "completion_tokens": 159,
      "total_tokens": 7163
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any links or statements about public availability of code related to data collection, preprocessing, feature extraction, or dataset generation. There is no mention of repositories or code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper documents the dataset creation process extensively in Section 2 (Data collection) describing participant recruitment, data collection procedures including speech tasks and questionnaires, and Section 3 (Feature Extraction and Data Preprocessing) detailing modalities, metrics extracted, preprocessing steps such as outlier removal and missing data handling. Thus, the dataset creation process is reasonably well documented in the paper."
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7520,
      "completion_tokens": 173,
      "total_tokens": 7693
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1, 4.1",
          "Reasoning": "The new dataset is a synthetic parallel audio corpus created by the authors using a voice conversion (VC) model. It consists of pairs of audio utterances with the same content and voice but different accents. The original recordings come from human-generated datasets (CMU-ARCTIC and L2-ARCTIC corpora) and the synthesized audio is then generated by AI voice conversion models (FragmentVC or VQMIVC). Therefore, the resulting dataset includes audio data that is both human recorded and model generated (synthesized) as described explicitly in Section 3.1.1 and the dataset composition in Section 4.1."
        }
      ]
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8372,
      "completion_tokens": 268,
      "total_tokens": 8640
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.2.1",
            "reasoning": "The subjective evaluation of the new synthetic dataset (approximately 9 hours of paired audio) was conducted by 10 Indian participants who rated accentedness and speaker similarity on a 5-point scale (Section 4.2.1). These participants are likely native speakers from the target demographic but are presumably non-expert annotators in linguistic analysis."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.2.1",
            "reasoning": "Participants were instructed to listen to provided audios and evaluate accent features on a 5-point scale describing degrees of accentedness and speaker similarity, indicating presence of some instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.2.1",
            "reasoning": "The rating scale of 1 to 5 with defined qualitative descriptors (1-bad to 5-excellent) acts as a rubric guiding the scoring of accentedness and speaker similarity."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide any examples of annotated audios or example ratings to annotators, nor does it mention example guidelines in the annotation instructions."
          }
        }
      ]
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9502,
      "completion_tokens": 332,
      "total_tokens": 9834
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any annotation or quality assurance performed by a single human expert or subject matter expert for the synthetic dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts validated or assessed the synthetic dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance by a single human non-expert on the synthetic dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.2.1 Subjective tests",
          "reasoning": "Section 4.2.1 describes subjective tests where 10 Indian participants (native speakers) evaluated the converted audios on accentedness and speaker similarity, providing a form of quality assurance by multiple human non-expert annotators from the target demographic."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2.2 Objective tests",
          "reasoning": "Section 4.2.2 shows that an accent classifier model and a speech recognition system are used to evaluate the synthesized outputs, which is a form of AI model based automated quality assessment of dataset outputs."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification of code or formulas for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some quality assurance is documented through multiple non-expert human evaluation and AI model evaluation, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9120,
      "completion_tokens": 465,
      "total_tokens": 9585
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was created entirely from scratch by humans for this work. Instead, existing corpora were used as source data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (Generating synthetic data) and Section 4.1 (Dataset and training description)",
          "reasoning": "The authors generate synthetic parallel datasets by applying voice conversion (VC) models (FragmentVC and VQMIVC) to existing audio corpora to create pairs of audios with the same speaker voice but different accents. This synthetic data is newly generated by AI models without direct referencing or simple modification of existing data, fulfilling the criteria for new data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves translation through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any generation of data via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is derived from existing corpora but not simply collected or aggregated without modification; the authors transform the data via voice conversion models."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "The starting point for synthetic data generation is existing speech corpora (CMU-ARCTIC and L2-ARCTIC). The authors apply voice conversion models to these existing audios to generate new audio pairs that differ in accent but maintain voice identity. This represents a transformation of existing data to produce derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and generation process of the datasets used."
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9638,
      "completion_tokens": 245,
      "total_tokens": 9883
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 - 3.2, Section 4.1",
          "reasoning": "The authors generate a synthetic dataset by voice conversion to create paired audios with the same voice but different accents, which is then used to train (fine-tune) the seq2seq Accent Conversion (AC) model using supervised learning methods. Training details with this synthetic dataset are described in Sections 3.1 and 3.2, and training setup is elaborated in Section 4.1."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10361,
      "completion_tokens": 482,
      "total_tokens": 10843
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset and training description",
          "reasoning": "The datasets used and synthesized in the paper involve English spoken with different accents (American and Indian accented English). No indication of any other human languages beyond these accented English variants is provided."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and training description",
          "reasoning": "The synthesized dataset consists of audio pairs containing English spoken with two distinct accents: native American English and Indian accented English. These represent two different accents of the same language (English), effectively making the dataset bilingual in terms of accent varieties."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and training description",
          "reasoning": "The original and synthesized datasets contain English speech only, albeit with different accents (American and Indian). The content in the speech is in English language, so the dataset entries are monolingual English audio entries with accent variation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains English speech only, with no indication of any non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists purely of speech audio data; there is no programming or code content included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication of mathematical or logical symbolic data entries in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech audio only; there is no biological sequence or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any constructed, fictional, or artificial languages; only natural English speech is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper clearly specifies the languages and accents involved (American and Indian accented English), so the language identity is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language audio data, specifically English speech; thus, it is language-relevant."
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7579,
      "completion_tokens": 198,
      "total_tokens": 7777
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link present",
          "reasoning": "The paper does not provide any link or mention of publicly available code repositories related to the synthetic data generation process or dataset construction. The only URLs given are to third-party implementations like tacotron2 and Wav2vec, not to the authors' own code for dataset synthesis."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Generating synthetic data) and Section 4.1 (Dataset and training description)",
          "reasoning": "The paper describes in detail how the synthetic dataset is generated using voice conversion models, including the process overview, architectures used (FragmentVC and VQMIVC), how data from existing corpora (CMU-ARCTIC and L2-ARCTIC) are utilized, and the total size and composition of the synthesized dataset. This provides a clear documentation of dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7667,
      "completion_tokens": 139,
      "total_tokens": 7806
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Data Collection and Section 3.1 Dataset",
          "Reasoning": "The dataset is derived from the MediaSum corpus, which consists of transcripts of media interviews (dialogues) originally created and recorded by humans as they are actual interview transcripts from NPR and CNN. The text modality here is the transcript text itself, human-generated through original media production and transcription processes. The authors sample and process these transcripts to create their SpeakerID training data by anonymizing speaker names and matching name mentions, but the underlying data originates from human-produced dialogue transcripts."
        }
      ]
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8519,
      "completion_tokens": 194,
      "total_tokens": 8713
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 Data Collection",
            "reasoning": "The new dataset is constructed by automatically processing the MediaSum corpus using automated text matching and named entity recognition methods without mention of human annotators performing manual labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe providing any instructions to human annotators since the dataset collection process is fully automatic and no manual annotation was reported."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No scoring rubrics or evaluation guidelines for annotation are mentioned since the dataset is generated via automatic text matching."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No annotation examples or guidelines are provided since no human annotation took place; dataset creation is automatic."
          }
        }
      ]
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9649,
      "completion_tokens": 314,
      "total_tokens": 9963
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Collection",
          "reasoning": "The dataset is created automatically from the MediaSum corpus by applying an automated data processing pipeline including named entity recognition using a state-of-the-art NER model (Trankit), anonymizing speaker names, and mapping detected person names to speaker identities using fuzzy text matching (Levenshtein Distance). There is no mention of manual verification or human annotators. This constitutes an automated process verifying or generating the annotations algorithmically."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process via automated verification and algorithmic techniques is documented and employed during dataset creation."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9267,
      "completion_tokens": 438,
      "total_tokens": 9705
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any new dataset from human contributors producing entirely original content from scratch. Instead, it uses existing transcripts from the MediaSum corpus."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating a dataset entirely using AI or machine learning models without reference to existing data. The data originates from existing transcripts."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involves human translations from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any machine translation was used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Collection",
          "reasoning": "The dataset is derived from the existing MediaSum corpus, which contains transcripts of media interviews. The authors sample and extract data from this existing dataset without fundamentally altering the transcripts themselves, indicating that the data is aggregated or collected from an existing source."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Collection",
          "reasoning": "The authors process the MediaSum transcripts by detecting person names using an NER model, anonymizing speaker identities, and mapping detected person names to speakers via text matching with Levenshtein Distance. These steps constitute modifications and transformations of existing data to create their SpeakerID dataset, thus the dataset is derived from original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the data source and its processing method; hence the data origin is documented."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9785,
      "completion_tokens": 516,
      "total_tokens": 10301
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the proposed dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses pretrained language models like RoBERTa and does not indicate training from randomly initialized parameters using the proposed dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1: Dataset and Section 3.4: Results",
          "reasoning": "The paper states that they randomly sample data from MediaSum to create a SpeakerID dataset used to fine-tune pretrained language models (e.g., RoBERTa) with supervised learning methods, as evidenced in Sections 2.3 and 3. Experiments and results show the model performance fine-tuned on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training methods using the proposed dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1: Dataset and Section 3.4: Results",
          "reasoning": "The proposed dataset sample is split into train/dev/test subsets and used to evaluate and benchmark the models, with metrics reported in Table 3 for precision, recall, and F1 on the test set."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not focus on analyzing trends or patterns in the dataset independently of its use for training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmentation or retrieval-augmented generation according to the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents practical use of the proposed dataset for supervised fine-tuning and evaluation of models; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10508,
      "completion_tokens": 506,
      "total_tokens": 11014
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is derived solely from the MediaSum corpus, which consists of interviews from sources like NPR and CNN in English. There is no mention of multiple languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as containing exactly two languages. It only references English-language data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The authors explicitly state that they sample 200 meetings from the MediaSum dataset in English language for their SpeakerID dataset. MediaSum is composed of English transcripts from NPR and CNN."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of dialogue transcripts and speaker annotations with no mention of programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Despite equation-like notation in model descriptions, these do not appear as dataset entries but are part of the methodological exposition."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human spoken dialogue transcripts, with no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses natural English language only; no fictional or constructed languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language transcripts and thus is not void of language."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7726,
      "completion_tokens": 212,
      "total_tokens": 7938
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 1 Introduction",
          "reasoning": "The paper explicitly states in the Abstract that \"The data and code are publicly available here: https://github.com/adobe-research/speaker-identification\", indicating that the code related to dataset creation, preprocessing, and model implementation is available in this repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 Data Collection",
          "reasoning": "Section 2.2 provides a detailed description of how the new SpeakerID dataset is constructed from the MediaSum corpus, including steps such as detecting person names, anonymizing speaker identities, mapping detected names to speaker identities using fuzzy text matching with Levenshtein Distance, and usage of a state-of-the-art NER model. The explanation is thorough and clear, reflecting transparent and complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8164,
      "completion_tokens": 112,
      "total_tokens": 8276
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Database Construction",
          "Reasoning": "The UY/CH-CHILD is an audio speech database composed of recordings of Uyghur children pronouncing Chinese words. The recordings were made using a laptop and microphone in controlled environments, involving direct human capture of speech data from 106 children, as detailed in Section 3.2 (Speech Recording). Thus, the modality is audio captured by human participants."
        }
      ]
    }
  },
  {
    "id": "nijat24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9016,
      "completion_tokens": 236,
      "total_tokens": 9252
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3 and 3.4",
            "reasoning": "Students from the International Cultural Exchange College and Xinjiang University performed speech segmentation and phonetic annotation, indicating multiple human annotators without explicit mention of expert status."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 and 3.4",
            "reasoning": "The recording process followed established guidelines [28], and annotators were given clear tasks to segment and label pronunciations at the syllable level in Pinyin with tone components."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.4",
            "reasoning": "Annotation required evaluating pronunciation accuracy and annotating deviations in a structured format with components (Initial~Final~Tone), implying scoring rubrics or criteria were provided."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.4",
            "reasoning": "Specific annotation examples such as the labeling of '\u5149' with 'g~uan*' to indicate mispronunciation and uncertainty were provided, showing concrete examples in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "nijat24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10146,
      "completion_tokens": 311,
      "total_tokens": 10457
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts or subject matter experts performing quality assurance or annotation in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was done by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The target speech segmentation and phonetic annotation were performed by students from Xinjiang University and the International Cultural Exchange College at Xinjiang University, who are likely non-experts or at least not explicitly described as experts. Multiple annotators were involved in these tasks as indicated in Sections 3.3 and 3.4."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used for quality assurance in the annotation process."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic rule-based quality assurance process is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving multiple non-expert human annotators for segmentation and annotation is described, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9764,
      "completion_tokens": 407,
      "total_tokens": 10171
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 3.1 to 3.4",
          "reasoning": "The paper explicitly describes the collection of new speech data from 106 Uyghur children who pronounced predefined Chinese words. The data includes newly recorded audio samples and carefully annotated pronunciations at the syllable level done by human annotators. The data was obtained through direct recording sessions under tutor supervision, following a vocabulary design tailored for this study. The resulting database, UY/CH-CHILD, is a brand new dataset created from scratch without deriving from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the dataset involves any translation of content from other languages performed by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of machine translation to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from pre-existing sources; the authors recorded new speech data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not created by modifying or adapting existing data; it is original data collected from recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10282,
      "completion_tokens": 209,
      "total_tokens": 10491
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Pronunciation Bias Analysis)",
          "reasoning": "The UY/CH-CHILD dataset is primarily used for analyzing pronunciation skill development and error patterns among Uyghur children learning Chinese as a second language, as evidenced by the detailed vowel, consonant, and tone bias analyses presented in Section 4."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11005,
      "completion_tokens": 610,
      "total_tokens": 11615
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on Uyghur children learning Chinese as their second language and contains Chinese words spoken by these children. There is no indication that more than two human languages are included in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the children are native Uyghur speakers learning Chinese as L2, the dataset entries contain only Chinese word pronunciations and their phonetic annotations in Pinyin. The dataset does not contain spoken samples or annotations in two languages per entry. Hence, this is not a bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain English content. The target words are in Chinese, and the annotations are in Pinyin, a Romanization of Chinese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3 (Database Construction)",
          "reasoning": "The UY/CH-CHILD dataset consists of audio recordings of Chinese words spoken by Uyghur children learning Chinese as a second language. The words are exclusively Chinese, and all phonetic annotations are in Pinyin (Romanized Chinese). The dataset is focused on Chinese language pronunciation data without inclusion of Uyghur or other languages as audio or annotated content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech only; there is no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains Chinese word pronunciations and is not related to any fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are clearly specified as Chinese words spoken by Uyghur children; the language content is well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech data with linguistic content (Chinese words) and is therefore not applicable for the non-language category."
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8223,
      "completion_tokens": 156,
      "total_tokens": 8379
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link mentions code availability",
          "reasoning": "The paper details the dataset construction process but does not provide any links, repository references, or mention of publicly releasing the code used for data collection, preprocessing, segmentation, or annotation. The focus is on data availability rather than on code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4 (Database Construction and Pronunciation Bias Analysis)",
          "reasoning": "The paper provides a detailed and clear description of the dataset creation process, including vocabulary design, speech recording methodology, segmentation, annotation, and then presents analysis based on the dataset. This documentation is thorough and sufficient to understand the construction of the new dataset."
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7320,
      "completion_tokens": 174,
      "total_tokens": 7494
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 PRIORI dataset, Section 3.1.1 Emotional MisMatch (EMM)",
          "Reasoning": "The PRIORI dataset is described as a longitudinal collection of audio recordings from individuals with bipolar disorder, recorded using a secure app on participants' smartphones capturing participants' side of conversations. This indicates human-generated audio data captured by humans via smartphones."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Emotional MisMatch (EMM)",
          "Reasoning": "Text transcripts were obtained using the Amazon Mechanical Turk (AMT) platform where crowd workers transcribed the audio recordings, indicating human-generated text data obtained via human transcription."
        }
      ]
    }
  },
  {
    "id": "niu23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8172,
      "completion_tokens": 210,
      "total_tokens": 8382
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1.1",
            "reasoning": "The paper states that transcripts of recordings were obtained using Amazon Mechanical Turk (AMT) crowdsourcing platform, indicating multiple non-expert annotators performed the text emotion annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1.1",
            "reasoning": "Use of AMT for emotion annotations typically requires task instructions to the crowd workers to guide emotion labeling, and the paper mentions identifying unreliable workers, implying the presence of instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "There is no explicit mention in the paper of any scoring rubrics provided to annotators for the emotion annotation task."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "The paper does not mention providing annotation examples or sample annotations to the AMT workers."
          }
        }
      ]
    }
  },
  {
    "id": "niu23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9302,
      "completion_tokens": 362,
      "total_tokens": 9664
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts with subject matter expertise performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.1, Section 3.2",
          "reasoning": "The transcript annotations for the PRIORI-Emotion dataset were obtained via Amazon Mechanical Turk (AMT) crowdsourcing platform, which typically involves multiple non-expert human annotators. Additionally, the paper mentions identifying unreliable AMT workers and averaging scores across annotators, indicating multiple non-expert annotators were involved in annotation and quality control."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models were used to automate emotion recognition and extract features, they were not described as performing quality assurance of the ground truth dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic emotion evaluation",
          "reasoning": "The paper describes an automatic process to identify unreliable AMT workers by computing mismatch distances and covariances between annotators' emotion ratings and acoustic emotion labels. This represents an automated verification method to assess annotation quality and filter unreliable annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple quality assurance steps including the use of multiple annotators and automated verification. Therefore, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8920,
      "completion_tokens": 493,
      "total_tokens": 9413
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.1 and 2",
          "reasoning": "The PRIORI-Emotion Dataset is a subset of the PRIORI dataset collected from 19 consenting subjects over a period of six to twelve months. Speech recordings were made from participants' smartphones capturing their side of the conversation. Transcripts of these recordings were obtained through the Amazon Mechanical Turk (AMT) crowdsourcing platform, involving human annotators. This represents original data collection involving human contributors creating new content (speech and annotations) from scratch rather than adapting existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses machine learning models to generate features (e.g., emotion recognition using BERT and CNN models), but does not generate new data solely from models. The data remains grounded in human-recorded speech and human-annotated transcripts."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence in the paper of data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence in the paper of data being produced through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not presented as being collated from pre-existing sources without significant modification. Instead, it is newly collected human speech and annotations."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.2 and 3.2",
          "reasoning": "The emotion recognition features and Emotional MisMatch (EMM) features are derived from the raw speech and text transcripts through transformations and computational methods (automatic speech recognition, emotion recognition models, statistical calculations). These represent derived data based on the original recorded and annotated data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins of the data are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9438,
      "completion_tokens": 241,
      "total_tokens": 9679
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Mood prediction",
          "reasoning": "The PRIORI-Emotion Dataset is used for mood prediction tasks where various features are evaluated and compared. The paper details experiments measuring predictive performance on mood severity and classification using this dataset, indicating its use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 EMM feature analysis",
          "reasoning": "The dataset is analyzed to characterize Emotional MisMatch (EMM) features and their statistical differences among mood classes. The analysis captures patterns and trends relevant to mood symptoms."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10161,
      "completion_tokens": 433,
      "total_tokens": 10594
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1, Section 2 PRIORI dataset",
          "reasoning": "The dataset consists of speech and transcripts from individuals with Bipolar Disorder, and the transcripts were obtained via Amazon Mechanical Turk in English. The paper references the use of the bert-base-uncased pre-trained language model, which is designed for English, and no other languages are mentioned. Thus, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language is mentioned or indicated in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech and text related to human language and emotion; there is no mention of programming or structured code-related content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe features and analyses, the dataset itself does not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human speech and text; there is no mention of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages such as Klingon or Esperanto."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the language of transcripts as English; thus, the language is known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language content (speech and text transcripts), so it is not applicable to label as having no language."
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7379,
      "completion_tokens": 174,
      "total_tokens": 7553
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link provided in the paper",
          "reasoning": "The paper does not provide any explicit link or mention of publicly available code related to the dataset collection, preprocessing, or generation. The only related links are to pretrained models and feature extraction code used as baselines, not code to reproduce the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (PRIORI dataset) and Section 3.1.1",
          "reasoning": "The paper documents the dataset creation process describing the longitudinal data collection from individuals with Bipolar Disorder using a smartphone app, mentions speech recording procedures, IRB approval, transcription via Amazon Mechanical Turk with details on annotator reliability, and the labeling procedure for emotion and mood states. Thus, the dataset creation process is transparently described."
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5914,
      "completion_tokens": 192,
      "total_tokens": 6106
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.2",
          "Reasoning": "The new dataset consists of audio recordings collected by the authors from 2012 to 2016 of participants naming objects either shown in pictures or described by the experimenter. The recordings were made with a Sony microphone and recorder, involving human participants speaking naturally. This indicates human-generated audio data collected specifically for this study."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The dataset includes transcriptions of the recorded words, as well as annotation files generated semi-automatically and then manually corrected by trained phoneticians. The transcriptions and annotations were produced through human involvement and careful manual correction, indicating human-generated text data linked to the audio."
        }
      ]
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6766,
      "completion_tokens": 222,
      "total_tokens": 6988
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Materials and procedures",
            "reasoning": "The data from speech recordings were semi-automatically segmented and then checked and corrected by a trained phonetician, with corrections confirmed by another phonetician, indicating multiple human experts involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Materials and procedures",
            "reasoning": "Participants were given specific tasks (picture naming and riddle tasks) with described procedures. The annotation process involved clear criteria such as marking onset of bursts and voicing, indicating the presence of instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 Materials and procedures",
            "reasoning": "No explicit mention of scoring rubrics or quantitative rating scales for annotation is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 Materials and procedures",
            "reasoning": "The paper does not present illustrative examples or sample annotations demonstrating the annotation process or guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7896,
      "completion_tokens": 435,
      "total_tokens": 8331
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.2 Materials and procedures",
          "reasoning": "The dataset quality assurance involved a trained phonetician who checked the automated segmentation results with Praat and corrected them wherever necessary. Another phonetician confirmed those changes, indicating expertise in the field of phonetics and linguistics was applied during the annotation validation process."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Materials and procedures",
          "reasoning": "Quality assurance was performed by more than one human expert: one trained phonetician checked and corrected the segmentation and another phonetician confirmed those corrections, showing that multiple experts were involved in the QA process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any non-expert human annotator conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any non-expert human annotators conducted quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "An AI model was not used as a judge for quality assurance; instead, automatic segmentation was conducted by Julius, but the review and correction were manual."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Materials and procedures",
          "reasoning": "Initial segmentation was performed semi-automatically using Julius speech recognition software and conversion scripts, thereby applying automated processes. However, this was followed by manual expert verification and correction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents a QA process involving both automated segmentation and manual expert validation."
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7514,
      "completion_tokens": 449,
      "total_tokens": 7963
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The study utilized speech data collected by one of the authors from 2012 to 2016 specifically for their research purposes involving 23 participants across eight cities in the Tohoku region of Japan. The data collection involved participants naming objects in picture and riddle tasks in their usual spoken language. This dataset was newly collected original human-produced speech data, not translated or adapted from existing datasets, as stated in Sections 2.1 and 2.2."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the study used some previously collected data from earlier studies (e.g., data from [1] and [5]), the focus in this paper is on newly collected original speech data and the reanalysis including fully pre-voiced tokens measured anew, rather than aggregation or collection without modification. Therefore, the data is not merely collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper mentions using data originally collected for another study [5] and previously analyzed in [1], but in the current study, the authors re-measured and included fully pre-voiced tokens using a method proposed in a prior study [6]. This involves deriving new measurements and extending analysis on existing data with modifications and adaptations, thus the data is partly derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8032,
      "completion_tokens": 255,
      "total_tokens": 8287
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Method) and Section 3 (Results and Analysis)",
          "reasoning": "The paper introduces a dataset of spoken words from Tohoku Japanese dialect speakers, collected across multiple cities, with detailed acoustic measurements such as voice onset time (VOT), fundamental frequency (F0), and preceding vowel duration. This dataset is employed primarily for analyzing phonetic and phonological characteristics of intervocalic voicing neutralization, covariation of VOT and F0, and other linguistic patterns, rather than for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8755,
      "completion_tokens": 368,
      "total_tokens": 9123
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset is composed of speech data in the Tohoku dialects of Japanese, a non-English human language. The participants produced words in their usual spoken language, which is Tohoku Japanese dialect, and the corpus consists entirely of this single language variety. No other languages are mentioned as part of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5973,
      "completion_tokens": 138,
      "total_tokens": 6111
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not mention or provide any link or reference to publicly available code repositories for the dataset construction, data collection, preprocessing, or analysis. There is no indication that the code is accessible for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including descriptions of recordings, participants, materials and procedures, annotation and measurement methods, and data analysis. The methodology is explained thoroughly in Section 2, which supports reproducibility and transparency despite no code availability."
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8262,
      "completion_tokens": 204,
      "total_tokens": 8466
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2 and Figure 1",
          "Reasoning": "The dataset consists of audio recordings of speech and respiration captured using a Shure MX150 microphone placed inside the mask of the AltiTrainer device worn by human participants during experimental conditions involving normoxia and hypoxia. This clearly indicates human-generated audio data collected via human-operated equipment."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2 and Figure 1",
          "Reasoning": "Physiological signals such as Near-infrared spectroscopy (NIRS) data measuring cerebral oxygenation and ECG data measuring heart rate were collected from participants using dedicated sensor hardware (Portalite NIRS sensors and Polar H10 ECG chest belt). These signals were recorded from human subjects using sensor devices, indicating human-generated signal sensor data."
        }
      ]
    }
  },
  {
    "id": "obrien23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9114,
      "completion_tokens": 230,
      "total_tokens": 9344
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2.1 and Section 2.2.3",
            "reasoning": "The paper specifies that voice activity detection segmentation was performed using the 'Voice Activity Detection' recipe of SpeechBrain (Section 2.2.1) and ECG features were extracted using a custom MATLAB script (Section 2.2.3). These descriptions indicate an automated, programmatic approach to data annotation rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention any provided instructions or protocols for manual annotators since annotation was performed automatically using software tools."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria for annotation; annotation was done automatically on raw signals and speech segments."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No annotation examples or guidelines including examples are described, as the annotation involves automatic segmentation and feature extraction."
          }
        }
      ]
    }
  },
  {
    "id": "obrien23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10244,
      "completion_tokens": 350,
      "total_tokens": 10594
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2, 2.3, and 2.5",
          "reasoning": "The dataset annotations such as segmentations into speech and non-speech were performed by an automated voice activity detection method (SpeechBrain VAD). Features extraction and segmentation for ECG and NIRS signals were done via custom MATLAB and Python code, relying on signal processing techniques and automated algorithms (e.g., filtering, peak detection). There is no indication of human annotators verifying or annotating the data manually. Thus, quality assurance relies on automated processing and verification of the processing steps."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes the use of automated methods for signal segmentation and feature extraction but does not mention any manual or human-based quality assurance, indicating that QA is performed via automatic processes."
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9862,
      "completion_tokens": 444,
      "total_tokens": 10306
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data; Section 2.1.1 Participants; Section 2.1.2 Experimental protocol",
          "reasoning": "The study involved recording acoustic, cerebral blood oxygenation, and cardiac signals from 20 human participants who underwent control and normobaric hypoxia experimental conditions. The data were collected directly from these human subjects during controlled experiments specifically conducted by the authors. This constitutes original data created entirely from scratch by human contributors, not adapted or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of data being human-translated from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of data being machine-translated."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating existing data from other sources; the dataset was newly collected."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Feature extractions",
          "reasoning": "While the raw data were newly collected from humans, the acoustic and physiological features were extracted and transformed from these raw recordings. For example, acoustic features (MFCCs), NIRS-based features, and ECG-based features were computed via established processing methods and feature extraction algorithms. Thus, the feature datasets can be considered derived from the raw recorded data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are explicitly described."
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10380,
      "completion_tokens": 322,
      "total_tokens": 10702
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.4.2 Model training",
          "reasoning": "The newly collected dataset, comprising acoustic, cerebral oxygenation, and cardiac signals from participants under normoxia and hypoxia conditions, was used to train Support Vector Machine models with a 4-fold cross-validation design to detect hypoxia. This indicates training models from scratch with the dataset features."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.5 Evaluation and Section 3.2 Evaluation results",
          "reasoning": "The dataset was partitioned into training and independent testing sets; models trained on the training dataset were evaluated on the testing dataset to measure hypoxia detection performance, indicating its use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3. Results and Section 4. Discussion",
          "reasoning": "The dataset was used to analyze the impact of speech and non-speech conditions on acoustic and physiological features related to hypoxia and to understand the characteristics and patterns associated with hypoxia-induced changes in speech production and physiological signals."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11103,
      "completion_tokens": 391,
      "total_tokens": 11494
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1.1 Participants and Section 2.1.2 Experimental protocol",
          "reasoning": "The dataset contains acoustic recordings of participants performing tasks and speech in a controlled experimental setting at Universit\u00e9 de Montpellier, France. Although the paper does not explicitly state the language spoken in the recordings, all participants are French university students and the only explicitly written commands or utterances mentioned (e.g., the word 'Top' for synchronization) are in English. The paper is written in English and describes the speech recordings without indication of other languages. Thus, the dataset is considered monolingual English in content, especially since the analysis focuses on acoustic features rather than on linguistic content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2.2.2 NIRS-based features and Section 2.2.3 ECG-based features",
          "reasoning": "The dataset includes physiological signals such as cerebral oxygenation data from near-infrared spectroscopy (NIRS) sensors and heart rate data from ECG chest belts. These data represent biological signals and non-human communication systems, as they are biological measurements rather than human spoken languages."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8321,
      "completion_tokens": 153,
      "total_tokens": 8474
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not include any links or references to publicly available code repositories or supplementary materials containing code for data collection, preprocessing, or generation related to the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods), subsections 2.1 (Data) and 2.2 (Feature extractions)",
          "reasoning": "The paper provides detailed descriptions of the dataset creation including participant recruitment, experimental protocol, data collection hardware and software setup, data synchronization, feature extraction methods for acoustic, NIRS, and ECG data; thus, the dataset creation process is well documented in the Methods section."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8904,
      "completion_tokens": 106,
      "total_tokens": 9010
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset: Afro-TTS",
          "Reasoning": "The Afro-TTS dataset consists of 136 hours of 16-bit, 48 kHz audio files recorded by 747 volunteers from 9 African countries speaking 86 different African English accents. The recordings were made by human contributors who read English texts, thus the data modality is audio and the origin is human generated."
        }
      ]
    }
  },
  {
    "id": "ogun24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9756,
      "completion_tokens": 259,
      "total_tokens": 10015
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4, Experiments and Evaluation protocol",
            "reasoning": "The subjective evaluation (listening tests) was conducted by 427 unique participants who provided human ratings and preferences on generated utterances, indicating multiple human non-expert raters performed annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Sections 3.1 Dataset and 4 Evaluation protocol",
            "reasoning": "The paper mentions that contributors were asked to read sequences of English texts with African-named entities and that for subjective evaluation, participants were instructed to rate various aspects such as naturalness and accentedness, indicating presence of instructions for both data collection and evaluation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4 Evaluation protocol",
            "reasoning": "Subjective metrics reported include naturalness MOS (Nat-MOS), accentedness (A-MOS), country-match, accent-match, and preference tests, showing that scoring rubrics with specific scales and dimensions were used in human annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide or mention any example annotations or example instructions in annotation guidelines for either the data collection or evaluation processes."
          }
        }
      ]
    }
  },
  {
    "id": "ogun24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10886,
      "completion_tokens": 429,
      "total_tokens": 11315
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator with subject matter expertise or membership in the target demographic for the Afro-TTS dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple human experts or annotators with expertise or from the target demographic. The annotations or data validation by experts are not described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided about multiple non-expert humans performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset pre-processing",
          "reasoning": "The dataset was processed using several AI-based models as part of quality assurance: a speech enhancement model was used for denoising, a bandwidth-extension model called VoiceFixer was applied to improve degraded utterances, and a quality estimator WVMOS was employed to select the best processed sample per utterance based on predicted MOS scores. These automated models serve as AI-based quality assurance steps for the audio data quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset pre-processing",
          "reasoning": "Automatic verification and normalization steps were performed, including normalization of volume levels using RMS-based normalization with ffmpeg and removal of long pauses using a voice activity detection tool. Also, text normalization (expanding abbreviations, numbers, and punctuation) was performed using the NeMo text normalization toolkit. These are algorithmic, rule-based processing steps constituting automatic quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents AI model-based and automatic quality assurance processes applied during data pre-processing. Therefore, it is not the case that no quality assurance process was documented or performed."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10504,
      "completion_tokens": 406,
      "total_tokens": 10910
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The Afro-TTS dataset was created by online crowd-sourcing recordings from 747 paid human contributors across 9 African countries who read specific English texts. The dataset is original and created from scratch by human speakers producing audio data, with no indication of translated, adapted, or derived content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any portion of the Afro-TTS dataset was generated by AI or machine learning models; all speech data is human recorded."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the dataset involving translation of content from other languages into English by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include data translated from other languages using machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Afro-TTS dataset was not collected or aggregated from existing sources; it was newly recorded and curated specifically for this work."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The raw recordings were processed by removing noise, enhancing quality with models like VoiceFixer, normalizing volume, and trimming long pauses, representing transformations applied to the original data before use."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11022,
      "completion_tokens": 566,
      "total_tokens": 11588
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Afro-TTS dataset is not described as used for pre-training large models on general patterns or in an unsupervised/self-supervised manner. Instead, it is used for supervised training and fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4 Experiments",
          "reasoning": "The paper states in Section 3.2 that a VITS model was trained from scratch (termed VITS-O) on the Afro-TTS dataset to validate the quality of the data for TTS experiments, indicating training from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4 Experiments",
          "reasoning": "The Afro-TTS dataset is used to fine-tune pre-trained models (VITS-FT and XTTS-FT) as described in Section 3.2 and the experiments section, showing supervised fine-tuning of TTS models on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training techniques such as RLHF used with the Afro-TTS dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The Afro-TTS dataset includes development and test splits used for evaluation of TTS model performance, including subjective and objective metrics as described in Sections 4 and 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 and throughout the paper",
          "reasoning": "The paper provides detailed analysis of accent representation, naturalness, accentedness, preference scores, and regional diversity derived from the Afro-TTS dataset, used to understand trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the Afro-TTS dataset is used as a knowledge base for augmenting models through retrieval or similar."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The Afro-TTS dataset is actively used throughout the pipeline for training, fine-tuning, evaluation, and analysis. Hence, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11745,
      "completion_tokens": 495,
      "total_tokens": 12240
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Afro-TTS dataset introduced focuses on English content with multiple African accents but does not contain entries in multiple different human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies the dataset consists of spoken English with African accents; it does not mention bilingual content involving exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset: Afro-TTS",
          "reasoning": "The Afro-TTS dataset contains English speech recordings only, as volunteers read sequences of English text. The paper repeatedly emphasizes that the dataset is pan-African accented English. No other spoken languages are included in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries contain English speech, not a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists only of human English speech and does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are not in constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content in the dataset is explicitly described as English; it is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains natural language speech data."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8963,
      "completion_tokens": 217,
      "total_tokens": 9180
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Dataset: Afro-TTS",
          "reasoning": "The paper states that the Afro-TTS dataset and models can be accessed via a link to Hugging Face (https://huggingface.co/intronhealth/afro-tts), but there is no explicit mention or link to the code used for data collection, preprocessing, or dataset construction. Hence, code for dataset creation is not indicated as publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset: Afro-TTS and Section 3.1.1 Dataset pre-processing",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including data collection (crowd-sourcing protocol), demographics of contributors, recording settings, consent, data preprocessing steps such as denoising, bandwidth extension, quality evaluation, normalization, splitting, and text normalization methods. This information is provided mainly in Sections 3.1 and 3.1.1, offering transparency about dataset creation."
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8323,
      "completion_tokens": 159,
      "total_tokens": 8482
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Introduction, Discussion",
          "Reasoning": "The new dataset introduced in the paper consists of real-time Magnetic Resonance Imaging (rtMRI) data and accompanying measurements of vertical larynx movements (VLMs) from five native speakers of Seoul Korean. This modality involves sensor data captured using rtMRI equipment during human speech production, thus it is human generated data from human subjects recorded with specialized sensors. The paper explicitly states that 'Target stimuli were produced by five native speakers of Seoul Korean using real-time Magnetic Resonance Imaging (rtMRI) of the larynx and tongue.' This confirms the data modality as sensor data and human generated origin."
        }
      ]
    }
  },
  {
    "id": "oh22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9175,
      "completion_tokens": 222,
      "total_tokens": 9397
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 and Discussion",
            "reasoning": "The data involve measurements of vertical larynx movements captured from speech produced by five native speakers using real-time MRI. The study references using an automated centroid tracking tool for data extraction, suggesting human expert involvement in data collection and annotation for prosodic focus identification, as well as interpretation of the articulatory patterns and prosodic focus effects."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not provide explicit description of detailed annotation instructions given to annotators for labeling or evaluating data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "No mention or description of scoring rubrics or formal rubric guidelines for annotation or evaluation is provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not furnish explicit annotation examples or annotated samples in guidelines or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "oh22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10305,
      "completion_tokens": 290,
      "total_tokens": 10595
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "General description of data collection involving five native speakers of Seoul Korean in Abstract and Introduction sections.",
          "reasoning": "The dataset comprises prosodic focus speech data collected from five native speakers of Seoul Korean. These speakers are members of the target demographic and presumably have expertise in natural speech production, thus serving as expert annotators for the data collection and validation process."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "Although the study uses an automated centroid tracking tool (ACT) for analysis, this is a measurement tool, not a quality assurance step for validating the dataset itself.",
          "reasoning": "The automated centroid tracking tool (ACT) is used for analyzing real-time MRI speech production data, but there is no description indicating that any automated verification process was applied for quality assurance of dataset annotations or labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the data collection process involving expert native speakers and analysis methods, indicating some QA approach was used; thus, QA is not absent."
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9923,
      "completion_tokens": 180,
      "total_tokens": 10103
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Abstract, Methods, Discussion",
          "reasoning": "The paper reports data collected through real-time Magnetic Resonance Imaging (rtMRI) from five native speakers of Seoul Korean producing designed target stimuli sentences. This data consists of original speech production recordings uniquely created by human contributors specifically for this study and not adapted or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10441,
      "completion_tokens": 297,
      "total_tokens": 10738
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the novel dataset for pre-training any machine learning models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset was used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for fine-tuning pre-trained models in a supervised manner."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning post-training methods using the dataset are described."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not indicated to be used purely for evaluation or benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Results and Discussion sections",
          "reasoning": "The dataset, consisting of real-time MRI data of vertical larynx movements under prosodic focus, is used primarily to analyze spatio-temporal articulatory patterns, correlations with prosodic focus, and phonetic characteristics rather than for any model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for augmenting models with external knowledge."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11164,
      "completion_tokens": 558,
      "total_tokens": 11722
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced involves only Seoul Korean utterances for prosodic focus analysis. No other languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset contains exactly two languages. Only one language, Korean, is used."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some example sentences are given in English for illustrative purposes, the actual dataset consists of Korean sentences produced by native speakers. The English is not part of the data but only used for explanation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Introduction; Discussion",
          "reasoning": "The dataset consists of Seoul Korean sentences produced by native Korean speakers (five speakers). The paper focuses on prosodic focus in Korean and measurements of vertical larynx movements for Korean utterances. Hence, the dataset is monolingual Korean (non-English)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any code or programming language data. Tools and methods (software) are mentioned, but the dataset entries are not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or logical symbolic expressions; it is composed of speech utterances and larynx movement measurements."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech articulations and vocal tract imaging, but it is linguistic data from human speakers, not biological sequences or non-human communications."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain fictional or constructed languages. The language used is Seoul Korean."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as Seoul Korean."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken Korean utterances and thus involves linguistic content."
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8382,
      "completion_tokens": 140,
      "total_tokens": 8522
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not include any links, appendices, or references to publicly available code repositories related to the dataset collection, preprocessing, or analysis."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data collection and IRB protocol) and Discussion sections",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process, including the use of real-time MRI data acquisition from five native Seoul Korean speakers, the stimuli design, and the analysis methods used to extract vertical larynx movement measures, indicating comprehensive documentation of the dataset's construction."
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7727,
      "completion_tokens": 232,
      "total_tokens": 7959
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 \"Design of CAPTDURE\", Table 3, Section 2.2 \"Recording environment and setup\"",
          "Reasoning": "The dataset includes 1,044 single-source sounds of various daily sound events that were recorded by the authors using multiple types of recording equipment and environments as detailed in Section 2.2 and Table 3. These sounds are captured from real-world sound events via human-operated recording setups, indicating human generation of the audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 \"Design of CAPTDURE\", Section 2.3 \"Captions collection\"",
          "Reasoning": "The dataset contains a total of 4,902 captions for the single-source sounds collected from Japanese crowdworkers using the Lancers crowdsourcing service as described in Section 2.3. The captions were manually written by human workers describing the single-source sounds, thus the text data is human generated."
        }
      ]
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8579,
      "completion_tokens": 216,
      "total_tokens": 8795
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3",
            "reasoning": "The captions were collected using Lancers, a crowdsourcing service in Japan, involving crowdworkers who provided captions for the sounds. This indicates multiple human non-experts authored the captions."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "Crowdworkers were instructed to write different sentences as captions for each sound and to avoid only naming the sound type or using only onomatopoeic words, demonstrating that specific instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "No mention of rubric definitions or scoring criteria for caption writing was provided in the paper; only appropriateness scoring is mentioned separately in Section 2.4."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "The paper does not mention providing examples of captions for annotation guidance during caption collection."
          }
        }
      ]
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9709,
      "completion_tokens": 411,
      "total_tokens": 10120
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human annotators with subject matter expertise performed quality assurance on the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA being conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.4 Captions evaluation",
          "reasoning": "The paper describes that crowdworkers (non-experts) were asked to score the appropriateness level of captions provided by others. Multiple non-expert human annotators (crowdworkers) performed quality evaluation by assigning appropriateness scores to each caption, reflecting a quality assurance process through multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that AI models were used as judges for quality assurance or annotation validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of any automatic or algorithmic verification process applied for data or annotations quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is described in the form of appropriateness scoring performed by multiple non-expert human annotators, so it is not the case that no QA was applied or documented."
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9327,
      "completion_tokens": 488,
      "total_tokens": 9815
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.3",
          "reasoning": "The dataset CAPTDURE was constructed by the authors who recorded 1,044 single-source environmental sounds themselves using various recording equipment and conditions (Section 2.1, 2.2). Captions for these recorded sounds were collected from Japanese crowdworkers via the crowdsourcing service Lancers, where human contributors wrote original captions describing the sounds (Section 2.3). Thus, both the audio data and the captions were newly created from scratch by human effort."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any dataset content entirely by AI or machine learning models; all data collection and captioning were performed by human recording and crowdworkers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Captions were originally collected in Japanese and then translated into English using the DeepL API, which is a machine translation system. No mention of human translation is made."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The captions collected in Japanese were translated into English using the DeepL API, a machine translation service, as explicitly stated in Section 2.3."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources; it was created by recording new sounds and collecting new captions."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "Multiple-source sounds were created by mixing two different single-source sounds recorded by the authors to simulate mixture sounds, representing data derived from the original single-source sounds with modifications (mixing and SNR adjustment)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are described clearly in the paper."
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9845,
      "completion_tokens": 281,
      "total_tokens": 10126
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The dataset CAPTDURE is used to train a neural network model from scratch using supervised learning to extract single-source sounds from mixture sounds based on captions, as described in the network architecture and training details in Section 3.3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The dataset is used for evaluation of the sound extraction performance, including test sets with mixtures created from the test single-source sounds, as described in Section 3.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.4, Fig. 1",
          "reasoning": "The dataset includes appropriateness scores collected from crowdworkers to analyze caption quality and appropriateness, as detailed in Section 2.4 and shown in Figure 1."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10568,
      "completion_tokens": 653,
      "total_tokens": 11221
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.3 Captions collection; Table 4 Dataset statistics",
          "reasoning": "The dataset CAPTDURE contains captions collected from Japanese crowdworkers in Japanese. These captions were also translated into English using the DeepL API (Section 2.3). Thus, the dataset contains captions in more than two human languages: Japanese (original captions) and English (machine-translated captions). Since two languages are explicitly presented, and the translation process suggests both languages are included, and no other languages are mentioned, the dataset can be considered bilingual rather than multilingual. However, the presence of original captions in Japanese and their translated versions in English indicates it includes exactly two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.3 Captions collection; Table 4 Dataset statistics",
          "reasoning": "CAPTDURE contains captions in Japanese collected from crowdworkers and English translations obtained via the DeepL API, explicitly mentioned in Section 2.3. No other human languages are mentioned. Thus, the dataset contains exactly two human languages: Japanese and English."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains Japanese captions as original data and English as translated captions. It is not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains Japanese captions but also English translations, so it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains sound recordings and natural language captions; there is no indication of programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio sounds and natural language captions only; no mathematical or logical symbolic notation is included in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human environmental sounds with captions and does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of the dataset containing fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly stated as Japanese and English; therefore, language coverage is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language captions (Japanese and English), so it is not language-free."
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7786,
      "completion_tokens": 212,
      "total_tokens": 7998
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention any link, repository, or location where the code for dataset construction or related processing is made publicly available. There is only a mention that the dataset is freely available online, but no indication the code to create the dataset has been released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Creation of CAPTDURE) and its subsections",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 2, including dataset design, recording environment and setup (Sec. 2.2), captions collection (Sec. 2.3), captions evaluation (Sec. 2.4), and data splitting (Sec. 2.5). It includes information on the number and types of recordings, recording equipment, microphone distance, duration, caption collection procedure, appropriateness scoring, and dataset split statistics. This constitutes thorough documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7127,
      "completion_tokens": 142,
      "total_tokens": 7269
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.1.1",
          "Reasoning": "The authors introduced a new dataset composed of question-answer pairs extracted from a subset of the Spotify 100,000 Podcast dataset, which consists of real spontaneous two-speaker conversations with audio recordings. They applied filtering steps for speaker diarisation, overlapping speech, laughter, and speaker verification, resulting in approximately 26,876 question-answer pairs (~38 hours of audio). This dataset contains audio modality recorded from human speakers in natural podcast conversations, hence is human generated and not model generated or unknown origin."
        }
      ]
    }
  },
  {
    "id": "omahony22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7979,
      "completion_tokens": 208,
      "total_tokens": 8187
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 and 2.1.1",
            "reasoning": "The corpus of question-answer pairs was created by automatically processing the Spotify Podcast dataset using speaker diarisation, punctuation detection, speaker embedding extraction, and audio filtering methods with no mention of human annotators performing manual labeling or correction."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions described",
            "reasoning": "Since the data annotation consists of an automatic pipeline for extraction and filtering, no human annotation instructions are provided or necessary."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of rubrics",
            "reasoning": "There is no indication that scoring rubrics or guidelines for human annotation were used or developed, as the process was automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples mentioned",
            "reasoning": "No annotation examples are provided in the paper regarding the data extraction or filtering procedures since these are automatic processes."
          }
        }
      ]
    }
  },
  {
    "id": "omahony22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9109,
      "completion_tokens": 363,
      "total_tokens": 9472
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process described in the paper does not use an AI model as a judge to validate dataset annotations or content. AI models are used for speaker embedding extraction and verification during filtering, but not for quality assurance judgment."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1. Data Filtering and Audio Filtering",
          "reasoning": "The dataset creation involves several automated verification and filtering steps, including automatic punctuation and diarisation to extract utterances with specific properties, speaker diarisation to ensure exactly two speakers, automatic removal of overlapping speech, laughter detection via known algorithms, and speaker verification using speaker embeddings and automatic speaker verification models. These are algorithmic and rule-based techniques that provide quality assurance of the dataset content without manual annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes an automated multi-stage filtering and verification process for dataset quality assurance; thus, quality assurance is present and documented."
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8727,
      "completion_tokens": 435,
      "total_tokens": 9162
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are not newly recorded or created from scratch by human contributors specifically for this paper. The authors did not create any original content by recording new speech data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any datasets are produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation to generate or adapt any dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.1.1",
          "reasoning": "The authors created a new dataset of question-answer pairs by extracting and filtering data from the existing Spotify 100,000 Podcast dataset. This involved selecting podcasts with two speakers, segmenting transcripts, filtering utterances, and applying speaker verification and audio filters. This process aggregates and selects data from an existing large corpus without generating original recordings, which fits the 'collated' category."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors applied filtering and processing to data from the Spotify podcast corpus, these steps are consistent with collating and selecting data rather than significant modifications or transformations to constitute derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the new datasets are clearly documented and described in detail in the paper."
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9245,
      "completion_tokens": 224,
      "total_tokens": 9469
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The new dataset of spontaneous question-answer pairs extracted from podcasts (Section 2.1) is combined with the read speech dataset to train two FastPitch models from scratch, one on read speech only and another combining read and spontaneous speech data. This demonstrates use of the new spontaneous speech dataset for training a model from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9968,
      "completion_tokens": 617,
      "total_tokens": 10585
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 and 3.1",
          "reasoning": "The new dataset introduced is derived from podcasts labeled with countries including UK, US, Canada or general English, but there is no indication of multiple languages being present. The dataset is filtered for English content only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1.1",
          "reasoning": "The dataset consists of question-answer pairs in English extracted from two-speaker podcasts, both speakers using English. There is no evidence of exactly two different human languages included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.1.1, and 3.1",
          "reasoning": "The new dataset extracted from the Spotify 100k Podcasts is in English, as indicated by country labels (UK, US, Canada) for filtering and by the text being punctuated and transcribed in English. The read speech data is from LJ Speech, an American English audiobook corpus. Therefore, both components of the new dataset contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Overall data description",
          "reasoning": "No non-English single language datasets are introduced in this work; all data is English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No datasets with programming or structured code content are introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The datasets contain speech and text transcripts but no mathematical or logical notations are part of the data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No fictional or constructed languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Sections 2.1 and 2.1.1",
          "reasoning": "The language content is clearly described as English and is documented through automatic transcriptions and filtering, so language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of speech with corresponding transcriptions in English; therefore, they do contain language."
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7186,
      "completion_tokens": 185,
      "total_tokens": 7371
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not mentioned",
          "reasoning": "The paper does not include any link or mention of publicly available code repositories for the dataset construction, filtering, or processing steps. No URLs or references to codebases are provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Data), Sections 2.1.1 (Data Filtering) and 2.1.2 (Audio Filtering)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process from the Spotify 100,000 Podcast dataset, including criteria for filtering the data by speaker count, utterance completeness, punctuation, speaker diarisation, extraction of question-answer pairs, removal of low-quality data via speaker verification, overlap and laughter detection, and final dataset statistics. This thorough description supports reproducibility of the dataset creation steps even if code is not provided."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8362,
      "completion_tokens": 167,
      "total_tokens": 8529
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Data and Evaluation Setup)",
          "Reasoning": "The new dataset introduced in the paper is derived from TED-LIUM 3 corpus, consisting of 450 hours of English speech audio. This is human-generated data as the TED talks are real human recordings."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4 (Data and Evaluation Setup)",
          "Reasoning": "The authors use GPT-3.5 (a model) to generate the Speech Comprehension Test Question-Answer (SQA) pairs from transcripts of the TED-LIUM 3 corpus, thus the QA pairs text data are model-generated."
        }
      ]
    }
  },
  {
    "id": "pan24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9214,
      "completion_tokens": 211,
      "total_tokens": 9425
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 4",
            "reasoning": "The question-answer pairs (SQA) for instruction tuning are generated by prompting GPT-3.5 on speech transcripts, indicating the annotation is performed by an AI model rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4",
            "reasoning": "The paper describes a process where GPT-3.5 is prompted with transcripts to generate question-answer pairs; this prompt effectively functions as detailed instructions guiding the AI model in the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "There is no mention or description of scoring rubrics used for evaluating or guiding the annotation generation by GPT-3.5."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not provide information about any example annotations given to GPT-3.5 or additional annotation examples provided as part of guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "pan24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10344,
      "completion_tokens": 345,
      "total_tokens": 10689
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by a single human expert on the generated dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human experts being involved in quality assurance of the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human performed quality assurance on the dataset or annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information on multiple non-expert humans conducting quality assurance on the dataset or annotations."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4, Data and Evaluation Setup",
          "reasoning": "The SQA (speech comprehension test question-answer) pairs are generated by prompting the GPT-3.5 large language model on the transcripts. This means that an AI model (GPT-3.5) was used to generate the instruction tuning data, effectively acting as an automatic judge in producing dataset annotations. However, no human quality assurance beyond this AI generation is described."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While an AI model generated questions and answers, there is no mention of an automated verification or rule-based technique to automatically verify or validate the quality of these annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documentation explaining that GPT-3.5 was used to generate the SQA data; hence, there is some form of quality assurance via AI model generation."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9962,
      "completion_tokens": 479,
      "total_tokens": 10441
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any entirely new speech or text data was created from scratch by human contributors. The speech data used (TED-LIUM 3) is pre-existing, and the transcripts are original transcripts, but the paper does not claim the authors themselves created new human-annotated data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4 Data and Evaluation Setup",
          "reasoning": "The authors use GPT-3.5 (an AI model) to generate Speech Comprehension Test Question-Answer (SQA) pairs based on the transcripts of their training corpus. This is model-generated new data in the form of QA pairs that do not exist originally."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that any human translators produced translations in the paper."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 4 Data and Evaluation Setup",
          "reasoning": "The authors use GPT-3.5 to translate English transcripts into Spanish, French, and German for unseen speech-to-text translation tasks. This counts as machine-translated data generated for evaluation purposes."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 Data and Evaluation Setup",
          "reasoning": "The main speech data source is TED-LIUM 3, which is an existing corpus. The authors utilize this existing dataset without fundamental changes, thus 'collated' applies."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that they derived a new dataset by modifying or adapting existing data in a significant way other than generating QA pairs or translations as noted."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the origins of all data used, including the source of speech data and generated QA pairs and translations."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10480,
      "completion_tokens": 490,
      "total_tokens": 10970
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly created SQA dataset or other generated data for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper initializes model components from pre-trained models (e.g., Whisper encoder and LLaMA-2 checkpoints) and does not report training any model from randomly initialized parameters with the new datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Speech Instruction-Tuning; Section 4 Data and Evaluation Setup; Section 5 Experiments and Results",
          "reasoning": "The new dataset, consisting of GPT-3.5 generated speech comprehension test question-answer (SQA) pairs from TED-LIUM 3 transcripts (about 450 hours English speech), along with ASR labels, is used for supervised instruction tuning of the COSMIC speech-Large Language Model system to develop instruction-following and in-context learning capabilities (Section 3.2 and Section 4). This supervised fine-tuning is explicitly described and experimentally validated in the paper (Section 5)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using the new datasets for reinforcement learning based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not used exclusively for evaluation or benchmarking; rather it is employed in training (instruction tuning). Existing standard datasets like TED-LIUM 3 test sets and FLEURS are used for evaluation, but these are pre-existing and not created by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the new dataset primarily for data analysis or to investigate trends or patterns; the focus is on instruction tuning and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the new dataset as a knowledge base to augment models via retrieval or other methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the new dataset in supervised fine-tuning (instruction tuning); thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11203,
      "completion_tokens": 635,
      "total_tokens": 11838
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced by the authors is based on TED-LIUM 3 English speech data only. Although the evaluation involves speech translation into multiple languages, the instruction-tuning dataset itself contains English speech and corresponding English text. Hence, the dataset does not include entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced contains only English speech and text in English. There is no mention of entries containing exactly two human languages in the dataset created by the authors."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 'Data and Evaluation Setup', Section 5 'Experiments and Results'",
          "reasoning": "The dataset constructed for instruction-tuning is derived from TED-LIUM 3 corpus which contains only English speech and transcripts. GPT-3.5 is prompted to generate question-answer pairs based on these English transcripts. The text responses and speech inputs used for training are exclusively English, establishing the dataset as monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset including non-English language data was introduced by the authors. The data used for instruction tuning is English speech and transcripts only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not mention inclusion of programming or code-related content in their dataset. The data consists of speech and textual natural language question-answer pairs."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly introduced dataset contains mathematical or formal logic notations or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of human English speech and corresponding text; no biological or non-human communication data is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificial languages in the new dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly stated to be English from TED-LIUM 3 corpus and GPT-3.5 generated English text; thus, the dataset language is fully known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries with English language (speech and text), so it does contain language."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8421,
      "completion_tokens": 196,
      "total_tokens": 8617
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention any public availability of code related to the dataset construction, data generation from speech transcripts, or instruction tuning data generation such as GPT-3.5 prompted question-answer pairs. There is no link or pointer to repositories or supplemental materials providing such code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 - Data and Evaluation Setup",
          "reasoning": "The paper provides detailed documentation on how the instruction tuning dataset is created: using TED-LIUM 3 speech corpus with 450 hours of English speech data; GPT-3.5 is prompted with speech transcriptions to generate question-answer pairs; data sizes and segmentations are reported (e.g., 50K audio segments, 856K QA pairs, average 17 QA pairs per utterance). The methodology for data preprocessing and generation is described clearly in Section 4."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8802,
      "completion_tokens": 116,
      "total_tokens": 8918
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Description of dataset",
          "Reasoning": "The paper explicitly states that they extended the Blizzard Challenge 2013 dataset with two new synthetic voices generated using WaveNET vocoders: 'Fastpitch Wavenet (Y)' and 'Tacotron Wavenet (Z)'. These are synthesized audio data generated by neural TTS models, thus the modality is 'audio', generated by models, not human recorded or natural recordings."
        }
      ]
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9654,
      "completion_tokens": 199,
      "total_tokens": 9853
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2",
            "reasoning": "The acoustic-phonetic features were extracted automatically using tools such as the Montreal Forced Aligner and Praat software, along with rule-based temporal boundary identification procedures. This indicates that annotation was performed via automatic and rule-based processes rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not describe providing explicit instructions to human annotators since the process was automatic."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No mention of scoring rubrics or criteria for human annotators is given; the feature extraction was algorithmic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No examples of annotation or sample annotations are provided since the feature extraction uses automated tools without manual annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10784,
      "completion_tokens": 325,
      "total_tokens": 11109
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information regarding quality assurance being performed by multiple human experts in the dataset preparation or annotation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance by AI models as judges is not described in the paper for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Feature extraction",
          "reasoning": "The feature extraction utilized automated techniques including the Montreal Forced Aligner (MFA) for phoneme-level alignment, the Burg formant-tracking algorithm in Praat, and a rule-based temporal boundary identification procedure. These automated and rule-based methods were employed to perform annotation and feature extraction, supporting an automated form of quality control over the consistency of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a systematic automated feature extraction process, indicating that some form of automated quality assurance was performed. Therefore, it is not accurate to conclude that no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10402,
      "completion_tokens": 506,
      "total_tokens": 10908
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states that they extended the existing 2013 Blizzard Challenge dataset by including two new voices generated using a WaveNET vocoder. The natural voice recordings used as a reference are original human speech recordings, created from scratch by human speakers as part of the Blizzard Challenge dataset. Thus, the natural voice data in the dataset is original human-generated data, not derived or adapted from pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The two new TTS voices (named Y and Z) in the extended dataset were generated entirely by neural text-to-speech models using the WaveNET vocoder, conditioned on FastPitch and Tacotron acoustic models respectively. These synthetic voices and their speech utterances are newly generated data created by AI models, not transformations or translations of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data created via machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset analyzed is an extension of the existing Blizzard Challenge 2013 corpus, which was aggregated from previous TTS systems and the natural voice recordings. The authors aggregated these pre-existing data sources along with their two new neural TTS voices to create a combined comparative dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although features are derived from the speech signals, the paper does not describe the dataset itself as derived or adapted from existing data with transformations. The dataset is primarily an extension aggregating natural and synthetic speech data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are explicitly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10920,
      "completion_tokens": 448,
      "total_tokens": 11368
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or indicate that the new dataset is used for pre-training large models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the new dataset is used for training a model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not reported to be used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of reinforcement learning or RLHF post-training methods using this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4 Results",
          "reasoning": "The extended BC-2013 corpus with two new WaveNET vocoder voices is used for comparative analysis and benchmarking against natural and other TTS systems, as described in Section 3.1 and analyzed in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The dataset is primarily used to analyze acoustic-phonetic features of obstruent consonants and surrounding vowels to characterize differences and trends in segmental properties across TTS systems (Section 4 Results, Section 5 Discussion)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models; no such utility is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clear uses of the dataset for evaluation and analysis."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11643,
      "completion_tokens": 367,
      "total_tokens": 12010
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described is exclusively English, with no indication of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries in two human languages; only English is used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The extended Blizzard Challenge 2013 dataset used in the study consists of English utterances only, as stated in the description of the dataset containing 100 identical English utterances synthesized by multiple TTS systems."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly English; no mention of non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of programming or code-related content as dataset entries; the dataset consists of audio utterances."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical symbolic content is included in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech utterances and does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are mentioned; the dataset is composed of natural English speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly specified as English in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (English) audio utterances, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8861,
      "completion_tokens": 199,
      "total_tokens": 9060
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention any availability of code or provide links to code repositories related to the data collection, preprocessing, or dataset generation. The description only mentions using the Blizzard Challenge 2013 corpus extended with two new WaveNET vocoder voices, but no code release or reference is provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Experimental setup - Description of dataset",
          "reasoning": "The paper documents the dataset construction process in Section 3.1, describing how the Blizzard Challenge 2013 data was extended with two new WaveNET vocoder voices (Fastpitch Wavenet and Tacotron Wavenet). It details the included systems, the number of utterances (100 identical utterances synthesized by 8 systems plus original natural voice), and the rationale for inclusion. This documentation is sufficient to understand the data used for the comparisons."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7309,
      "completion_tokens": 136,
      "total_tokens": 7445
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.3",
          "Reasoning": "The Greek Podcast Corpus (GPC) is a newly introduced dataset composed of audio recordings collected from publicly available podcasts in Greek. These podcasts were recorded by humans and downloaded via web crawling and open-source tools. The audio data is single-channel, 16 kHz sampled speech, representing real human speech from different speakers and domains. This clearly establishes the data modality as audio and the origin as human-generated since it consists of original human speech recordings collected from human-operated podcast production."
        }
      ]
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8161,
      "completion_tokens": 205,
      "total_tokens": 8366
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3",
            "reasoning": "The Greek Podcast Corpus (GPC) ASR corpus is automatically transcribed using the WhisperX pipeline, which performs segmentation and transcription with Whisper large-v3 and forced alignment models without human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No description of annotation instructions or detailed human guidelines is provided, as the transcription is generated automatically by the WhisperX pipeline."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No mention of scoring rubrics or formal evaluation criteria for human annotators in the creation of the dataset; the process is fully automatic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No examples for annotators are provided since no human annotation guidelines or training were involved; the process uses automatic transcription only."
          }
        }
      ]
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9291,
      "completion_tokens": 367,
      "total_tokens": 9658
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance by a single human annotator expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention of multiple human expert annotators performing quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates that quality assurance was performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Automatic Transcription",
          "reasoning": "The dataset is transcribed using the WhisperX pipeline leveraging the Whisper large-v3 model, which is an AI model used to generate silver transcriptions. The process includes removing segments with hallucinations and filtering segments with inaccurate timestamps due to code-switching; this indicates that the AI model is effectively used as the primary source of annotations and for quality filtering."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Automatic Transcription",
          "reasoning": "Automated processing steps include Voice Activity Detection (VAD) to segment speech, post-processing to remove problematic segments based on patterns (e.g., subtitles indicating hallucinations), and filtering based on phoneme-level aligner outputs. These algorithmic and rule-based filtering steps constitute an automatic process for quality assurance of the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although no human annotation quality assurance is reported, the dataset utilizes an AI model for transcription alongside automated filtering processes, so some form of quality assurance is performed."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8909,
      "completion_tokens": 397,
      "total_tokens": 9306
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is collected from existing podcasts and not created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the speech transcriptions are generated by the Whisper large-v3 model, the audio data itself is not generated by a model but collected from podcast recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data done by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection and preprocessing",
          "reasoning": "The Greek Podcast Corpus is compiled by collecting existing podcast audio through a web crawler and podcatching tools, aggregating open-source audio data from multiple podcasts and episodes without significant modification of the audio content."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 Automatic Transcription",
          "reasoning": "The transcripts are obtained by applying the WhisperX pipeline with the Whisper large-v3 model to the collected podcast audio, thus the transcriptions are derived from existing audio data with modifications via automatic speech recognition processing and post-processing filtering."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data collection and transcription process; the origin of the data is clearly stated."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9427,
      "completion_tokens": 529,
      "total_tokens": 9956
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The GPC includes a large 3124-hour untranscribed pre-training corpus of podcast audio collected for Modern Greek, indicating it is used for pre-training large models on general speech patterns in an unsupervised or self-supervised fashion."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes fine-tuning pre-trained Whisper models rather than training models from scratch, so there is no indication that the dataset is used to train a model from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2, 3.3 and Section 4.1",
          "reasoning": "The authors use the transcribed subset of GPC (800 hours with pseudo-labels) to fine-tune pre-trained Whisper-small and Whisper-medium models in a supervised manner, demonstrating improvements in ASR performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning post-training techniques such as RLHF being applied using the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new GPC dataset is not used exclusively for evaluation; instead, it is used for fine-tuning and pre-training. The paper uses other established datasets exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The paper conducts thorough multi-domain and out-of-training analysis using the GPC test set to evaluate ASR performance across domains and data scales, indicating analysis use of the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the GPC corpus serves as a knowledge base for retrieval-augmented generation or similar augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There are documented practical usages of the GPC dataset in pre-training, fine-tuning, and analysis."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10150,
      "completion_tokens": 549,
      "total_tokens": 10699
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper introduces the Greek Podcast Corpus (GPC) which is exclusively for Modern Greek. Although the Whisper model used is multilingual, the dataset itself contains only Greek speech."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.3",
          "reasoning": "The dataset is primarily Modern Greek. Some segments with English code-switching were removed during post-processing, so the dataset does not include bilingual entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any English-only content; it is focused on Modern Greek podcasts."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Sections 3.1 to 3.3",
          "reasoning": "The Greek Podcast Corpus is composed exclusively of Modern Greek speech collected from podcasts, covering 16 distinct domains. The authors explicitly state it is a corpus for Modern Greek and remove segments containing English code-switching."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of code or programming language content within the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is speech recordings with transcriptions; it does not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech and does not include biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are included; only Modern Greek is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is well specified as Modern Greek."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly consists of human language content (Modern Greek speech), so it is not language free."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7368,
      "completion_tokens": 234,
      "total_tokens": 7602
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3",
          "reasoning": "The paper states in the abstract and in the contributions section (Section 3) that the trained model checkpoints and recipes to recreate the corpora and models are publicly available at the provided GitHub link (https://github.com/georgepar/greek_podcasts_asr). They also specify that they do not redistribute the original podcast audio data, but provide scripts for corpus reproduction, indicating that the code for data collection and preprocessing is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.1, 3.2, 3.3",
          "reasoning": "The paper provides a detailed description of the data collection process (Section 3.1), including the use of RSS feed crawling, audio downloading, format conversion, and VAD segmentation. They further document the process for building training, validation, and test splits stratified by domain (Section 3.2), and explain the automatic transcription and post-processing steps (Section 3.3). This detailed description constitutes thorough documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9189,
      "completion_tokens": 196,
      "total_tokens": 9385
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data-set and Table 1",
          "Reasoning": "The Gram Vaani ASR Challenge 2022 introduced a new dataset consisting of spontaneous telephone speech audio recordings collected from users across India using the Mobile Vaani platform. The dataset is explicitly described as audio (.mp3 files) with sampling rates ranging from 8kHz to 48kHz, and recordings were done in real conditions, implying human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data-set and Table 1",
          "Reasoning": "The labeled portion of the dataset contains human-generated transcriptions created by crowd workers recruited via the Uliza platform. These transcriptions are manually created text corresponding to the audio recordings, hence human-generated text data."
        }
      ]
    }
  },
  {
    "id": "patel22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10041,
      "completion_tokens": 178,
      "total_tokens": 10219
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1",
            "reasoning": "Transcriptions for the Gram Vaani dataset were done by crowd workers recruited via the Uliza platform, indicating multiple non-expert humans performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe providing detailed annotation instructions to the crowd workers transcribing the data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no description of scoring rubrics or criteria used in the transcription process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not discuss providing examples of annotations or transcriptions to the annotators."
          }
        }
      ]
    }
  },
  {
    "id": "patel22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11171,
      "completion_tokens": 344,
      "total_tokens": 11515
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1, 'Details of the Data-set'",
          "reasoning": "The transcriptions were done by crowd workers recruited via the Uliza platform, indicating that multiple human non-experts performed the transcription. There is no evidence suggesting these annotators were experts or subject matter knowledgeable, so this corresponds to multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance by AI models as judges is not described; the AI models are used for pseudo-transcribing unlabeled data but not explicitly for QA of existing transcriptions."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1.1, 'Analysis on the Labeled Data'",
          "reasoning": "The paper describes an automated verification process where the training data transcriptions were analyzed by decoding with an in-domain acoustic model and biased language model and comparing decoded output with reference via edit distance. This automated approach was used to estimate transcription accuracy or errors, representing an automatic verification process as quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10789,
      "completion_tokens": 406,
      "total_tokens": 11195
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 3.1.1",
          "reasoning": "The Gram Vaani ASR Challenge 2022 dataset consists of original spontaneous telephonic speech collected through the Mobile Vaani platform across India, including regional/dialectal variations of Hindi. The transcriptions were done by crowd workers via the Uliza platform, indicating that both audio and transcripts were created from scratch by human participants and annotators, not adapted or translated from existing sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any part of the dataset itself was generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data generated by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any usage of machine translation to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was collected directly from human participants via the Mobile Vaani platform and not aggregated or collected from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is presented as original spontaneous speech data, not derived from existing datasets or modified versions thereof."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data collection and annotation process, so the origin is not unknown."
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11307,
      "completion_tokens": 316,
      "total_tokens": 11623
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 3.1.2",
          "reasoning": "The Gram Vaani dataset labeled portion (100 hours train-set and 5 hours dev-set) is used to train End-to-End (E2E) ASR models from scratch without use of pre-trained models, as described in the analysis and experiments sections."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 3.3, Table 5",
          "reasoning": "The provided labeled and blind test sets are used for evaluation and benchmarking of ASR models, including model performance measurements on dev and blind sets in all challenge tracks."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1.1, Section 3.2.1, Figure 2, Figure 3",
          "reasoning": "The dataset is used to analyze the transcription quality, speaker variations, sampling frequencies, confidence of decoded utterances, and data characteristics to guide model training and data selection."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12030,
      "completion_tokens": 591,
      "total_tokens": 12621
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The dataset is described as consisting of spontaneous telephone speech in Hindi, including regional dialectal variations of Hindi, but does not mention inclusion of more than two distinct human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The dataset is only described as containing Hindi speech; while it mentions removal of English text from transcripts for model training, there is no indication that the dataset entries themselves contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.1 and 3.1.2",
          "reasoning": "The dataset is explicitly Hindi speech data and not described as containing only English. English text is removed from transcripts, indicating the data is not English monolingual."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and 3.1.2",
          "reasoning": "The dataset exclusively contains Hindi speech, including regional and dialectal variations of Hindi, without mention of other languages included in the entries. English text in transcripts is removed, indicating the dataset is monolingual non-English in Hindi."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset comprises speech and transcriptions only; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of spontaneous Hindi speech and corresponding text transcriptions; it does not include mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Dataset contains human speech only; no biological sequences or non-human communication data is mentioned."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset entries is clearly specified as Hindi."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain Hindi language speech and are therefore not language-free."
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9248,
      "completion_tokens": 215,
      "total_tokens": 9463
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories or mention that code used for data collection, preprocessing, or dataset construction is released. The dataset is referenced as provided by the Gram Vaani ASR Challenge 2022, but no code release related to dataset construction is indicated."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 'Details of the Data-set' and relevant discussions in Section 2 and 3",
          "reasoning": "The paper provides detailed descriptions of the dataset characteristics, including the origin, recording conditions, language characteristics, and segmentation information. Specifics such as duration, format, sampling rates, annotation method (crowd-sourced transcription via the Uliza platform), and presence of regional and speaker variations are documented. The paper also documents preprocessing steps such as voice activity detection segmentation for unlabeled data. These details constitute a reasonably transparent documentation of the dataset creation and characteristics."
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8057,
      "completion_tokens": 138,
      "total_tokens": 8195
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Sections 3.2 and 4.1",
          "Reasoning": "The new dataset consists of synthetic reverberant mixtures of speech created by convolving human-recorded clean speech (from the Libri-light dataset, which is human-generated audio) with simulated room impulse responses generated via an acoustic room simulator (model-generated). Thus, the audio data modality includes both human-generated (clean speech recordings) and model-generated (simulated reverberation and mixtures) components explicitly described in Sections 3.2 Acoustic Simulation and 4.1 Data Preparation."
        }
      ]
    }
  },
  {
    "id": "patterson22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8909,
      "completion_tokens": 199,
      "total_tokens": 9108
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 Acoustic Simulation",
            "reasoning": "The training data for the new task is generated synthetically using an acoustic room simulator that produces reverberant mixtures with sources at random locations and distances; this simulation process is automatic and does not involve human labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2 Acoustic Simulation",
            "reasoning": "There is no mention of instructions provided to annotators because the data generation is fully simulated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2 Acoustic Simulation",
            "reasoning": "Since the data is automatically generated, there are no scoring rubrics or annotation guidelines for human annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2 Acoustic Simulation",
            "reasoning": "No examples of annotation are provided as no manual annotation is performed; the data is created via simulation."
          }
        }
      ]
    }
  },
  {
    "id": "patterson22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10039,
      "completion_tokens": 332,
      "total_tokens": 10371
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human annotator involvement, expert or otherwise, in quality assurance for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of multiple human experts for quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single human non-expert performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information suggesting multiple non-expert humans performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic Simulation, Section 4.1 Data Preparation",
          "reasoning": "The dataset is synthetically generated using an acoustic room simulator and scripted audio processing involving convolution with simulated room impulse responses. The near and far labels for sources are assigned algorithmically based on exact source-to-microphone distances in the simulated environment. The entire dataset preparation is automated and reproducible as described, constituting automated verification or quality assurance through algorithmic means rather than human annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is implied via the consistent automated simulation and labeling approach, so it is not the case that no quality assurance or validation is performed or documented."
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9657,
      "completion_tokens": 434,
      "total_tokens": 10091
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data created entirely from scratch by human contributors. Instead, it uses existing speech datasets (Libri-light and LibriSpeech) combined with simulated room impulse responses."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated by AI or machine learning models. Instead, data is constructed by convolving clean speech recordings with acoustically simulated impulse responses."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The work does not involve translation or any data produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated through machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Preparation",
          "reasoning": "The authors use existing speech recordings from the Libri-light dataset and LibriSpeech corpus, aggregating these sources without significant modification as audio content to be used in their experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic Simulation and Section 4.1 Data Preparation",
          "reasoning": "The dataset used for training is derived by convolving the existing clean speech recordings with simulated room impulse responses generated by an acoustic room simulator. This processing modifies the original data to simulate reverberation and various distances, effectively transforming existing speech data into synthetic reverberant mixtures reflecting near and far sound sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and method of generation of the training data, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10175,
      "completion_tokens": 361,
      "total_tokens": 10536
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic Simulation, Section 4.1 Data Preparation, Section 5 Results",
          "reasoning": "The authors create a synthetic dataset using an acoustic room simulator with randomized microphone and source locations and reverberation effects to generate mixtures of near and far sounds. They use this synthetic data to train their neural network model from randomly initialized weights (Adam optimizer with training for one million steps), demonstrating usage in training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Metrics, Section 5 Results",
          "reasoning": "The synthetic datasets generated are used to evaluate separation performance using SI-SDR improvement and noise reduction metrics on test sets, demonstrating the use of the dataset for performance benchmarking and evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 Effect of distance threshold, Section 5.2 Effect of model size, Section 5.3 Effect of varied speaker count in evaluation set, Section 5.4 Effect of silent targets",
          "reasoning": "The authors analyze model performance trends across variations in distance thresholds, model size, speaker count, and presence of silent targets using their synthetic datasets, indicating use primarily for analyzing patterns and effects related to the task."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10898,
      "completion_tokens": 569,
      "total_tokens": 11467
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses speech recordings exclusively from the Libri-light dataset and LibriSpeech for validation and testing. There is no mention of multiple human languages; the speech data is English as implied by the use of LibriSpeech."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages. Only English speech is used, so bilingual is not applicable."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Preparation",
          "reasoning": "The training data is constructed using speech recordings from the Libri-light dataset with validation and test partitions from LibriSpeech. Both are well-known English speech corpora, indicating that the dataset contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses English speech recordings (Libri-light, LibriSpeech), so it is not a non-English monolingual dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are audio speech signals and simulated reverberant mixtures; there is no mention of programming or structured code-related content within the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulas appear in the paper to describe loss functions and signal processing methods but these are not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only human speech audio; there is no biological sequence or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No artificial or constructed languages are mentioned in the dataset; only English human speech is included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are explicitly documented as English, so the language is known and specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language human speech content; therefore it does contain language and is not N/A."
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8116,
      "completion_tokens": 174,
      "total_tokens": 8290
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section specifies code repository or access",
          "reasoning": "The paper does not provide any explicit links or references to publicly available code repositories for the dataset construction or data generation process. Although demos are available online, these pertain to model outputs rather than dataset or data construction code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.2 Acoustic Simulation and 4.1 Data Preparation",
          "reasoning": "The paper contains detailed documentation of the dataset construction and simulation process including the use of the acoustic room simulator, randomization of microphone and source locations, and mixing procedures. It describes the creation of near and far targets based on distance thresholds and the distributions of distances and source counts within the synthetic rooms, providing transparency into how the data was generated and processed for training and evaluation."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 2508,
      "completion_tokens": 120,
      "total_tokens": 2628
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Augmentation and Section 2.2 Augmentation with Machine Translation",
          "Reasoning": "The new dataset is generated by a data augmentation system that algorithmically converts written text into multiple variant spoken forms using rewrite rules. Additionally, neural machine translation (NMT) models translate the English spoken-written pairs into target languages. Both steps are automated, with no indication of manual human creation of these pairs, thus the data is model-generated text."
        }
      ]
    }
  },
  {
    "id": "paul22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 3360,
      "completion_tokens": 251,
      "total_tokens": 3611
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 and Section 2.2",
            "reasoning": "The new dataset is generated synthetically using a data augmentation system that applies rewrite rules to generate spoken forms from written texts and further augmented via neural machine translation models to produce spoken-written pairs in target languages. This process is described as automated and synthetic without mention of human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe any instruction guidelines provided to annotators for creating or validating these synthetic datasets since the data is generated automatically by the augmentation system and NMT models."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no indication of scoring rubrics or annotation evaluation measures applying to human annotations in dataset creation as the data is produced by automatic augmentation and translation processes."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 2 and Table 3",
            "reasoning": "The paper provides multiple examples of the generated spoken-written pairs from the data augmentation system and the machine translation pipeline, illustrating the output data format and variations."
          }
        }
      ]
    }
  },
  {
    "id": "paul22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 4490,
      "completion_tokens": 288,
      "total_tokens": 4778
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple non-expert annotators conducted QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although neural machine translation models are used for data augmentation, the paper does not describe AI models being used as a quality assurance judge for the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper describes filtering out translated spoken-written pairs where text normalization forms are not intact and choosing NMT models with reasonable BLEU scores to ensure quality of generated data. This filtering and validation can be considered an automated verification process using algorithmic techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automatic filtering process as quality assurance; thus, QA is present."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 4108,
      "completion_tokens": 444,
      "total_tokens": 4552
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of spoken-written pairs entirely from scratch by human contributors. The data augmentation and text normalization system is automated and no explicit human-generated novel data is described."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "Section 2.1 describes a data augmentation method that generates diversified spoken forms for written text using an automated process involving rewrite rules and a data augmentation system. This produces new spoken-written pairs synthetically and is generated by a model or automated system rather than humans. Hence, this is new data generated by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of human translations being used or the target data being acquired via human translators."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "Section 2.2 details the use of neural machine translation models to translate English spoken-written pairs generated by the augmentation system into other target languages. This process generates new spoken-written pairs in other languages via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as directly collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The data augmentation system derives new spoken forms by applying rewrite rules to existing written text and converting them into multiple spoken variations. This represents data derived from existing written texts with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation methodology is explicitly described, thus not unknown or undocumented."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 4626,
      "completion_tokens": 410,
      "total_tokens": 5036
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new dataset for pre-training large models typically in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the models are trained from scratch on the new dataset without pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The paper describes training ITN models with synthetic spoken-written pairs generated by their data augmentation and machine translation pipeline to improve inverse text normalization performance, which involves supervised fine-tuning of models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of reinforcement learning post-training methods using the new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is used primarily for training, not exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the new dataset primarily for analysis of trends or patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not serve as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The usage of the new dataset is clearly described in the paper for supervised fine-tuning; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 5349,
      "completion_tokens": 487,
      "total_tokens": 5836
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.2 Augmentation with Machine Translation",
          "reasoning": "The new dataset introduced is synthesized via neural machine translation from English spoken-written text pairs to multiple target languages, explicitly including French, Italian, and Spanish as shown in experimental results. Hence, it contains entries with more than two human languages, qualifying it as multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Augmentation",
          "reasoning": "The initial synthetic spoken-written text pair dataset generated from English text normalization augmentation contains only English language content before the machine translation step."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains spoken-written text pairs, not programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset includes numerical and symbolic expressions in text form (e.g., '20:20'), it does not explicitly contain formal mathematical notation or symbolic logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly documented and identified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain human natural language text."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 2567,
      "completion_tokens": 146,
      "total_tokens": 2713
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any link to publicly available code repositories or mention that the code used for the data augmentation system or the machine translation data generation is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 and Section 2.2",
          "reasoning": "The paper documents the dataset creation process in detail, including the data augmentation pipeline that generates diverse spoken forms from written text, and the machine translation approach to produce spoken-written pairs in target languages. Steps such as extraction, reformatting, augmentation with rewrite rules, and filtering of translated pairs are described, providing transparency about the dataset creation method."
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7681,
      "completion_tokens": 131,
      "total_tokens": 7812
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3",
          "Reasoning": "The SIAEW Corpus consists of Spanish-accented English word tokens created by recording bilingual speakers producing words, from which accented-to-native continua were generated by splicing segments and applying acoustic weighting as per a computational procedure. Hence, the audio data was originally human speech recordings (human generated) subsequently manipulated algorithmically to generate continuums with graded accentedness (model generated). The paper describes selection of speakers, recordings, and the method to generate accented tokens, confirming the mixed origin."
        }
      ]
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8533,
      "completion_tokens": 226,
      "total_tokens": 8759
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3",
            "reasoning": "The annotations for the new SIAEW Corpus were obtained through perceived accentedness judgements by 17 native English listeners (non-expert annotators) in a categorisation task. These listeners are multiple non-expert humans as they are native English speakers but not described as experts in phonetics or linguistics."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "The participants performed a two-alternative forced choice categorisation task with specific instructions to classify tokens as foreign-accented or native, as described in the experiment protocol."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No explicit scoring rubrics for annotation quality or categorisation criteria beyond forced-choice instructions are described in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No mention of example annotations or trial exemplars provided to annotators is present in the paper for this annotation task."
          }
        }
      ]
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9663,
      "completion_tokens": 509,
      "total_tokens": 10172
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the study involved native English listeners performing accentedness judgements, there is no indication that these annotators were experts or belonged to the target demographic specifically as experts, nor is there indication that their judgement was considered expert annotation for QA purposes."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single non-expert human."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3, last three paragraphs",
          "reasoning": "The quality assurance process for the dataset involved a cohort of 17 native English listeners (non-experts in annotation or phonetics) who performed a two-alternative forced choice task categorizing each stimulus as native or non-native accented. These listeners were students without advanced knowledge of Spanish and non-bilingual; thus they qualify as non-expert human annotators. Multiple such annotators contributed to the validation of perceptual ratings used to normalize the accentedness steps and select speakers. Hence, quality assurance was performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset generation involved algorithmic acoustic blending and logistic function fitting to transform acoustic steps into iso-perceptual steps, this process relates to dataset creation rather than quality assurance. The perceptual quality assurance relies on human listener judgments, not automated verification of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a quality assurance process involving multiple human non-expert listeners performing perceptual categorization to validate the accentedness gradation and select suitable speakers for the finalized dataset."
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9281,
      "completion_tokens": 449,
      "total_tokens": 9730
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The SIAEW Corpus was created by recording bilingual speakers producing English words and Spanish non-words, specifically recruited and evaluated for minimal accent. The recordings are original speech data produced entirely by human speakers from scratch. The paper details the recording process and speaker evaluation, indicating these are original human-generated speech stimuli, not translations or adaptations of pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated entirely by AI or machine learning models. Although the paper uses computational techniques such as acoustic manipulation and blending derived from existing recordings, the original data are human speech recordings, not model-generated data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was created via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine translation being used to create the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources; it was newly recorded from human speakers."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 and Section 3",
          "reasoning": "The original human recordings are manipulated via acoustic splicing and weighting methods to create continua of accented speech tokens with graded levels of accentedness. This involves transforming and generating new stimuli that are adaptations based on existing human recordings, thus making the final dataset 'derived' due to these modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data generation methodology; thus, this label does not apply."
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9799,
      "completion_tokens": 316,
      "total_tokens": 10115
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The SIAEW Corpus is used to evaluate the perception of segmental foreign accent by listener cohorts with different L1 backgrounds, as shown in the native listener categorization task detailed in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The dataset enables analysis of trends and differences in accent perception across listener groups differing in phonological inventory and familiarity with the accented language, as discussed in Section 4."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10522,
      "completion_tokens": 491,
      "total_tokens": 11013
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset, called SIAEW Corpus, contains English words produced with Spanish-accented English sounds, involving only two human languages: English and Spanish; thus it is bilingual but not multilingual (more than two languages)."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The SIAEW Corpus contains lexical tokens of English words produced with graded Spanish-accented English segments. The data involves precisely two human languages: English (native language of the words) and Spanish (accent language) as evidenced throughout Section 3."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "Although the words are English, the collected speech tokens contain Spanish-accented English segments, representing accentedness from a second language (Spanish). Therefore, the dataset is not monolingual English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset does not contain only non-English language entries; it contains English words pronounced with Spanish-accented features, maintaining the English lexical content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists entirely of speech recordings and no programming code or structured code data is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of speech tokens and contains no mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech data only and does not include any biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset involves fictional or constructed languages; it involves natural human languages English and Spanish."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages involved in the dataset are clearly documented as English and Spanish accented English; thus language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of speech recordings of human languages (English and Spanish-accented English); therefore, it includes language."
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7740,
      "completion_tokens": 195,
      "total_tokens": 7935
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3",
          "reasoning": "The paper provides a download link for the SIAEW Corpus dataset itself (https://doi.org/10.5281/zenodo.6371655) but does not mention or provide any code repository or link for the code used to generate the dataset or any data processing scripts."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The dataset creation process is documented in detail across sections 2 and 3. Section 2 describes the methodology for generating iso-perceptual steps from acoustic continua using logistic fitting and inversion. Section 3 details the selection of target consonants and vowels, the recording procedure with bilingual speakers, the generation of continua, native listener experiments for perceptual testing, and selection of speakers for the final corpus. The paper thoroughly explains the procedures used, enabling understanding of dataset construction."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8319,
      "completion_tokens": 183,
      "total_tokens": 8502
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2",
          "Reasoning": "The new dataset introduced in this paper is the Chilean Spanish AD corpus recorded by Universidad de Chile and Hospital del Salvador (Chile), consisting of semi-spontaneous speech recordings from Chilean Spanish speakers describing the \"cookie theft picture\". These audio recordings were manually segmented and transcribed, indicating human involvement in recording and creation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2",
          "Reasoning": "The Chilean Spanish AD dataset includes manual transcriptions (text) of the semi-spontaneous speech recordings. These transcriptions were created by humans as they were manually segmented and transcribed, demonstrating human generation of text data."
        }
      ]
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9171,
      "completion_tokens": 198,
      "total_tokens": 9369
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1.2",
            "reasoning": "The Chilean Spanish AD dataset recordings were evaluated by an expert neurologist following clinical criteria, and the recordings were manually segmented and transcribed, indicating annotation by a single human expert."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1.2",
            "reasoning": "The data were manually segmented and transcribed by experts following established clinical criteria, implying specific instructions were provided for transcription and segmentation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention any scoring rubrics or annotation guidelines for segmentation or transcription in the new Chilean Spanish AD dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention or evidence of annotation examples or samples provided in the paper for the Chilean Spanish AD dataset annotation."
          }
        }
      ]
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10301,
      "completion_tokens": 183,
      "total_tokens": 10484
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1.2 Chilean Spanish AD",
          "reasoning": "The Chilean Spanish AD dataset was evaluated by an expert neurologist following recognized clinical criteria for Alzheimer's Disease, indicating quality assurance by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly mentions expert evaluation for the Chilean Spanish dataset, so quality assurance is documented."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9919,
      "completion_tokens": 412,
      "total_tokens": 10331
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1.2 Chilean Spanish AD",
          "reasoning": "The paper describes a new Spanish Alzheimer's Disease dataset composed of speech recordings and manual transcriptions from 39 Chilean Spanish speakers. This dataset was recorded by the Universidad de Chile and Hospital del Salvador (Chile) and is presented as a newly created data collection by human participants, not derived from or translated from other datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data in the paper is described as generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper that any dataset content was produced by translating material from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention usage of machine translation to create data in any dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are either original collections or publicly available corpora; the paper does not describe assembling data from multiple existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset is described as being transformed or adapted from existing datasets; the Spanish dataset was newly recorded."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origins of the datasets used, especially for the Spanish dataset; therefore, the data source is specified."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10437,
      "completion_tokens": 376,
      "total_tokens": 10813
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper uses the new Chilean Spanish AD dataset to fine-tune pre-trained embeddings for Alzheimer's Disease classification through supervised learning methods. Section 3.1 describes classification experiments with the Spanish dataset alone, and Section 3.2 reports transfer learning approaches where English pre-trained embeddings are fine-tuned for Spanish data, demonstrating use for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The Chilean Spanish dataset is also used as an evaluation benchmark to assess model performance for Alzheimer's detection. Classification performance (F-score) is reported for this dataset alone and in combination with English data, evaluating the impact of transfer learning."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4",
          "reasoning": "The dataset is used for analyzing language dependencies and differences between linguistic and acoustic embeddings for AD detection in Spanish speech, as shown in Sections 3.1 and 4. The paper discusses patterns, such as stronger acoustic importance in Spanish and language differences influencing embeddings."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The Chilean Spanish AD dataset introduced by the authors is directly used in supervised fine-tuning, evaluation, and analysis as detailed in multiple sections of the paper."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11160,
      "completion_tokens": 611,
      "total_tokens": 11771
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 (Data)",
          "reasoning": "The two datasets introduced are separate: one comprises English speakers (Pitt Corpus) and the other comprises Spanish speakers (Chilean Spanish AD corpus). There is no indication that any single dataset entry contains multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 (Data)",
          "reasoning": "Neither dataset contains entries with exactly two languages. The English dataset contains English only, and the Spanish dataset contains Spanish only. The study uses both languages but not combined within same data entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (The Pitt Corpus)",
          "reasoning": "The Pitt Corpus subset used in this study consists exclusively of native American English speakers describing a picture. Thus, the dataset entries contain English speech and transcripts only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1.2 (Chilean Spanish AD)",
          "reasoning": "The Chilean Spanish AD corpus consists exclusively of native Spanish speakers from Chile describing the same picture. The speech and transcripts are in Spanish only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced are composed of human speech and transcripts, not containing any code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets consist of audio speech recordings and text transcripts describing a picture; no entries contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not include biological sequences or non-human communication; the content is human natural language speech."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication or mention of any fictional or constructed language data in the introduced datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages of the datasets are clearly documented and explicitly described as English (Pitt Corpus) and Spanish (Chilean Spanish AD)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human speech and corresponding transcripts, so they contain language data."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8378,
      "completion_tokens": 190,
      "total_tokens": 8568
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper suggests the availability of code for dataset construction.",
          "reasoning": "The paper does not provide any direct link, repository, or indication that the code related to the construction, collection, or preprocessing of the new datasets (specifically the Chilean Spanish AD corpus) is publicly available. The datasets are described, but no code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1.2 (Chilean Spanish AD)",
          "reasoning": "The paper provides detailed information on the Chilean Spanish AD dataset: number of subjects, demographic details, speech task description, clinical evaluation criteria, recording and transcription process, and average duration of recordings. Although the dataset is previously reported in [18], the key aspects relevant to dataset creation and characteristics are documented within the paper to enable understanding and reproducibility to some extent."
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6906,
      "completion_tokens": 353,
      "total_tokens": 7259
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 4",
          "Reasoning": "The dataset includes audio recordings of speech from 79 participants captured during stress induction tasks, recorded using human participants speaking and recorded by audio devices."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 4",
          "Reasoning": "The dataset includes video recordings (face and multiple postures) of 79 participants performing tasks, captured by cameras operated and recording the humans during the experiment."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 4 and Section 2.5",
          "Reasoning": "Physiological data streams were captured from sensors worn by participants, including electrodermal activity, skin temperature, acceleration, ECG and heart activity from Empatica E4 and Faros 180 devices, reflecting human physiological signals during tasks."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 4",
          "Reasoning": "Psychological questionnaires self-reported by participants, and performance results (Rebus performance) are provided as tabular data reflecting manually collected human-generated responses and measurements."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.5 and overall protocol sections",
          "Reasoning": "Semantic segmentation labels were manually annotated using the BESS-TiANNO tool by humans, creating structured text labels contextualizing portions of the session."
        }
      ]
    }
  },
  {
    "id": "pesan24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7758,
      "completion_tokens": 207,
      "total_tokens": 7965
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.5",
            "reasoning": "Manual semantic labeling of session segments was conducted using the BESS-TiANNO tool, implying involvement of human annotators with expertise to assign semantic labels accurately."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 and 4.5",
            "reasoning": "The annotation of semantic segments corresponds to specific, well-defined parts of the experimental protocol and is performed manually with the BESS-TiANNO tool, indicating instructions were likely provided to ensure consistency in labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly discussed",
            "reasoning": "There is no explicit mention of scoring rubrics or detailed evaluation criteria for the manual semantic labeling task in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly discussed",
            "reasoning": "The paper does not provide examples of annotated segments or examples in annotation guidelines for semantic labeling."
          }
        }
      ]
    }
  },
  {
    "id": "pesan24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8888,
      "completion_tokens": 287,
      "total_tokens": 9175
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4.4 and Section 4.5",
          "reasoning": "Semantic labeling of segments was performed manually using the BESS-TiANNO tool. The paper mentions manual semantic labeling but does not specify multiple annotators or expert teams; the detailed process implies expert involvement (e.g., ECG expert verifying QRS complex detection), suggesting that at minimum a single human expert conducted quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.4 and 4.1",
          "reasoning": "Validation includes automatic preprocessing steps such as speaker-level volume normalization for audio, timestamp alignment for video, automated artifact removal for physiological signals, and algorithmic QRS complex detection using combination methods. These are automated or algorithmic verification processes ensuring data quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes multiple validation and quality assurance steps for each modality, including both manual expert verification and automated preprocessing; thus, QA is documented and present."
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8506,
      "completion_tokens": 358,
      "total_tokens": 8864
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.2, and 3",
          "reasoning": "The BESST dataset was created by collecting new experimental data from human participants performing stress-inducing tasks, such as Hand Immersion Task and Reading Span Task. The participants were native Czech speakers and the data, including audio, video, physiological signals, and psychological assessments, were recorded specifically for this dataset as described in the protocol sections and dataset description. This indicates original content generated entirely from scratch by human subjects."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "Derived data streams such as Voice Activity Detection segmentations, semantic segmentations, and RR interval calculations were produced by applying post-processing and transformations to native recordings (audio, physiological data). These processed data representations are based on the original recorded data but transformed non-trivially to enhance usability."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9024,
      "completion_tokens": 248,
      "total_tokens": 9272
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 and Section 6",
          "reasoning": "The BESST dataset is introduced as a resource to facilitate research and analysis on the interplay between stress, cognitive load, and speech. The paper discusses the dataset's design, collection protocols, and its role in enabling investigations into stress-induced speech variations, cognitive load effects on speech, and multimodal interaction rather than describing direct use for training or evaluation of machine learning models. Thus, the dataset is primarily intended for analyzing patterns and relationships in multimodal stress and speech data."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9747,
      "completion_tokens": 427,
      "total_tokens": 10174
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "The dataset exclusively contains speech data recorded from native Czech speakers, with all content in the Czech language only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "No indication of data collected in exactly two languages; the dataset is solely in Czech."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "The dataset content is not in English; it is exclusively Czech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "The dataset consists solely of Czech speech data, confirmed as native Czech speakers only. This indicates the dataset is monolingual and non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No programming or structured code-related content is part of the dataset; all data pertains to speech, physiological, and video signals."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No mathematical or logical notation is present as dataset entries; only physiological signals and speech data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "Though physiological signals like ECG and skin conductance are included, the data does not include biological sequences or non-human communication systems as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Section 2.6 and overall dataset description",
          "reasoning": "No mention or indication of artificial or constructed languages present in the dataset; only natural Czech language is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "The language of the dataset is explicitly specified as Czech."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech data, thus it does contain language."
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6965,
      "completion_tokens": 203,
      "total_tokens": 7168
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 5",
          "reasoning": "The paper explicitly states in Section 5 that the BESST dataset, along with relevant code and tools, is accessible for research and development at https://speech.fit.vutbr.cz/besst. This indicates that code related to the dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2, 3, and 4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including participant recruitment and criteria (Section 2.1), stress induction protocols (Section 2.2), session procedures (Section 2.3), psychological measurements (Section 2.4), data streams and devices used (Section 2.5, Table 3), dataset structure and subsets (Section 3 and 3.1), and validation and preprocessing steps of all modalities (Section 4). This extensive documentation covers the dataset creation comprehensively."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6959,
      "completion_tokens": 419,
      "total_tokens": 7378
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 and 3",
          "Reasoning": "The dataset comprises over 87,000 utterances spoken by 1,000 Vietnamese celebrities, collected from real-world video sources on YouTube and TikTok (Section 2.4 and 3). These audio utterances capture spontaneous speech under noisy conditions, and are thus human generated speech recordings from natural conditions, not synthesized or generated by any model."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 and 2.5",
          "Reasoning": "The dataset construction pipeline involves crawling videos from YouTube and TikTok channels of Vietnamese celebrities as the raw data source. These videos are human-generated content (interviews, podcasts, shows) recorded by humans and posted online. The authors applied visual-aided processing techniques such as face tracking, face verification, and active speaker verification to process the videos, confirming that the videos themselves originate from human-generated content online."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "During the dataset construction, images were crawled from Google Images and TikTok profile pictures that depict the faces of persons of interest (POIs). These images are photographs manually captured by humans and publicly available online, hence their origin is human generated. They are used for face verification steps in the pipeline."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.6; Table 3 and 4",
          "Reasoning": "The dataset includes annotated labels for each speaker, specifically gender and dialect information, which were manually assigned by human annotators through majority voting on sampled utterances. Therefore, the tabular data representing speaker metadata (gender, dialect, speaker ID) is human-generated annotation data included in the dataset."
        }
      ]
    }
  },
  {
    "id": "pham23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7811,
      "completion_tokens": 210,
      "total_tokens": 8021
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.6",
            "reasoning": "Section 2.6 states that gender and dialect labels for each speaker were manually created by 3 annotators originating from different Vietnamese regions to classify dialects by majority voting on 5 utterances per speaker, indicating multiple human annotators involved rather than experts or single annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.6",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to the human annotators for gender and dialect labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.6",
            "reasoning": "There is no mention of specific rubrics or scoring criteria used for annotation in gender or dialect labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.6",
            "reasoning": "No examples of annotations or illustrative samples are shown or mentioned in the paper for the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "pham23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8941,
      "completion_tokens": 407,
      "total_tokens": 9348
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is a subject matter expert or member of the target demographic alone."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.6 Annotating speaker genders and Vietnamese dialects",
          "reasoning": "The dataset's gender and dialect labels are manually annotated by majority voting among 3 annotators from three different Vietnamese regions (Northern, Central, Southern). This indicates multiple human annotators from the target demographic participated in quality assurance for this aspect of the data."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information indicates quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as being from different Vietnamese regions, which implies relevant expertise or at least native knowledge, thus they are not considered non-experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.5 Audio-visual processing",
          "reasoning": "The paper utilizes Wav2Vec2, a pre-trained AI speech representation model, for duplication detection and audio similarity calculation to remove duplicates and invalid speakers, indicating AI-based QA methods."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.5 Audio-visual processing and Section 2.3 Image crawling and processing",
          "reasoning": "The dataset construction pipeline involves automatic processes such as face detection (RetinaFace), face embedding extraction (ArcFace), clustering (k-means), and automated outlier detection using interquartile range rules for noisy utterance removal, constituting automated rule-based quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes multiple quality assurance processes involving human annotators, AI models, and automatic rule-based filtering, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8559,
      "completion_tokens": 458,
      "total_tokens": 9017
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by human contributors; instead, it is collected from existing media sources such as YouTube and TikTok."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated or synthesized by AI or machine learning models; it is collected from real-world videos of speakers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involves translating content from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involves translating content using machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset collection & construction), specifically 2.4 Video crawling and 2.3 Image crawling and processing",
          "reasoning": "The dataset is collected from existing online media sources (YouTube, TikTok, Google Images) without creation of new content. It involves crawling and downloading pre-existing videos, images, and audio of Vietnamese speakers."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.5 Audio-visual processing and Section 2.6 Annotating speaker genders and Vietnamese dialects",
          "reasoning": "The dataset is based on existing audio-visual data but includes significant processing and transformations such as removing duplicates, face detection, clustering, filtering, active speaker verification, merging duplicated speakers, and manual annotation of gender and dialect labels. These processing steps transform the raw collected data into a refined, annotated dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset's data sources and construction methods are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9077,
      "completion_tokens": 496,
      "total_tokens": 9573
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using Vietnam-Celeb for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Experimental setup and Section 4.3 Experimental results",
          "reasoning": "The dataset (Vietnam-Celeb-T) is used to train ECAPA-TDNN models from scratch. The paper details training procedures and reports results comparing models trained from scratch on Vietnam-Celeb-T versus other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Experimental setup and Section 4.3 Experimental results",
          "reasoning": "The dataset is used for supervised fine-tuning of pre-trained VoxCeleb models, showing that fine-tuning with Vietnam-Celeb-T improves performance on Vietnamese speaker verification tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning based post-training methods using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.4 Final dataset and Section 4.3 Experimental results",
          "reasoning": "The dataset is split into training and test subsets (Vietnam-Celeb-E and Vietnam-Celeb-H) used for benchmarking and performance evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is used primarily for analysis of trends or patterns separate from training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or similar tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has clear documented uses in training (from scratch), supervised fine-tuning, and evaluation."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9800,
      "completion_tokens": 584,
      "total_tokens": 10384
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 and Table 1",
          "reasoning": "The Vietnam-Celeb dataset contains only Vietnamese language utterances, as explicitly stated in Section 3 and Table 1; there is no mention of other languages included. The paper mentions the dataset is specifically for Vietnamese speaker recognition."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 and Table 1",
          "reasoning": "The dataset contains only Vietnamese language speaker utterances. There is no indication or mention of it containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3 and Table 1",
          "reasoning": "The dataset contains Vietnamese speech only, not English, so it is not monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 3; Table 1",
          "reasoning": "The dataset exclusively contains spoken Vietnamese language data. The entire dataset is described as a Vietnamese speaker recognition dataset, with no mention of any other languages. Hence, it is monolingual (non-English) with the language being Vietnamese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper includes mathematical formulas for processing steps, the dataset itself does not contain mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech data only; there is no biological or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes only natural Vietnamese language; there is no indication of any constructed or fictional language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly documented as Vietnamese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data (Vietnamese), so it is not applicable."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7018,
      "completion_tokens": 204,
      "total_tokens": 7222
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 and Conclusion",
          "reasoning": "The paper mentions that Vietnam-Celeb is publicly available under a GitHub repository; however, it does not provide any link or explicit mention of the code repository for the dataset construction pipeline or related scripts. The paper describes the data collection and preprocessing pipeline in detail, but does not indicate that the code for these processes is publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 2 (Dataset collection & construction) and Section 3 (The Vietnam-Celeb dataset). It explains each stage of the data collection, preprocessing, filtering, and annotation pipeline, including the visual-aided techniques used (face detection, verification, active speaker verification), the handling of duplicates, and gender and dialect annotation. This detailed documentation supports reproducibility and transparency of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9262,
      "completion_tokens": 299,
      "total_tokens": 9561
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, paragraph 1; Table 1",
          "Reasoning": "The datasets m-MUSIC-AVQA and m-AVQA are built upon the existing benchmark AVQA datasets MUSIC-AVQA and AVQA which contain videos as one modality. These videos were originally recorded and provided as part of these datasets, indicating human involvement in the acquisition of the video data."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, paragraph 1; Table 1",
          "Reasoning": "The audio modality in the datasets originates from the original benchmark datasets MUSIC-AVQA and AVQA, which contain natural audio recordings corresponding to the videos. The audio thus comes from human-recorded real-world events captured in the videos, not generated by models."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3, paragraphs 2 and 3",
          "Reasoning": "The question-answer pairs in multiple languages were created by machine-translating the original question-answer pairs from the English datasets MUSIC-AVQA and AVQA using Google's machine translation API, an algorithmic system, followed by human verification and refinement. Hence, the textual QA data in various languages is model-generated (machine translation) with subsequent human curation."
        }
      ]
    }
  },
  {
    "id": "phukan24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10114,
      "completion_tokens": 243,
      "total_tokens": 10357
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3: Multilingual AVQA Datasets",
            "reasoning": "The multilingual datasets m-MUSIC-AVQA and m-AVQA were created by translating original English datasets using machine translation (Google Translate API), followed by human verification and refinement of the translated question-answer pairs to ensure accuracy."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3: Multilingual AVQA Datasets",
            "reasoning": "The paper does not mention providing specific detailed annotation instructions or guidelines to the human annotators involved in the verification and refinement process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3: Multilingual AVQA Datasets",
            "reasoning": "There is no description of scoring rubrics or criteria used for annotation or verification in the multilingual datasets creation process."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 2 and Section 3: Multilingual AVQA Datasets",
            "reasoning": "Examples of m-MUSIC-AVQA entries are provided (Figure 2) illustrating the translated question-answer pairs, which implies that examples were included for annotation verification or demonstration."
          }
        }
      ]
    }
  },
  {
    "id": "phukan24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11244,
      "completion_tokens": 382,
      "total_tokens": 11626
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper about multiple human expert annotators performing quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that quality assurance was performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3, paragraph discussing dataset creation and translation",
          "reasoning": "The multilingual datasets were created by translating the original English question-answer pairs into seven additional languages using Google's machine translation API, followed by human verification and refinement of the translated question-answer pairs. Although the paper does not explicitly state the number or expertise of the human annotators, the mention of 'human verification and refinement' implies involvement of multiple human non-experts rather than experts, as no expertise detail is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process described does not include the use of AI models as judges for verifying the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3, last paragraph",
          "reasoning": "The dataset creation process involved machine translation using Google's machine translation API, which constitutes an automated process for generating multilingual question-answer pairs. Additionally, standard machine translation evaluation metrics such as BLEU, ROUGE, and METEOR were used to confirm the reliability of translations, indicating automated verification techniques employed for quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a quality assurance process involving both machine translation and human verification and refinement, so QA is present."
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10862,
      "completion_tokens": 471,
      "total_tokens": 11333
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new data was created entirely from scratch by human contributors. The new datasets are derived from existing datasets and involve translation rather than original human annotation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not generated by AI or machine learning models creating new content from scratch. Instead, they leverage machine translation applied to existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "Section 3, paragraphs 2 and 3",
          "reasoning": "Although the paper mentions human verification and refinement of machine-translated question-answer pairs, the primary translation to create the datasets was performed by machine translation (Google Translate). There is no indication that the translations were entirely produced by human translators originally."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 3, paragraphs 2 and 3",
          "reasoning": "The multilingual datasets m-MUSIC-AVQA and m-AVQA were created by translating existing English question-answer pairs from benchmark AVQA datasets into seven additional languages using Google's machine translation API, as explicitly stated in Section 3."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the datasets are based on existing sources (MUSIC-AVQA and AVQA), the paper specifies translation to produce new multilingual datasets rather than simple aggregation without modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3, paragraphs 2 and 3",
          "reasoning": "The multilingual datasets are created by transforming existing English datasets via translation into new languages, which constitutes deriving new data from existing sources with adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly described as machine translation from existing datasets; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11380,
      "completion_tokens": 311,
      "total_tokens": 11691
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Methodology) and Section 5 (Experimental Results)",
          "reasoning": "The multilingual AVQA datasets (m-MUSIC-AVQA and m-AVQA) are used to fine-tune and train downstream models (MERA suite models) for supervised learning to answer questions in multiple languages, as described in Sections 4 and 5."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Experimental Results)",
          "reasoning": "The datasets are used for evaluation to benchmark model performance across languages and question types, as presented in Table 3 and related discussions."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Multilingual AVQA Datasets), Table 2 and Section 5 (Experimental Results)",
          "reasoning": "The authors analyze question types and answer distributions (Table 2) and investigate model performance trends across languages and question types (Section 5). Hence, the datasets support analysis of patterns and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12103,
      "completion_tokens": 520,
      "total_tokens": 12623
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3, Introduction, and Abstract",
          "reasoning": "The paper introduces two new multilingual AVQA datasets: m-MUSIC-AVQA and m-AVQA. These datasets are created by translating existing English AVQA datasets into seven additional languages: French, Hindi, German, Spanish, Italian, Dutch, and Portuguese, along with the original English. Hence, the datasets contain entries in eight human languages, making them multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain entries for more than two languages (eight languages), so the bilingual label does not apply."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors explicitly extend the dataset to multiple languages and do not present a new monolingual English-only dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are available in multiple languages, not restricted to a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of natural language questions and answers but no mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are included in the datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain entries in any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included are explicitly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language entries in multiple human languages."
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9321,
      "completion_tokens": 168,
      "total_tokens": 9489
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 3",
          "reasoning": "The paper explicitly provides a GitHub link (https://github.com/swarupbehera/mAVQA) for both the dataset and code. This indicates the code related to creating the multilingual AVQA datasets from existing datasets (including translation steps) is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Multilingual AVQA datasets)",
          "reasoning": "The paper clearly documents the dataset creation process, including the source datasets (MUSIC-AVQA and AVQA), the use of Google's machine translation API to translate questions and answers into seven additional languages, and mentions human verification and refinement of translations. Statistical analyses and detailed descriptions are also included, providing transparency about the dataset creation."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7431,
      "completion_tokens": 160,
      "total_tokens": 7591
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3",
          "Reasoning": "The authors introduced newly extracted additional audio data from the original MALACH speech corpus, increasing training and test data by 18 hours and 3 hours respectively, along with 150 hours of unlabeled audio. This is human generated data recorded from Holocaust survivor interviews using microphones."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3",
          "Reasoning": "Manual transcriptions (text) of the additional data were processed and segmented, including annotations such as speaker turns and overlaps. These transcriptions were manually created by human transcribers associated with the original dataset."
        }
      ]
    }
  },
  {
    "id": "picheny23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8283,
      "completion_tokens": 204,
      "total_tokens": 8487
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3",
            "reasoning": "Section 3 describes manual speaker turn annotation and transcription processes for the additional MALACH data; long pauses and speaker overlap were manually annotated, indicating human expert involvement in producing accurate transcriptions."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "The paper indicates manual speaker turn and pause annotations, as well as transcription involving non-English named entities marked with '@', implying specialized instructions were provided to transcribers to handle these challenges."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "There is no mention of scoring rubrics or rating schemes for annotation quality; the focus is on transcription and segmentation rather than subjective scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No explicit annotation examples or sample annotated data excerpts are provided in the paper describing the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "picheny23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9413,
      "completion_tokens": 347,
      "total_tokens": 9760
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the dataset involved a single human expert annotator or transcriber."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper that multiple human experts conducted quality assurance or validation of the dataset transcriptions or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not detail any quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide explicit information that multiple non-expert humans performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used extensively for speech recognition experiments, the paper does not describe that an AI model was used as a judge for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated processes such as Kaldi VAD segmentation and scoring tools (NIST SCTK) are used for processing and evaluation, the paper does not document any automated verification being used as a quality assurance process for the dataset annotations themselves."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied to validate or verify the accuracy of the transcription annotations in the new data extracted. It only mentions manual transcription and use of existing transcripts but does not discuss any verification, correction, or annotation validation procedures. Therefore, no quality assurance process is documented for the dataset."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9031,
      "completion_tokens": 529,
      "total_tokens": 9560
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3: New Data Extraction",
          "reasoning": "The paper describes extracting and processing approximately 18 hours of additional transcribed data and 3 hours of test data from the original MALACH audio tapes. These manual transcriptions were created by human annotators working with the recordings and XML files, including manual annotation of speaker turns and transcriptions with special prefix notation for named entities. This constitutes new human-created data beyond the previous release."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 5: Use of Pseudo-labeled Data",
          "reasoning": "The authors generate pseudo-labels for previously untranscribed audio data by using a fine-tuned speech recognition model to automatically transcribe unlabeled data. These pseudo-labels are thus newly generated data by a model and then used to augment training data. This process creates new machine-generated transcriptions from unlabeled audio."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3: New Data Extraction",
          "reasoning": "The newly extracted data comes from the existing large MALACH archive of recordings and transcripts. The authors processed and aggregated this existing data (including unlabelled speech and partially transcribed data) to produce new training and test sets for speech recognition experiments. This reuse and aggregation of existing archival data is characteristic of collated data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the data was processed and segmented, the paper does not describe the new datasets as transformations or adaptations derived from existing data beyond aggregation and transcription. The pseudo-labels are new model-generated data rather than derived in the sense of modification of existing text data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins and generation methods of the new datasets introduced in the paper are clearly described."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9549,
      "completion_tokens": 585,
      "total_tokens": 10134
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe usage of the MALACH dataset for pre-training large models in an unsupervised or self-supervised manner. Instead, pre-training is done on other corpora with MALACH used for fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "End-to-end ASR models trained from scratch solely on MALACH data were attempted (Section 4.2), but results were worse than fine-tuned models, and the main focus is on fine-tuning pretrained models rather than training from scratch on MALACH."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.4 and 5",
          "reasoning": "The MALACH dataset is used for supervised fine-tuning of large pretrained wav2vec2 models (Section 4.4). Both the original labeled MALACH data and pseudo-labeled data generated from unlabeled MALACH data are used in fine-tuning to improve recognition performance (Section 5)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of using the MALACH dataset for reinforcement learning post-training techniques such as RLHF is made in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.1, 4.3, 4.5, and 5",
          "reasoning": "The MALACH data is extensively used for evaluation and benchmarking word error rates (WER) of various speech recognition systems (ESPNET architectures, Whisper, wav2vec2 models) and pseudo-labeling methods throughout Sections 4 and 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6",
          "reasoning": "Section 6 provides error analysis and discusses sources of remaining errors, effects of segmentation, challenges with named entities, and accents in MALACH, demonstrating use of the dataset for detailed analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the MALACH dataset as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents practical and multiple uses of the MALACH dataset; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10272,
      "completion_tokens": 600,
      "total_tokens": 10872
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper describes the MALACH Interviews and Transcripts English corpus, which contains English language interviews. It mentions foreign named entities but does not state that multiple human languages appear in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is described as English language survivor testimonies. While there are many foreign named entities (names and places), the content is in English and does not include a second fully supported human language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 1 (Introduction), Section 3 (New Data Extraction), Section 4 (Recognition Systems)",
          "reasoning": "The paper consistently refers to the MALACH English corpus containing English interviews of Holocaust survivors. The data used for speech recognition experiments is English speech with transcripts in English. Though it contains many foreign named entities, the predominant linguistic content is English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset focuses on English language interviews; non-English only datasets are not described as a new contribution."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of audio interviews and transcripts; there is no indication that code or programming language entries are present in the dataset itself."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention is made of mathematical or formal logical expressions as part of the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is human speech testimony; no biological sequences or non-human communication systems are present."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of any fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly documented as English with many foreign named entities. The language content is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains spoken human language data and corresponding transcripts, so it is not devoid of any language."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7490,
      "completion_tokens": 144,
      "total_tokens": 7634
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3",
          "reasoning": "The paper describes the process of extracting and processing additional data from the original MALACH dataset, including segmentation and handling cross-channel audio, but no mention is made of any publicly available code repository or software accompanying these data extraction or preprocessing steps."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3",
          "reasoning": "Section 3 provides detailed documentation of the data extraction and processing methods used to prepare additional MALACH corpus data, including descriptions of the original data format, channel summation, segmentation criteria, handling of speaker overlap, and transcription conventions. This constitutes transparent documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 4695,
      "completion_tokens": 98,
      "total_tokens": 4793
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Procedure",
          "Reasoning": "The new dataset consists of 48 minimal pair items contrasting Bern and Zurich German variants, recorded as single-word utterances by two 24-year-old female native speakers and a trained professional actress. These audio recordings were made specifically for this study, with human speakers and human involvement in data collection clearly stated."
        }
      ]
    }
  },
  {
    "id": "pistor24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 5547,
      "completion_tokens": 195,
      "total_tokens": 5742
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.3 Procedure",
            "reasoning": "The annotation of phonetic features and the recording quality control were performed by native speakers and a professional actress, with quality evaluated by native speakers, indicating expert involvement in annotating and preparing the dataset."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Procedure",
            "reasoning": "Training sessions with a professional actress and quality control by native speakers imply detailed instructions or guidelines were given to ensure accurate and natural imitations of dialect features."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention the use of formal scoring rubrics in annotation guidelines for the phonetic feature dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No explicit examples of annotated items or instructions/examples for annotators are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "pistor24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 6677,
      "completion_tokens": 166,
      "total_tokens": 6843
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.3 Procedure",
          "reasoning": "The paper states that quality control measures were implemented by native speakers to ensure the accuracy and naturalness of recorded and imitated stimuli, implying that a single human expert performed quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 6295,
      "completion_tokens": 420,
      "total_tokens": 6715
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.3",
          "reasoning": "The authors created new original speech stimuli by recording 66 individual words from two native female speakers of Bern and Zurich German dialects. Additionally, a professional actress trained in these dialects recorded imitations as part of a matched-guise design. The words and phonetic features were carefully selected and controlled for semantic affectivity, resulting in new recorded data not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the linguistic features were selected informed by scholarly literature and a Google questionnaire, the main data\u2014the speech stimuli\u2014are newly recorded. The paper does not describe aggregating pre-existing audio data without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the selection of phonetic variables was influenced by existing studies and a questionnaire, the data itself consists of newly recorded audio, not adapted from prior audio recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented in Section 2.1 and 2.3."
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 6813,
      "completion_tokens": 229,
      "total_tokens": 7042
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Methods and Materials), Section 3 (Results)",
          "reasoning": "The new dataset of 48 minimal pair stimuli contrasting 24 Bern and Zurich German variants constitutes the core data used for analyzing perceptual evaluations by different rater groups. The dataset is used primarily for analyzing trends and patterns in linguistic feature evaluations, such as aesthetic perceptions and stereotype associations, rather than for training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 7536,
      "completion_tokens": 350,
      "total_tokens": 7886
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 Materials",
          "reasoning": "The dataset consists of spoken single-word utterances representing 24 linguistic features that contrast Bern German and Zurich German dialect variants, which are two dialects of the German language. The dataset entries therefore contain two human languages: Bern German and Zurich German."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 4754,
      "completion_tokens": 170,
      "total_tokens": 4924
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section mentioning code availability",
          "reasoning": "The paper does not provide any links to code repositories or mention that the code for data collection, preprocessing, or dataset generation is publicly available. There is no indication that code used in the study has been shared for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods and materials), subsections 2.1 (Materials) and 2.3 (Procedure)",
          "reasoning": "The paper thoroughly documents the dataset creation process by describing the selection of linguistic features, minimal pair stimuli design, recording procedures including actors and native speakers, validation steps for naturalness, and the rating procedure involving participants and tasks. These sections provide transparency and detail about how the dataset was constructed and processed."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8620,
      "completion_tokens": 131,
      "total_tokens": 8751
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 - Results for streamed ASR transcripts; footnote 3",
          "Reasoning": "The new dataset consists of manually corrected transcripts of automatic speech recognition (ASR) outputs from radio talks and TV political debates in Czech. The authors have created and are publishing this dataset, which includes transcripts and corresponding recordings annotated with punctuation. The data are in text format because they represent transcripts, and are human generated as they come from manual correction and human annotation of ASR outputs, not model-generated or unclear provenance data."
        }
      ]
    }
  },
  {
    "id": "polacek23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9472,
      "completion_tokens": 193,
      "total_tokens": 9665
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5",
            "reasoning": "Section 5 states that the test set transcripts were manually punctuated, implying manual annotation. The use of 'manually added punctuation' suggests human annotators, likely multiple experts to ensure quality, annotated the new dataset compiled from radio talks and TV debates."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not explicitly mention providing annotation instructions for the manual punctuation annotation of the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no explicit mention of scoring rubrics or guidelines for annotation evaluation regarding the new dataset's punctuation annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide annotation examples or guidelines illustrating how punctuation should be annotated in the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "polacek23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10602,
      "completion_tokens": 303,
      "total_tokens": 10905
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert annotator for the new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The authors state that the test set includes manually added punctuation annotations created from automatic transcripts of radio talks and TV debates. They describe these as \"manually added punctuation\", indicating human annotation. While specific details on the number or expertise of annotators are not explicitly stated, the description implies that multiple human annotators with subject matter knowledge performed the annotations to create these datasets, as these annotations require linguistic expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance performed by an AI model as a judge for dataset validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification processes used as quality assurance for dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documented quality assurance via manual annotation; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10220,
      "completion_tokens": 425,
      "total_tokens": 10645
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The authors mention that they are publishing new data consisting of manually added punctuation to automatic transcripts of radio talks and TV debates, created using an ASR system. These transcripts are manually annotated with punctuation marks, indicating original content created by human contributors from scratch (i.e., not translations, collated, or derived)."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was created by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that any dataset was generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the training data includes multiple sources such as newspaper articles, diploma theses, and legal texts, these are used only for training and not introduced as new datasets by the authors. The only new dataset mentioned consists of manually annotated transcripts."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new dataset is derived from existing sources via transformation or adaptation. The new transcripts appear to be newly created manual annotations on ASR outputs."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and creation of the new test set with manually added punctuation is specified in the paper (Section 5)."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10738,
      "completion_tokens": 514,
      "total_tokens": 11252
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The authors mention training the ELECTRA-Small model from scratch using the training corpus consisting of 23 GB of Czech texts including manually corrected automatic transcripts of Czech TV/R broadcasts (Section 4.2). The model trained from scratch yields good performance, as shown in Table 1 in Section 4.3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.3 and 4.2",
          "reasoning": "The training data includes 1.5 GB of manually corrected transcripts used for fine-tuning the model. Fine-tuning is performed on this dataset with supervised learning methods to optimize the classification head and improve punctuation restoration performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for reinforcement learning or RL-based post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The authors publish the test dataset consisting of automatic transcripts of radio talks and TV debates with manual punctuation annotation. This dataset is explicitly used for evaluation and benchmarking of the proposed APR models."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses analysis of performance and accuracy, it does not present the dataset primarily for analyzing characteristics or trends outside of training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used or described as serving as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has documented use cases within training from scratch, fine-tuning, and evaluation as described above."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11461,
      "completion_tokens": 416,
      "total_tokens": 11877
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any dataset containing multiple human languages beyond one or two; only Czech language datasets are discussed."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced and used are exclusively related to Czech language transcripts; there is no mention of datasets containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced are not English language datasets; the focus is on Czech language transcripts."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Sections 4.1, 4.2, 5",
          "reasoning": "The new datasets introduced consist of manually corrected transcripts and ASR automatic transcripts of Czech TV and radio broadcasts. The text and evaluation data are exclusively in Czech, a non-English language, thus the dataset is monolingual (Non-English)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although models and code are discussed, the dataset itself contains only natural language transcripts, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No parts of the introduced datasets contain mathematical or logical symbolic expressions; all data are textual transcripts."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain human spoken language transcripts and do not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention that the dataset involves any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages are clearly specified as Czech with manual and ASR transcripts; language identity is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains natural language transcripts, so it is not without any language."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8679,
      "completion_tokens": 272,
      "total_tokens": 8951
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5",
          "reasoning": "The paper mentions that the authors are publishing the test data (transcripts and recordings) at a specified URL (https://owncloud.cesnet.cz/index.php/s/fHqtWwZ5G9V5pPN) but there is no mention or link provided regarding the availability of code for data collection, preprocessing, or dataset construction. The paper does not provide explicit information about the release of code related to the dataset construction process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5 and throughout Section 4.2",
          "reasoning": "The paper provides documentation on the dataset creation process, describing that the training data includes 23 GB of Czech texts from various sources such as newspaper articles, manually corrected transcripts of Czech TV/radio broadcasts, diploma theses, and legal texts. Also, the fine-tuning data includes an additional 1.5 GB of manually corrected transcripts. Further details on pre-processing and tokenization steps are described in Section 3.2, and the dataset composition and sizes are elaborated in Section 4.2. Moreover, Section 5 describes the test datasets (radio talks and TV debates) and their manual annotation. These descriptions indicate transparent and detailed documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10809,
      "completion_tokens": 117,
      "total_tokens": 10926
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction; Section 2 Methodology; Abstract",
          "Reasoning": "The paper explicitly states that the authors have collected and annotated a new balanced dataset containing voice recordings from both healthy participants and participants diagnosed with CHF. These voice recordings originate from patients and healthy subjects, indicating that the data was recorded from humans (i.e., human generated audio). There is no indication the audio data is synthetic or model generated, nor is the origin unknown."
        }
      ]
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11661,
      "completion_tokens": 158,
      "total_tokens": 11819
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5 Conclusion",
            "reasoning": "The dataset contains speech recordings from patients diagnosed with CHF by medical professionals, indicating experts assigned the CHF diagnosis labels."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide details about specific annotation instructions given to annotators or labelers."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention of scoring rubrics or criteria for the labeling process in the text."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not describe providing annotation examples or guidelines for labeling speech data."
          }
        }
      ]
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12791,
      "completion_tokens": 216,
      "total_tokens": 13007
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 (Methodology), especially the paragraph starting with 'The collected dataset contains speech recordings from patients diagnosed with CHF by medical professionals.'",
          "reasoning": "The dataset consists of speech recordings from patients diagnosed with CHF by medical professionals, implying that a subject matter expert (medical professional) was involved in annotating or labeling the data regarding CHF diagnosis status. There is no mention of multiple experts performing annotation or consensus annotation, thus this quality assurance is conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12409,
      "completion_tokens": 404,
      "total_tokens": 12813
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Methodology, Section 5 Conclusion",
          "reasoning": "The authors explicitly state that they have collected and annotated a novel speech dataset consisting of voice recordings from patients diagnosed with CHF and healthy subjects. This dataset was created from scratch using recordings from human participants, as described in Section 2 and confirmed in the Conclusion where they mention 'We have collected and annotated a speech dataset...'. There is no indication that the data was translated, derived, or adapted from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data generated entirely by AI or machine learning models. The dataset consists of human speech recordings collected from participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any machine translation being involved in data creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources, but rather as newly created from fresh recordings."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived from existing sources with modifications or transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the new dataset is explicitly documented as newly collected human speech recordings."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12927,
      "completion_tokens": 473,
      "total_tokens": 13400
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Methodology) and Section 4 (Experiments)",
          "reasoning": "The newly introduced CHF speech dataset is used to train models, including transfer learning on pretrained SoundNet and VGG-16 models, fine-tuning them on this dataset to learn CHF-specific representations. The paper benchmarks conventional machine learning and deep learning methods trained on this dataset, indicating it is actively used for supervised model training from scratch or via fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Methodology) and Section 4 (Experiments)",
          "reasoning": "The paper applies transfer learning to pre-trained SoundNet and VGG-16 models and fine-tunes them on the new CHF speech dataset. This supervised fine-tuning helps the models adapt to the CHF detection task, as explicitly described in the methodology and evaluation sections."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate any use of reinforcement learning or RL-based post-training techniques with the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The dataset is also used for evaluation and benchmarking various models' performance under subject-independent evaluation protocols such as LOSO and LOSGO cross-validation. This shows explicit usage of the dataset for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Tables 1 and 2, Figure 3",
          "reasoning": "The paper analyzes model performances, class-wise accuracy, and subject-wise results using the dataset, discussing patterns such as higher accuracy for CHF diagnosed subjects. This constitutes analysis of trends and characteristics in the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation; no such usage is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has multiple practical utilities described and demonstrated in the paper, including training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13650,
      "completion_tokens": 438,
      "total_tokens": 14088
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper describes a dataset of voice recordings from patients diagnosed with CHF and healthy participants, but does not mention multiple languages. No evidence that more than two languages are present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not indicate presence of exactly two languages in the dataset. No information on bilingual content provided."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 3 (Dataset Description)",
          "reasoning": "The dataset consists of voice recordings collected from patients and healthy participants speaking English. The paper does not mention any other language being used. Hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication in the paper that the dataset contains recordings in any other single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of voice recordings only; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Though the paper contains some formulas and notation in the methodology, the dataset itself contains voice recordings, not mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech recordings, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves natural human language (English) only; no constructed or artificial languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper clearly describes the dataset as human voice recordings used for CHF detection, with English as the language; language is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains voice recordings which are human language; therefore, it does contain language."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10868,
      "completion_tokens": 202,
      "total_tokens": 11070
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5 Conclusion",
          "reasoning": "The paper mentions in the conclusion that the dataset will be made publicly available to researchers after acquiring the required clearances, but does not provide any link or mention availability of code associated with dataset collection, preprocessing, or generation. There is no indication that any code has been released or is accessible to the public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Methodology and Section 3 Dataset description (implied)",
          "reasoning": "The paper provides a description of the dataset's collection and annotation process, including that speech recordings were collected from patients diagnosed with CHF and healthy participants. Although the dataset details like number of samples, segmentation into 5-second windows, and class labels are described, the paper does not provide exhaustive detail such as full acquisition protocols or exact recording conditions. Still, a reasonable level of documentation on dataset creation is presented within the methodology and experimental sections."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7234,
      "completion_tokens": 88,
      "total_tokens": 7322
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "The MOCKS testset uses audio data extracted from existing human-recorded speech datasets LibriSpeech and Mozilla Common Voice, which contain human speech recordings. The paper specifies that these source datasets contain recordings of human speech, thus the audio data is human generated."
        }
      ]
    }
  },
  {
    "id": "pudo23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8086,
      "completion_tokens": 186,
      "total_tokens": 8272
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.2",
            "reasoning": "The paper states that after automatic processing using MFA alignments and phonetic transcription, the final step consisted of manual checking of the transcriptions to exclude obviously incorrect samples. This implies multiple human experts validated the data manually."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not describe any provided annotation instructions or guidelines for manual annotation or checking steps."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention of scoring rubrics or formal criteria used for annotation or validation in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No annotation examples or exemplars are provided or discussed in the paper regarding the manual checking process."
          }
        }
      ]
    }
  },
  {
    "id": "pudo23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9216,
      "completion_tokens": 399,
      "total_tokens": 9615
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.2 Creation of our testset",
          "reasoning": "The paper describes a manual checking step of the transcriptions to exclude obviously incorrect samples; it mentions the use of language experts choosing the most popular phonetic transcriptions, implying multiple human annotators contributed to phonetic transcription selection and verification but does not specify expert qualifications for these checks. Given this and no explicit statement of expert annotations, it is appropriate to consider multiple non-expert human annotators conducted quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.2 Creation of our testset",
          "reasoning": "Quality assurance involved automated processes such as rule-based grapheme-to-phoneme (G2P) conversion and the use of Montreal Forced Aligner (MFA) for automatic alignments and phonetic distance calculations. These automated verifications and rule-based selections serve as an algorithmic QA step to ensure correct alignment and selection of similar and different phrases."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly mentions manual transcription checks and automated alignment and verification processes, so quality assurance was performed and documented."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8834,
      "completion_tokens": 468,
      "total_tokens": 9302
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any audio data was recorded or created entirely from scratch by human contributors specifically for this dataset. It uses pre-existing datasets as the base."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain content generated entirely by AI or machine learning models. The only model-related data generation mentioned is the use of forced alignments and G2P transcriptions, but these are processes applied to existing data, not original data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation from another language by humans in the creation of this dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any use of machine translation to produce or modify the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 4.1",
          "reasoning": "The dataset is not merely an aggregation or collection of existing data from multiple sources without significant modification; the paper describes deriving new data from existing datasets via segmentation and processing."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The MOCKS testset is created by deriving new data from existing large public datasets (LibriSpeech and Mozilla Common Voice). This includes using forced alignments to segment audio into keywords, applying phonetic transcription with an internal G2P algorithm, selecting positive and negative samples, and creating offline and online audio variants with additional audio padding. The process involves modifications, transformations and adaptations of existing recordings rather than direct reuse."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9352,
      "completion_tokens": 282,
      "total_tokens": 9634
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5: Initial experiments with MOCKS",
          "reasoning": "The MOCKS dataset is explicitly introduced and used as a benchmark testset for evaluating custom keyword spotting (KWS) models, as detailed in Section 5 where baseline evaluation results are presented comparing performance on MOCKS and other testsets. This demonstrates its utility for evaluation and benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3: MOCKS description and analysis",
          "reasoning": "The authors conducted detailed analysis of MOCKS dataset properties such as keyword lengths distribution and gender distribution of speakers, characterizing the dataset beyond just its use for evaluation, indicating usage for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the MOCKS dataset for evaluation and analysis of KWS models."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10075,
      "completion_tokens": 553,
      "total_tokens": 10628
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 4.1.2; Table 1",
          "reasoning": "The MOCKS dataset contains audio data in multiple human languages, specifically English, French, German, Italian, and Spanish as stated in the Abstract and detailed in Section 4.1.2. Table 1 lists keywords and other statistics for subsets corresponding to these five languages, confirming inclusion of more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than two languages, so the bilingual label does not apply."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although English subsets exist within MOCKS, the dataset is not restricted to only English and includes other languages as well."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While there are subsets for single non-English languages such as German, French, Italian, and Spanish, the overall proposed dataset includes multiple languages, so the monolingual (non-English) label does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises audio recordings of spoken phrases, with no indication of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of mathematical or logical expressions in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human speech audio and does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset are explicitly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural human language audio data; thus, this label does not apply."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7293,
      "completion_tokens": 230,
      "total_tokens": 7523
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link providing code",
          "reasoning": "The paper mentions that the MOCKS dataset has been made publicly available at a Hugging Face URL, but it does not provide any information about publicly releasing the code or scripts used for data collection, preprocessing, keyword extraction, or dataset generation. There is no link or description about code repositories or implementations to reproduce the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (especially 4.2 Creation of our testset and 4.3 MOCKS description and analysis)",
          "reasoning": "The paper gives detailed documentation about the dataset creation process, including data sources (LibriSpeech and MCV), the requirements guiding keyword selection, the use of grapheme-to-phoneme algorithms, alignment via Montreal Forced Aligner (MFA), selection criteria for positive and negative samples, and details about offline and online audio segments. They also describe some manual checking steps and provide extensive statistics and analysis in Section 4. This information provides a transparent and thorough overview of the dataset construction methodology."
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 1239,
      "completion_tokens": 84,
      "total_tokens": 1323
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2",
          "Reasoning": "The paper explicitly introduces Vox-CA as a new dataset for cross-age speaker verification, consisting of audio pairs of speech recordings. These audio samples are recorded speech segments from human speakers, implying the data is human generated rather than simulated or model generated."
        }
      ]
    }
  },
  {
    "id": "qin22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 2091,
      "completion_tokens": 229,
      "total_tokens": 2320
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2",
            "reasoning": "The annotation of the proposed Vox-CA dataset involves generating positive and negative audio pairs automatically based on known speaker identities and age groups, as described in Section 2. There is no indication of human annotators reviewing or labeling the data, suggesting an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for the annotators, consistent with the annotation being an automatic pairing process rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "There is no description of scoring rubrics or rating guidelines associated with the annotation process for the Vox-CA dataset, indicating rubrics are not applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No examples of annotation or labeling are provided in the paper for the Vox-CA dataset, as the data creation is through automatic pairing based on metadata rather than explicit annotation tasks."
          }
        }
      ]
    }
  },
  {
    "id": "qin22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 3221,
      "completion_tokens": 250,
      "total_tokens": 3471
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of multiple human experts for quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert human annotated or validated the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using AI models to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic verification or rule-based checking for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the Vox-CA dataset but does not document or describe any quality assurance process for the dataset's annotations or content."
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 2839,
      "completion_tokens": 241,
      "total_tokens": 3080
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The dataset Vox-CA is described as constructed by selecting and pairing audio segments from existing VoxCeleb datasets to create cross-age positive pairs, indicating data aggregation from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 3357,
      "completion_tokens": 466,
      "total_tokens": 3823
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the newly introduced Vox-CA dataset is used for pre-training purposes."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit indication in the paper that Vox-CA is used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results)",
          "reasoning": "The dataset Vox-CA is used to fine-tune or improve speaker verification systems as shown in experiments where the model\u2019s performance improves on Vox-CA, indicating supervised use of the dataset for fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning or RL-based post-training methods using the Vox-CA dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results)",
          "reasoning": "Vox-CA dataset is employed as a benchmark to evaluate speaker verification system performance, such as reporting EER in tables, indicating its usage for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Description and Comparison), Section 4",
          "reasoning": "The authors analyze characteristics of Vox-CA compared to other datasets and discuss patterns related to cross-age speaker verification, indicating analytical use."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that Vox-CA is used as an external knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset Vox-CA has documented applications in supervised fine-tuning, evaluation, and analysis as described in multiple sections."
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 4080,
      "completion_tokens": 236,
      "total_tokens": 4316
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 and Table 1",
          "reasoning": "The proposed Vox-CA dataset is introduced and appears to be derived from VoxCeleb, which consists primarily of English speech. The paper does not mention any other languages being included; hence the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 1298,
      "completion_tokens": 115,
      "total_tokens": 1413
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any information or link regarding the public availability of code related to the dataset construction, collection, or preprocessing for the new dataset Vox-CA."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "Section 2 of the paper describes the details of the new dataset Vox-CA and compares it with another dataset (Interspeech 2022). This section documents the dataset creation process to some extent through explanation and analysis."
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6440,
      "completion_tokens": 120,
      "total_tokens": 6560
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. Performance Evaluation",
          "Reasoning": "The paper states that a new database was created in a studio environment consisting of 384 audio tracks recorded by 15 professional singers familiar with the oktoechos singing modes. The recordings are of Malayalam hymns collected from the liturgical book of the Indian Orthodox church. The recordings were done at 44.1 kHz, indicating audio modality data that was produced through human performance and captured via studio recording."
        }
      ]
    }
  },
  {
    "id": "rajan22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7292,
      "completion_tokens": 172,
      "total_tokens": 7464
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3 Performance Evaluation",
            "reasoning": "The dataset was created with participation from 15 professional singers familiar with the singing modes; thus, domain experts performed the annotation and classification labels are expert-derived for the eight modes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to the experts during dataset creation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No mention or description of scoring rubrics or criteria for annotation evaluation are found in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide or refer to any annotation examples or exemplar annotations."
          }
        }
      ]
    }
  },
  {
    "id": "rajan22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8422,
      "completion_tokens": 407,
      "total_tokens": 8829
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3 Performance Evaluation",
          "reasoning": "The dataset was created in a studio environment with participation of 15 professional singers aged 12 to 50, all very familiar with the oktoechos singing modes. These singers, being experts or knowledgeable members of the target demographic, provided the recordings, implying quality assurance occurred through their expert performance."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3 Performance Evaluation",
          "reasoning": "A total of 15 professional singers participated in the dataset recording. Since multiple experts (professional singers familiar with oktoechos) contributed, the quality assurance was effectively performed by multiple human experts ensuring the dataset authenticity."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge or for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any automated verification or algorithmic QA process applied to the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper details the dataset creation involving multiple professional singers, implying quality assurance was performed and documented."
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8040,
      "completion_tokens": 487,
      "total_tokens": 8527
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Performance Evaluation",
          "reasoning": "The paper states that a database was created in a studio environment consisting of eight nirams (colours) with 384 audio tracks recorded by 15 professional human singers familiar with the 'oktoechos' singing modes. The hymns used were collected from the liturgical book of the Indian Orthodox church and recorded specifically for this work. This indicates that the audio data is original content created from scratch by human contributors for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated or synthesized entirely by AI or machine learning models. All data pertains to recordings or features extracted from human-created music."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that data was produced by translating content from another language through human translators. The hymns are in Malayalam and collected from liturgical books; no translation process is described."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any machine translation process for the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While hymns were collected from existing liturgical books, the dataset itself consists of newly recorded audio tracks, not merely aggregated or collected recordings from existing datasets. Therefore, it is not merely collated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though musical texture features and Mel-spectrograms are derived from the raw audio data through signal processing, the rubric targets data novelty at the dataset level, not feature extraction. The dataset itself is original recordings, not derived from existing data with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of the dataset, which was newly created through studio recordings by human singers."
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8558,
      "completion_tokens": 354,
      "total_tokens": 8912
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 - Performance Evaluation, Section 4 - Results and Analysis",
          "reasoning": "The newly created liturgical music corpus of 384 audio tracks was used to train the proposed stacked bidirectional and unidirectional LSTM and GRU architectures from scratch, as described in the performance evaluation and results sections. The corpus was used for supervised training with 60% training split, 5% validation, and the rest for testing, indicating training models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 - Performance Evaluation, Section 4 - Results and Analysis",
          "reasoning": "The dataset serves as both training and test data to evaluate and benchmark the performance of different models including DNN, CNN, vanilla LSTM/GRU, and the proposed stacked architectures in terms of classification accuracy and other metrics."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 - Results and Analysis",
          "reasoning": "The dataset is used to analyze classification results, including per-class accuracy and F1-score analyses, confusion matrices, and detailed discussion on performance per mode (niram), helping to understand the characteristics and challenges of the oktoechos classification task."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9281,
      "completion_tokens": 380,
      "total_tokens": 9661
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3. Performance Evaluation",
          "reasoning": "The newly created corpus consists of 384 audio tracks of liturgical music in Malayalam language, as explicitly stated in Section 3. The hymns used are collected from the Indian Orthodox church liturgical book, and all are in Malayalam, a non-English language. No other languages are mentioned as part of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of liturgical music in Malayalam language, hence it contains language."
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6499,
      "completion_tokens": 191,
      "total_tokens": 6690
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention any link, repository, or reference to code related to the dataset collection, preprocessing, or generation. No public availability of such code is indicated in any section of the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Performance Evaluation)",
          "reasoning": "The paper provides explicit details on dataset creation, including the studio recording environment, number of audio tracks (384), duration of each track, number and demographics of singers (15 professional singers aged 12 to 50), the language (Malayalam hymns), and references the source of hymns (liturgical book of the Indian Orthodox church). Additionally, specifics about recording sampling rate (44.1kHz) and data splits (60% training, 5% validation, rest testing) are described, constituting documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 4555,
      "completion_tokens": 168,
      "total_tokens": 4723
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Protocol description",
          "Reasoning": "The new dataset introduced involves collection of voice, speech, and respiratory sound recordings through acoustic tasks performed by participants. These audio data are captured via human participants speaking or performing tasks, therefore originating from human-generated recordings."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Protocol description",
          "Reasoning": "The protocols include demographic questions, patient-reported outcome measures (PROMs), validated patient questionnaires (e.g., GAD-7, VHI-10), and clinical validation questions. This metadata and questionnaire data are text-based and recorded from human participants, thus human-generated."
        }
      ]
    }
  },
  {
    "id": "rameau24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 5407,
      "completion_tokens": 216,
      "total_tokens": 5623
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.4",
            "reasoning": "The protocol development and review involved multiple expert groups including clinical experts, bioethicists, standards experts, diversity experts, and software developers who reviewed and amended each question in the protocols."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Sections 2.3 and 2.4",
            "reasoning": "The protocols were developed with detailed questions and tasks, incorporating literature review and clinical expertise, and reviewed iteratively to create standardized instructions for data collection across cohorts."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "Clinical experts assessed and rated the clinical significance and validity of each task, implying the presence of rubrics or scoring guidelines during protocol review."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None explicit",
            "reasoning": "The paper does not explicitly mention provision of annotation or data collection examples in the guidelines or protocols, only reviews and amendments of protocol questions."
          }
        }
      ]
    }
  },
  {
    "id": "rameau24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 6537,
      "completion_tokens": 299,
      "total_tokens": 6836
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.4 Validation of the protocols for Standards, Ethics, Clinical expertise, Tools and PEDP",
          "reasoning": "Quality assurance was performed by multiple human experts including clinical experts, bioethicists, standards experts, diversity experts, and software developers who reviewed each section of the protocol in multiple review sessions and amended the questions based on expert input."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge for quality assurance of the dataset or protocols."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification methods used for quality assurance of the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents a quality assurance process involving multiple human experts, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 6155,
      "completion_tokens": 403,
      "total_tokens": 6558
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 and 3.1",
          "reasoning": "The paper describes protocols developed by interdisciplinary human experts over a 6-week period to collect new voice, speech, and related clinical data from human participants across multiple disease cohorts and a control cohort. These datasets are generated through direct data acquisition from human subjects following these newly developed protocols, indicating original data created from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets generated entirely by AI or machine learning models without reference to pre-existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention in the paper that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate any dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on the development of new data collection protocols for prospective data acquisition, not aggregation of existing datasets without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset is based on existing sources with modifications or adaptations; rather, the protocols were designed for new data collection."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors explicitly describe the protocols for new human data collection and no ambiguous or undocumented data origin is indicated."
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 6673,
      "completion_tokens": 248,
      "total_tokens": 6921
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 and Sections 2.3-2.4",
          "reasoning": "The paper focuses on developing standardized multi-disorder voice data collection protocols, harmonizing data acquisition methods, and establishing a foundation for future clinical and voice AI biomarker research. There is no description of actual data usage for training or evaluation of models, but rather a detailed development and analysis of the protocols and data characteristics, including clinical, ethical, and DEI considerations, indicating the dataset is primarily used for analysis and future research facilitation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 7396,
      "completion_tokens": 540,
      "total_tokens": 7936
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, implicit in all sections describing the protocols and tasks",
          "reasoning": "The paper describes the development of voice, speech, and respiratory sounds collection protocols intended for clinical research use. All protocols, questionnaires, acoustic tasks, and screening tools mentioned (e.g., GAD-7, VHI-10) and descriptions are presented solely in English. There is no mention of inclusion of other human languages or translations in the dataset. Thus, the dataset entries are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions software developers and data scientists involved in the protocol development and references to a GitHub repository, there is no indication that the dataset entries themselves contain programming or structured code-related content. The dataset consists of clinical, acoustic, demographic, and questionnaire data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention inclusion of mathematical or formal logical expressions within the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes human voice, speech, and respiratory sounds but does not include biological sequences (e.g., DNA) or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any fictional or constructed languages in the dataset or protocols."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's linguistic content is clearly specified as English; language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries with human language (English) in the form of questionnaires, acoustic tasks, and clinical metadata."
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 4614,
      "completion_tokens": 137,
      "total_tokens": 4751
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3 (Results)",
          "reasoning": "The paper states that the full protocols are available for download on their GitHub repository (https://github.com/eipm/bridge2ai-redcap), indicating the associated code and protocols for data acquisition are publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper extensively documents the development process of the protocols, including literature review, expert reviews, iterative development, alignment of protocols across cohorts, ethical and DEI considerations, and finalization. This provides transparency and completeness about dataset creation."
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7749,
      "completion_tokens": 274,
      "total_tokens": 8023
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.3",
          "Reasoning": "The new dataset consists of audio recordings of six monolingual Australian English speakers pronouncing target monosyllables in carrier phrases. The recordings were made using an electret condenser microphone during the experimental sessions, indicating human-generated data through recorded natural speech."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "Laryngeal activity was captured using electroglottography (EGG) via a Laryngograph microProcessor EGG-D200. This sensor data was collected synchronously with audio during human speech production, thus the sensor data originates from human involvement in the recording process."
        },
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.4 and 3",
          "Reasoning": "Statistical analysis was performed on measured open quotient (OQ) values, including data processing and modeling with generalized additive mixed models (GAMMs) implemented in R. The resulting processed data tables and statistical outputs are model generated, derived from human-generated original data but produced by computational modeling."
        }
      ]
    }
  },
  {
    "id": "ratko23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8601,
      "completion_tokens": 171,
      "total_tokens": 8772
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.3",
            "reasoning": "The paper describes that acoustic and EGG analysis was undertaken by the first author, indicating that annotation and analysis were performed by a single human expert."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "There is no mention of any detailed annotation instructions provided for the manual annotations or analyses performed by the expert."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "The paper does not describe any scoring rubrics or formal criteria used during annotation or analysis."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No annotation examples or guidelines including examples are provided or described in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "ratko23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9731,
      "completion_tokens": 167,
      "total_tokens": 9898
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.3 Data collection and analysis",
          "reasoning": "The paper indicates that vowel onsets and offsets and mispronunciations were determined by the first author, suggesting a single human expert performed the quality control and annotation validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9349,
      "completion_tokens": 407,
      "total_tokens": 9756
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 (Participants), Section 2.2 (Experimental materials), Section 2.3 (Data collection and analysis)",
          "reasoning": "The paper reports original electroglottographic recordings collected from six monolingual Australian English speakers producing designed CVC stimuli. The data were elicited specifically for this study via controlled experimental methods, including new recordings and subsequent analysis of vocal fold activity. This dataset represents original human data created from scratch by the authors' experimental procedure."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any dataset generated entirely by AI or machine learning models. All data analyzed are empirical recordings captured from human participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any data being produced by translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Data were not collated or aggregated from existing sources; rather, they were newly collected through experiments."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although statistical and signal processing analyses were performed, the data themselves are original recordings rather than derived or adapted from pre-existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data collection method and origin are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9867,
      "completion_tokens": 242,
      "total_tokens": 10109
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 and 4 (Results and Discussion)",
          "reasoning": "The newly collected dataset of electroglottographic (EGG) recordings of Australian English speakers' productions is used primarily for analyzing trends and characteristics of glottal activity during voiceless coda consonants, specifically investigating open quotient (OQ) changes relating to different coda consonants. The paper describes the analysis of these data to reveal phonetic patterns rather than any training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10590,
      "completion_tokens": 494,
      "total_tokens": 11084
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced consists solely of Australian English speech tokens; no other human languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is exclusively Australian English; no second human language is present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 Experimental materials; Section 2.1 Participants",
          "reasoning": "The dataset consists of recordings of Australian English speakers producing Australian English words; all linguistic content is in English as confirmed by participant description and stimuli."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is in English, specifically Australian English, so non-English monolingual does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming language or code entries are included in the dataset; it consists of speech recordings of English words only."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although statistical modelling methods (GAMMs) are used in analysis, the dataset itself does not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech recordings, not biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content is specified as Australian English; there is no ambiguity or undocumented language content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English speech recordings, so it does contain language."
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7808,
      "completion_tokens": 194,
      "total_tokens": 8002
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any links, URLs, or references to repositories hosting code related to data collection, preprocessing, analysis scripts, or dataset generation. There is no mention of publicly available code or software."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods), including 2.1 (Participants), 2.2 (Experimental materials), 2.3 (Data collection and analysis), and 2.4 (Statistical analysis)",
          "reasoning": "The paper provides detailed and transparent documentation of the dataset creation process including recruitment of participants, the design of the experimental materials with target words and phonetic contexts, equipment used for data collection, analysis methodology including signal processing for EGG data, and statistical modeling approaches. This sufficiently documents the construction process and characteristics of the collected data for reproducibility and downstream use."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7657,
      "completion_tokens": 137,
      "total_tokens": 7794
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, 'Compositional Benchmark Datasets'",
          "Reasoning": "The paper introduces new compositional splits of existing benchmark SLU datasets (ATIS and SNIPS) to evaluate compositional generalization. These splits are created by selecting and manipulating subsets of human-generated utterances originally from these datasets, which are textual data representing spoken language understanding utterances. The process involves removing or selecting utterances based on slot combinations and lengths, and substituting slot values with values from the training set, but all original data is human-generated text."
        }
      ]
    }
  },
  {
    "id": "ray23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8509,
      "completion_tokens": 236,
      "total_tokens": 8745
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2",
            "reasoning": "The authors create compositional train/test splits of existing SLU benchmark datasets (ATIS and SNIPS) by algorithmically selecting and filtering utterances according to slot combinations and number of slots per utterance. This process is described as systematic data manipulation and filtering rather than explicit human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not describe providing specific annotation instructions for human annotators for the newly created compositional splits, as the splits are generated algorithmically from existing annotated datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No mention of annotation scoring rubrics or evaluation criteria related to manual annotation is present for the compositional splits, indicating no rubrics were used."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "Although example utterances from compositional splits are shown (e.g., Figure 1), these are examples of data splits, not of annotation guidelines provided to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "ray23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9639,
      "completion_tokens": 347,
      "total_tokens": 9986
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts for the new compositional splits."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance by a single human non-expert annotator mentioned in the paper for the constructed datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about multiple non-expert human annotators performing quality assurance on the dataset splits introduced."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that an AI model was used as a judge or for quality assurance on the new dataset splits."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 (Compositional Benchmark Datasets)",
          "reasoning": "The compositional splits are created by an automated process involving selecting and removing utterances based on slot type combinations and slot counts, and replacing OOV slot values with random slot values from the training set of the same slot label. This systematic selection and modification process constitutes an automatic, rule-based dataset splitting and augmentation procedure, representing an automatic verification or quality control through algorithmic means, rather than manual quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the dataset splitting is automatic, the process is documented and algorithmic rather than absent, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9257,
      "completion_tokens": 470,
      "total_tokens": 9727
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or imply that any new dataset was created entirely from scratch by human contributors. Instead, it uses existing benchmark datasets (ATIS and SNIPS) and creates new compositional splits from them."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for training and evaluation are derived from existing benchmark datasets. The paper does not mention any dataset generated entirely by AI or model outputs."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation or multilingual datasets in the paper. All data is in English and no human translation process is described."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that any dataset was produced or modified through machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 'Compositional Benchmark Datasets'",
          "reasoning": "The paper creates new compositional splits from existing benchmark datasets ATIS and SNIPS by selecting subsets of existing utterances with certain slot type combinations removed or selected to test compositional generalization. Thus, the data is collected and aggregated from existing sources without significant modification of the individual utterances."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 'Compositional Benchmark Datasets'",
          "reasoning": "The compositional splits are derived from the original datasets by filtering training and test utterances according to slot type combination criteria as well as replacing out-of-vocabulary slot values with randomly selected in-vocabulary slot values of the same slot label. This involves transformations and adaptations applied to the existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the compositional splits are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9775,
      "completion_tokens": 446,
      "total_tokens": 10221
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The authors train their compositional SLU models from pre-trained BERT checkpoints but further train (fine-tune) on their compositional splits; the datasets are used directly to train models on intent classification and slot tagging tasks (jointly trained). This corresponds to supervised training on these datasets, not pre-training from scratch, but there is no mention of random initialization training, so does not apply."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2",
          "reasoning": "The compositional splits of ATIS and SNIPS datasets created by the authors are used for supervised fine-tuning of pre-trained BERT SLU models, as described in the experiments training setup. The datasets serve as supervised fine-tuning data to improve compositional generalization."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RLHF methods involving the new datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2, 4.2",
          "reasoning": "The compositional splits (novel slot combination and length generalization) made from ATIS and SNIPS are explicitly used as new benchmarks to evaluate compositional generalization of SLU models. The splits provide evaluation sets to measure performance on compositionality challenges."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The compositional splits enable analysis of model behavior regarding novel slot combination and length generalization, facilitating in-depth analysis on why baseline SLU models fail in these scenarios."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the introduced datasets serve as knowledge bases for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are actively used for supervised fine-tuning, evaluation, and analysis as described in multiple sections."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10498,
      "completion_tokens": 462,
      "total_tokens": 10960
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces compositional splits of benchmark SLU datasets (ATIS and SNIPS), which are English language datasets. There is no mention of multiple languages or multilingual content in the datasets introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced (compositional splits of ATIS and SNIPS) contain English utterances only, with no indication of any second human language being included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2: Compositional Benchmark Datasets",
          "reasoning": "The compositional splits are created from the ATIS and SNIPS datasets, which contain English utterances. The paper provides example utterances such as 'show flights to Boston' and 'play rock from the eighties', clearly indicating English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the composed splits contain non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of spoken language utterances related to slot filling and intent detection; no programming or code snippets are present in these datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical notation to describe model losses, the datasets themselves do not contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are human spoken language utterances; there is no biological or non-human communication data reported."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no presence of constructed or fictional languages such as Klingon or Esperanto in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets' linguistic content is clearly specified as English from known benchmark datasets; the language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language utterances in English; thus they include language and are not inapplicable to the language metric."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7716,
      "completion_tokens": 163,
      "total_tokens": 7879
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2, Compositional Benchmark Datasets",
          "reasoning": "The paper describes the method to create compositional splits of benchmark SLU datasets ATIS and SNIPS in detail within Section 2 but does not mention any link, URL, or location for publicly available code or scripts for constructing these dataset splits."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, Compositional Benchmark Datasets",
          "reasoning": "The paper provides a clear and detailed description of how the compositional train/test splits are created, including specific steps for novel slot combination split and length generalization split, with explanation of data selection and substitution of OOV slot values, making the dataset creation process explicit and transparent within Section 2."
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8400,
      "completion_tokens": 91,
      "total_tokens": 8491
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 (SMC data description)",
          "Reasoning": "The SSPNet Mobile Corpus (SMC) is composed of 60 phone conversations recorded from 120 participants engaged in spontaneous speech tasks. The audio data was created through human conversations during an experimental task setting, making it human-generated audio data."
        }
      ]
    }
  },
  {
    "id": "rennie22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9252,
      "completion_tokens": 217,
      "total_tokens": 9469
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2",
            "reasoning": "The SSPNet Mobile corpus (SMC) is a newly introduced dataset composed of spontaneous phone conversations recorded independently. The laughter events are identified and counted by processing audio recordings and feature extraction rather than by human labeling described explicitly. The paper mentions feature extraction and overlapping speech handling using independent recordings, indicating an automatic approach to identifying laughter events."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not explicitly state any annotation instructions provided to annotators for the SMC dataset labeling or processing."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No mention or description of rubric usage for annotators in creating the SMC dataset is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not show any examples or mention the provision of annotation examples regarding laughter identification or labeling for the SMC dataset."
          }
        }
      ]
    }
  },
  {
    "id": "rennie22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10382,
      "completion_tokens": 349,
      "total_tokens": 10731
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human expert conducted quality assurance on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description in the paper of multiple human experts performing quality assurance or annotation validation for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that quality assurance was performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about the use of multiple human non-expert annotators for quality assurance of the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models as judges or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule based quality assurance applied to dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the SSPNet Mobile corpus (SMC) as a dataset used in the experiments, which was collected via spontaneous conversations around the winter survival task. However, the paper lacks any description of an annotation or quality assurance process for labeling laughter events. There is no information about the involvement of human annotators, expert or non-expert, nor mention of any quality assurance methods employed or documented. Hence, no quality assurance process is described or documented in the paper for the introduced dataset."
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10000,
      "completion_tokens": 437,
      "total_tokens": 10437
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The SSPNet Mobile corpus (SMC), which is used as the primary dataset in this paper, was generated through 60 spontaneous telephone conversations between 120 human participants engaged in 'The winter survival task'. This data was created from original human conversational speech recorded specifically for this corpus, representing naturalistic interaction without experimenter influence during the conversations, thus qualifying as original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generation by AI or machine learning models as part of the dataset creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was involved in the data preparation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SMC dataset was not assembled from existing sources without modification, but rather recorded specifically for this study, so it is not considered collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The SSPNet Vocalisation Corpus (SVC) is derived from the SMC by selecting clips of 11 seconds containing at least one laughter or filler event, effectively a subset of the SMC, representing a modified and transformed version of an existing dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly specified for both datasets."
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10518,
      "completion_tokens": 320,
      "total_tokens": 10838
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.5 Training and Testing",
          "reasoning": "The paper describes using the SSPNet Mobile corpus (SMC), a new and more naturalistic conversational dataset introduced by the authors, to train neural networks from randomly initialized parameters to detect laughter. The k-fold training setup and multiple model initializations show that the dataset was actively used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Results and Discussion",
          "reasoning": "The SMC dataset is used as a benchmark for evaluating the performance of various laughter detection methods, with performance metrics such as AUC, precision, recall, and F1 scores reported to compare model effectiveness."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Results and Discussion and Section 5 Conclusions",
          "reasoning": "The dataset is analyzed to understand characteristics of laughter detection methods, model ability to detect laughter event durations, and to critique evaluation metrics used in the field. The paper discusses trends and challenges in laughter detection using the SMC dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11241,
      "completion_tokens": 585,
      "total_tokens": 11826
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper describes the new dataset (SMC) as consisting of spontaneous conversations with participants who are all native English speakers (Section 2.2). There is no indication that multiple languages are present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of two human languages in the proposed dataset. The paper explicitly states all participants were native English speakers (Section 2.2)."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The authors state that all participants in the SSPNet Mobile Corpus (SMC), the new and largest dataset introduced in the paper, were native English speakers (Section 2.2). The conversations are spontaneous dialogues in English, therefore the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of English conversations only, so this label does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of audio recordings of human speech; no programming or code snippets are part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No evidence that the dataset contains mathematical or logical expressions; it is composed of speech audio."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human vocalizations (speech and laughter), but not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of constructed or fictional languages in the dataset; the language used is English."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language(s) of the dataset are explicitly stated as English; therefore, the dataset is not of unknown language origin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language (English speech), so the label N/A does not apply."
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8459,
      "completion_tokens": 196,
      "total_tokens": 8655
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention found regarding code availability",
          "reasoning": "The paper does not provide any link, repository, or mention of publicly available code related to the construction, preprocessing, or generation of the dataset. There is no indication that code has been shared to facilitate reproduction of the dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data), specifically Sections 2.1 (SVC) and 2.2 (SMC)",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process for both the SVC and SMC datasets, including participant demographics, experimental design (e.g., 'The winter survival task' for the SMC), recording conditions, and statistics regarding laughter events. While the SVC is an extracted subset of the SMC, the parent corpus (SMC) is thoroughly described, indicating clear documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6647,
      "completion_tokens": 191,
      "total_tokens": 6838
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Data",
          "Reasoning": "The dataset consists of audio recordings sampled at 44.1 kHz from 250 sessions where participants interacted with a virtual agent while performing speech tasks. The audio data is recorded from human participants during these tasks, thus it is human generated data captured via recording devices during the study."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Data",
          "Reasoning": "The dataset also consists of corresponding video recordings with resolution 320 x 240 pixels and 15 fps from the same 250 sessions. These videos capture the participants' facial movements during the speech tasks. Since these videos were recorded from actual human participants in the study, the data is human generated, captured using cameras during the experiment."
        }
      ]
    }
  },
  {
    "id": "richter23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7499,
      "completion_tokens": 219,
      "total_tokens": 7718
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 Methods",
            "reasoning": "The paper states that all recordings were rated by one speech researcher, with a second researcher rating a subset for inter-rater agreement, indicating annotation was performed by expert human raters in speech research."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 Methods, Sections 3.1 and 3.2",
            "reasoning": "Detailed annotation guidelines including instructions for rating audio and video quality were provided, including descriptions of distortion categories and criteria for rating scores."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Sections 3.1 and 3.2",
            "reasoning": "The paper details explicit rubrics with Likert scale definitions from 1 to 5 for both audio and video quality ratings, specifying how to rate distortions and their severity."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention or provide any annotation examples or exemplar ratings in the guidelines or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "richter23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8629,
      "completion_tokens": 451,
      "total_tokens": 9080
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3 Methods and Section 4 Results",
          "reasoning": "The dataset annotations for audio and video quality ratings were performed by one speech researcher with expertise relevant to speech and audiovisual quality assessment in clinical settings, indicating a single human expert performed the quality assurance. This is explicitly stated in Section 3 'All recordings were rated by one speech researcher...'."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3 Methods and Section 4 Results",
          "reasoning": "A second speech researcher, also an expert, rated a subset (125 samples) of the dataset to assess inter-rater agreement, demonstrating that multiple human expert raters participated in quality assurance albeit partially. This is detailed in Section 3 'To assess inter-rater agreement, a second speech researcher rated the audio and video corresponding to 125 DDK samples...'."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that an AI model was used as a judge for quality assurance of the data."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance described involves human raters evaluating recordings manually; automated verification methods or rule-based techniques for quality assurance are not described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are clearly described, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8247,
      "completion_tokens": 178,
      "total_tokens": 8425
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The dataset consists of audiovisual recordings collected directly from human participants (people with ALS and healthy controls) interacting with a virtual agent to perform speech tasks. These recordings were newly recorded specifically for this study and are original data generated organically by human subjects rather than adapted from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8765,
      "completion_tokens": 215,
      "total_tokens": 8980
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 - Results",
          "reasoning": "The dataset introduced is primarily used to analyze data quality, feasibility, and task compliance of the multimodal dialog-based remote assessment system. The paper presents detailed analysis of audio and video quality ratings, compliance rates, and correlations with speech impairment severity, rather than using the dataset for training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9488,
      "completion_tokens": 494,
      "total_tokens": 9982
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing more than two human languages. The data consists solely of English speech tasks."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages; only English is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The participants interacted with the virtual agent in English performing speech tasks such as picture description and syllable repetition (/pAtAk/), with no mention of any other language. The instructions, tasks, and assessments all appear to be in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a dataset containing a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio and video recordings of human speech; no programming or structured code content is present in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains no mathematical or formal logical expressions or symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though medical in nature, the dataset consists of human speech recordings and video, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial languages are mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human speech recordings and thus contains language."
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6706,
      "completion_tokens": 146,
      "total_tokens": 6852
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any links, references, or mentions of publicly available code related to the dataset construction, data collection, or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data), Section 3 (Methods)",
          "reasoning": "The paper documents the dataset creation process including participant recruitment, the speech tasks used (picture description, DDK), the spoken and video recording parameters, and detailed rubrics for quality rating, audio and video assessment criteria, and compliance evaluation. The methods section provides clear, structured documentation of how data was collected, assessed, and the rationale behind these steps."
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10878,
      "completion_tokens": 100,
      "total_tokens": 10978
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 (EARS dataset description)",
          "Reasoning": "The EARS dataset is introduced as a high-quality speech dataset with 107 speakers recorded in an anechoic chamber. The speech data are captured via microphones in controlled environments, involving human speakers' recordings, confirmed in the Abstract and Section 2 where recording equipment and conditions are described."
        }
      ]
    }
  },
  {
    "id": "richter24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11730,
      "completion_tokens": 578,
      "total_tokens": 12308
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 (Dataset description), Section 3.1 (EARS-WHAM), Section 3.2 (EARS-Reverb)",
            "reasoning": "The EARS dataset comprises recordings from 107 speakers recorded in anechoic conditions. Speakers are participants from diverse backgrounds, implying human non-expert annotators provided speech data. The paper does not state expert annotation for labeling or complex annotations but mentions transcriptions of the reading portion, indicating multiple human annotators likely transcribed, typically non-experts or crowd-sourced transcribers. Speech content such as reading styles and emotional speech were recorded but no mention of expert evaluators is given for the original dataset annotation. Noise and room impulse responses used for creating derivative datasets are from public sets, but their annotation is not introduced in this paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2 and 3",
            "reasoning": "The paper does not provide or refer to any annotation instructions or guidelines for transcribers or annotators involved in the dataset creation. Instructions for recording procedures are implied but not explicitly detailed as annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "There is no mention of scoring rubrics or formalized criteria used during annotation or transcription for the EARS dataset in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No explicit examples of annotations or transcriptions are included or referenced as part of the dataset release documentation within the paper."
          }
        },
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 (EARS-WHAM) and 3.2 (EARS-Reverb)",
            "reasoning": "The EARS-WHAM and EARS-Reverb datasets are generated via mixing the clean EARS speech data with noise or reverberation impulse responses, using signal processing techniques such as mixing at specified SNRs or convolution with recorded RIRs. These are deterministic, automatic processes without human annotation involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1 and 3.2",
            "reasoning": "No annotation guidelines or instructions were needed because the data augmentation employed automatic signal processing rather than human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 and 3.2",
            "reasoning": "No scoring rubrics were described or necessary for these automatic processing steps."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1 and 3.2",
            "reasoning": "No annotation examples apply to this automatic data generation process."
          }
        }
      ]
    }
  },
  {
    "id": "richter24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12860,
      "completion_tokens": 305,
      "total_tokens": 13165
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human annotator with subject matter expertise or membership in the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance being performed by multiple human experts in the dataset collection or annotation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human non-expert conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple human non-experts performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for benchmarking and evaluation of speech enhancement performance, the paper does not state that an AI model was used as a quality assurance judge for the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process applied to validate the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process applied during the collection or preparation of the EARS dataset or its derived datasets. There is no mention of human or automated QA procedures to validate the annotations or recordings."
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12478,
      "completion_tokens": 485,
      "total_tokens": 12963
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (EARS dataset)",
          "reasoning": "The paper describes the EARS dataset as containing 100 hours of anechoic speech recorded from 107 human speakers with diverse demographics and various speaking styles, recorded in an anechoic chamber. The data was collected directly from human participants performing speech tasks, indicating original content created entirely from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described is based on human speech recordings; no indication exists that any part of the dataset was generated artificially by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced via machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The EARS dataset itself is newly recorded data, not collected insignificantly modified from pre-existing datasets. Although other RIR datasets and noise datasets are aggregated for creating reverberant or noisy versions (such as EARS-Reverb and EARS-WHAM), these are not independently released datasets introduced by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2 (EARS-WHAM and EARS-Reverb)",
          "reasoning": "The authors introduced EARS-WHAM and EARS-Reverb datasets derived from the newly recorded EARS clean speech data mixed with or convolved by existing noise and RIR datasets. These datasets are derived by applying transformations to the original clean recordings (adding noise or reverberation), thus based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset sources and methods are well documented in the paper."
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12996,
      "completion_tokens": 338,
      "total_tokens": 13334
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the EARS dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper that the EARS dataset is used to train models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate using the EARS dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of reinforcement learning (RL) based post-training methods involving the EARS dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The EARS dataset is used to construct benchmark datasets (EARS-WHAM and EARS-Reverb) for speech enhancement and dereverberation evaluation. The paper presents instrumental metric evaluations and listening tests on these benchmarks, and a blind test set with an evaluation server is introduced for performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on benchmarking and evaluation rather than analysis of dataset trends or characteristics independently."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the EARS dataset as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13719,
      "completion_tokens": 503,
      "total_tokens": 14222
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the EARS dataset as being composed of speech recordings from English speakers only. There is no mention of other languages included in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only recordings in English; no mention of two languages present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 and Abstract",
          "reasoning": "The EARS dataset contains speech from 107 English speakers recorded in an anechoic chamber. The content includes various speaking styles and emotions, but all material is in English. Transcriptions and metadata confirm that the language of the dataset is English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly English; no non-English language is included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio speech data only; no programming language or structured code content is present."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains some technical and mathematical descriptions but the dataset itself comprises recorded speech audio only, not math or logic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech recordings, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages in the dataset; all speech is natural English."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language coverage is specified clearly as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language speech recordings."
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10937,
      "completion_tokens": 227,
      "total_tokens": 11164
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper about publicly available code for dataset construction.",
          "reasoning": "The paper mentions download links for the dataset and an automatic evaluation server but does not mention any publicly available code repository or code for the data collection, preprocessing, or generation process for the EARS dataset or the derived datasets (EARS-WHAM, EARS-Reverb). Hence, the code is not made publicly available according to the paper content."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 and Section 3 (subsections 3.1 and 3.2) describe the dataset creation and processing.",
          "reasoning": "The paper provides detailed descriptions of dataset creation, including recording conditions in an anechoic chamber, microphone details, number and diversity of speakers, recording quality, speaking styles, and the creation of derived datasets EARS-WHAM and EARS-Reverb with details on mixing procedures, noise sources, impulse response usage, and test set details. This documentation is sufficient for transparency and reproducibility understanding from the paper content."
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6340,
      "completion_tokens": 157,
      "total_tokens": 6497
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Method, 2.2 Procedure, and Section 3. Results",
          "Reasoning": "The paper introduces a new dataset of acoustic recordings from seven male native Mehri speakers producing a set of items containing ejective and pulmonic alveolar and velar stops in various word positions. The data consists of manually recorded speech audio of these speakers during fieldwork in Oman, as described in Section 2.2, yielding 420 tokens which were analyzed acoustically (Section 3). This is explicitly newly recorded human speech data involving human participants; thus, the modality is audio and the data is human generated from direct field recordings."
        }
      ]
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7192,
      "completion_tokens": 203,
      "total_tokens": 7395
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.2 Procedure",
            "reasoning": "The annotation involved acoustic data being segmented and labeled manually using Praat by a phonetics expert, as indicated by the detailed manual acoustic segmentations and analysis described in the procedure section."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Procedure",
            "reasoning": "The paper specifies the use of a structured protocol with a fixed set of stimuli and precise measurement points for acoustic parameters, implying that detailed instructions were given to the annotator on how to segment and label the data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No explicit mention or description of scoring rubrics or rating scales for annotations is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide sample annotated data examples or annotation guidelines including examples."
          }
        }
      ]
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8322,
      "completion_tokens": 293,
      "total_tokens": 8615
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving multiple human expert annotators."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by an AI model as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although acoustic measurements were extracted using automatic tools (Parselmouth and VoiceSauce), this refers to data extraction rather than a quality assurance or validation step on annotation or data labels. There is no mention of an automated verification process to QA labels or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process related to dataset annotation, labeling, or validation. The dataset consists of recorded speech items with acoustic measurements extracted automatically, but no specific QA of the data or annotations is reported."
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7940,
      "completion_tokens": 413,
      "total_tokens": 8353
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 and throughout Results section",
          "reasoning": "The paper reports acoustic data recorded from seven native speakers of Omani Mehri during fieldwork. The stimuli consisted of 12 items designed for eliciting contrasts in ejective and pulmonic alveolar and velar stops. Each item was produced multiple times by each speaker, yielding a newly collected dataset of 420 tokens. The data were manually segmented and labeled before acoustic analysis, indicating entirely original content created from scratch by human contributors during fieldwork."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the data was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from other languages by human translators as part of dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources without modification; it was explicitly recorded anew from speakers."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not appear to be based on existing sources with modifications; it is original data from human speakers."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and data collection methods for the dataset are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8458,
      "completion_tokens": 237,
      "total_tokens": 8695
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Results) and Section 4 (Discussion and Conclusion)",
          "reasoning": "The dataset collected from seven native Mehri speakers was used primarily for analyzing acoustic correlates of ejective stops, examining durational and non-durational parameters, and studying sound change and phonetic variability in the endangered Mehri language. No mention of model training or evaluation is made; the dataset serves as a basis for phonetic and linguistic analysis of patterns and trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9181,
      "completion_tokens": 638,
      "total_tokens": 9819
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of recordings and acoustic measurements from Omani Mehri speakers. While speakers are bilingual in Mehri and dialectal Arabic, the dataset entries themselves contain only Mehri language items, not entries in multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of recordings of Mehri language items; although speakers are bilingual and the context discusses Arabic influence, the dataset entries are exclusively in Mehri, not bilingually coded."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset recordings contain speech in Omani Mehri, a non-English language. English is only used as the language of the paper and documentation, not in dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (Procedure) and throughout the paper",
          "reasoning": "The new dataset introduced consists of speech recordings and acoustic data of words from Omani Mehri only (a Modern South Arabian Semitic language) as produced by native Mehri speakers. The dataset items are exclusively in one non-English language, Mehri, as detailed in Section 2.2 and implied throughout the study."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are speech tokens in Mehri and associated acoustic measurements. Although computational tools like Parselmouth and Praat were used for analysis, the dataset itself does not contain code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses some mathematical notation (e.g., for D-prime calculation), but the dataset itself does not contain entries with mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech recordings in Mehri language; no biological sequences or non-human communication signals are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains data from a natural language (Mehri), not any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified and documented as Omani Mehri."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data in Omani Mehri, so it is not non-linguistic data."
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6399,
      "completion_tokens": 166,
      "total_tokens": 6565
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention or include any link to the code used for data collection, preprocessing, acoustic measurement extraction, or analysis for constructing the dataset. There is no indication that any code was made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 - Method (2.2 Procedure)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including information about the speakers, recording procedure, stimuli used (Table 1), the phonetic items, annotation and measurement methods (tools like Praat, Parselmouth, and VoiceSauce), acoustic parameters measured, and the analytical methods used to analyze the dataset. This constitutes comprehensive documentation of the dataset creation within the paper."
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7782,
      "completion_tokens": 140,
      "total_tokens": 7922
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4 and 5.2",
          "Reasoning": "The paper introduces a new synthetic dataset generated by a small Generative Adversarial Network (GAN) model trained to distill discriminative speech emotion representations from the original human-generated speech data (IEMOCAP). The synthesized data consists of SSL feature representations in time series format for use in downstream speech emotion recognition tasks. Thus, the new dataset modality is time series, and the data is model generated by the proposed GAN method, not directly human recorded or raw audio, nor of unknown origin."
        }
      ]
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8634,
      "completion_tokens": 229,
      "total_tokens": 8863
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 (Dataset Distillation for Speech Emotion) and Section 5 (Implementation details)",
            "reasoning": "The paper introduces a new synthetic dataset approach generated by a GAN-based generative model trained on the original IEMOCAP dataset. The synthetic dataset is created by the trained GAN model automatically without human annotators involved in annotation or labeling of the synthetic data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention any instructions given to annotators because the synthetic dataset is generated automatically by the GAN model, so no annotation instructions are needed or provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "Since no human annotations were performed, no scoring rubrics for annotation are described or applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No annotation examples are provided because the synthetic data generation is automated and does not involve manual annotation processes."
          }
        }
      ]
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9764,
      "completion_tokens": 344,
      "total_tokens": 10108
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being conducted by any human annotator, expert or otherwise, on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information in the paper indicates that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset is generated using a GAN (an AI model), the paper does not describe a process where an AI model is used to judge or verify the quality of annotations or dataset content as a QA process. The GAN is used to generate the dataset itself rather than serving as a verifier for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any automated verification, rule-based checks, or algorithmic validation steps explicitly performed for quality assurance on the synthetic dataset or its annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any explicit quality assurance process applied to the newly generated distilled dataset. There is no mention of human or automated quality checks or validation of dataset annotations or content. The dataset is generated by the proposed GAN-based method and evaluated indirectly via downstream model performance, but no QA process is documented."
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9382,
      "completion_tokens": 414,
      "total_tokens": 9796
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of new datasets by human contributors. It focuses on distilling existing data and generating synthetic data via models."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4, 5.2 and multiple mentions throughout the paper",
          "reasoning": "The paper proposes a generative model (GAN) trained on existing speech emotion data to generate synthetic datasets. These synthetic datasets are new data generated entirely by the AI model, distinct from original data points, intended to train downstream models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data is produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of machine translation to produce data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not merely collected or aggregated; rather, synthetic datasets are produced via GANs, which transform and generate data representations."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 and 4: Dataset Distillation and Dataset Distillation for Speech Emotion",
          "reasoning": "The synthetic datasets are derived from the original IEMOCAP dataset by learning a distribution and generating data adapted from the original, but significantly transformed into synthetic representations for downstream tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method and source of the data are clearly described; thus, this label is not applicable."
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9900,
      "completion_tokens": 198,
      "total_tokens": 10098
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2 and Table 2",
          "reasoning": "The synthetic dataset generated by the proposed GAN-based dataset distillation method is used to train downstream speech emotion recognition models in a supervised manner, with the evaluation performed on real data test sets to measure task performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10623,
      "completion_tokens": 595,
      "total_tokens": 11218
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on dataset distillation for speech emotion recognition using the IEMOCAP dataset which is an English dataset. There is no indication that multiple languages are included in the proposed synthetic dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset discussed is the IEMOCAP, which is English only. No mention of any second human language was made for the newly generated synthetic dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 4 (Dataset Distillation for Speech Emotion); usage of IEMOCAP dataset",
          "reasoning": "The new dataset is distilled from the IEMOCAP dataset, which consists of English speech data. The paper explicitly uses IEMOCAP for speech emotion recognition and does not mention any other language, indicating the synthetic data is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states usage of the IEMOCAP dataset which is English; no non-English language is described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code data entries are included in the proposed synthetic dataset. The paper focuses on generative modeling of speech feature representations, not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes mathematical formulae to describe loss functions and GAN training, the dataset itself does not contain mathematical or logical notation as content entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns human speech emotion recognition, no biological sequences or non-human communication signals are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial languages are present in the dataset or discussed in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English from the use of IEMOCAP dataset; hence the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language speech data (or their synthetic representations), so it contains language entries."
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7841,
      "completion_tokens": 211,
      "total_tokens": 8052
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not mention any link, URL, or repository where the code for the dataset distillation generative model (GAN-based) or dataset generation is made publicly available. There is no indication of code release for reproducing the dataset generation or GAN training."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3, 4, and 5",
          "reasoning": "The paper documents the dataset distillation process in detail, including the methodology of training the GAN generator as a dataset distillator, the specific losses introduced (softmax matching loss, diversity penalty), architectures used (GAN-CNN and GAN-ATT), and experimental set-up. It describes the conditioning on IEMOCAP emotion labels, the dataset size budgets, and evaluation procedures. Although the original dataset (IEMOCAP) is publicly known, the method to distill it into a generative model dataset is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7728,
      "completion_tokens": 104,
      "total_tokens": 7832
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Table 1",
          "Reasoning": "The paper describes synthetic audio produced by running target language text through trained TTS systems of higher-resource pivot languages, generating synthetic text-speech pairs used for ASR augmentation. This synthetic audio is generated by AI models (Microsoft TTS and Google Cloud TTS neural models) without direct human recording, hence model generated."
        }
      ]
    }
  },
  {
    "id": "robinson22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8580,
      "completion_tokens": 176,
      "total_tokens": 8756
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "The synthetic data is generated automatically by feeding authentic text into pretrained TTS systems (Microsoft and Google Cloud TTS) for several languages, producing synthetic text-speech pairs without human manual annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "There is no mention of instructions given to annotators since the synthetic data generation is automatic, not a manual annotation task."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "No scoring or rubric system is described for this automatic synthetic data generation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "No annotation examples or guidelines are provided for this automatic synthetic data generation."
          }
        }
      ]
    }
  },
  {
    "id": "robinson22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9710,
      "completion_tokens": 399,
      "total_tokens": 10109
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for the datasets. There is no information about expert annotation or verification."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA by multiple human experts or expert annotators. There is no mention of expert involvement in dataset validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert. The paper does not provide details on human annotation or verification by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance by multiple non-expert annotators or human validators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using any AI model to perform quality assurance on the new datasets introduced."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an automatic verification process such as rule-based or algorithmic QA applied to dataset annotations or quality."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process for the new datasets presented. The datasets are described in terms of sources and sizes, but no QA procedures or validation by humans, AI, or automated verification is reported."
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9328,
      "completion_tokens": 497,
      "total_tokens": 9825
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not claim to have created original content entirely from scratch by human contributors. The data used such as Kiswahili authentic recordings, Guaran\u00ed data from Common Voice, and Suba corpus are pre-existing or collected from other sources."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (TTS Augmentation)",
          "reasoning": "The authors generate synthetic audio data by inputting authentic text into pretrained TTS neural models (Microsoft TTS and Google Cloud TTS) from high-resource pivot languages. This synthetic speech data is newly generated by AI models and does not directly copy or transform existing audio data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of machine translation to generate data; even transliteration described is done via hand-crafted phone maps, not automatic translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 (Data setup) and Table 1",
          "reasoning": "Text and audio datasets such as the Helsinki Swahili corpus, Mozilla Common Voice data for Italian and Guaran\u00ed, and AfricanVoices Suba corpus are collected or aggregated from existing resources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Transliteration)",
          "reasoning": "The paper describes deriving transliterated text from the original target language texts using hand-crafted phone maps to approximate orthography for pivot languages. This transliterated text is a modified form of the original data, used as input to the pivot language TTS."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The sources and method of generation of all datasets used or created are documented explicitly."
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9846,
      "completion_tokens": 362,
      "total_tokens": 10208
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 Experimental Setup; Section 4 Results",
          "reasoning": "The novel datasets of synthetic TTS-generated audio paired with text, produced by passing target language text through high-resource pivot language TTS systems, are used to train ASR models from scratch. Experiments involve varying amounts of authentic and synthetic data combined to train an end-to-end ASR system, without mention of pre-training or fine-tuning existing models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Synthetic data amount; Section 4.4 Application to low-resource languages",
          "reasoning": "The datasets including synthetic TTS-augmented training sets and authentic test sets are used to evaluate and benchmark ASR performance, in terms of word error rate and character error rate. Authentic test sets remain constant to measure improvements from training with synthetic data."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Choice of pivot language; Section 4.3 TTS quality",
          "reasoning": "The datasets are used to analyze the effects of different pivot languages and TTS quality on ASR performance and characteristics of recognition. The paper reports trends and surprising findings, analyzing the relationship between TTS quality and augmentation benefit."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10569,
      "completion_tokens": 556,
      "total_tokens": 11125
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 and throughout the paper",
          "reasoning": "The new datasets introduced for experiments contain entries in multiple distinct human languages: Kiswahili, Guaran\u00ed, Italian, and Suba. The authors use text and speech data for these four languages as part of their augmentation and ASR experiments, demonstrating multilingual coverage."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While bilingual datasets might appear in the context of paired languages, the datasets explicitly described and used are monolingual in separate languages, with experiments conducted across multiple such languages rather than a bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not consist solely of English content; rather, they focus on other languages like Kiswahili, Guaran\u00ed, Italian, and Suba."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Table 1",
          "reasoning": "Each dataset is monolingual in a non-English language, e.g., Kiswahili, Guaran\u00ed, Italian, or Suba. The data entries are distinctly in each one language, making the individual datasets monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets containing code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not involve biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets are clearly specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language data."
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7787,
      "completion_tokens": 185,
      "total_tokens": 7972
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.1 Data setup and Table 1",
          "reasoning": "The paper provides a link to the data with train/val/test splits and details about subsets for each experiment at https://github.com/n8rob/MultilingualTTSAugmentation, indicating publicly available code and data-related resources for dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data setup and related paragraphs describing authentic and synthetic data preparation, transliteration, and Table 1",
          "reasoning": "The paper documents the dataset creation process in detail, including authentic data sources for Kiswahili, Guaran\u00ed, Suba, and Italian, text corpora used for TTS prompts, synthetic data generation via Microsoft and Google TTS, transliteration methods, and data quantity used, which ensures transparency and completeness in dataset creation."
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8039,
      "completion_tokens": 181,
      "total_tokens": 8220
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The new dataset is simulated two-speaker overlapped utterances created from a corpus of single speaker audio-visual YouTube utterances. The overlapped audio waveform was created by offsetting and adding single speaker audio signals, thus involving both human-recorded audio and algorithmic signal mixing."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The video data consists of mouth-tracks extracted from YouTube videos (human-recorded video data). To simulate overlap, video frames are shifted and non-speech regions are filled with repeated frames, which is a model-generated manipulation, making the origin both human and model generated."
        }
      ]
    }
  },
  {
    "id": "rose22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8891,
      "completion_tokens": 241,
      "total_tokens": 9132
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 describes that the training set was created from a corpus of single speaker A/V YouTube utterances containing aligned face and mouth tracks, and that overlapped and single speaker test sets were obtained from human transcribed utterances with aligned mouth tracks taken from YouTube videos, implying multiple human experts performed the transcription annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to annotators. The description focuses on dataset creation and mentions human transcription but does not mention specific annotation guidelines or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of scoring rubrics or criteria for annotation quality or decision making in the paper regarding dataset annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide annotation examples or example guidelines illustrating the annotation process for the human transcriptions included in the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "rose22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10021,
      "completion_tokens": 335,
      "total_tokens": 10356
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert annotator for the new simulated two-speaker overlapped utterances dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the use of AI models for speech recognition and attention mechanisms but does not indicate that any AI model was employed to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of any automated verification or algorithmic QA process applied to validate dataset annotations or content; the creation process is described but no automated quality verification is documented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset is simulated by combining existing single-speaker YouTube utterances with offsets and overlaps, and while human transcriptions are used for test sets, the paper does not document any explicit quality assurance process applied to these annotations or the simulated overlapping speech dataset. Therefore, no quality assurance process is described or performed for the new dataset."
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9639,
      "completion_tokens": 548,
      "total_tokens": 10187
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states that a training set and test sets of simulated two-speaker overlapped utterances were created from a corpus of single speaker audio-visual YouTube utterances containing aligned face and mouth tracks. The YouTube utterances are collected as original data, and the simulated overlapped utterances are constructed by mixing pairs of these human-created utterances. Thus, the original single-speaker audio-visual utterances are new data collected from human contributors, as described in Section 3.1."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets were not generated entirely by AI or machine learning models. Instead, existing YouTube utterances recorded by humans were used to create the datasets, and overlapping utterances were simulated by mixing these utterances."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being produced by translation from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper explains that the training data are created by aggregating single-speaker utterances from an existing YouTube corpus. These utterances, collected from different videos, are combined to form overlapped speech utterances by mixing audio and aligning mouth tracks. This constitutes a form of data collation from existing sources without significant modification to the base data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The two-speaker overlapped utterances are derived datasets created by mixing two single-speaker utterances with offsets and combining their videos to simulate overlapping speech. Video frames in non-speech regions are modified by repeating frames forward and backward to ensure continuous visual presence, which is a form of modification and adaptation of the original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides explicit details about the derivation and creation of the datasets, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10157,
      "completion_tokens": 269,
      "total_tokens": 10426
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper describes creation of a new simulated two-speaker A/V overlapped utterance dataset from single speaker YouTube utterances, which is used as training data for the multi-talker models including the VCAM. The dataset of 20k hours is specifically used for training models from scratch as indicated in Sections 2 and 3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The paper uses the same newly created simulated overlapped and single speaker datasets as test sets for benchmarking and evaluating the model performance (WER, active speaker detection mAP). This indicates the datasets are used for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10880,
      "completion_tokens": 582,
      "total_tokens": 11462
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The new dataset introduced, created from YouTube videos, is described as a corpus of single speaker audio-visual utterances. There is no indication of multiple languages; all content is implied to be in English. Therefore, the dataset is not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The dataset consists of single speaker utterances synthesized into overlapping speech, but no mention is made of two languages per entry. Thus, it is not bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is created from YouTube utterances with aligned face and mouth tracks, as per references [18, 22, 5, 8], which are known to be English language datasets. The paper does not indicate any other languages are included. Hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of a dataset containing only a non-English language is made for the new data introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains audio-visual speech data, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The entries of the dataset are audio-visual speech data without mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech and visual mouth tracks, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is specified to be English, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language speech data (English), so language is present."
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8098,
      "completion_tokens": 233,
      "total_tokens": 8331
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any link, URL, or reference to publicly available code related to the dataset creation, data collection, or preprocessing steps. There is no explicit statement indicating that code used for constructing the dataset is available to the public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Simulated audio-visual overlapping speech corpora)",
          "reasoning": "Section 3.1 extensively documents the process of constructing the simulated two-speaker overlapped utterances dataset from a corpus of single speaker A/V YouTube utterances. The paper describes the method of offsetting and mixing audio signals, alignment and offsetting of video mouth tracks, handling of non-speaking video frames by repetition, simulation of single speaker scenarios (TwoFace and OneFace), offset selection for overlap intervals, and size and composition of the training and test sets. This description provides sufficient detail on dataset construction for understanding and potential reproduction of the simulation methodology, although it does not indicate publication of the original single speaker source data."
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7508,
      "completion_tokens": 176,
      "total_tokens": 7684
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 and Section 2",
          "Reasoning": "The kidsTALC dataset introduced in this paper consists of recorded audio from German children's speech collected through examiner-child interactions using human-operated devices (Olympia dictaphone). The speech is naturally spoken by children aged 3 \u00bd to 11 years, captured and manually transcribed by humans, indicating human-generated origin."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The dataset includes manual orthographic and phonetic transcriptions of the audio recordings by trained human transcribers and transcription agencies, ensuring text data originates from human generation directly related to the collected audio speech data."
        }
      ]
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8360,
      "completion_tokens": 197,
      "total_tokens": 8557
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.3",
            "reasoning": "The transcriptions and annotations are completed by trained graduate students of speech language therapy, trained speech language therapists, and a professional transcription agency. The process involves multiple annotators including second checks and consensus discussions, indicating multiple human experts were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "A transcription protocol incorporating a three step procedure for each recording is applied to ensure reliability and consistency of transcriptions and annotations. This protocol implies detailed instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No explicit mention of scoring rubrics or detailed criteria for scoring annotations is provided in the text."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention providing annotation examples in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9490,
      "completion_tokens": 394,
      "total_tokens": 9884
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset QA process involves multiple annotators; it is not limited to a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Data Transcription and Annotation",
          "reasoning": "The paper states that manual transcriptions and annotations are completed by trained graduate students of speech language therapy, trained speech language therapists, and a professional transcription agency, indicating multiple human experts with subject matter expertise performed the quality assurance. A three-step procedure involving initial transcription, checking by a different transcriber, and final checking by a professional agency with consensus discussion was used to ensure reliability and consistency."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of single human non-expert QA; all transcribers are described as trained or expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotators are trained or expert; non-expert annotators are not mentioned."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that an AI model was employed to judge or quality assure the annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based algorithmic verification processes are described for QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A detailed quality assurance process is described involving multiple expert annotators and consensus checking."
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9108,
      "completion_tokens": 488,
      "total_tokens": 9596
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Data Acquisition and Section 2.1 Participants",
          "reasoning": "The paper details the collection of a novel dataset called kidsTALC consisting of audio recordings and manual orthographic and phonetic transcriptions of spontaneous German children's speech. The data was collected directly from human participants (children aged 3\u00bd\u201311 years) in examiner-child interactions, recorded in their natural environments (home or kindergarten), with no indication that the data was adapted or translated from existing sources. Section 2 explains that the data was recorded and transcribed by trained humans following specific transcription protocols ensuring high quality. Therefore, the data is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated solely or partially by AI or machine learning models. The speech data consists of real recordings of children's spontaneous speech and manually created transcriptions."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and transcriptions do not involve translation from another language. The speech recordings are originally in German. No mention of translation or bilingual corpus creation is made."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that machine translation was used for generating the dataset or its annotations."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not an aggregation of existing datasets or corpora. It is newly recorded and transcribed from scratch according to the paper."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not derived from existing datasets or transformed from previous sources. Instead, it was newly collected and transcribed manually."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9626,
      "completion_tokens": 296,
      "total_tokens": 9922
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4. Automatic Speech Recognition",
          "reasoning": "The dataset is used to train an end-to-end automatic speech recognition model from scratch using supervised learning methods, as described in Section 4, where a baseline model is trained using the kidsTALC dataset for phone recognition with connectionist temporal classification (CTC) loss."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4. Automatic Speech Recognition",
          "reasoning": "The dataset is also used as a benchmark for evaluating the performance of the baseline ASR model by measuring the phone error rate (PER) on the train and test subsets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3. Statistical Analysis",
          "reasoning": "The dataset is analyzed statistically in Section 3 to assess transcription quality (transcriber agreement), age distribution of participants, word and phone distributions, providing insights into the characteristics and challenges of the data."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10349,
      "completion_tokens": 420,
      "total_tokens": 10769
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 2.1 Participants",
          "reasoning": "The kidsTALC dataset contains speech data collected specifically from monolingual German children aged 3 \u00bd to 11 years. The paper explicitly states that the dataset focuses on 'typically developing, monolingual German children' (Section 1) and the participants' eligibility criteria include monolingual German speakers (Section 2.1). Therefore, the dataset entries contain exactly one language which is non-English (German)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data consisting of German child speech with orthographic and phonetic transcriptions, so it clearly contains entries with language."
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7567,
      "completion_tokens": 146,
      "total_tokens": 7713
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not specified in the paper",
          "reasoning": "The paper mentions using SpeechBrain for the ASR baseline and adjusting the recipe for their dataset, but does not provide any direct link or statement about making the code used for data collection, transcription, or preprocessing publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 through 3",
          "reasoning": "The paper thoroughly documents the dataset creation process including participant recruitment, recording setting, transcription and annotation protocols, phonetic token set, and statistical analysis of the dataset. These details are found primarily in Sections 2 and 3, providing transparency and completeness on how the dataset was constructed."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6494,
      "completion_tokens": 96,
      "total_tokens": 6590
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Experimental Setting",
          "Reasoning": "The kidsTALC corpus is described as consisting of speech of 3 \u00bd \u201311 years old German speaking children, indicating human recorded audio data of children's speech. The corpus is explicitly stated as publicly available and used for phonetic transcription, confirming its human-generated origin as recorded speech audio."
        }
      ]
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7346,
      "completion_tokens": 217,
      "total_tokens": 7563
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3 and Section 4.2",
            "reasoning": "The re-annotation of 500 utterances of kidsTALC was performed by trained speech language therapists, which are experts in the field as described in Section 3 and referenced in Section 4.2."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "It is stated that for re-annotation, annotators used the orthographic transcription and were provided multiple pronunciations to choose from with the ability to edit or add new pronunciations, implying structured instructions to guide annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not mention any scoring rubrics or explicit criteria for scoring during the re-annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No examples or illustrative annotation cases are described or provided in the paper regarding the re-annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8476,
      "completion_tokens": 258,
      "total_tokens": 8734
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Setting) and Section 4.2 (Relation to token changes during re-annotation)",
          "reasoning": "The paper describes re-annotation of 500 utterances of the kidsTALC corpus done by trained speech language therapists, who are subject matter experts. These experts performed the annotation individually, indicating a single human expert quality assurance process for the re-annotated data."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by an AI model as a judge on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic checking process is described for quality assurance of the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a clear expert human re-annotation quality assurance process; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8094,
      "completion_tokens": 427,
      "total_tokens": 8521
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Setting) and Section 4.2",
          "reasoning": "The paper describes re-annotation of 500 utterances from the kidsTALC corpus by trained human annotators (speech language therapists) creating new phonetic transcriptions. This re-annotation represents original content created entirely from scratch by human contributors and not derived or adapted from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any completely synthetic data generated solely by models; all datasets used are either existing corpora or human-produced annotations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by human translation from another language is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation from another language is present in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets (TIMIT, MCV, kidsTALC) are used as-is; there is no indication that the authors collected or aggregated data from existing sources to create new datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors utilize existing datasets and provide re-annotations, but do not appear to create derived datasets by modification or transformation of existing corpora beyond annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the origin of the re-annotated data as human-created, clearly outlining the source and method of its generation."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8612,
      "completion_tokens": 503,
      "total_tokens": 9115
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate using the introduced dataset for pre-training models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced dataset kidsTALC is not used for training models from scratch; the models are pre-existing and the authors do not report training from scratch using kidsTALC."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses pre-trained models and fine-tuned wav2vec2.0 models on other data but does not explicitly state that the newly introduced kidsTALC dataset is used for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training techniques such as RLHF related to the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3 (Experimental Setting), 4 (Results and Discussion), especially 4.1 and 4.2",
          "reasoning": "The newly introduced kidsTALC corpus is used as a test set for evaluating uncertainty estimation methods. The paper holds out test and validation sets from kidsTALC and uses them to benchmark predictive uncertainty and error detection capabilities of the ASR models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The paper uses the kidsTALC dataset to analyze the relationship between model uncertainty and human annotator uncertainty by re-annotating 500 utterances and comparing token changes with model uncertainty estimates."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the new dataset as an external knowledge base for augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new dataset has demonstrated practical use in both evaluation and analysis as documented in the paper."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9335,
      "completion_tokens": 562,
      "total_tokens": 9897
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The new dataset introduced by the authors is kidsTALC, which contains only German children's speech. There is no mention of more than two human languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The proposed dataset, kidsTALC, contains speech in exactly one language: German. There is no indication it contains two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "KidsTALC is a corpus of German children\u2019s speech; it is not in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Setting); Section 1 (Introduction)",
          "reasoning": "The kidsTALC corpus is described as 'a publicly available child speech corpus' containing speech of German-speaking children. Therefore, it contains exactly one non-English language, German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The dataset is speech data and phonetic transcriptions, with no programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "While the paper contains mathematical formulas describing the CTC uncertainty calculation, the dataset entries themselves do not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The dataset consists of human speech recordings and annotations, no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "No presence of fictional or artificially created languages is mentioned for the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The language of the dataset is clearly stated as German for kidsTALC; hence, language is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The kidsTALC dataset is human speech data and thus contains language."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6553,
      "completion_tokens": 172,
      "total_tokens": 6725
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not include any link or mention of publicly available code related to dataset construction, collection, preprocessing, or generation. The paper notes using existing datasets (TIMIT, kidsTALC, Mozilla Common Voice) and pretrained models without own training or code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Experimental Setting)",
          "reasoning": "The paper provides information about the used datasets, their composition, and the re-annotation process for kidsTALC including the selection procedure and treatment of annotations. While it does not describe the original dataset collection in detail (as it uses existing datasets), it documents the re-annotation protocol for kidsTALC in sufficient detail relevant to the new annotations introduced for this study."
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7764,
      "completion_tokens": 151,
      "total_tokens": 7915
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data collection",
          "Reasoning": "The MuPTA corpus consists of audio recordings of spontaneous and read speech collected from 30 native Russian speakers using handheld devices, explicitly recorded by humans, as described in Section 3.1."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data collection",
          "Reasoning": "The MuPTA corpus includes video recordings captured simultaneously with audio via smartphones and tablet cameras, recording facial expressions and behavior during speech, thus human-generated video data as per Section 3.1."
        }
      ]
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8616,
      "completion_tokens": 230,
      "total_tokens": 8846
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.2",
            "reasoning": "The annotation derives from a self-evaluation questionnaire filled by each speaker (informant) themselves. Each speaker is thus a human expert of their own personality by definition, implying a single human expert per annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper only mentions that each speaker completed a standard self-evaluation questionnaire of 60 questions. There is no description of instructions given to annotators, since these annotations come from self-report questionnaires rather than separate annotation procedures."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No scoring rubrics for annotation are discussed or provided in the paper for the annotation procedure; the personality traits scores are derived directly from standard self-evaluation questionnaires."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No examples of annotation or questionnaire completion are presented or mentioned in the paper as part of the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9746,
      "completion_tokens": 257,
      "total_tokens": 10003
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single human expert annotating or validating the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance or validation of the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset annotation quality assurance was not described as being conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of AI models being used to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification, algorithmic or rule-based QA process applied to validate the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance or validation process performed on the dataset annotations or collected data."
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9364,
      "completion_tokens": 352,
      "total_tokens": 9716
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection",
          "reasoning": "The MuPTA corpus was collected by recording 30 native Russian speakers performing specific tasks including spontaneous speech and reading scripted sentences. The data consists of original audio-visual recordings created entirely from scratch by human contributors (the speakers), not adapted or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus data consists of human recordings; no data was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation systems as per the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MuPTA corpus is not an aggregation of existing sources; it was newly recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the dataset is based on or adapted from existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation process are explicitly documented."
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9882,
      "completion_tokens": 269,
      "total_tokens": 10151
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The MuPTA corpus is utilized to train and fine-tune the proposed audio-visual neural network models for personality traits assessment using supervised learning methods as described in the methodology and results sections."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The MuPTA corpus is used to evaluate and benchmark the performance of the proposed approach in assessing personality traits, with analysis of metrics such as accuracy and concordance correlation coefficient."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The authors analyze the impact of speech type (spontaneous vs. read) on PTA performance and investigate performance trends across different signal lengths and modalities using the MuPTA corpus."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10605,
      "completion_tokens": 390,
      "total_tokens": 10995
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection; Table 1",
          "reasoning": "The MuPTA corpus contains data from 30 native Russian speakers. The paper explicitly states that the corpus is in the Russian language only (midly-resourced Russian language), including speech data and scripted sentences in Russian. Table 1 compares MuPTA to other corpora and lists its language as 'Ru' (Russian). Therefore, the dataset entries are monolingual in a non-English language, namely Russian."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7823,
      "completion_tokens": 156,
      "total_tokens": 7979
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 6, Conclusions",
          "reasoning": "The paper states in the Conclusions section that both the source code of the proposed approach and the MuPTA corpus can be found at the web-page https://oceanai.readthedocs.io, indicating that code related to dataset processing and the approach is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3, Data Collection and Data Annotation",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 3, describing data collection devices, recording setup, speech tasks (spontaneous and read speech), synchronization procedures, and annotation via self-evaluation questionnaires. This provides transparency on the dataset construction process."
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 2917,
      "completion_tokens": 243,
      "total_tokens": 3160
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, paragraph describing MuPTA corpus",
          "Reasoning": "The MuPTA corpus is introduced by the authors (reference [3]) as their own multimodal personality traits assessment dataset. Given it includes video modality as used in their evaluation, and such data is recorded from human participants, this video data is human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, paragraph describing MuPTA corpus",
          "Reasoning": "The MuPTA corpus includes audio recordings corresponding to the personality traits assessment tasks. As this data is recorded from humans speaking, it is human generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, paragraph describing MuPTA corpus",
          "Reasoning": "The text modality in the MuPTA corpus comes from speech transcripts of humans. The Whisper model is used for automatic speech recognition, converting human spoken language into text, indicating that the original text content is human generated."
        }
      ]
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 3769,
      "completion_tokens": 187,
      "total_tokens": 3956
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 and Section 3",
            "reasoning": "The paper describes a new MuPTA corpus introduced by the authors, but it does not specify any human annotation process. Instead, it emphasizes the use of automatic feature extraction using pretrained models and neural networks rather than human-labeled annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention of annotation instructions provided for human annotators since no human annotation process is described for MuPTA."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or human-based rating guidelines are mentioned for annotation of the MuPTA corpus."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples are provided or referenced since no human annotation guidelines are given."
          }
        }
      ]
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 4899,
      "completion_tokens": 290,
      "total_tokens": 5189
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information described in the paper about multiple human experts performing quality assurance on dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for analysis and feature extraction, the paper does not specify that AI models perform quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification process as a form of quality assurance for dataset annotation or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the MuPTA corpus but provides no description or mention of any quality assurance process applied to its annotations or content. There is no information about validation or checking of the dataset quality; thus, no quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 4517,
      "completion_tokens": 401,
      "total_tokens": 4918
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: 'Using our framework, we also obtained initial baselines for our MuPTA corpus [3]'",
          "reasoning": "The MuPTA corpus is introduced by the authors as their own dataset, implying it was collected anew, presumably from human participants, as typically collected multimodal human behavior data. The paper mentions it alongside the existing First Impressions v2 corpus but differentiates MuPTA as introduced by the authors. This suggests original content collected from human contributors rather than derived or collated data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data introduced is not generated by AI or machine learning models; rather, models are applied to real human data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was machine-translated."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "MuPTA is presented as a newly introduced dataset rather than aggregated from existing data without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the introduced dataset is constructed by modifying or adapting existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the MuPTA dataset is specified as new data introduced by the authors."
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 5035,
      "completion_tokens": 465,
      "total_tokens": 5500
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MuPTA corpus or any new dataset introduced for pre-training models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper of training models from scratch using the new MuPTA corpus. The training is mentioned generally but without clarifying that it is from scratch on this new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2, last paragraph",
          "reasoning": "The paper states that the OCEAN-AI framework was tested on their new MuPTA corpus and achieved initial baselines. This implies the dataset is used for supervised fine-tuning of the model to predict personality traits."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that reinforcement learning or RL-based methods are applied to the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2, last paragraph",
          "reasoning": "The MuPTA corpus is used to establish initial baselines and evaluate the framework's performance, indicating its use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MuPTA corpus specifically for trend or pattern analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The MuPTA corpus is actively used for supervised fine-tuning and evaluation as described in the paper."
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 5758,
      "completion_tokens": 225,
      "total_tokens": 5983
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2, Text information analysis module",
          "reasoning": "The text information analysis module uses the LIWC set intended for psychological analysis of English texts, indicating that the new MuPTA corpus introduced contains English language content only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 2976,
      "completion_tokens": 254,
      "total_tokens": 3230
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2, paragraph starting with 'Source codes [7] and a software documentation [8] of OCEAN-AI are free-available.'",
          "reasoning": "The paper mentions that source code and software documentation for the OCEAN-AI framework are freely available, but this relates to the PTA framework and not explicitly to the code used for constructing the new MuPTA dataset, which is introduced by the authors. There is no explicit mention or link indicating that the code used for collecting or preprocessing the MuPTA dataset itself is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 and throughout the paper, especially the paragraph describing the OCEAN-AI Framework Architecture and the mention of their MuPTA dataset in evaluation.",
          "reasoning": "The paper introduces the MuPTA corpus as a new dataset and references prior publications [3] for more details. While not extensively detailed in this specific paper, some documentation about the dataset creation process is provided, notably in the context of its use and initial baselines. The paper provides enough description to understand the dataset's purpose and general characteristics, implying some degree of documentation on the dataset's creation."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7105,
      "completion_tokens": 156,
      "total_tokens": 7261
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Voice recording",
          "Reasoning": "The CALLS corpus contains speech audio data recorded by the same female speaker acting as an operator, who uttered phone-call lines in a recording studio, thus the audio data are human-recorded."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Crowdsourcing phone-call dialogue lines",
          "Reasoning": "The dialogue lines (text) were written by human crowdworkers via microtask crowdsourcing, following specified instructions to create dialogues between operator and customers, constituting human-generated text data for the corpus."
        }
      ]
    }
  },
  {
    "id": "saito23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7957,
      "completion_tokens": 256,
      "total_tokens": 8213
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2, Section 3.1",
            "reasoning": "The paper states that dialogue lines were collected by microtask crowdsourcing where many crowdworkers wrote dialogue lines. Specifically, 150 crowdworkers for complaint handling and 1,400 for attentive listening participated. These crowdworkers wrote the dialogue lines according to instructions, indicating multiple non-expert humans performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Crowdworkers were given task descriptions and specific instructions such as writing each dialogue line following a given format and anonymizing sensitive information. Also, for attentive listening, instructions included creating four-turn dialogues with positive emotion. This shows clear, detailed instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention any scoring rubrics or criteria used to evaluate or score the annotations performed by crowdworkers."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe providing explicit examples of dialogue lines to crowdworkers in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "saito23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9087,
      "completion_tokens": 329,
      "total_tokens": 9416
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper indicating that multiple human experts conducted quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 3.1",
          "reasoning": "The dataset dialogue lines were crowdsourced via microtask crowdsourcing from multiple crowdworkers who are not described as experts. The paper states that 150 and 1,400 crowdworkers participated to collect dialogue lines for complaint handling and attentive listening subsets, respectively. The authors screened and proofread the collected dialogues to exclude spam and inappropriate content, indicating a quality check process involving multiple non-expert annotators (crowdworkers)."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA process does not involve an AI model judging the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification techniques as part of quality assurance for the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented via the crowdsourcing screening and proofreading process described in Sections 2.2 and 3.1."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8705,
      "completion_tokens": 467,
      "total_tokens": 9172
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 and 2.3",
          "reasoning": "The paper states that dialogue lines for simulated customer-center phone calls were crowdsourced from human workers, who wrote these dialogues based on provided scenarios. The speech recordings of these lines were performed by a single female speaker acting as the operator. Hence, the new dataset consists of original content created from scratch by human contributors via crowdsourcing and human voice recordings, without translation or derivation from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated entirely by AI or machine learning models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of the dataset being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation was applied or stated as part of data generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset scenarios for complaint handling were created by selecting and referencing existing complaint data from the FKC corpus, which contains approximately 5 million text complaints. Although the dialogue lines themselves were written by crowdsourcing, they were based on existing complaint data extracted from the FKC corpus, indicating the dataset incorporates collated content from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset uses complaint data from the FKC corpus to create dialogue scenarios, the dialogues themselves are newly written rather than modified versions of existing dialogues. There is no clear evidence of transformations or adaptations applied to existing dialogue lines."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data collection methodology and source of content."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9223,
      "completion_tokens": 423,
      "total_tokens": 9646
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "In Section 4, the authors describe training EDSS models from randomly initialized parameters using the CALLS corpus for developing empathetic dialogue speech synthesis. They explicitly mention the use of the CALLS corpus for training an acoustic model and a neural vocoder, indicating training from scratch after pretraining on a separate corpus (JSUT)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper details fine-tuning the FastSpeech 2 model pretrained on the JSUT corpus with the CALLS corpus training data for EDSS tasks, representing supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning or RL-based post-training methods using the CALLS corpus."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The authors perform MOS tests to evaluate naturalness and speaking-style similarity of synthetic speech generated using models trained with the CALLS corpus, indicating its use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4",
          "reasoning": "Section 3.4 provides analysis of domain differences between the CALLS and STUDIES corpora in prosodic and textual features, showing use of the CALLS corpus primarily for analyzing characteristics of empathetic spoken dialogues."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CALLS corpus is not described or used as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates multiple practical uses of the CALLS corpus in training, analysis, and evaluation stages."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9946,
      "completion_tokens": 399,
      "total_tokens": 10345
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CALLS corpus introduced in the paper contains only Japanese language data. There is no mention of multiple languages being used in the corpus."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CALLS corpus contains dialogue lines entirely in Japanese. There are no indications that a second human language is included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is recorded in Japanese and the paper extensively discusses Japanese speech data, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 2. Corpus Construction Methodology",
          "reasoning": "The CALLS corpus is a Japanese speech corpus, as stated in the abstract and throughout the paper. All dialogues and speech recordings are in Japanese only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of code or programming language data in the CALLS corpus. The dataset consists of spoken dialogue and text transcripts in Japanese."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical notations are present in the dataset according to the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human dialogue speech only; no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any fictional or artificial languages in the corpus. It only contains Japanese language data."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly described as Japanese, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset explicitly contains spoken and textual Japanese dialogue data containing human language."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7164,
      "completion_tokens": 161,
      "total_tokens": 7325
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not mention any link or repository containing the code used for data collection, preprocessing, or corpus generation. While a project page URL is provided for the corpus, no explicit mention or link to code resources is found."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Corpus Construction Methodology, Sections 2.1 to 2.3",
          "reasoning": "The paper provides a detailed explanation of the corpus construction process, including dialogue scenario design (Section 2.1), crowdsourcing procedure for dialogue lines (Section 2.2), and voice recording methodology (Section 2.3). This clear and transparent documentation enables understanding of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7391,
      "completion_tokens": 117,
      "total_tokens": 7508
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.2; Section 3.2",
          "Reasoning": "The SRC4VC corpus contains 11 hours of speech recorded on smartphones by 100 Japanese speakers, captured through crowdsourcing with humans actively recording their voices using smartphones in their own rooms. The data consists of actual human voice recordings, verified by manual validation, and is explicitly described as smartphone-recorded audio samples, establishing human generation and excluding model or unknown origins."
        }
      ]
    }
  },
  {
    "id": "saito24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8243,
      "completion_tokens": 238,
      "total_tokens": 8481
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3, Table 1",
            "reasoning": "The paper states that speaker-wise recording quality scores and utterance-wise perceived emotion labels were crowdsourced from multiple crowdworkers. Table 1 specifies hundreds of workers participated in recording quality rating and emotion labeling, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Section 2.2 describes that the recording webpage contained instructions for recording for each subset, implying that guidelines and instructions were provided to the crowdworkers performing the tasks."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "Recording quality was rated using a five-scale Mean Opinion Score (MOS) from 1 (very bad) to 5 (very good), indicating presence of rubrics for scoring quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not explicitly mention providing annotation examples in the guidelines for the crowdworkers performing quality scores or emotion labeling."
          }
        }
      ]
    }
  },
  {
    "id": "saito24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9373,
      "completion_tokens": 499,
      "total_tokens": 9872
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being conducted by a single human annotator with subject matter expertise or being a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human annotators with subject matter expertise or belonging to the target demographic performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Sections 2.3, 3.1, and 3.5",
          "reasoning": "The annotation of speaker-wise recording quality scores and utterance-wise perceived emotion labels was crowdsourced to multiple crowdworkers (non-experts). Specifically, 400 crowdworkers rated recording quality, and 500 annotated emotion labels (Section 2.3 and Table 1). The annotators were recruited via crowdsourcing platforms without mention of subject matter expertise, implying they were non-experts. Multiple crowdworkers annotated each sample, indicating multiple non-expert human QA."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.5",
          "reasoning": "The paper reports use of the NISQA model to compute speech quality scores (Noisiness, Coloration, Discontinuity, Loudness, and Naturalness) which were correlated with human-annotated MOS values. This indicates quality assessment using an AI model was performed alongside human annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention automated verification of code or formulas as part of the QA process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes multiple quality assurance processes involving multiple non-expert human annotators and AI model-based evaluation, so it is not accurate to consider QA as absent."
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8991,
      "completion_tokens": 462,
      "total_tokens": 9453
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 1, 2.1, 2.2",
          "reasoning": "The SRC4VC dataset consists of 11 hours of speech data recorded by 100 Japanese speakers using their own smartphones, collected via crowdsourcing. The speech samples cover various styles such as read-aloud, expressive, conversational, and singing, and were recorded freshly by human participants specifically for this corpus. The paper explicitly describes that speakers recorded their voices in their own environments using a web-based recording platform. This data is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by AI or machine learning models; it consists of human voice recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any of the data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation being used in generating the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SRC4VC dataset was recorded specifically for this work and is not a mere aggregation of existing data without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The paper mentions that the corpus includes sentences selected from existing corpora (ITA, JVNV, STUDIES, CALLS) and Japanese songs, which were read or sung by the speakers. Thus, while the audio recordings are newly produced, the verbal content is derived from existing sources with adaptation for recording."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is well documented and described in the paper."
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9509,
      "completion_tokens": 273,
      "total_tokens": 9782
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 - Any-to-Any VC Experiment",
          "reasoning": "The SRC4VC dataset is used as evaluation data in an any-to-any voice conversion task where a model trained on high-quality speech is tested on SRC4VC speakers' voice samples. The study demonstrates how recording quality mismatch affects VC performance, thus using SRC4VC primarily for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 - Corpus Analysis",
          "reasoning": "The authors perform detailed analyses of SRC4VC including signal-to-noise ratio statistics, recording device diversity, speaker demographics, and annotation results for recording quality and perceived emotions. This indicates usage of the dataset for analyzing trends, patterns, and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10232,
      "completion_tokens": 614,
      "total_tokens": 10846
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SRC4VC corpus contains speech recorded from Japanese speakers only, as repeatedly referenced in the abstract, Sections 1, 2, and 3. There is no mention of more than two languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus includes only Japanese language speech samples, with no indication of a second human language included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not English-only; it is specifically described as Japanese speech corpus. No English speech content was indicated as the core data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 1 Introduction, Section 2 Construction of SRC4VC",
          "reasoning": "The dataset SRC4VC contains speech recorded exclusively in Japanese by 100 Japanese speakers, as noted throughout the paper (e.g., Abstract: \"100 Japanese speakers\", Section 2: \"Japanese copyright-free songs\", Section 3: \"speech corpora related to SRC4VC with Japanese language descriptions\"). Therefore, it is a monolingual, non-English dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code-related content is present in the dataset. The corpus consists solely of speech recordings and their annotations."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are part of the dataset entries. The references to mathematical notation in citations or methods are unrelated to the dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed only of human speech recordings, with no biological sequences or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any constructed or fictional languages in the dataset. All data is real Japanese language speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as Japanese throughout the paper; hence, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are speech recordings containing human language (Japanese); thus, the dataset does contain language."
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7450,
      "completion_tokens": 178,
      "total_tokens": 7628
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper mentions that the SRC4VC corpus is open-sourced for research purposes at a given URL, but does not provide any link or mention of code availability related to data collection, preprocessing, or dataset construction. No code repository or code release is cited anywhere in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides extensive documentation of the dataset creation process including the design of SRC4VC (Section 2.1), voice recording procedure by crowdworkers via a web platform (Section 2.2), detailed annotation procedures (Section 2.3), and corpus analysis including speech quality and speaker distribution (Section 3). This thorough description provides transparency and completeness of the data collection and annotation."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7245,
      "completion_tokens": 617,
      "total_tokens": 7862
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 CSF2022 Dataset",
          "Reasoning": "The new dataset introduced in the paper consists of videos recorded using a webcam from a professional French cuer uttering sentences in French Cued Speech. The paper explicitly states that videos of 1920x1080 pixels at a target framerate of 60fps were recorded. This video data was captured via human involvement, specifically a human cuer performing and being recorded, and using human-operated recording tools."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 CSF2022 Dataset",
          "Reasoning": "Alongside the video recording, audio was also recorded at 44.1 kHz PCM, 16 bits, from the same human cuer during the performance of French Cued Speech sentences. Though not used for this study, it is part of the new dataset and was recorded via human involvement."
        },
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Visual Features",
          "Reasoning": "From the recorded videos, visual features consisting of hand and lips pose coordinates were automatically extracted via the Mediapipe software. The 21 hand and 42 lip x/y coordinates per frame were processed and reduced by PCA and clustering (k-means) to generate fixed-dimension time series feature vectors. These feature sequences are algorithmically generated representations, thus model generated rather than human generated."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 CSF2022 Dataset",
          "Reasoning": "The phonetic transcription for the new dataset was created automatically using the LIA-Phon phonetizer and then manually verified and corrected. Thus, the initial phonetic transcription is model generated (automatic phonetic transcription), but then corrected by humans. While the final data includes human revision, the origin includes model generation due to automatic initial transcription."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 CSF2022 Dataset",
          "Reasoning": "The sentences themselves were originally selected from existing text sources such as the SIWIS database, French parliament debates, and French novels. These texts are human-generated language content. The new dataset consists of recordings of these sentences being cued by a human speaker."
        }
      ]
    }
  },
  {
    "id": "sankar23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8097,
      "completion_tokens": 213,
      "total_tokens": 8310
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1",
            "reasoning": "The dataset comprises 1087 sentences recorded by a professional French cuer (a single expert user specialized in Cued Speech), and the phonetic transcription was manually verified and corrected after automatic phonetic transcription, indicating expert annotation involvement by a single individual."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not describe any detailed annotation instructions or guidelines provided to the cuer or annotator for manual annotation or phonetic correction."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of scoring rubrics or formal criteria given for the manual annotation or phonetic transcription correction process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide or cite examples of annotated data or annotation examples within the annotation guidelines or manual annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "sankar23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9227,
      "completion_tokens": 399,
      "total_tokens": 9626
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 CSF2022 Dataset",
          "reasoning": "The phonetic transcription of the recorded sentences was obtained automatically using the LIA-Phon phonetizer and manually verified and corrected. Given that this verification and correction process is done manually and the dataset involves professional French cuers and linguistic expertise (phonetic annotations with 36 phonetic categories), it is reasonable to infer that the annotations were quality assured by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating QA was done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating QA was done by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance is described as being performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification of annotations (e.g. code verification or rule-based validation) for the dataset annotations is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a manual verification and correction process of phonetic transcriptions for quality assurance, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8845,
      "completion_tokens": 402,
      "total_tokens": 9247
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states that a new dataset called CSF2022 was recorded consisting of 1087 sentences by a professional French cuer. The sentences were selected from various French language sources and recorded specifically for this study, indicating that the dataset was created entirely from scratch by human contributors through new recordings."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any part of the dataset was generated by AI or machine learning models. The data consists of human recorded videos."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains translated content produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely collected or aggregated from existing sources. It is newly recorded data, not existing video or data aggregated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the phonetic transcriptions were obtained using LIA-Phon phonetizer and verified manually, the core dataset consists of newly recorded videos and not derived from existing data with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation is clearly specified; thus, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9363,
      "completion_tokens": 473,
      "total_tokens": 9836
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new dataset being used for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "The new CSF2022 dataset is used to train the proposed automatic cued speech recognition model from scratch, as described in Section 3.1 where the dataset is introduced and in Section 4.1 where model performance is reported after training on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new dataset for supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset is used for reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The new dataset is used for evaluation purposes, including performance measurement and benchmarking of the proposed recognition system."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The dataset is used to analyze hand-lip temporal dynamics by extracting and interpreting attention maps and segmentation boundaries, thus serving to study characteristics and patterns of cued speech production."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Practical usages of the dataset for training, evaluation, and analysis are documented in the paper."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10086,
      "completion_tokens": 397,
      "total_tokens": 10483
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The new dataset CSF2022 is recorded for French Cued Speech (LfPC), using sentences selected from French language sources such as French parliament debates and French novels. The phonetic transcription uses 36 phonetic categories to describe French, as stated in Section 3.1. Thus, the dataset entries are monolingual in French, which is a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language data, specifically French Cued Speech with corresponding phonetic transcriptions."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7304,
      "completion_tokens": 182,
      "total_tokens": 7486
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 (CSF2022 Dataset) and other sections",
          "reasoning": "The paper mentions that the dataset will be made publicly available along with the paper, including raw data and annotations, but does not mention or provide any link to source code related to data collection, preprocessing, or generation. There is no indication that the code used to collect or process the dataset is available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (CSF2022 Dataset)",
          "reasoning": "The paper provides documentation on the dataset creation process, including details about the recording setup, the professional cuer, the sentence sources, recording conditions, frame rate, video resolution, phonetic transcription process, manual corrections, and manual annotation for a subset. This documentation is sufficiently detailed for understanding the dataset creation."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7802,
      "completion_tokens": 129,
      "total_tokens": 7931
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Sections 1 Abstract, 3.1, and 3.2",
          "Reasoning": "The dataset is audio data created by convolving human-generated speech samples from the LibriSpeech dataset with simulated room impulse responses generated by a geometric acoustic solver, resulting in spatial audio with 19 microphone channels and first-order ambisonics. Hence, the original audio is human-generated, but the spatial augmentation and RIR simulation are model-generated, making the final dataset a combination of human-generated and model-generated audio."
        }
      ]
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8654,
      "completion_tokens": 198,
      "total_tokens": 8852
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "The dataset Spatial LibriSpeech is generated by augmenting LibriSpeech samples with synthetic acoustic simulations involving parametric room generation, room impulse response simulations, and mixing. These steps describe a simulation pipeline rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No mention of explicit annotation instructions for human annotators is provided since labels come from synthetic simulation parameters, not manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No scoring rubrics or quality assessment criteria are described as the labels are derived automatically from simulation outputs."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No section",
            "reasoning": "No examples of annotation or labeling guidelines are included since the dataset is generated automatically via simulation rather than manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9784,
      "completion_tokens": 342,
      "total_tokens": 10126
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance done by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process does not involve AI model evaluation or judgment as described in the paper."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Generation Methodology",
          "reasoning": "The dataset is synthesized by parametric room generation, simulation of room impulse responses with a geometric acoustic solver, and automatic mixing with LibriSpeech samples. These steps imply an automated pipeline that ensures dataset consistency. Although the paper does not explicitly state a separate QA step, the entire generation and labeling process is algorithmic and rule-based, indicating that quality assurance is performed through automated verification of code and formulas inherent to the simulation and mixing steps."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes an automated generation and labeling procedure based on simulations and parametric models, implying some form of automated quality assurance. Therefore, quality assurance is not absent or undocumented."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9402,
      "completion_tokens": 383,
      "total_tokens": 9785
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by humans. It uses pre-existing LibriSpeech audio samples and Microsoft Deep Noise Suppression noise samples, which are then augmented."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated entirely by AI or machine learning models; rather, it uses simulated room impulse responses applied to existing audio samples."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any translation of data from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not simply collected or aggregated from existing sources without modification; it involves further processing."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Generation Methodology",
          "reasoning": "Spatial LibriSpeech is derived by augmenting existing datasets (LibriSpeech and Microsoft Deep Noise Suppression Challenge samples) through convolution with simulated room impulse responses and scaling. The data is therefore based on existing sources with applied modifications and transformations (simulated acoustic conditions and mixing)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of data generation for Spatial LibriSpeech is explicitly described."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9920,
      "completion_tokens": 533,
      "total_tokens": 10453
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, especially 4.1 and 4.3",
          "reasoning": "The paper clearly states that models were trained on Spatial LibriSpeech from scratch using the dataset annotations for multiple spatial audio tasks. Section 4 describes training models on the dataset without pre-training, and Section 4.3 discusses training models with subsets of the dataset, confirming its use for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper performs supervised fine-tuning of the models pre-trained on Spatial LibriSpeech on the ACE Challenge Dev set to improve target data performance, especially for T30 and DRR estimation, demonstrating post-training supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning or RLHF post-training techniques using Spatial LibriSpeech is found in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Spatial LibriSpeech is used primarily as a training dataset. Evaluations on external datasets like TUT Sound Events 2018 and ACE Challenge are performed, but not primarily on Spatial LibriSpeech as an evaluation dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The authors analyze model embeddings and dataset representations (e.g., UMAP visualizations in Section 4.2), as well as the effect of training on smaller subsets in Section 4.3, indicating the dataset is used for analyzing characteristics and trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval or augmentation purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates multiple practical uses of Spatial LibriSpeech, including training and analysis, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10643,
      "completion_tokens": 546,
      "total_tokens": 11189
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the Spatial LibriSpeech dataset as augmenting LibriSpeech samples, which are in English, and noise from the Microsoft Deep Noise Suppression Challenge, without mention of additional languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are not described as containing exactly two languages; only English is indicated."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 3.1 Generation Methodology",
          "reasoning": "Spatial LibriSpeech is generated by augmenting LibriSpeech samples (an English speech corpus), with simulated acoustic conditions and distractor noise, but no other languages are mentioned; hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset originates from LibriSpeech samples which are in English; no non-English language only data is described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content consists of audio data, labels, and simulated audio conditions; no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions as part of the entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only human speech audio and simulated room impulse responses; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses natural English speech from LibriSpeech; no fictional or constructed languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content is clearly stated as English speech from the LibriSpeech corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language audio (English speech); hence, language is applicable."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7861,
      "completion_tokens": 161,
      "total_tokens": 8022
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any link or mention of publicly available code for the dataset generation pipeline or related preprocessing steps. There is no indication that the code used to generate the Spatial LibriSpeech dataset is shared or accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Generation Methodology and Section 3.2 Dataset Characteristics",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including parametric room generation, room impulse response simulation, and mixing with LibriSpeech samples. Specific details about room distributions, material properties, microphone array modeling, and dataset splits are documented, providing transparency and completeness of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 3733,
      "completion_tokens": 529,
      "total_tokens": 4262
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Recordings",
          "Reasoning": "The MUNDEX corpus contains video recordings of 88 dyads performing board game explanations and gameplay, captured from six cameras at 1920x1080 resolution and 50fps. These videos were recorded with human participants in dyadic interaction settings, thus human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Recordings and Section 2.2.1 Transcription and Acoustic Information",
          "Reasoning": "Separate headset microphones recorded the speech audio of the explainer and explainee during the interactions. This audio data originates from human speech recorded in the sessions, hence human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Transcription and Acoustic Information",
          "Reasoning": "Transcriptions of the recorded speech were created manually with corrections following automatic transcription from BAS Web-service or OpenAI's Whisper, including orthographic transcriptions and annotations for disfluencies and other speech phenomena. Since these transcriptions are manually corrected, they are human generated text data derived from the recordings."
        },
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.2 Multimodal Annotations",
          "Reasoning": "Annotations include head movement, gaze direction, and manual gestures, all of which are temporal behavioral signals tracked over time from the video data. These were annotated by humans observing the recordings, reflecting human-generated time series data."
        },
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.2 Multimodal Annotations",
          "Reasoning": "The corpus includes annotations of non-verbal behavior such as torso movements, facial expressions, and adaptors (e.g., fidgeting). These are behavioral annotations derived from human observation and manual annotation, considered human generated non-standard modality data capturing behavioral phenomena."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.3 Discourse Annotation",
          "Reasoning": "Discourse functions and understanding signals (e.g., backward looking functions, feedback utterances) are annotated based on dialogue transcripts, manually annotated by humans following modified DAMSL and DIT++ schemes, constituting human generated text annotations."
        }
      ]
    }
  },
  {
    "id": "schade24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 4585,
      "completion_tokens": 218,
      "total_tokens": 4803
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2",
            "reasoning": "Annotations include manual correction of transcriptions and multimodal annotations (head movement, gaze, manual gestures, non-verbal behavior, and discourse) which are typically performed by multiple human experts as stated in the context of manually corrected transcriptions and annotation standards."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2.3",
            "reasoning": "Discourse annotations are based on modified versions of DAMSL and DIT++ annotation schemes, which imply the presence of defined annotation instructions and guidelines."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2.3",
            "reasoning": "Annotations of discourse functions and understanding levels (understanding, non-understanding, misunderstanding, partial understanding) correspond to defined categories implying the use of rubrics to guide annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not explicitly mention providing annotation examples in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "schade24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 5715,
      "completion_tokens": 347,
      "total_tokens": 6062
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance (QA) process performed solely by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that multiple human experts conducted quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a QA process performed by a single non-expert human annotator in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper mentions planned and ongoing machine learning-based investigations, it does not indicate using AI models to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although automatic transcription tools (BAS Web-service or OpenAI's Whisper) were used for initial transcriptions, the paper does not describe an automated verification or QA process for the annotations that constitutes QA; manual corrections and annotations followed these automatic processes."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the annotation efforts (manual corrections of transcriptions, manual ELAN annotations of multimodal behavior, and discourse annotations) but does not document any explicit quality assurance procedure or validation process for these annotations. There are no mentions of verification, cross-checking, or inter-annotator agreement evaluations. Thus, no formal QA process is described or applied."
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 5333,
      "completion_tokens": 434,
      "total_tokens": 5767
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Recordings",
          "reasoning": "The MUNDEX corpus data was newly created by human participants through recording 88 dyadic interactions involving explanations of a board game by a self-trained explainer to a naive explainee, capturing natural multimodal communication from scratch. This data was not translated, adapted, or derived from pre-existing material but collected as original human-human interaction data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus data was recorded from human interactions; there is no indication that the dataset itself was generated or synthesized by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no statement indicating that the data was produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset was generated by machine translation, as the data consists of original German language interactions."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not a collection or aggregation of existing sources without significant modification; it is newly recorded human interaction data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotations",
          "reasoning": "The rich multimodal annotations such as transcriptions, acoustic information, head movement, gaze, manual gestures, facial expressions, and discourse functions are derived from the original recorded data with modifications and transformation applied to produce various annotation layers."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are well documented and specified."
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 5851,
      "completion_tokens": 241,
      "total_tokens": 6092
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3, Discussion",
          "reasoning": "The MUNDEX corpus is primarily used for analyzing multimodal interaction dynamics and levels of understanding in dyadic explanations, including investigations of listeners' gaze behavior, deictic co-speech gestures, turn-taking dynamics, and non-verbal signals, as described in Section 3. There is no indication that the dataset is actually used for training or evaluation of machine learning models; rather, it facilitates analysis of communication patterns and multimodal behaviors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 6574,
      "completion_tokens": 377,
      "total_tokens": 6951
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Recordings",
          "reasoning": "The MUNDEX corpus recordings consist exclusively of dyadic interactions where all speakers are speakers of German, a non-English language. The paper explicitly states in Section 2.1 that all speakers of the corpus are German speakers, indicating the dataset is monolingual in German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes transcriptions and annotations of human spoken language, specifically German, thus it contains language."
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 3792,
      "completion_tokens": 145,
      "total_tokens": 3937
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Sections 2 and 4",
          "reasoning": "The paper describes the corpus creation process, annotation methods, and tools developed or used for annotation but does not provide any links to repositories or locations where the code related to the dataset construction, data collection software, or preprocessing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (especially 2.1 and 2.2)",
          "reasoning": "The paper provides detailed documentation of the data collection process, participant details, annotation procedures, modalities annotated, tools used or planned for annotation, and discourse annotation schemes employed, giving a comprehensive overview of the dataset creation."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7293,
      "completion_tokens": 123,
      "total_tokens": 7416
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1, Introduction; Section 2, Dataset Description",
          "Reasoning": "The study introduces the CRSS-UTDallas Fearless Steps Apollo 11 audio corpus, a naturalistic multi-channel audio dataset capturing voice communications from the NASA Apollo-11 mission. This dataset consists of around 100 hours of audio manually transcribed with speaker labels collected from multiple communication channels. The data was collected through human-operated recording during the Apollo 11 mission and annotated by professional annotators."
        }
      ]
    }
  },
  {
    "id": "shekar23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8145,
      "completion_tokens": 167,
      "total_tokens": 8312
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2",
            "reasoning": "Section 2 states that the 100 hours of audio data were manually transcribed by professional annotators, indicating multiple human experts were involved in annotating speaker labels."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not describe any specific annotation instructions provided to annotators for the speaker labels."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "There is no mention of scoring rubrics or detailed criteria used during the annotation process in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not provide or reference annotation examples or illustrative samples used to guide annotators."
          }
        }
      ]
    }
  },
  {
    "id": "shekar23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9275,
      "completion_tokens": 174,
      "total_tokens": 9449
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 Dataset Description",
          "reasoning": "The dataset annotations, specifically the 100 hours of audio data, were manually transcribed by professional annotators. These annotators are described as 'professional,' which implies subject matter expertise in transcribing and labeling speaker data accurately."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8893,
      "completion_tokens": 444,
      "total_tokens": 9337
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2, Dataset Description",
          "reasoning": "The paper states that the UTDallas Fearless Steps Apollo corpus was manually transcribed by professional annotators for speaker labels, involving 100 hours of audio data selected from mission-critical events. This indicates that the data annotations and labels were created from scratch by human contributors specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is audio recordings and human annotations. There is no indication that the dataset was generated by AI or machine learning models; rather, models are applied to the dataset for analysis."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation activity or generation of data via translation by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is discussed or applied to the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2, Dataset Description",
          "reasoning": "The dataset is described as a subset of 100 hours drawn from a larger 19,000 hour Apollo corpus, which itself consists of naturalistic audio collected during actual NASA missions. The authors selected and organized this subset and channels based on speech activity, indicating that the data is an aggregation from existing historical sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of modified or adapted data derived from other sources; the data is raw or annotated recordings without significant transformations altering the original content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as archival NASA Apollo mission audio recordings manually annotated by humans."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9411,
      "completion_tokens": 527,
      "total_tokens": 9938
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset Description; Section 5 Implementation Details",
          "reasoning": "The paper describes using the Fearless Steps Apollo-11 audio corpus subset (100 hours of hand-labeled audio) to train graph attention network models from scratch for speaker classification. It details the training setup, including 70% of the data used for training, a cross-entropy loss, Adam optimizer, and batch sizes, indicating the dataset was used for model training with randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the dataset for fine-tuning pre-trained models; instead, pretrained speaker embeddings are used as features, but training the graph networks is done from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of reinforcement learning based post-training methods using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 Discussion",
          "reasoning": "The dataset is split into training and test sets, and the test set (30% of data) is used for evaluating the speaker classification accuracy of models, thus serving as evaluation data to benchmark performance of the proposed method against baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6 Tracking Speaker-of-Interest on Mission Phases",
          "reasoning": "The authors perform analysis on the dataset to track key speakers of interest across different mission phases, analyzing and visualizing speaker durations and interactions, which is an analytical use of the dataset rather than model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset serves as a knowledge base for retrieval-augmented generation or similar use cases."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset usage is clearly documented for training, evaluation, and analysis tasks."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10134,
      "completion_tokens": 578,
      "total_tokens": 10712
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Fearless Steps Apollo-11 audio corpus is described as capturing communications from over 400 personnel during NASA Apollo-11 mission, which was conducted primarily in English. There is no indication in the paper of multiple human languages being present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or imply the presence of exactly two human languages in the dataset entries. The linguistic content is consistently English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 1 (Introduction), Section 2 (Dataset Description)",
          "reasoning": "The dataset consists of NASA Apollo 11 audio communications involving mission personnel. The paper consistently refers to English speech data (i.e., astronaut and mission specialist English voice communication). There is no indication of any other language spoken or transcribed in the corpus."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any single non-English language being the exclusive content of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio speech recordings and associated speaker annotations. The paper does not mention inclusion of any programming or structured code content in the dataset itself."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are audio speech recordings and speaker labels; mathematical and logical notations are present only in the paper's methodology and equations, not the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset solely contains human speech from mission personnel without any biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly specified as English; thus unknown language status does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset is composed of spoken language audio and speaker transcripts, so it indeed contains human language."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7352,
      "completion_tokens": 223,
      "total_tokens": 7575
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper indicates availability of code for dataset construction.",
          "reasoning": "The paper does not provide any link, appendix, or reference to code repositories related to the dataset construction, preprocessing scripts, or data preparation. It only mentions that the dataset is a subset of the Fearless Steps Apollo 11 audio corpus with manual transcription provided, but there is no indication that dataset construction code has been released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Dataset Description",
          "reasoning": "The paper provides some documentation about the dataset construction process in Section 2. It describes the source corpus (Fearless Steps Apollo 11 audio corpus), the subset selection criteria (100 hours from three mission-critical events), the selection of channels (five channels with most speech activity), the amount of manual transcription by professionals, and the number and duration of speakers and utterances. While this is not exhaustive code-level documentation, it offers transparency and completeness about the dataset creation and selection process sufficient for understanding and reproducibility considerations."
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7460,
      "completion_tokens": 173,
      "total_tokens": 7633
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 ITA Dataset",
          "Reasoning": "The ITA dataset consists of 76 conversational in-class speech recordings from 54 adult learners collected as new speech data for this study, explicitly described as recordings from international students serving as teaching assistants. It is original audio captured from human speakers."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Human labeled Transcripts and Prosody Ratings",
          "Reasoning": "Human transcriptions (human speech recognition - HSR) and prosodic labels were generated by fifteen trained linguists who manually transcribed and rated the ITA speech data, thus these text annotations are human generated."
        }
      ]
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8312,
      "completion_tokens": 220,
      "total_tokens": 8532
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1.1",
            "reasoning": "Section 3.1.1 states that fifteen trained linguists rated the speech for accentedness and intelligibility and provided prosodic annotations, indicating that multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1.1",
            "reasoning": "The annotators were linguists trained specifically to rate accentedness, intelligibility, speech rate, pause durations, and lexical stress against the CMU Pronouncing Dictionary, implying the presence of detailed instructions and guidelines for consistent annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1.1",
            "reasoning": "Ratings for accentedness and intelligibility along with calibrated lexical stress scores suggest a structured scoring system or rubrics were used during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any example annotations or example data provided to annotators in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9442,
      "completion_tokens": 334,
      "total_tokens": 9776
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The ITA dataset includes speech rated by fifteen trained linguists who evaluated accentedness and intelligibility, indicating multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models (such as Wav2vec 2.0 and GOPT) are used for assessment, they are applied to the dataset for scoring and feature extraction rather than conducting quality assurance on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of automated verification of annotations or quality assurance through algorithmic or rule-based techniques is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A detailed human expert annotation process involving multiple trained linguists for intelligibility and prosodic annotation is documented, so QA process is present."
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9060,
      "completion_tokens": 519,
      "total_tokens": 9579
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 ITA Dataset",
          "reasoning": "The ITA dataset was collected from 54 adult learners with diverse L1 backgrounds producing conversational in-class speech recordings. The data includes human-annotated transcripts and prosodic ratings completed by trained linguists. The dataset was prepared specifically for this study to analyze L2 speech intelligibility, indicating original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset is described as being originally generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data in the study was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was described as generated via machine translation from other languages; the ASR transcripts were generated from speech recognition, not translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ITA dataset is original speech data collected for this study, not a mere aggregation of existing datasets without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic Model and GOP Feature Extraction; Section 3.3 GOPT Inference and Scoring; Section 3.4 MDD using Wav2vec 2.0",
          "reasoning": "The study derives features such as GOP features using pre-trained acoustic models (Kaldi TDNN-F trained on Librispeech) and applies trained models (GOPT trained on Speechocean762) to the ITA data. Also, mispronunciation detection (MDD) scores leverage pre-trained Wav2vec 2.0 fine-tuned models. Therefore, some data representation used is derived from existing data or models with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origins of the data clearly, particularly the ITA dataset and modeling techniques."
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9578,
      "completion_tokens": 373,
      "total_tokens": 9951
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Results) and 4.1 (Multicollinearity Test and Regression Analysis)",
          "reasoning": "The ITA dataset is used to assess the performance of multi-level pronunciation scoring systems and mispronunciation detection models, serving as the test set to benchmark and evaluate the correlation between model outputs and human-rated intelligibility scores."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Characterization of L2 Intelligibility using Feature Importance) and Section 5 (Discussion)",
          "reasoning": "The dataset is utilized to analyze the relationships between various pronunciation scores, prosodic features, and intelligibility factors to identify dominant predictors influencing L2 speech intelligibility, thus it is primarily used for analyzing trends and characteristics rather than for training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10301,
      "completion_tokens": 584,
      "total_tokens": 10885
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The ITA dataset described consists of non-native speakers of English with diverse first languages; however, all recordings and transcriptions are in English. There is no indication that the dataset includes speech data in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The dataset involves L2 English spoken by speakers of various L1s, but speech data is only in English. There is no evidence that the dataset contains exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The ITA dataset consists of English conversational speech produced by adult learners from diverse language backgrounds, with transcripts and annotations all in English. The dataset entries contain only English utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No data presented in the paper indicates presence of non-English monolingual speech in the proposed dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset focuses on natural spoken language (English speech) and associated annotations; there is no inclusion of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include content with formal mathematical or logical symbols; it concerns natural language speech data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech data (English) only, without any biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset's language content is well specified as English; there is no lack of documentation regarding language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language data (English speech); therefore, this label does not apply."
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7519,
      "completion_tokens": 163,
      "total_tokens": 7682
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention any publicly available code repository or provide links to code related to data collection, preprocessing, or generation for the ITA dataset or any datasets introduced by the authors."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 ITA Dataset and subsections 3.1.1, 3.1.2",
          "reasoning": "The paper documents the dataset creation process for the ITA dataset in detail, including number of speakers, utterances, details on annotation (human and ASR), types of labels (transcripts, prosodic ratings), and usage of Whisper ASR for transcript generation, providing transparency about dataset composition and labeling procedure."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6701,
      "completion_tokens": 268,
      "total_tokens": 6969
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Methods",
          "Reasoning": "The paper describes speech utterances recorded from 8 children (4 male, 4 female) across grades 1 to 4, using a SHURE KSM30 microphone capturing speech audio during a picture naming task. This audio data is human-generated as it is recorded speech from human children during an experiment conducted by the authors."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Methods",
          "Reasoning": "Subglottal resonance data are recorded from a K&K Sound HotSpot accelerometer positioned on the neck below the thyroid cartilage during the children's speech. This sensor data is recorded from human participants and manually measured, thus human-generated."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Methods and Figure 1",
          "Reasoning": "Ultrasound images of the tongue's midsagittal profile were captured for each child concurrently with microphone and accelerometer data during speech production. These ultrasound tongue images are captured using human-operated ultrasound equipment during data collection, thus are human-generated image data."
        }
      ]
    }
  },
  {
    "id": "shetty23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7553,
      "completion_tokens": 260,
      "total_tokens": 7813
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 Methods",
            "reasoning": "The manual extraction and measurement tasks (e.g., measuring formant frequencies using Praat, manual segmentation of ultrasound tongue contours) were done by trained individuals such as speech-language pathology students or researchers, indicating expertise and single-expert annotation rather than crowdsourced or automated methods."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 Methods",
            "reasoning": "Speech utterances were elicited using a standardized picture naming task commonly used by speech-language pathologists (GFTA-3), implying provided instructions for elicitation and annotation processes to ensure consistency."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 Methods",
            "reasoning": "No mention is made of any scoring rubrics or grading scales guiding the manual measurements or annotations; the tasks involved objective acoustic and articulatory measurements rather than subjective ratings requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 Methods and Figures 1 and 6",
            "reasoning": "While Figures illustrate the measurement process and results (e.g., tongue contours and measurement points), no explicit annotation examples or sample annotated data points or transcripts are described as part of annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "shetty23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8683,
      "completion_tokens": 213,
      "total_tokens": 8896
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2: Methods, Paragraph 2 and 3",
          "reasoning": "The speech utterances were elicited by a speech-language pathology student-clinician, which is an expert in speech-language pathology, indicating quality assurance through a single human expert during data collection and labeling."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes manual measurement of acoustic features and manual extraction of tongue contours by a student-clinician, indicating quality assurance was performed by human expert; thus, QA process is documented."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8301,
      "completion_tokens": 443,
      "total_tokens": 8744
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper explicitly states that data were collected longitudinally from eight typically developing children between grades 1 and 4 using ultrasound imaging, microphone, and neck accelerometer signals during speech tasks. This data was newly collected by the authors from human participants and is original raw data created from scratch, not derived or adapted from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating data via AI or machine learning models; the data consists of recorded acoustic and articulatory features from human subjects."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not aggregated from existing sources; it was collected directly from human participants as original recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 (Articulatory Analysis) and Section 2 (Methods)",
          "reasoning": "The articulatory features such as Tongue Curvature Degree (TCD) and Tongue Curvature Position (TCP) were derived by processing and quantifying ultrasound tongue contours obtained from the novel collected data. Thus, these features are derived data formed by transforming the original ultrasound images."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is documented clearly, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8819,
      "completion_tokens": 325,
      "total_tokens": 9144
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4 (Results and Discussion; Conclusions)",
          "reasoning": "The paper introduces a longitudinal dataset of children's acoustic and articulatory speech features and uses it primarily to analyze developmental trends, sex-based differences, and articulatory-acoustic relationships in speech over time. The dataset is used to study patterns and characteristics rather than for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for analysis and characterization within the study, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9542,
      "completion_tokens": 510,
      "total_tokens": 10052
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2: Methods; Section 3: Results and Discussion",
          "reasoning": "The speech data in the new dataset consists exclusively of utterances by native speakers of Midwestern American English. Words used for vowel analysis are English words such as \"Apple\", \"Vacuum\", \"Teeth\", \"Zebra\", \"Shoe\", \"Zoo\", \"Watch\", and \"Frog\". All participants are native English speakers and the focus is on the English vowel system. Hence, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 2: Methods",
          "reasoning": "The dataset includes articulatory feature measures such as Tongue Curvature Degree (TCD) and Tongue Curvature Position (TCP) which are defined mathematically as ratios of line segment lengths (e.g., TCD = len(ZO)/len(XO), TCP = len(YO)/len(XO)). These formal symbolic expressions of articulatory parameters imply the presence of mathematical and logical notation as part of the dataset's quantified features."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are speech utterances by children and contain linguistic content, so they do contain language."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6760,
      "completion_tokens": 184,
      "total_tokens": 6944
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not explicitly mentioned in the paper",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories related to data collection, preprocessing, or generation for the dataset. There is no mention of code release or software availability for reproducing the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods) and throughout the paper",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the participant selection criteria, longitudinal data collection from specific grades and ages, methods of acoustic and articulatory data recording (microphone, accelerometer, ultrasound), and specific measurements taken (formant frequencies, subglottal resonances, tongue contour extraction). These details are described mainly in Section 2 and elaborated in the rest of the paper, enabling understanding of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7109,
      "completion_tokens": 91,
      "total_tokens": 7200
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset",
          "Reasoning": "The paper states that the new dataset used for training and validation consists of 325 hours of internal Mandarin speech data obtained from various websites such as YouTube, indicating that the recordings are presumably human-generated audio data collected from real speech sources rather than synthesized or simulated audio."
        }
      ]
    }
  },
  {
    "id": "shi23c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7961,
      "completion_tokens": 191,
      "total_tokens": 8152
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "The paper states that punctuation labels per frame are obtained by forced alignment, which is an automatic alignment technique from text to audio, indicating an automatic process is used for annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention or describe any detailed annotation instructions given to annotators or guidelines for forced alignment annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No mention of scoring rubrics or criteria for annotation quality is present related to the forced alignment labeling or endpoint labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no presentation of annotation examples or samples for punctuation or endpoint labels in the paper or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "shi23c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9091,
      "completion_tokens": 314,
      "total_tokens": 9405
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single non-expert human conducted quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no details about quality assurance by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset includes forced alignment for punctuation labeling, the paper does not explicitly state that an AI model was used as a judge for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The punctuation labels per frame are obtained by forced alignment, which indicates an automatic process for annotation. The forced alignment method is an algorithmic approach that automatically generates frame-level punctuation labels, but the paper does not mention any manual verification or correction of these labels. Thus, quality assurance is conducted through automatic verification and labeling via forced alignment techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is implied through the use of forced alignment automatic labeling; therefore, the absence of QA process is not applicable."
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8709,
      "completion_tokens": 504,
      "total_tokens": 9213
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The authors specify that they use 325 hours of internal Mandarin speech data collected from various websites such as YouTube, used as training and validation sets. This indicates data collected and curated by the authors or their institution. The paper describes that punctuation labels per frame are obtained by forced alignment, implying manual or automatic alignment on human-generated speech data. There is no indication that the data was translated or generated by models; thus, the data can be considered newly collected original data by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the dataset or its portions were generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the data was produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper of data generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper states the training and validation data were obtained from various websites such as YouTube, indicating collection from existing public sources. The data was aggregated into internal datasets for training and evaluation without explicit mention of significant modification. Thus, the data can be considered collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "Punctuation labels per frame are obtained via forced alignment, which is a process of deriving time-aligned labels from existing textual and audio data. Additionally, endpoint labels are artificially defined by combining punctuation and silence duration. These labels constitute data derived or transformed from the original collected speech data, indicating that parts of the dataset contain derived annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation process are specified sufficiently in the paper."
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9227,
      "completion_tokens": 345,
      "total_tokens": 9572
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.3",
          "reasoning": "The internal Mandarin speech dataset of 325 hours is used for training the proposed semantic VAD model from scratch with the specified multi-task objectives including VAD, punctuation prediction, and ASR auxiliary loss. This is described in Section 3.1 under Dataset and Section 3.3 under Model configuration where training procedures are detailed."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4",
          "reasoning": "The paper uses internal Mandarin long speech test sets (Test1, Test2, Test3) for performance evaluation of the proposed semantic VAD, measuring latency, character error rate, and detection cost function. This is explicitly described in Section 3.1 and Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and Figures 4 and 5",
          "reasoning": "The paper conducts visual and quantitative analysis of segmentation timing and judgment proportions using the test sets, especially shown in Section 4, Figure 4, and Figure 5, to analyze characteristics and benefits of the semantic VAD method."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9950,
      "completion_tokens": 498,
      "total_tokens": 10448
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used is described as an internal Mandarin speech dataset only, with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages. Only Mandarin is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as containing only English. The dataset is specified as Mandarin speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper states in Section 3.1 that the dataset is 325 hours of internal Mandarin speech data used for training and evaluation, indicating it contains only Mandarin, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any programming or structured code-related content is included in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas for loss functions and metrics, these are not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional, artificial, or constructed languages are mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly specified as Mandarin, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language (Mandarin) data, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7168,
      "completion_tokens": 179,
      "total_tokens": 7347
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link to publicly available code related to data collection, preprocessing, or dataset generation. There is no indication that the code has been released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper provides some details about the dataset, including that the training data consists of 325 hours of internal Mandarin speech from various websites such as YouTube. The test data sets are described (meeting, recording pen, live webcast) along with their durations. Punctuation labels per frame are obtained by forced alignment. Table 1 shows label distributions in the training set. These provide some level of documentation on the dataset creation process, although the dataset is internal and sourcing details are brief."
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11435,
      "completion_tokens": 106,
      "total_tokens": 11541
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection, Table 1",
          "Reasoning": "ML-SUPERB gathers data from a wide range of multilingual speech corpora, which are collections of spoken language audio recordings. These datasets originate from human speech recordings with various licenses allowing their use in research, and the benchmark uses subsets of this audio data for training, development, and testing across 143 languages."
        }
      ]
    }
  },
  {
    "id": "shi23g_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12287,
      "completion_tokens": 248,
      "total_tokens": 12535
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 Data Collection, Section 2.4 Framework and Benchmark Settings",
            "reasoning": "The paper states that ML-SUPERB collects data from existing public multilingual corpora but does not describe any manual annotation process. The data annotation (e.g., transcriptions) originates from the source corpora. The preparation of the benchmark datasets and training/testing splits are performed in an automated, scripted manner using established toolkits (S3PRL, ESPnet). Thus, no human annotators or new manual annotations are described."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention providing any annotation instructions relevant to new manual annotations, nor does it imply any manual labeling was done by humans for this benchmark."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No scoring rubrics related to annotation guidelines are described because no manual annotation process is presented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No examples of annotation guidelines or annotated examples are provided since the benchmark uses existing datasets without newly introduced manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "shi23g_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13417,
      "completion_tokens": 268,
      "total_tokens": 13685
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or indication in the paper that multiple human experts performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert humans performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automatic or algorithmic verification of the dataset content or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process applied to the collected datasets used in ML-SUPERB. The dataset is assembled from existing corpora, but no QA or validation protocol is described for the benchmark's datasets."
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 13035,
      "completion_tokens": 403,
      "total_tokens": 13438
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any new speech data entirely from scratch by human contributors. Instead, it uses existing publicly available multilingual speech datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data being generated by AI or machine learning models for the benchmark dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "ML-SUPERB gathers data from a wide range of existing multilingual speech corpora such as Multilingual Librispeech, Commonvoice, Voxforge, Voxpopuli, Googlei18n, and others, all collected under permissive licenses. The benchmark essentially collates these existing datasets without significant modifications."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the data has been significantly modified, transformed, or adapted from existing sources to form the benchmark dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear documentation on the origins of the data used in ML-SUPERB; therefore, this category does not apply."
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13553,
      "completion_tokens": 357,
      "total_tokens": 13910
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.4 (Downstream model and training details) and Section 3.2 (Experimental Results)",
          "reasoning": "The ML-SUPERB dataset is used to fine-tune pre-trained self-supervised learning (SSL) speech models. The paper specifically states that frozen SSL features are used and a lightweight downstream model is trained on ML-SUPERB data, including 10-minute and 1-hour training sets, confirming supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Throughout the paper, especially Sections 2.4 (Evaluation metric) and 3.2 (Experimental Results)",
          "reasoning": "ML-SUPERB serves as a benchmark dataset used exclusively for evaluating various SSL models on multilingual automatic speech recognition (ASR) and language identification (LID) tasks, as detailed in multiple sections and through performance tables."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 (Layerwise analysis)",
          "reasoning": "The ML-SUPERB dataset is used for analyzing layer importance and model behavior across languages and tasks, as shown by the presented layerwise analysis and weight visualization for models like XLSR-128."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14276,
      "completion_tokens": 558,
      "total_tokens": 14834
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Section 2.1 Data Collection, Section 2.3 Multilingual Track",
          "reasoning": "The dataset introduced in the paper, ML-SUPERB, contains speech data for 143 languages ranging from high-resource to endangered languages (Abstract, Section 2.1). It includes multilingual speech corpora such as Multilingual Librispeech, Commonvoice, Voxforge, Voxpopuli, and others covering many languages (Section 2.1). The multilingual track explicitly involves multilingual ASR and LID tasks with combined training data from all these languages (Section 2.3). Thus, the dataset is clearly multilingual with entries across more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes many more than two languages, so it cannot be classified as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although English data is part of the dataset, the dataset is not restricted to English entries only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages, not a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset contains entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only, with no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language data."
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11494,
      "completion_tokens": 190,
      "total_tokens": 11684
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.4 (Framework and Benchmark Settings)",
          "reasoning": "The paper states the use of established toolkits (S3PRL and ESPnet) and mentions that ML-SUPERB will be published as an all-in-one recipe in ESPnet's egstraining collection, but there is no explicit indication or link in the paper stating that the code for dataset collection, preprocessing, or generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Data Collection) and Table 1",
          "reasoning": "The paper provides detailed documentation on dataset construction, including the various source corpora used, their licenses, dataset sizes, statistics in Table 1, training/validation/test splits, inclusion of few-shot languages, and reflections on data selection rationale. This constitutes transparent and comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6745,
      "completion_tokens": 182,
      "total_tokens": 6927
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The authors introduced the LibriPhrase dataset derived from LibriSpeech, which consists of audio recordings. These audio data originate from human speech recordings as part of LibriSpeech, which is based on human-read audiobooks. Thus, the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The LibriPhrase dataset includes text transcriptions corresponding to the audio short phrases. These texts represent human-generated written text from the LibriSpeech corpus derived from human-produced materials. The text data stems from human-generated literary sources and manual transcription alignment, therefore it is human generated text."
        }
      ]
    }
  },
  {
    "id": "shin22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7597,
      "completion_tokens": 212,
      "total_tokens": 7809
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1",
            "reasoning": "The paper states that the LibriPhrase dataset is created by automatically processing the LibriSpeech dataset using forced alignment with the Montreal Forced Aligner to determine word-level timestamps and then splitting utterances into shorter phrases. There is no indication of human annotators involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not mention any instructions provided to annotators since the dataset creation is done automatically via forced alignment and programmatic splitting."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No rubric or scoring criteria are mentioned for annotation because the dataset is generated automatically without subjective human judgment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No annotation examples or guidelines are described as it is an automatic data processing pipeline without explicit annotation steps requiring such examples."
          }
        }
      ]
    }
  },
  {
    "id": "shin22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8727,
      "completion_tokens": 302,
      "total_tokens": 9029
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the LibriPhrase dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the LibriPhrase dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert annotators carried out quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance of dataset annotations is not described as being performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental settings, Datasets",
          "reasoning": "The LibriPhrase dataset was constructed by processing the LibriSpeech dataset with the Montreal Forced Aligner to determine word-level timestamps and then splitting full utterances into short phrases. This process relies on automated forced alignment tools for annotation, which constitutes an automatic verification or automated annotation method rather than manual quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors document the dataset creation process involving forced alignment and segmentation, indicating some form of automated QA rather than none."
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8345,
      "completion_tokens": 305,
      "total_tokens": 8650
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The LibriPhrase dataset is built from the existing LibriSpeech dataset by extracting and segmenting existing utterances into short phrases of 1 to 4 words, without being created from scratch. This involves collecting and organizing existing speech data without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "LibriPhrase is derived by processing the LibriSpeech corpus using forced alignment to determine word-level timestamps and then splitting long utterances into shorter phrases, representing a transformation and adaptation of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8863,
      "completion_tokens": 322,
      "total_tokens": 9185
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The LibriPhrase dataset is explicitly introduced and used as a training dataset in an open-vocabulary keyword spotting task to train the proposed cross-modal correspondence detector (CMCD) model from scratch. The paper details its construction and usage for training, as seen in Sections 4.1 and 4.2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The LibriPhrase dataset is used for evaluation by constructing positive and negative audio-text pairs to measure the model's performance on keyword spotting under various conditions including easy and hard negatives, as described in Sections 4.1 and 4.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and Figure 5",
          "reasoning": "The paper utilizes the LibriPhrase dataset to analyze the impact of keyword length on detection performance and ablation studies on training losses, demonstrating analysis beyond training or simple evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9586,
      "completion_tokens": 268,
      "total_tokens": 9854
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1, Datasets",
          "reasoning": "The new dataset introduced by the authors is LibriPhrase, which is derived from LibriSpeech, an English-language speech corpus of public domain audiobooks. The paper explicitly states the use of the CMU pronouncing dictionary for English grapheme-to-phoneme conversion and the evaluation datasets and examples are in English, indicating that the entries in LibriPhrase are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6804,
      "completion_tokens": 151,
      "total_tokens": 6955
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code or scripts for the construction or processing of the LibriPhrase dataset or any other dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper documents the dataset creation process of LibriPhrase in Section 4.1, explaining that it is built from LibriSpeech using the Montreal Forced Aligner for word-level time stamps and splitting utterances into shorter phrases of 1 to 4 words. They also specify the training set sizes and how noise is added for robustness."
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7629,
      "completion_tokens": 305,
      "total_tokens": 7934
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Dataset and pre-processing",
          "Reasoning": "The authors introduce a new labeled speech data subset from the JSUT dataset called 'basic5000 subset' consisting of 6.78 hours of speech by a single female speaker, which was manually annotated for TTS labels. This data is human generated because the speech was recorded by humans and labels were manually annotated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Dataset and pre-processing",
          "Reasoning": "The authors collected a proprietary Japanese speech corpus from six male and eleven female professional speakers totaling 207.96 hours of speech with manually annotated labels, called the LARGE dataset. This new dataset was recorded and annotated by humans, thus is human generated audio data."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 TTS data augmentation and Section 2.3 Text-to-speech data augmentation",
          "Reasoning": "To augment training data for the annotation model, an auxiliary TTS model was first trained on a limited set of paired data and then used to synthesize speech from a large text-only corpus. The resulting synthesized speech paired with generated labels constitutes a model-generated audio dataset created by the authors for training augmentation."
        }
      ]
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8481,
      "completion_tokens": 175,
      "total_tokens": 8656
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1.1 Dataset and pre-processing",
            "reasoning": "The paper mentions the use of a proprietary Japanese speech corpus recorded by multiple professional speakers with manually annotated labels, implying annotation by multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "The paper does not describe any annotation guidelines, instructions, or protocols given to annotators for creating the manual labels."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria provided to annotators in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "The paper does not provide or reference annotation examples or samples as part of the dataset creation process."
          }
        }
      ]
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9611,
      "completion_tokens": 468,
      "total_tokens": 10079
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that multiple human experts performed quality assurance or annotation on the new datasets. While it is mentioned that the large dataset (LARGE) has manually annotated labels, the paper does not specify that multiple human expert annotators performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human non-expert annotators were involved in quality assurance for the dataset labels."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using an AI model as a quality assurance judge for the dataset annotations. The AI models are used for annotation, but no indication is given that AI assesses or verifies annotation quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset and pre-processing; Section 3.1.2 Evaluation on annotation accuracy",
          "reasoning": "The datasets involve annotations that are manually labeled in the LARGE dataset and publicly available data (JSUT) with manual TTS labels. While the annotations are manual, no detailed QA process by humans is described. Instead, the evaluation uses algorithmic metrics such as CER and F1 score for phonemic and prosodic label annotation, which represent automatic and objective verification of annotation quality. Additionally, the proposed annotation model is trained and evaluated strictly by cross-entropy loss and error-rate based metrics, indicating an automated evaluation and verification process. Therefore, the quality assurance for the new datasets is primarily via automatic verification methods rather than explicit human quality assurance processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper discusses manual annotation of labels for the LARGE and JSUT datasets as the source of ground truth. Thus, there is some form of quality assurance implied by manual annotations. Therefore, 'N/A' is not appropriate."
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9229,
      "completion_tokens": 466,
      "total_tokens": 9695
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset and pre-processing",
          "reasoning": "The paper uses manually annotated labels for the JSUT dataset and a proprietary Japanese speech corpus, both of which are created by human contributors and represent original content not derived or adapted from other sources."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.3 Text-to-speech data augmentation and Section 3.1.1 TTS data augmentation",
          "reasoning": "The paper generates pseudo label-speech paired data using an auxiliary TTS model trained on existing labeled data and text-only corpora, thus creating new speech data synthesized entirely by the TTS model based on text-derived labels."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data is produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating data via machine translation systems or translating data from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are either manually annotated or synthetically generated; there is no mention of aggregating data from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 Text-to-speech data augmentation and Section 3.1.1 TTS data augmentation",
          "reasoning": "The pseudo label-speech paired data are derived by transforming text-only corpora into paired data using the auxiliary TTS model and a text processing module, representing data derived from existing text through generation and augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are explicitly described in the paper, with no unspecified or undocumented sources."
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9747,
      "completion_tokens": 260,
      "total_tokens": 10007
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2.1 and 3.2.2",
          "reasoning": "The newly created TTS datasets produced by the authors' annotation model are used to train and fine-tune TTS models using supervised learning methods, as described in the experiments where TTS models are trained with these datasets and evaluated for naturalness and pronunciation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1.2",
          "reasoning": "The annotated datasets created by the proposed method are used to objectively evaluate the annotation accuracy (e.g., computing CER and F1 score) to benchmark the performance of the annotation model compared to baselines."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10470,
      "completion_tokens": 600,
      "total_tokens": 11070
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1.1 and 3.2",
          "reasoning": "The datasets used and created in the paper are all in Japanese language only. No mention of multiple languages appears for the proposed datasets or experiments."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1.1 and 3.2",
          "reasoning": "The datasets and experimental data described involve only a single language: Japanese. No bilingual language pairs are mentioned or included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1.1 and overall paper",
          "reasoning": "The datasets introduced are Japanese speech corpora and text data. There is no English content in the datasets introduced by the authors."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset and pre-processing, Section 3.2 Experimental conditions",
          "reasoning": "The new datasets introduced (or augmented) by the authors are all Japanese speech datasets and accompanying Japanese phonemic and prosodic labels. The datasets such as JSUT, LARGE, and JVS are Japanese speech corpora. Therefore, the proposed dataset is monolingual non-English, specifically Japanese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The datasets contain speech samples and text labels (phonemic and prosodic annotation), not programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "While some mathematical notation is used in model formulation, the datasets themselves do not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The datasets contain human speech and text labels only, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No fictional or constructed languages are present in the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The language of the datasets is clearly documented as Japanese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language (Japanese) data and annotations."
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7688,
      "completion_tokens": 153,
      "total_tokens": 7841
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not mentioned in the paper",
          "reasoning": "The paper does not provide any links or mention that the code used for constructing the annotation models, dataset generation, or data augmentation is publicly available in any repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper documents the dataset creation process in detail, including the use of existing datasets (JSUT, LARGE), the method of annotation with the proposed model, the architecture details of the annotation model, and the TTS data augmentation procedure. Experimental conditions and evaluation protocols related to dataset construction and annotation are explicitly described in Sections 2 (Method) and 3 (Experiments)."
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8156,
      "completion_tokens": 357,
      "total_tokens": 8513
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Labelled datasets; Table 1; Section 4.2 Bemba, Nyanja, Tonga, Lozi",
          "Reasoning": "The labelled datasets consist of read speech recordings obtained from human speakers. The paper states these are recorded using the LIG-AIKUMA mobile application by native speakers or students of the University of Zambia reading sourced text, thus human involvement is explicit in both data creation and recording."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Labelled datasets; Nyanja dataset description; Tonga and Lozi datasets description",
          "Reasoning": "The text transcriptions used for read speech are sourced from publicly available literature books or human translations (e.g., translations of English captions into Nyanja by qualified high school teachers). Therefore, the text data is human generated through manual translation or public literary sources."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Unlabelled datasets",
          "Reasoning": "The unlabelled datasets are collected from radio news and talk show recordings sourced from various radio stations and YouTube. These are recordings of natural speech by humans, captured using human-operated equipment and collected by participants during a data sourcing sprint event."
        }
      ]
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9008,
      "completion_tokens": 256,
      "total_tokens": 9264
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.2 Labelled datasets",
            "reasoning": "The labelled speech data for Nyanja, Tonga, Lozi, and Bemba involved multiple speakers who are native language speakers. Additionally, crowd-sourcing for Nyanja translation was done by five qualified high school teachers who are native speakers, indicating multiple human experts participated in annotation (text translation and speech recording). Speakers and validators were students with native language proficiency."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.2 Labelled datasets",
            "reasoning": "Sentence-level tokenization of text was performed and recordings were gathered using the LIG-AIKUMA mobile application in elicitation by text mode, implying instructions were provided for consistent sentence selection and recording procedures."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.2 Labelled datasets",
            "reasoning": "The paper describes preprocessing steps for cleaning and formatting data but does not mention any specific scoring rubrics or annotation scoring guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.2 Labelled datasets",
            "reasoning": "No examples of annotation or recording guidelines are provided or illustrated in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10138,
      "completion_tokens": 449,
      "total_tokens": 10587
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 4.2 Labelled datasets",
          "reasoning": "The Nyanja dataset involved five qualified high school teachers who are native speakers to translate English captions, indicating multiple human experts contributed to the annotation. Furthermore, speakers/recorders are identified as native speakers and university students, which suggests involvement of multiple experts from the target demographic who likely contributed to validation and recording. The paper mentions validators and annotators in the process, suggesting multiple human experts participated in quality verification."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.2 Labelled datasets",
          "reasoning": "The speakers/recorders are students of the University of Zambia and identified as black (target demographic) but there is no explicit mention that they are experts in linguistics or annotation, implying they are non-experts. Multiple such students participated in recording the read speech, indicating that multiple human non-experts were involved in quality assurance by recording and validating the data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.3 Unlabelled datasets",
          "reasoning": "For the unlabelled audio collections, segmentation and removal of non-speech audio was done automatically using pyannote audio and voice activity detection modules, constituting an automated quality assurance step for the unlabelled data to ensure usability and quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents quality assurance processes involving human annotators and automated segmentation tools."
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9756,
      "completion_tokens": 497,
      "total_tokens": 10253
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.2 Labelled datasets",
          "reasoning": "The labelled datasets contain read speech recorded from text sourced directly from publicly available literature books and native translations of English captions by qualified human translators. The speech recordings were made by native speakers (students of the University of Zambia) reading sentences recorded via the LIG-AIKUMA application. This data was newly created from scratch by human contributors through direct speech recording, not derived from existing audio data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any data generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 4.2 Labelled datasets (Nyanja dataset)",
          "reasoning": "The Nyanja labelled data was created from human translations of English captions of images from Flickr30K by five qualified High School teachers who are native speakers. This indicates data produced by human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine translation being used in generating the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.3 Unlabelled datasets",
          "reasoning": "The unlabelled datasets consist of radio and TV news and talk show broadcast recordings collected from various radio stations and YouTube, aggregated by inviting students to collect these existing recordings. This is data collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.3 Unlabelled datasets - Segmentation",
          "reasoning": "The unlabelled audio data was further processed by automatic segmentation and voice activity detection using pyannote tools to remove non-speech events and produce smaller audio chunks. This represents derivation from existing audio data with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins and methods of data generation are clearly documented throughout the paper."
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10274,
      "completion_tokens": 307,
      "total_tokens": 10581
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The labelled datasets introduced in the paper are used to train end-to-end ASR Transformer models from scratch, as described in Section 5.1, demonstrating their use in training models with randomly initialized weights."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The labelled datasets are used to fine-tune a large-scale pretrained multilingual Wav2Vec2.0 model (XLS-R) for speech recognition tasks, as outlined in Section 5.2, showing supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used exclusively for evaluation; instead, the paper uses them for training and fine-tuning models, with evaluation metrics reported but not exclusive dataset usage for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not primarily used for analysis of trends or characteristics; rather, they serve in training/fine-tuning ASR models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10997,
      "completion_tokens": 602,
      "total_tokens": 11599
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 4.1 Description; Section 4.2 Labelled datasets; Section 4.3 Unlabelled datasets",
          "reasoning": "The paper introduces Zambezi Voice, a multilingual speech corpus that includes labelled and unlabelled speech data for multiple Zambian languages. Specifically, the labelled dataset contains speech data for four languages: Bemba, Nyanja, Tonga, and Lozi. The unlabelled dataset contains data for five languages: Bemba, Nyanja, Tonga, Lozi, and Lunda. This clearly indicates that the dataset includes entries in more than two human languages, fulfilling the criteria for a multilingual dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two languages, so it does not fit the bilingual category."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises African Zambian native languages and does not consist solely of English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple non-English languages (five in total), so it is not monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset contains any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is speech data for human languages only, with no biological or non-human communication systems involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes natural Zambian languages; there is no mention or indication of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All the languages in the dataset are specified and documented (Bemba, Nyanja, Tonga, Lozi, Lunda)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains multiple natural human languages, so it is not without language content."
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8215,
      "completion_tokens": 195,
      "total_tokens": 8410
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or appendix",
          "reasoning": "The paper does not provide any explicit link or reference to a publicly accessible code repository related to the data collection, preprocessing, or generation. Although the dataset is released publicly, the paper does not mention release or availability of code used to create or process the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Zambezi Voice) and its subsections 4.2 (Labelled datasets) and 4.3 (Unlabelled datasets)",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process including data sources, collection methods (e.g., recordings from literature books and translations, crowdsourcing speaker participation), preprocessing steps (e.g., segmentation, voice activity detection, filtering corrupted files), and data splits. This documentation is thorough and informative, supporting transparency and reproducibility although no code is shared."
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6056,
      "completion_tokens": 98,
      "total_tokens": 6154
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 (Speech material)",
          "Reasoning": "The new dataset introduced by the authors consists of a corpus of Greek utterances recorded by a 31-year-old native Greek male talker in a sound studio, using professional recording equipment. The recordings were manually checked and repeated as needed, indicating human generation and recording of the audio data."
        }
      ]
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6908,
      "completion_tokens": 225,
      "total_tokens": 7133
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 2.5 Procedure",
            "reasoning": "The annotation involves participants (listeners) adjusting spectral tilt and typing what they heard; these participants are likely general listeners recruited for the study, not described as experts, thus considered non-experts performing the rating and transcription tasks."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.5 Procedure",
            "reasoning": "Participants received instructions to listen to speech and adjust spectral tilt to recognize as many words as possible, indicating annotation instructions were given to guide responses."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.5 Procedure",
            "reasoning": "No explicit scoring rubric or rubric-like grading criteria were provided for the annotation tasks; correctness was judged by keyword recall count, which is a direct measurable outcome rather than a rubric-based scoring system."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.5 Procedure",
            "reasoning": "No annotation examples or example annotations for participants were mentioned in the paper guiding their responses."
          }
        }
      ]
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8038,
      "completion_tokens": 218,
      "total_tokens": 8256
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.3 Speech Material",
          "reasoning": "The corpus consists of Greek utterances recorded by a single native Greek male talker, and recordings were manually checked for quality and correctness by humans, indicating quality assurance by a human expert during data collection and segmentation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Speech Material",
          "reasoning": "The utterances were segmented using a custom amplitude-based pause detector algorithm, with recordings screened manually. Thus, an automated process was used for segmentation, representing automatic verification in part of quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7656,
      "completion_tokens": 522,
      "total_tokens": 8178
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.3 Speech material",
          "reasoning": "The paper describes the creation of a new corpus of Greek utterances recorded from a 31-year-old native Greek male talker reading the complete corpus. The recordings were performed in a controlled studio setting using professional equipment, and the sentences were segmented and checked for quality manually. This indicates that the data was newly created from scratch by human contributors and is original content not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is provided that the dataset involves data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated via machine translation methods."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the speech-shaped noise masker was generated by filtering random noise with the spectrum of a female talker's concatenated sentences from an existing corpus (Sharvard corpus [19]), the corpus itself was not newly created by the authors but an existing resource. This existing corpus is only used as a source for noise shaping; it is not a new dataset introduced by the authors. Therefore, the collated category does not apply to the newly introduced dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.4 Stimuli",
          "reasoning": "The paper describes that speech spectral tilt modifications were applied to the new Greek speech recordings by filtering speech signals through digital filters to create 25 modification levels ranging from steeper to flatter spectral tilts. These manipulations represent derived data based on the new raw speech recordings, with modifications applied to generate stimuli for the experiments."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and creation process are clearly documented in the paper; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8174,
      "completion_tokens": 226,
      "total_tokens": 8400
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2.3 (Speech material), 2.4 (Stimuli), 3 (Results), and 4 (Predicting listener preferences)",
          "reasoning": "The newly introduced Greek speech corpus is used primarily to analyze listener preferences for spectral tilt in quiet and noisy conditions, study trends and patterns in listener behavior, and develop a predictive model for preferences rather than for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8897,
      "completion_tokens": 337,
      "total_tokens": 9234
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The newly introduced dataset consists of Greek utterances recorded from a native Greek male speaker, as stated in Section 2.3. The speech material comprises sentences in the Greek language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6115,
      "completion_tokens": 146,
      "total_tokens": 6261
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not stated in the paper",
          "reasoning": "The paper does not mention any publicly available code repository or link related to the dataset collection, preprocessing, or generation. No code URLs or supplemental materials regarding code availability are provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.3 (Speech material)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including information about the native speaker, recording environment and equipment, sampling rate, segmentation method (custom amplitude-based pause detector), and data preparation steps such as downsampling and normalization. This documentation is sufficient to understand how the dataset was constructed and prepared for experiments."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6729,
      "completion_tokens": 82,
      "total_tokens": 6811
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset",
          "Reasoning": "The new dataset introduced consists of audio recordings of vocal breath sounds collected from 106 subjects recorded at a hospital using a Zoom H6 microphone, with human subjects producing the sounds and data recorded by a human-operated device under clinical supervision."
        }
      ]
    }
  },
  {
    "id": "solanki23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7581,
      "completion_tokens": 178,
      "total_tokens": 7759
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 Dataset",
            "reasoning": "The dataset was labeled to indicate boundaries of inhalation, exhalation, and breath cycles under the guidance of a pulmonologist at a hospital, implying expert human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "The paper mentions labeling under pulmonologist guidance but does not describe any detailed instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "There is no discussion or mention of scoring rubrics or specific criteria in annotation guidelines for labeling breath boundaries."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "The paper does not provide or reference any examples used for annotation or labeling guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "solanki23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8711,
      "completion_tokens": 171,
      "total_tokens": 8882
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2: Dataset",
          "reasoning": "The data was recorded under the guidance of a pulmonologist, indicating that a subject matter expert oversaw the data collection and labeling process for breath sounds and boundaries, which implies quality assurance by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8329,
      "completion_tokens": 415,
      "total_tokens": 8744
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset consists of audio files recorded from 106 subjects over a span of 4 years (2016 to 2019) using a Zoom H6 microphone at a hospital under the guidance of a pulmonologist. This indicates original data collection involving human subjects and manual annotation of breath boundaries (inhale, exhale, breath cycle), thus representing new data created entirely from human contributions."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated entirely by AI or machine learning models; instead, models are trained on the collected human audio data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data translated from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected from existing sources; rather, it was recorded newly over several years specifically for the study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While features such as MFCC statistics are computed from the audio data, these are feature extractions and not new datasets derived from existing data sources. The dataset itself is not described as derived or adapted from prior datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is clearly specified and documented as original human recordings."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8847,
      "completion_tokens": 506,
      "total_tokens": 9353
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for pre-training any models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Experiments and Results), specifically 3.1, 3.2, 3.3, and 3.5",
          "reasoning": "The dataset is used to train models from scratch such as the 2-D CNN on mel-spectrograms and the 1-D CNN LSTM on MFCC statistics. The paper describes training these models from randomly initialized parameters using the breath sound dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using pre-trained models that are then fine-tuned on this dataset. All training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any reinforcement learning based post-training methods such as RLHF applied on the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used exclusively for evaluation or benchmarking; instead, it is the primary dataset used to train and test models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Discussion)",
          "reasoning": "The dataset and experimental results are analyzed to understand gender cues in vocal breath sounds, effects of breath segment lengths, number of frames, and role of breath boundaries. The paper uses the dataset to analyze patterns and characteristics beyond simple model training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not employed as a knowledge base or in retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for training and analysis purposes as described in multiple sections of the paper."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9570,
      "completion_tokens": 578,
      "total_tokens": 10148
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of vocal breath sounds collected from subjects without reference to multiple human languages. No indication of multiple languages is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not specify that exactly two languages are included; no language-based content is present or mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises audio recordings of breath sounds, coughs, and certain vowel/fricative phonations, but does not contain English language speech content or text. No use of English language is indicated."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although sustained vowels like /A:/, /i:/, etc. are present, these are phonetic sounds, not language content. No single non-English language is indicated in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries with programming or structured code-related content; it contains audio signals only."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2: Dataset",
          "reasoning": "The dataset consists of audio recordings of human vocal breath sounds and related respiratory sounds; these are biological signals and non-speech vocalizations, constituting biological communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The type of sounds and their nature are documented and described; language content is not involved but the dataset modality is well specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While the dataset does not involve language, it consists of biological vocal breath sounds, which are a form of non-human biological communication signals. Therefore, 'N/A' does not apply since these sounds can be considered a communication system."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6788,
      "completion_tokens": 189,
      "total_tokens": 6977
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link in the paper",
          "reasoning": "The paper does not mention any public availability of code related to data collection, preprocessing, or dataset generation. There is no link or reference to a code repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Dataset",
          "reasoning": "The paper provides detailed information about the dataset creation process, including recording equipment (Zoom H6 microphone), sampling rate (44.1 kHz), recording conditions (non-laboratory, hospital, with pulmonologist guidance), microphone placement (approx. 10 cm from mouth), duration of data collection (2016 to 2019), subject details (106 subjects, 55 male and 51 female), and annotation labels (inhalation, exhalation, breath cycles). This detailed description documents the dataset creation process sufficiently for understanding and reuse."
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8047,
      "completion_tokens": 270,
      "total_tokens": 8317
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Human Data Collection",
          "Reasoning": "The authors collected new human-spoken audio data from crowd-workers on Amazon Mechanical Turk where participants spoke the user turns verbatim or paraphrased them. This data was recorded as audio waveforms, representing human-generated audio modality data."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Text-to-Speech Version",
          "Reasoning": "The authors generated a new TTS-Verbatim version of the evaluation and training sets by converting written user inputs into speech waveforms using a text-to-speech (TTS) system. This audio was generated by an algorithmic system, not human-recorded."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Redesigned DSTC11 Evaluation Sets",
          "Reasoning": "The authors created redesigned evaluation sets (dev-dstc11 and test-dstc11) by programmatically replacing slot values in the original MultiWoz dataset with new slot values drawn from external lists. This manipulation of text data was performed algorithmically to reduce memorization bias."
        }
      ]
    }
  },
  {
    "id": "soltau23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8899,
      "completion_tokens": 237,
      "total_tokens": 9136
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3",
            "reasoning": "The data collection was performed via Amazon Mechanical Turk with crowd-workers instructed to read dialogs and speak user turns verbatim or paraphrased, indicating multiple non-expert annotators contributed to creating human-spoken versions of the user inputs."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Crowd-workers were instructed explicitly to utter user turns verbatim or paraphrased preserving semantic meaning and entities, showing that instructions were provided for data collection."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Quality criteria such as missing audio, incomplete dialog, unintelligible speech, background noise, and transcription accuracy thresholds were used to measure and control quality, indicating rubric-like criteria guided acceptance of collected data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not mention providing specific examples of annotation or data collection to the crowd-workers; only instructions and quality criteria are described."
          }
        }
      ]
    }
  },
  {
    "id": "soltau23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10029,
      "completion_tokens": 525,
      "total_tokens": 10554
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human expert review or quality assurance by expert annotators for the datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention in the paper that multiple experts or subject matter experts performed quality assurance on the newly introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Human Data Collection",
          "reasoning": "Quality assurance was performed via Amazon Mechanical Turk crowd-workers who were instructed to read prompts and speak user turns either verbatim or paraphrased. The paper explicitly states that crowd-workers performed the data collection and various quality control measures were applied. Since Mechanical Turk workers are generally non-expert annotators (i.e., crowd-workers without domain expertise), and multiple were involved, this fits the category of multiple human non-experts performing quality assurance by verifying recordings and supplying transcripts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Human Data Collection; Section 5.1 ASR and Related Outputs",
          "reasoning": "The paper describes using an ASR system to transcribe collected utterances and measure transcription accuracy against the provided transcripts as a quality control step. This automated ASR model acts as a judge to detect unintelligible speech and high background noise by measuring deletion rates, indicating the use of an AI model for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Human Data Collection",
          "reasoning": "Objective, bulk quality control measures were developed that programmatically detected missing audio and incomplete dialogs. This automated verification of data completeness and consistency qualifies as an automatic process for quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents quality assurance processes including human annotation with multiple crowd-workers, AI-model-based transcription checking, and automated detection of missing data, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9647,
      "completion_tokens": 460,
      "total_tokens": 10107
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The authors collected new spoken user utterances directly from human contributors via Amazon Mechanical Turk, instructing them to produce verbatim and paraphrased speech based on the written user turns. This constitutes original content created from scratch by humans, not derived or translated from existing data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The authors generated a version of the spoken input data by synthesizing speech using a Text-to-Speech (TTS) system from the existing written user inputs, producing original audio data generated entirely by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset builds upon the existing MultiWoz dataset, the newly introduced spoken data (human recordings and TTS) are not merely collated but newly produced forms of the original written data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The authors created redesigned evaluation sets by modifying slot values from the existing MultiWoz evaluation sets\u2014replacing cities, hotel and restaurant names, and time slots with new values\u2014to reduce overlap with the training set, which constitutes derived data based on an existing source with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are well documented in the paper; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10165,
      "completion_tokens": 425,
      "total_tokens": 10590
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Sections 3.2, 5.3",
          "reasoning": "The paper describes the use of newly created spoken versions of the MultiWoz dataset (TTS-Verbatim, Human-Verbatim, Human-Paraphrased) for training dialog state tracking (DST) models, including training models from scratch such as the seq-to-seq model and fine-tuned D3ST models with ASR hypotheses and data augmentation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.3",
          "reasoning": "The dataset is used to fine-tune pre-trained large language models (D3ST models based on T5x) to perform dialog state tracking on spoken dialog inputs, particularly when retraining on ASR transcripts."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning based post-training methods such as RLHF applied using the new datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.1, 4, 6",
          "reasoning": "The redesigned evaluation sets (dev-dstc11, test-dstc11) and spoken versions are explicitly used for benchmarking and measuring the performance of participating dialog state tracking models in the DSTC11 challenge."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1, 5.3, 6",
          "reasoning": "The paper conducts analysis of the performance gap between written and spoken dialog inputs, ASR error impacts, memorization bias in the original MultiWoz datasets, and the effect of human versus TTS speech input on model performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10888,
      "completion_tokens": 566,
      "total_tokens": 11454
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described in the paper is exclusively based on English language dialogs. There is no mention of the dataset containing more than two or multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the presence of exactly two human languages in the dataset. It focuses solely on English dialogs."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (Data), Section 3.3 (Human Data Collection)",
          "reasoning": "The dataset is a spoken version of the MultiWoz corpus, which is a widely recognized English language dialog dataset. The data collection involved English dialogs only, as explicitly referenced in Section 3 and Section 3.3, with user turns read or paraphrased in English. The ASR training is also on English data (PeopleSpeech corpus), and there is no indication of other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is present that the dataset includes dialogs in any non-English language only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains dialog data and audio data only. There is no indication of code or programming language content within the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain mathematical or logical symbolic representations or formal notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data such as DNA or animal signals are included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificial constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is explicitly stated as English, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains language data\u2014English spoken dialogs\u2014so this category does not apply."
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8106,
      "completion_tokens": 213,
      "total_tokens": 8319
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Entire paper",
          "reasoning": "The paper mentions that the corpus is freely available at https://dstc11.dstc.community but does not provide any explicit information or links regarding the availability of the code used for data collection, preprocessing, or generation of the dataset. There is no mention of any public code repositories or software released alongside the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Data), including subsections 3.1, 3.2, and 3.3",
          "reasoning": "The paper provides detailed documentation on the dataset creation process, including the redesign of the evaluation sets with new slot values (Section 3.1), generation of TTS-Verbatim versions (Section 3.2), and the human data collection procedures for Human-Verbatim and Human-Paraphrased versions (Section 3.3). It also describes quality control measures and data augmentation. Hence, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7867,
      "completion_tokens": 130,
      "total_tokens": 7997
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (SC-100 Dataset), and Abstract",
          "Reasoning": "The SC-100 dataset is newly introduced in this paper as a large-scale keyword spotting dataset comprising 100 speech commands from daily life, with detailed timestamp annotations for keyword beginning and end times. The audio data are segments extracted from the LibriSpeech dataset, which consists of recordings by humans (speech audiobook data). The paper explicitly states that keywords were extracted from human speech and meticulously annotated for timing, indicating the data is human recorded audio."
        }
      ]
    }
  },
  {
    "id": "song24c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8719,
      "completion_tokens": 220,
      "total_tokens": 8939
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3",
            "reasoning": "The SC-100 dataset's keyword utterances and their beginning and end timestamps were generated by applying the Keyword-Miner tool to segment keywords and obtain timestamps from the LibriSpeech dataset, an automatic alignment and segmentation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper describes the use of an automatic tool for segmentation and timestamp extraction, and does not mention any specific human annotation instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "There is no mention of any scoring rubrics or criteria used for annotation; the process is described as automatic segmentation and filtering based on duration and meaningfulness."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not include or refer to any examples provided for annotation guidelines or processes; the dataset preparation relied on the Keyword-Miner tool output followed by selection criteria."
          }
        }
      ]
    }
  },
  {
    "id": "song24c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9849,
      "completion_tokens": 309,
      "total_tokens": 10158
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the SC-100 dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or mention of multiple human experts performing quality assurance or verification of the SC-100 dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any involvement of a single human non-expert for quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are provided regarding multiple human non-expert annotators conducting quality assurance or verification."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that AI models were used to perform quality assurance or verification of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic verification process for the quality assurance of the SC-100 dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "Although the paper describes the process of creating the SC-100 dataset and annotating the beginning and end timestamps using the Keyword-Miner tool and manual selection, it does not provide any explicit details about quality assurance steps, procedures, or verification of the timestamps annotations. Therefore, no quality assurance process is documented for the dataset."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9467,
      "completion_tokens": 411,
      "total_tokens": 9878
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The SC-100 dataset is not described as newly created from scratch by human contributors independently, but rather derived through processing and selection methods from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "There is no mention of the dataset being generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "There is no indication that the dataset involves translation from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "There is no indication that the dataset involves machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "Although the SC-100 dataset uses existing data from the LibriSpeech corpus, the data is further processed, selected, and annotated, which goes beyond simple aggregation without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (SC-100 Dataset)",
          "reasoning": "The SC-100 dataset is constructed by extracting keywords and corresponding timestamps from the LibriSpeech dataset using the Keyword-Miner tool, selecting 100 keyword classes, filtering them, and adding detailed begin/end time annotations. This shows that the dataset is derived from existing sources with additional selection, filtering, and annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and processing methods are explicitly documented in the paper."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9985,
      "completion_tokens": 524,
      "total_tokens": 10509
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SC-100 dataset is not described as being used for pre-training large models or unsupervised/self-supervised learning in the paper."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.4",
          "reasoning": "The paper describes experiments training the ED-sKWS model using both the Google Speech Commands and SC-100 datasets. The SC-100 dataset is used directly for supervised training of the KWS model from scratch, as indicated in the experimental setup and the loss function optimization involving the SC-100 dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of SC-100 dataset for fine-tuning a pre-trained model; training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the SC-100 dataset is used for reinforcement learning based post-training methods like RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4.4",
          "reasoning": "The SC-100 dataset is explicitly introduced to evaluate early-decision performance with detailed timestamp annotations, allowing precise assessment of early stopping capabilities, early-decision accuracy, and energy efficiency measures. The paper uses SC-100 primarily as an evaluation and benchmarking dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some analyses of model performance on SC-100 are presented, the primary usage is evaluation rather than exploratory analysis of dataset characteristics or trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SC-100 dataset is not used as a knowledge base to augment models or for retrieval-augmented generation as per the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The SC-100 dataset is clearly used in the paper with described purposes in training and evaluation."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10708,
      "completion_tokens": 532,
      "total_tokens": 11240
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (SC-100 Dataset), Abstract",
          "reasoning": "The SC-100 dataset introduced in the paper is built from keywords extracted from the LibriSpeech dataset (English audiobooks) and contains 100 English speech commands. The dataset entries consist solely of English words and commands used in daily life, with no mention of other languages. Therefore, the dataset is monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes algorithms and methods in code-like equations and pseudocode, the SC-100 dataset itself contains speech commands only and no programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Sections 2.1, 2.2",
          "reasoning": "Mathematical notations and equations are used to describe the SNN model and loss functions in the paper, but these are part of the methodology, not the dataset content. The dataset entries themselves are speech audio with labeled timestamps and do not contain math expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech commands only, no biological sequences or non-human communication is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is clearly specified as English; thus, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are speech commands in English, which constitutes language content."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7926,
      "completion_tokens": 161,
      "total_tokens": 8087
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3: SC-100 Dataset",
          "reasoning": "The paper describes in detail the process of creating the SC-100 dataset, including usage of Keyword-Miner tool and selection criteria, but it does not provide any link or mention of publicly available code repositories or scripts to replicate the dataset creation process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: SC-100 Dataset",
          "reasoning": "The paper provides a detailed description of the dataset creation process for SC-100, explaining the extraction method (using Keyword-Miner tool on LibriSpeech dataset), the selection criteria for keywords, partitioning into categories, dataset size, and special timestamp annotations. This amounts to substantial documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10561,
      "completion_tokens": 165,
      "total_tokens": 10726
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 and 5 Conclusions",
          "Reasoning": "The paper states in Section 2.2 and the Conclusions that the authors created a new labeling of a subset of the AMI meeting corpus for emotional hotspots. The AMI Meeting Corpus consists of multi-channel audio recordings of meetings, captured from research institute settings, thus the data is audio and captured from real human meetings. The authors added manual hotspot labels to a subset, indicating human involvement in labeling. Therefore, the new dataset is audio modality with human-generated origin (recorded from humans and manual labeling by humans). There is no indication that the data was generated or simulated algorithmically, nor is it of unknown origin."
        }
      ]
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11413,
      "completion_tokens": 321,
      "total_tokens": 11734
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2",
            "reasoning": "The paper states that for the AMI Meeting Corpus, 'we used existing hotspot annotations for ICSI and created labels for the AMI corpus.' It also mentions that the raters labeled the AMI data and plans to finalize labeling a more significant portion of the corpus, implying human annotators, likely multiple, were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 and 6",
            "reasoning": "The paper references prior work and guidelines for hotspot labeling such as [2] and [17], and mentions labeling uses the concept of hotspots with degrees (lukewarm, warm, hot). This implies that annotation instructions were provided to raters to ensure consistent labelling on the AMI corpus."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 and references [2,17]",
            "reasoning": "The hotspot labels include intensity levels (lukewarm, warm, hot), which implies a rubric or scale for annotators to follow in assessing hotspot intensity. Reference [17] is cited as a labeling guide which most likely contains scoring rubrics for hotspot intensity."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not explicitly state providing examples in the annotation guidelines for the new AMI labeling. While it references prior work and guides, no direct mention of examples in guidelines for AMI dataset annotation is present."
          }
        }
      ]
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12543,
      "completion_tokens": 246,
      "total_tokens": 12789
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.2 Data, specifically the new labeling of a subset of the AMI meeting corpus",
          "reasoning": "The paper states that they created new hotspot labels for the AMI corpus subset; although the paper does not explicitly describe the annotator(s), the context and the nature of such annotations suggest that subject matter experts or trained annotators familiar with emotional hotspot labeling performed the quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for predicting emotional hotspots, the paper does not describe AI models being used as judges or for quality assurance of the labels."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated or algorithmic verification process for the quality of the new annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12161,
      "completion_tokens": 445,
      "total_tokens": 12606
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper introduces a new labeling of a subset of the AMI meeting corpus with hotspot annotations created by the authors. This labeling was performed by human raters to provide reference labels for evaluation. Therefore, this is original content created entirely from scratch by human contributors, not merely translations or adaptations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets that are generated entirely by AI or machine learning models. The emotion classification model is pretrained, but the paper does not present any new data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by human translators translating from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of machine translation or generation of data via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data used are existing corpora (ICSI, AMI, Switchboard, SSPNet, IEMOCAP) but the novel aspect is the labeling performed for AMI, not simple collation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The new hotspot labels for the AMI corpus are obtained by human labeling, which involves interpreting and annotating existing audio data (AMI corpus). Thus, the new labeled data is derived from the existing AMI meeting corpus with modifications (annotation) applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are specified in the paper."
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12679,
      "completion_tokens": 460,
      "total_tokens": 13139
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states that the ICSI and AMI meeting corpora are not used for training models from scratch; instead, they reuse models trained on other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly labeled AMI subset or the processed ICSI data are used for supervised fine-tuning of any pre-trained model; models applied are pre-trained on different corpora."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning or RL-based post-training using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 3, Section 4",
          "reasoning": "The authors created new hotspot labels for a subset of the AMI meeting corpus (Section 2.2) and processed the ICSI meeting corpus to prepare labels. These datasets are then exclusively used to evaluate the cross-corpus hotspot detection models (Sections 3 and 4). It is clear that the new AMI labeling and processed ICSI data function as test/evaluation sets in the experiments."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper analyzes results and discusses label distributions, the new labeled AMI subset is not primarily used for analysis purposes independent of evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases to augment models or retrieval systems."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13402,
      "completion_tokens": 555,
      "total_tokens": 13957
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (ICSI Meeting Corpus), Section 2.2 (AMI corpus), and Section 2.3 (Training Corpora)",
          "reasoning": "The newly introduced labeling of a subset of the AMI meeting corpus (Section 2.2) and the use of the ICSI Meeting Corpus (Section 2.1) both exclusively contain English language meetings. The paper specifically states that the ICSI meetings are in English and the AMI corpus meetings are also conducted in English. Additionally, training corpora such as Switchboard, IEMOCAP, and Libri-Light are English datasets. Thus, the new dataset labeling introduced for the AMI corpus and the use of ICSI are monolingual English datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is based on speech audio recordings and labels of emotional hotspots; it does not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises speech audio segments annotated for emotional involvement; it does not include mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are human speech segments; there is no indication of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of constructed or fictional languages in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the language of data used; it is not unspecified or unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are spoken language audio recordings with annotated labels, thus contain human language content."
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10620,
      "completion_tokens": 156,
      "total_tokens": 10776
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.2 Data",
          "reasoning": "The paper states that new labeling was created for the AMI corpus subset but does not include any links or references to code repositories for the labeling process or dataset construction. The only code links provided refer to pretrained models used from other works, not for dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 Data",
          "reasoning": "The paper describes in detail the new labeling process for a subset of the AMI meeting corpus in Section 2.2, including the number of meetings labeled, segments, hotspots, and annotation details. While not exhaustive, this constitutes sufficient documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7919,
      "completion_tokens": 141,
      "total_tokens": 8060
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Prepared Datasets",
          "Reasoning": "The corpus consists of audiobook recordings selected from Librivox and Blizzard Challenge 2013 dataset, with three different speakers per book, which are human-read performances of literary texts, thus human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Prepared Datasets",
          "Reasoning": "Transcripts corresponding to the audiobook recordings were obtained from Project Gutenberg, which are human-authored literary texts, thus human-generated text data."
        }
      ]
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8771,
      "completion_tokens": 248,
      "total_tokens": 9019
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2, Contrastive personal pronoun subcorpus",
            "reasoning": "Three native English speakers validated 200 pronouns for contrastive focus by listening to audio clips and annotating, and inter-annotator agreement was measured with Cohen-kappa scores indicating strong agreement."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2, Contrastive personal pronoun subcorpus",
            "reasoning": "Annotators were provided with audio clips and transcriptions with the target pronoun highlighted, and were instructed to assign a binary label indicating presence of contrastive focus, indicating specific instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2, Contrastive personal pronoun subcorpus",
            "reasoning": "A binary scoring rubric was used where annotators assigned 1 for contrastive focus and 0 otherwise, and inter-annotator agreement was computed, showing a clear rubric for annotation decisions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not describe providing explicit examples or sample annotations to the annotators during their validation task."
          }
        }
      ]
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9901,
      "completion_tokens": 412,
      "total_tokens": 10313
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single human annotator with subject matter expertise or from the target demographic conducted quality assurance on the data."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2, Contrastive personal pronoun subcorpus",
          "reasoning": "Three native English speakers manually verified and validated the presence of contrastive focus on pronouns in selected audio samples. They assigned labels based on listening and reading the transcript, and their high inter-annotator agreement (Cohen-kappa scores 0.85\u20130.90) indicates reliable quality assurance performed by multiple human experts (native speakers of the language). The expertise as native speakers and their manual validation denotes subject matter expertise relevant to the task."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence was provided that quality assurance was done by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators were described as native speakers and hence presumably experts, so this label does not apply."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an AI model (BERT) was used for prominence prediction, it was not used to perform quality assurance or validation of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2, Prosodic feature extraction",
          "reasoning": "Prosodic features were extracted automatically using the continuous wavelet transform (CWT) method implemented in a toolkit, which combines pitch, energy, and duration signals into prominent scores per word. This automatic extraction serves as an initial annotation before manual verification, constituting an automatic quality assurance process for prosodic prominence labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are clearly documented: automatic prosodic extraction and manual multi-expert validation for contrastive pronouns."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9519,
      "completion_tokens": 453,
      "total_tokens": 9972
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are not newly created original content authored by humans from scratch. The paper states that the corpus is collected from existing audiobooks read by multiple speakers and aligned with existing text transcripts from Project Gutenberg."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not generated by AI or machine learning models. Models were trained on the data, but no new data was generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation, human or otherwise, in the creation of the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was machine-translated from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The data is collected and aggregated from existing sources: audiobooks from Librivox and Blizzard Challenge 2013, and text transcripts from Project Gutenberg. The authors selected and aligned this data but did not create new original speech data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The authors applied modifications and transformations to the collated data such as segmenting audio into utterances, phoneme alignment with Montreal Forced Aligner, and extracting prosodic features using continuous wavelet transform methods. They then quantized the continuous prominence scores into qualitative labels, effectively deriving new annotation layers based on the existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and processing methods are explicitly described in Section 2; hence, the origin of the data is well documented."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10037,
      "completion_tokens": 474,
      "total_tokens": 10511
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for training models from scratch. The paper describes training a randomly initialized BERT but this uses existing architecture rather than the dataset alone for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The dataset, composed of audiobook recordings with prosodic prominence labels, is used to fine-tune a pretrained BERT model to predict word-level prominence labels in a supervised manner, as described in the prominence prediction task."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of reinforcement learning based methods or RLHF with the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The test corpus including multiple speakers reading the same text is used for evaluating the accuracy of prominence prediction, particularly for contrastive focus on pronouns. Additionally, the synthesized speech conditioned on prominence labels is evaluated perceptually by human listeners in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The authors analyze speaker agreement on contrastive focus through multiple renditions of the same book by different speakers, examining inter-annotator agreement and characteristics of prominence patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models but rather as annotated data for prediction and control tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced in the paper is clearly used for supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10760,
      "completion_tokens": 554,
      "total_tokens": 11314
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The new dataset described consists exclusively of English literary texts sourced from Librivox and Blizzard Challenge 2013, with transcripts from Project Gutenberg, all in English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The dataset consists only of English language audiobooks; no indication of a second language is presented."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2: Prepared Datasets; Section 3: Predicting Prominence",
          "reasoning": "The paper states that the audiobook corpus and transcripts consist of English literary texts only, and all experiments including prominence prediction and TTS control are performed on English data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The dataset is explicitly English; no non-English single-language datasets are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Throughout paper",
          "reasoning": "While programming methods and models (e.g., BERT, FastSpeech 2) are used, the dataset itself does not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Throughout paper",
          "reasoning": "The dataset entries consist of spoken and textual English literary content without mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Throughout paper",
          "reasoning": "No biological sequences or non-human communication data are present in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Throughout paper",
          "reasoning": "The dataset contains only natural English language material; no constructed or fictional languages are involved."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The language of the dataset is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language entries (English)."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7978,
      "completion_tokens": 253,
      "total_tokens": 8231
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper about dataset construction code availability.",
          "reasoning": "The paper does not provide any link, URL, or reference to a code repository containing the code used for data collection, preprocessing, or dataset construction. While it mentions the use of several tools and libraries (e.g., Aeneas, Montreal Forced Aligner, Wavelet Prosody Toolkit), these are existing external tools, not code released by the authors. There is no indication that the authors made their processing or extraction code publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 \"Prepared Datasets\"",
          "reasoning": "The paper provides detailed information about the dataset preparation process in Section 2, including selection criteria for audiobooks, source datasets (Librivox and Blizzard Challenge 2013), transcript sourcing (Project Gutenberg), text segmentation tools used, audio segmentation methods, forced alignment, and the methodology for prosodic feature extraction (continuous wavelet transform) along with quantization of prominence scores. Additionally, the creation of a contrastive personal pronoun subcorpus is described along with manual verification and validation procedures. This constitutes substantial documentation on dataset creation."
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6649,
      "completion_tokens": 133,
      "total_tokens": 6782
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.2 Corpus design; Section 2.3 Listener attributes",
          "Reasoning": "The paper introduces the CocoNut-Humoresque corpus which consists of 1800 speech segments of meaningful Japanese utterances collected from human speakers, as described in abstract and Section 2.2. These audio segments were recorded in-the-wild, representing natural speech, and are thus human generated. Listener attributes and their subjective likability ratings were collected via crowdsourcing, further indicating the human origin of the audio data."
        }
      ]
    }
  },
  {
    "id": "suda24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7501,
      "completion_tokens": 262,
      "total_tokens": 7763
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 (Corpus design) and Section 2.3 (Listener attributes)",
            "reasoning": "The paper clearly states that 885 listeners from an online crowdsourcing service evaluated the speech segments. These listeners were randomly assigned to evaluation subsets, were not domain experts but typical crowd workers, and there is no mention of expert annotators or a specialized group."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 (Corpus design)",
            "reasoning": "Listeners were instructed to rate likability on a 6-point scale and to ignore linguistic content and focus only on the voice quality, indicating clear instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 (Corpus design)",
            "reasoning": "The paper defines a 6-point rating scale with explicit descriptions for each point (1 to 6) indicating degrees of dislike or like, which serves as a scoring rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe providing specific examples of rated utterances or sample annotations to the annotators as part of the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "suda24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8631,
      "completion_tokens": 324,
      "total_tokens": 8955
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human expert annotators performing quality assurance for the dataset annotations or ratings."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process was not described as being conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 and 2.3 (Corpus design and Listener attributes)",
          "reasoning": "The dataset annotations (likability scores) were collected via an online crowdsourcing service involving 885 listeners. These listeners evaluated speech segments, providing subjective likability ratings. The listeners are non-expert humans recruited from the crowd, likely without expert training in voice evaluation. Each speech segment was rated by multiple such non-expert listeners, implying quality assurance is through multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by AI model judges is reported in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic verification process for quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving multiple human non-expert annotators is explicitly described, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8249,
      "completion_tokens": 536,
      "total_tokens": 8785
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Corpus design",
          "reasoning": "The paper explicitly states that 1800 speech segments were selected from the Coco-Nut corpus and then evaluated by 885 human listeners who rated the likability on a 6-point scale. The speech segments are natural Japanese utterances, collected in 'in-the-wild' conditions. The listeners were human participants who provided subjective likability ratings, and their demographic attributes were also collected. This indicates that the likability score data is original content created by human contributors and not derived from pre-existing material or generated by models."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was generated entirely by AI or machine learning models without reference to existing data. Although x-vectors are used as speaker embeddings extracted from the utterances, the dataset itself is not newly generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate any data in the corpus."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 Corpus design",
          "reasoning": "The speech segments were selected from the existing Coco-Nut corpus, which is a large-scale speech description corpus. The authors curated 1800 segments from this existing corpus to form the new CocoNut-Humoresque corpus for likability ratings. Thus, the speech data is collected and aggregated from an existing source without modification to the speech recordings themselves."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors selected subsets of segments based on speaker embedding diversity (using x-vectors), there is no evidence that the data itself was transformed or modified from existing sources; the speech data is presented as-is from the Coco-Nut corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the data source and methodology clearly, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8767,
      "completion_tokens": 246,
      "total_tokens": 9013
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the CocoNut-Humoresque corpus as being used exclusively or primarily for evaluation, benchmarking, or performance measurement of models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The CocoNut-Humoresque corpus is used primarily for analyzing trends, patterns, and characteristics of voice likability, including gender and age biases and the relationship to acoustic features such as fundamental frequency and x-vectors. The paper details extensive statistical analyses on the collected subjective likability ratings and listener/speaker attributes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9490,
      "completion_tokens": 504,
      "total_tokens": 9994
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly mentions the dataset contains Japanese utterances. There is no indication of any other human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only Japanese speech segments, no second language is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states the utterances are in Japanese language, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 2.2 Corpus design; Section 5 Conclusions",
          "reasoning": "The new dataset introduced, CocoNut-Humoresque, comprises natural Japanese speech utterances, as repeatedly described (e.g., Section 2.2 states listeners understand Japanese and that the utterances are from the Japanese Coco-Nut corpus). No other language data are included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech utterances and metadata, but no programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical symbolic representations are present in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial languages are included; dataset consists of natural Japanese speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of entries is specified as Japanese clearly, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language (Japanese), thus language applies."
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6708,
      "completion_tokens": 164,
      "total_tokens": 6872
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1 Introduction",
          "reasoning": "The paper states in the Abstract and Introduction that the CocoNut-Humoresque corpus is open-source and publicly available from a GitHub repository (https://github.com/sarulab-speech/Coco-Nut). This implies that the code related to the dataset, including scripts for data collection and preprocessing, is available there, supporting reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Methodology",
          "reasoning": "Section 2 of the paper provides detailed documentation on the dataset creation process, including corpus design, subset construction algorithm, listener recruitment and attributes, and the evaluation protocol. This thorough description documents the dataset construction process adequately for reproducibility and downstream usage."
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7470,
      "completion_tokens": 89,
      "total_tokens": 7559
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Datasets",
          "Reasoning": "The paper introduces two new datasets: YouTube City and YouTube Dramas, which consist of audio collected from YouTube videos. The data represents speech audio from Arabic dialects and is naturally produced human speech, indicating human generation of the audio modality."
        }
      ]
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8322,
      "completion_tokens": 183,
      "total_tokens": 8505
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5: Human Analysis",
            "reasoning": "The paper describes a human study where a native speaker expert manually listens to and annotates a random sample of utterances. This indicates that a single expert performed the annotation analysis."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5: Human Analysis",
            "reasoning": "There is no mention or description of detailed annotation instructions provided to the annotator in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5: Human Analysis",
            "reasoning": "The paper does not describe any scoring rubrics or formal evaluation guidelines used for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5: Human Analysis",
            "reasoning": "No examples of annotation cases or guidelines examples are provided or described in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9452,
      "completion_tokens": 565,
      "total_tokens": 10017
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance for the new datasets was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document a quality assurance process involving multiple human expert annotators for the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance carried out by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments) - Human Analysis",
          "reasoning": "In the Human Analysis subsection, a native speaker expert manually listened to and annotated 25 utterances from several dialects to verify the quality of self-training labels. Although only one native speaker is explicitly mentioned, the description implies assessments were performed manually by a human with expertise. Because the paper mentions a 'native speaker expert' in singular and does not indicate multiple annotators, this is closer to Single Human Expert. However, since the rubric states if no information about multiple annotators, we cannot confirm multiple experts; so by strict interpretation, this label should not be set to true. Therefore, this label is false."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Preprocessing Pipeline and Section 4.3 Self-Training",
          "reasoning": "The preprocessing pipeline and data labeling rely on AI models (voice activity detection model pyannote.audio and multilingual speech identification model lang-id-voxlingua107-ecapa) for segmentation, language identification, and silver-label dialect identification. These AI models act as judges to automatically label and filter the data, enabling large scale data curation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Preprocessing Pipeline",
          "reasoning": "The preprocessing pipeline includes automatic segmentation using VAD, language identification, and filtering based on model confidence thresholds, constituting an automatic verification/cleaning process without manual intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documentation of quality assurance processes via AI models and automatic preprocessing pipeline as described, as well as some manual human analysis; therefore, it is incorrect to say no quality assurance was applied or documented."
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9070,
      "completion_tokens": 405,
      "total_tokens": 9475
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset created entirely from scratch by human contributors without use of existing data. The new datasets introduced are collected from existing sources such as YouTube videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any new dataset was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The newly introduced datasets, YouTube City and YouTube Dramas, are collected from existing YouTube videos using keyword-based search and manual collection respectively. The data is aggregated from existing publicly available sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper applies a preprocessing pipeline including segmentation, language identification, and silver-label dialect identification to the collected data, producing processed datasets derived from existing YouTube audio data with transformations and labeling."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the sources and processing methods of the new datasets introduced, so the origin is specified."
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9588,
      "completion_tokens": 449,
      "total_tokens": 10037
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The YouTube City dataset is used for self-training from scratch as part of the self-training approach to improve dialect identification models (Section 4.3). Table 7 and discussion in Section 6 indicate that models trained from scratch with this dataset see benefits in domain shift performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The ADI-17MSA dataset, constructed by supplementing ADI-17 with MSA data, is used to fine-tune pretrained SSL models in supervised learning settings for dialect identification (Section 4.1). The paper also describes fine-tuning HuBERT and XLS-R models on ADI-17 datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning or RL-based post-training methods using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 5 and 6",
          "reasoning": "The YouTube Dramas dataset, newly collected by the authors, is used exclusively for testing and evaluation purposes to assess domain shift performance of models (Tables 7, Section 5 Experiments, and Section 6 Results). This dataset is used to benchmark model robustness on out-of-domain dialect identification."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Experiments, Human Analysis subsection",
          "reasoning": "The newly collected YouTube City and YouTube Dramas datasets are used in human analysis to examine label quality and model prediction errors, characterizing the performance and limitations of self-training methods (Section 5). This is primarily for analysis rather than training or direct evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any of the new datasets serve as a knowledge base for augmenting models via retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10311,
      "completion_tokens": 585,
      "total_tokens": 10896
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The newly introduced datasets (YouTube City and YouTube Dramas) are described as containing Arabic speech only, focusing on Arabic dialects, without mention of any other languages, indicating they are not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The new datasets contain only Arabic speech data from various Arabic dialects; there is no indication of a second language present in the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The paper clearly describes the datasets as Arabic speech data targeted at dialect identification; there is no indication that the datasets contain English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The newly collected datasets (YouTube City and YouTube Dramas) are comprised exclusively of Arabic speech utterances reflecting different Arabic dialects, thus containing only one non-English language \u2014 Arabic."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "No programming or code-related content is reported as part of the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "The datasets consist of audio speech data; no mathematical or logical expressions or symbolic notation are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "The datasets contain human speech only (Arabic dialects); there is no mention of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "The datasets do not include fictional or artificially constructed languages; all content is Arabic speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The language(s) in the datasets are well specified as Arabic dialects; the language is documented and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain spoken language content in Arabic; thus, they are not labeled as not having any language."
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7529,
      "completion_tokens": 147,
      "total_tokens": 7676
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper describes a preprocessing pipeline and data collection procedure for the two new datasets (YouTube City and YouTube Dramas), but does not provide any links or references to publicly available code repositories for these processes."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The paper provides detailed descriptions of the new dataset collection, including data source (YouTube), collection keywords, preprocessing steps (segmentation, language identification, silver-label dialect identification), and filtering strategies, thereby documenting the dataset creation process thoroughly in the paper."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7648,
      "completion_tokens": 255,
      "total_tokens": 7903
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 MultiTalk dataset",
          "Reasoning": "The MultiTalk dataset is described as comprising over 420 hours of in-the-wild 2D talking videos across 20 languages collected from YouTube using an automated pipeline. These videos capture natural human talking faces, indicating human-generated video data."
        },
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1 MultiTalk dataset",
          "Reasoning": "The dataset includes pseudo-3D mesh vertices generated by applying the off-the-shelf 3D reconstruction model SPECTRE to the collected 2D videos. These 3D meshes are not directly recorded but produced by a model, thus are model-generated tabular data representing 3D coordinates."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1 MultiTalk dataset",
          "Reasoning": "The dataset provides pseudo-transcripts generated using the Whisper model from the audio in the videos. These transcripts are automatically generated by the model, hence are model-generated text data."
        }
      ]
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8500,
      "completion_tokens": 235,
      "total_tokens": 8735
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 2.1 MultiTalk dataset",
            "reasoning": "The dataset annotations such as pseudo-transcripts were generated automatically using the Whisper model [24], and the 3D mesh vertices were reconstructed using the off-the-shelf SPECTRE model [15], indicating annotation was done via automated AI models without human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1 MultiTalk dataset",
            "reasoning": "The paper does not provide details or mention any human-curated annotation instructions or guidelines for the pseudo-transcript or 3D mesh annotations as these were generated by automated models."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 MultiTalk dataset",
            "reasoning": "There is no mention or description of scoring rubrics or criteria used for annotation quality assessment in the automated annotation pipeline."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 MultiTalk dataset",
            "reasoning": "The paper does not include example annotations or example guidelines for annotators since annotations are produced automatically by models."
          }
        }
      ]
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9630,
      "completion_tokens": 490,
      "total_tokens": 10120
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 3.4 (Discussion and conclusion)",
          "reasoning": "The dataset uses automated pseudo-annotations including pseudo-transcripts generated by the Whisper AI model (Section 2.1) and pseudo 3D mesh vertices reconstructed from 2D videos using the SPECTRE model, which is an off-the-shelf AI 3D reconstruction model (Section 2.1). The authors note these annotations are pseudo-annotations generated by AI models rather than human annotations (Section 3.4). Thus, quality assurance is primarily performed by AI models serving as annotators and judges of the data quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 (Active speaker verification, Frontal face verification)",
          "reasoning": "The data collection pipeline involves automated filtering steps such as active speaker verification using TalkNet and frontal face verification through angle measurements by Mediapipe, which are rule/algorithmic-based automated verification processes. This automatic verification ensures the dataset quality by filtering invalid or low-quality samples without manual intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper details automated AI-based pseudo-annotation and automated filtering steps, indicating some form of quality assurance is present."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9248,
      "completion_tokens": 476,
      "total_tokens": 9724
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MultiTalk dataset is collected by scraping videos from YouTube rather than being specially recorded or created from scratch by human contributors. The paper does not mention any original content created entirely from humans for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of real 2D videos collected from YouTube and their corresponding pseudo 3D reconstructions using an existing 3D reconstruction model (SPECTRE). The data itself is not originally generated by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human translation involved in producing the dataset transcripts or labels."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is reported as part of the data preparation or annotation process for the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The MultiTalk dataset is collected by retrieving in-the-wild 2D talking videos from YouTube using keyword queries and automated filtering pipelines. This process involves collecting and aggregating publicly available videos without significantly modifying the original video content."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset includes pseudo 3D mesh vertices generated by applying an off-the-shelf 3D reconstruction model (SPECTRE) on the collected 2D videos. These 3D meshes are derived data, created by transforming existing 2D videos into 3D representations for use as annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset's collection and generation processes are explicitly described in detail in Section 2.1, so the data origin is clearly documented."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9766,
      "completion_tokens": 507,
      "total_tokens": 10273
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 2.2 Learning discrete facial motion",
          "reasoning": "The MultiTalk dataset is used to train a vector-quantized autoencoder (VQ-VAE) to learn a discrete codebook of facial motions in an unsupervised manner without using speech data, indicating pre-training on general facial motion patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state training a model from randomly initialized parameters without pre-training using the dataset; instead, it describes a two-stage training process including pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.2 Learning speech-driven motion synthesis",
          "reasoning": "After pre-training the VQ-VAE, the dataset is used to train a temporal autoregressive model supervised by speech input and discrete facial motion codes to synthesize 3D faces, representing supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of the dataset for reinforcement learning post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Experiments and Evaluation Metric",
          "reasoning": "The MultiTalk dataset is used as the test set to benchmark the proposed MultiTalk model and compare with existing methods across multiple languages, assessing lip-sync accuracy and realism."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the dataset for analyzing trends or characteristics apart from training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is used as a knowledge base to augment models through retrieval or similar means."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for pre-training, supervised fine-tuning, and evaluation as described in multiple sections of the paper."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10489,
      "completion_tokens": 530,
      "total_tokens": 11019
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 2.1; Table 1",
          "reasoning": "The MultiTalk dataset introduced in this paper contains over 420 hours of 2D talking videos in 20 different languages. The paper explicitly states in multiple locations (e.g., Abstract, Section 2.1, and Table 1) that the dataset consists of multilingual data covering exactly 20 distinct human languages, which is more than two languages, satisfying the criteria for being multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to exactly two languages; it covers 20 languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages and is not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages and is not limited to exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains video, audio, pseudo 3D mesh vertices, and transcripts but does not contain programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech-based and does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include constructed or fictional languages; it contains natural human languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included in the dataset are explicitly stated and documented as 20 human languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains multiple human natural languages."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7707,
      "completion_tokens": 157,
      "total_tokens": 7864
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and footnote",
          "reasoning": "The paper explicitly states in the abstract and provides a URL (https://multi-talk.github.io/) for the code and datasets, indicating that code for the dataset collection and related resources is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (MultiTalk dataset)",
          "reasoning": "Section 2.1 describes in detail the automated pipeline for collecting the 2D multilingual talking videos, including video collection queries, active speaker verification using TalkNet, frontal face verification with Mediapipe, transcription using Whisper, and lifting to 3D meshes using SPECTRE, providing transparent and fairly complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7042,
      "completion_tokens": 219,
      "total_tokens": 7261
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2 Audio recording and Section 2.2 Corpus statistics and dialect details",
          "Reasoning": "The paper describes the creation of the Thai-central and Thai-dialect corpora from audio recordings collected from volunteer speakers using a crowd-sourcing platform and local recruitment during 2020-2021. The recordings are of human speakers reading scripted prompts in various Thai dialects, hence the audio data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.1 Text prompts and Section 2.2 Corpus statistics and dialect details",
          "Reasoning": "The datasets include transcripts and Thai translations of the recorded speech. The text prompts were sourced from seven different sources including specially created templates for e-commerce and survival domains and translated into dialectal transcriptions by specialists. These text transcriptions and tokenizations are manually curated, indicating human-generated textual data."
        }
      ]
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7894,
      "completion_tokens": 230,
      "total_tokens": 8124
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1.2 and 2.1.4",
            "reasoning": "The audio recordings for Thai-dialect were done locally by recruiting participants from target regions and recorded mostly online. For transcription tokenization, dialectal specialists were relied upon and maximal matching tokenizers were used to verify and correct inconsistencies, indicating involvement of multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1.1 and 2.1.2",
            "reasoning": "Text prompts were carefully prepared from multiple sources and dialectal transcriptions were made for each dialect, suggesting instructions were given for transcription and recording procedures."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 and 2.1.3",
            "reasoning": "No explicit mention of annotation or transcription scoring rubrics was found in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit reference found",
            "reasoning": "The paper does not describe inclusion of annotation examples or illustrative samples in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9024,
      "completion_tokens": 436,
      "total_tokens": 9460
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1.2 Audio recording and Section 2.1.4 Tokenization",
          "reasoning": "The paper states that Thai-central recordings were validated by another expert-level crowd worker to ensure data quality, indicating multiple expert reviewers. Additionally, for tokenization in dialects, dialectal specialists performed tokenization and any disagreements with automatic tokenizers were re-examined, implying multiple experts involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1.2 Audio recording",
          "reasoning": "Audio recordings were collected through a crowd-sourcing platform which only allowed speakers who passed a prerequisite test to record audio. These crowd workers performed recording and initial validations, implying the involvement of multiple non-expert annotators in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models used as judges or validators for annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.3 Audio Verification",
          "reasoning": "The paper describes using automated checks including Voice Activity Detection (VAD), Signal to Noise Ratio (SNR), and signal energy thresholds, using an in-house VAD tool to ensure quality of audio recordings. This algorithmic verification constitutes automated quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents several quality assurance processes including expert verification, multiple non-expert validations, and automated verification."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8642,
      "completion_tokens": 518,
      "total_tokens": 9160
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 (Data collection procedure) and Abstract",
          "reasoning": "The paper clearly states that the authors collected 840 hours of read speech corpora from volunteers through a crowdsourcing platform and local recordings. The utterances were read by human speakers, and the data includes transcripts and their translations. The collection involved human participants recording original speech data based on prompts from multiple sources, indicating the data was created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by models or AI. The speech data and transcripts all come from human sources."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Text prompts) and Section 2.2 Corpus statistics and dialect details",
          "reasoning": "The paper describes that the Thai-dialect corpora contain E-commerce and Survival sentences translated to their respective dialectal transcriptions, produced by dialectal specialists. These translations are done by human experts to ensure accuracy and tokenization consistency."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation used to generate any part of the dataset or translations."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Text prompts)",
          "reasoning": "The text prompts for recording were collected from seven different sources such as BEST2010 articles, Dek-D teen webboard, Pantip general webboard, Wikipedia, and others, which implies aggregation of existing textual sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Text prompts)",
          "reasoning": "The E-commerce and Survival subsets were generated using templates where nouns, verbs, product names, etc. were replaced to create varied sentences, indicating that some data was created by modifying existing templates, thus derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and creation processes are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9160,
      "completion_tokens": 582,
      "total_tokens": 9742
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "Sections 4.2 and 4.3 describe the use of the Thai-central and Thai-dialect corpora for pre-training models before fine-tuning, demonstrating that these datasets serve as pre-training data to adapt models to the target dialectal languages."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the proposed datasets. Instead, it uses pre-trained models (e.g., HuBERT) as starting points."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.2, 4.1, 4.2, 4.3",
          "reasoning": "The paper describes fine-tuning pre-trained models on the Thai-central and Thai-dialect datasets using supervised learning methods for dialect ASR, as outlined in various experimental sections."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of reinforcement learning or RL-based post-training methods such as RLHF using the proposed datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed datasets are not used exclusively for evaluation; rather, they are predominantly used for training (pre-training and fine-tuning). Evaluation is conducted on subsets of the datasets, but this is a consequence of model training rather than the dataset being reserved solely for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 4.4",
          "reasoning": "The authors analyze the dataset's characteristics, such as vocabulary overlap (OOV metrics) between dialects (Section 2.2) and the impact of including the Pattani dialect on overall performance (Section 4.4), indicating use of the dataset for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for model augmentation or retrieval-augmented generation based on the paper content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates multiple practical uses of the introduced datasets for pre-training, supervised fine-tuning, and analysis."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9883,
      "completion_tokens": 529,
      "total_tokens": 10412
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2, Section 2.2, Table 2, and Table 3",
          "reasoning": "The proposed dataset consists of multiple Thai dialects: the main Thai-central dialect and three local dialects (Khummuang, Korat, Pattani), each representing different regions in Thailand. These dialects are distinct enough linguistically; Pattani is noted as closely related to Malay, an Austronesian language, while Khummuang and Korat are Zhuang-Tai languages. This indicates the dataset includes entries in more than two human languages or dialects."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two dialects/languages rather than exactly two."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset primarily contains Thai and Thai dialect speech, not English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple dialects/languages, not just a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech in various dialects only."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages/dialects of the dataset are clearly documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data, thus language is present."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7101,
      "completion_tokens": 183,
      "total_tokens": 7284
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract",
          "reasoning": "The paper mentions that the corpus data will be available on Github, but it does not provide any direct link or mention availability of the code used for data collection, preprocessing, or dataset generation. There is no indication that the authors have released the code associated with the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, Data collection procedure (2.1), subsections 2.1.1 to 2.1.4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the sources of text prompts, procedures for audio recording, audio verification criteria, and tokenization methods with specific details about dialectal tokenization by specialists. This extensive description covers the data collection and preprocessing steps, enabling some level of reproducibility and understanding of the dataset construction."
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8085,
      "completion_tokens": 185,
      "total_tokens": 8270
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Speech collection",
          "Reasoning": "The corpus introduced contains speech samples with different placements of focus, collected from human speakers via a web-based recording application. The speech data were recorded by human speakers recruited through Amazon Mechanical Turk, indicated by detailed recording procedures and quality controls."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Paraphrase collection and Section 2.4 Filtering paraphrases",
          "Reasoning": "The corpus includes paraphrased text corresponding to the recorded speech that reflects the paralinguistic focus information. These paraphrases were generated by native English speakers performing a free-form paraphrasing task on crowdsourcing platform MTurk, hence the text data originate from human generation."
        }
      ]
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8937,
      "completion_tokens": 303,
      "total_tokens": 9240
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2.2 and Section 2.3",
            "reasoning": "Speech recordings were collected from 10 British native speakers recruited via MTurk, a crowdsourcing platform, indicating multiple human non-expert annotators. For paraphrase collection, 550 native English speakers were selected from MTurk and divided into groups to generate paraphrases, confirming multiple human non-expert annotators involved in annotation tasks."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2.2 and Section 2.3",
            "reasoning": "Clear instructions were given to MTurk Workers for speech recording, including how to emphasize underlined words and to speak naturally, and for paraphrase collection, Workers were instructed to listen to focused speech and produce text that conveyed implied meaning without adding capitalization, exclamation marks, or inferences."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "Subjective evaluation HITs included a scoring rubric from 1 to 5 on two criteria: accuracy of paraphrase conveying implied meaning and quality of paraphrase sentence formatting, used to filter and remove low-quality paraphrases."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific location described",
            "reasoning": "The paper does not mention providing specific example annotations or paraphrases as part of annotation guidelines for either speech recording or paraphrase collection tasks."
          }
        }
      ]
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10067,
      "completion_tokens": 327,
      "total_tokens": 10394
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance carried out by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts with subject matter expertise were involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.4 Filtering paraphrases",
          "reasoning": "Multiple human annotators (MTurk workers who are native English speakers but not explicitly stated as experts) performed subjective quality evaluations of paraphrases from two perspectives (accuracy in conveying implied meaning and formatting). Each paraphrase was evaluated by three different workers, and selection was based on median ratings. This constitutes quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model used for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 Text design",
          "reasoning": "Automatic verification of original text captions was performed using GECToR, a grammatical error correction model, to filter out sentences with grammatical errors, constituting an automatic verification process in dataset preparation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes multiple quality assurance procedures; thus, QA is documented and present."
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9685,
      "completion_tokens": 498,
      "total_tokens": 10183
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 - Speech collection, Section 2.3 - Paraphrase collection",
          "reasoning": "The dataset consists of speech recordings collected from human speakers via Amazon Mechanical Turk, as described in Section 2.2. The crowdworkers produced speech samples with different placements of focus. Furthermore, the corresponding paraphrases reflecting implied meaning were collected from native English speakers also via Mechanical Turk, as described in Section 2.3. Both speech and paraphrases were created directly by human contributors and are original content generated specifically for this corpus."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models. All the dataset components were created by human contributors."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved translation from another language produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of data generated by machine translation from another language in the construction of the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 - Text design",
          "reasoning": "The base captions for text design were selected and filtered from the existing Flickr8k dataset. While the authors carefully filtered and selected items, this portion of the dataset is an aggregation of existing text captions with minimal modification, qualifying as collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 - Text design",
          "reasoning": "The authors filtered the Flickr8k captions to meet criteria such as length, punctuation, and uniqueness, resulting in adapted source sentences used for recordings. Thus, the text data is derived from existing sources with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides detailed documentation of the data sources and collection methods, so the data origin is known."
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10203,
      "completion_tokens": 238,
      "total_tokens": 10441
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Analysis) and Section 4 (Discussion and conclusion)",
          "reasoning": "The corpus is primarily used for analyzing how focus expressed in speech maps into the linguistic domain, involving both qualitative and quantitative analyses of transformation patterns from paralinguistic to linguistic representation. The paper discusses detailed analysis of lexical and grammatical transformations associated with different parts of speech, demonstrating the dataset's role in revealing trends and characteristics of focus transformation rather than direct training or evaluation of models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10926,
      "completion_tokens": 385,
      "total_tokens": 11311
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Corpus construction, specifically 2.2 Speech collection and 2.3 Paraphrase collection",
          "reasoning": "The dataset introduced is an English corpus containing speech samples with different focused words and their corresponding paraphrased English text reflecting the implied meaning of the speech. All speech and paraphrase data are exclusively in English as evidenced by participant selection criteria (native English speakers from English-speaking countries) and the instructions that require English utterances and English paraphrases."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8144,
      "completion_tokens": 173,
      "total_tokens": 8317
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to the dataset construction, data processing, or collection methods."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Corpus construction) and subsections 2.1 to 2.4",
          "reasoning": "The paper contains detailed documentation of the dataset creation process including text design (Section 2.1), speech collection procedures (Section 2.2), paraphrase collection (Section 2.3), filtering and quality control (Section 2.4), as well as analysis in Section 3. The documentation explains data sourcing, filtering, crowdsourcing platform use, participant criteria, evaluation, and payment, providing transparency and completeness for reproducibility."
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6531,
      "completion_tokens": 313,
      "total_tokens": 6844
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data - Experiment 2, lines discussing training data usage from TIMIT corpus",
          "Reasoning": "The new dataset introduced in Experiment 2A is the use of the TIMIT corpus, which contains recordings of a child learning British English including both spoken utterances and their corresponding text transcriptions. This dataset is human generated as it consists of recorded speech from humans (the child and caregivers)."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data - Experiment 2, lines discussing training data usage from LibriSpeech train-clean-100",
          "Reasoning": "In Experiment 2B, the authors use LibriSpeech train-clean-100 as training data, which is a corpus consisting of human-recorded audio speech and their transcriptions. This data is human generated as it consists of real speech recordings from humans."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Language model inputs - Synthesized speech description",
          "Reasoning": "The authors generate synthetic speech utterances using TTS (Coqui TTS model) trained on LJ SPEECH dataset for Experiment 1C. This synthetic speech is model generated, created by the TTS system rather than a human speaker, and is introduced by the authors to support their method."
        }
      ]
    }
  },
  {
    "id": "sy23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7383,
      "completion_tokens": 196,
      "total_tokens": 7579
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Data",
            "reasoning": "The paper states that the child speech data contains phonetically and orthographically transcribed utterances, with transcriptions in CHAT format by experts following linguistic transcription standards, indicating expert human annotators produced the transcriptions."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide explicit mention of detailed annotation instructions given to annotators for the transcription or annotation of the newly introduced dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not describe any scoring rubrics or evaluation rubrics used for annotation in the newly introduced dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention providing annotation examples alongside guidelines or in any appendix for the newly introduced transcriptions."
          }
        }
      ]
    }
  },
  {
    "id": "sy23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8513,
      "completion_tokens": 331,
      "total_tokens": 8844
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert on the newly introduced dataset or its annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the new dataset's annotations or transcription quality."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance carried out by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by multiple non-expert human annotators is made for the newly introduced dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used extensively for processing and measuring entropy, no specific role of AI models as a quality assurance judge for dataset quality or annotations is documented."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an automated verification process or algorithmic verification serving as quality assurance specifically for the dataset annotations or transcriptions."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper utilizes existing datasets (e.g., PROVIDENCE) for testing and trains models on existing datasets (e.g., LibriSpeech, TIMIT). The paper does not introduce any new dataset or describe any new annotation or quality assurance process for any new data. Therefore, no quality assurance process is applied or documented for any newly introduced dataset."
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8131,
      "completion_tokens": 449,
      "total_tokens": 8580
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created entirely from scratch by human contributors. Instead, the authors use existing datasets such as PROVIDENCE, LibriSpeech-960, and TIMIT for training and testing."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is generated entirely by AI or machine learning models without reference to or transformation of existing data. The paper focuses on training models and methods to analyze existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 Data",
          "reasoning": "The datasets used for training and testing (LibriSpeech-960, PROVIDENCE, TIMIT) are existing datasets collected previously. The authors aggregate these existing data sources to train language models and evaluate their entropy metric, indicating that the data is collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Language model inputs",
          "reasoning": "The authors generate discrete acoustic units from speech using clustering on Hubert model features and also generate synthetic speech from orthographic transcriptions. These transformations and adaptations of existing data represent derived data derived from existing speech or text corpora."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of all data used; no unknown or undocumented data sources are present."
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8649,
      "completion_tokens": 288,
      "total_tokens": 8937
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3, Experiment 2",
          "reasoning": "The new dataset composed of paired spoken utterances and their text-based entropy values is used to train a supervised model that predicts entropy from speech features, effectively fine-tuning a pre-trained speech feature extractor (WHISPER) with supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 3",
          "reasoning": "The child-centered PROVIDENCE dataset (newly processed by the authors) is used to evaluate and benchmark the entropy metrics and speech-based prediction models against standard language development measures."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.3 and Discussion",
          "reasoning": "The dataset is analyzed to observe patterns of entropy changes in children's speech over age and to correlate with traditional language development metrics, focusing on trends and characteristics rather than model training only."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9372,
      "completion_tokens": 557,
      "total_tokens": 9929
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used for evaluation, PROVIDENCE, contains only English-learning children and adults' speech. The training data is LibriSpeech and TIMIT, both English corpora. No mention of multiple languages beyond English is found for the new datasets introduced."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of datasets containing exactly two human languages is present. All datasets and corpora mentioned are English-only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 Data; Section 3.1 Data; Abstract",
          "reasoning": "The new datasets introduced use the PROVIDENCE dataset, which contains recordings of English-learning children and their mothers. The language modeling training corpora (LibriSpeech, TIMIT) are also English-only. Therefore, these datasets contain entries with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with exactly one non-English language are introduced or described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described consist of child-directed speech audio and transcriptions. No datasets with programming or structured code content are introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe concepts (e.g., entropy formula), the datasets themselves do not contain mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are based on human child-directed speech. No biological sequences or non-human communication data are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned or used in any introduced datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of all datasets introduced is clearly described as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced clearly contain human language data (English)."
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6590,
      "completion_tokens": 162,
      "total_tokens": 6752
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract",
          "reasoning": "The paper explicitly states in the Abstract that the source code of the experiments is released at https://github.com/yaya-sy/EntropyBasedCLDMetrics, indicating that the code related to the experiments and dataset processing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 Data",
          "reasoning": "The paper provides detailed descriptions of the data used, including the training and test datasets (LibriSpeech-960 and PROVIDENCE), the preprocessing steps (e.g., phonemization, speech discretization using k-means clustering on HUBERT features), and data preparation for the language model inputs. This indicates thorough documentation of dataset creation and processing steps within the paper."
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6805,
      "completion_tokens": 83,
      "total_tokens": 6888
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Corpus of gender expansive speakers",
          "Reasoning": "The paper introduces a new corpus called the MAGES Corpus, composed of recordings from 14 gender expansive speakers reading 400 sentences each. This data is audio recorded from human speakers specifically collected for this study."
        }
      ]
    }
  },
  {
    "id": "szekely24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7657,
      "completion_tokens": 232,
      "total_tokens": 7889
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 5.2",
            "reasoning": "Section 5.2 and 6.2 describe a community evaluation where three nonbinary SGD users (non-expert community members) participated in qualitative evaluation and survey of the synthetic voices, indicating multiple human non-expert annotators performing subjective evaluation of voice qualities."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5.2",
            "reasoning": "No detailed instructions for annotation or evaluation are described; the participants listened to voices and provided feedback based on personal preferences and perceptions without mention of structured annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5.2",
            "reasoning": "The paper does not describe any formal scoring rubric or rating scale provided to annotators; evaluations are qualitative and survey-based, not using graded rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5.2",
            "reasoning": "No examples are presented as part of the annotation guidelines; the participants evaluated the voices directly without reference examples or training examples."
          }
        }
      ]
    }
  },
  {
    "id": "szekely24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8787,
      "completion_tokens": 408,
      "total_tokens": 9195
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single human expert performed quality assurance on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by multiple human experts or members of the target demographic for dataset validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4 Objective evaluations",
          "reasoning": "The paper uses a state-of-the-art speaker identification model (ECAPA-TDNN) to compute speaker similarity scores for assessing speaker consistency and identity preservation of the synthetic voices, which constitutes an AI model performing quality assurance on the dataset-related synthetic samples."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4 Objective evaluations",
          "reasoning": "Objective metrics including automatic measurements of acoustic Vocal Tract Length (aVTL) on synthesized samples and automatic Mean Opinion Score (MOS) prediction are employed to assess quality and consistency. These automated verification techniques contribute to quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents quality assurance performed via AI models and automated metrics; thus, QA is present."
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8405,
      "completion_tokens": 430,
      "total_tokens": 8835
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.3 Corpus of gender expansive speakers",
          "reasoning": "The paper introduces a novel corpus recorded by 14 gender expansive human speakers from the mid-Atlantic region of the United States, recruited specifically for this research. This dataset consists of 400 sentences per speaker recorded to cover phonetic coverage, with ethical approval and original recordings collected from human participants."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The synthetic voices generated by the TTS model are outputs of the model, but these are not datasets introduced by the authors; rather, they are synthesized results based on the corpus. The paper does not describe generating an entirely new dataset from the model alone."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data generated by machine translation systems in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a collection or aggregation of existing data sources without modification; it is original recordings collected from participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While existing x-vector models and pretrained TTS architectures are used for synthesis, the dataset itself is not derived from existing data with modifications; it is original speech recordings from human subjects."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is described explicitly as original recordings from human participants (MAGES corpus)."
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8923,
      "completion_tokens": 315,
      "total_tokens": 9238
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The MAGES corpus of fourteen gender expansive speakers is used to fine-tune a pre-trained Tacotron 2 TTS model using supervised learning. The model training is described as transfer learning starting from a pre-trained gender-ambiguous model, which is then adapted to the new corpus."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 and Section 5",
          "reasoning": "The MAGES corpus is used in objective evaluations to assess consistency and synthesis quality of the TTS outputs. Additionally, the synthesized voices based on this dataset are evaluated by nonbinary SGD users in qualitative community evaluations."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 6.2",
          "reasoning": "The MAGES corpus is analyzed to extract features such as acoustic Vocal Tract Length (aVTL) and to understand variability among gender expansive speakers. The community evaluation also analyzes user perceptions of synthesized voices derived from this dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9646,
      "completion_tokens": 594,
      "total_tokens": 10240
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described, the MAGES corpus, records 14 gender expansive speakers in the mid-Atlantic region of the United States, all speaking English (Section 2.3). There is no indication of multiple languages being represented."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English speech data from gender expansive speakers with no indication of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.3 (Corpus of gender expansive speakers)",
          "reasoning": "The MAGES corpus used for training the TTS system comprises recordings of 14 speakers of English, with the speech material taken from the ModelTalker database which covers English diphones and triphones. The corpus is specified as being from speakers in the mid-Atlantic US region and is English-only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is explicitly described as English speech; no other languages or non-English monolingual datasets are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses technical models (Tacotron 2, x-vectors, PCA), there is no indication that the dataset itself contains entries of programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken utterances; mathematical or logical notation is not part of it, even though the paper discusses mathematical methods applied during modeling."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus contains human speech from gender expansive speakers; no biological sequences or non-human communications are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on English speech from real speakers. There is no mention of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is clearly stated as English; it is documented and specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data (English spoken utterances). It is not language-free."
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6864,
      "completion_tokens": 176,
      "total_tokens": 7040
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.3 Corpus of gender expansive speakers",
          "reasoning": "The paper describes the collection of the MAGES Corpus and references an online link to the corpus dataset (https://maxwell-hope.com/mages-corpus/), but it does not include any information or links related to publicly available code for data collection, preprocessing, or generation associated with the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.3 Corpus of gender expansive speakers",
          "reasoning": "The paper provides detailed documentation about the dataset creation process including recruitment method (word-of-mouth and social media), number of speakers (14 gender expansive speakers), IRB approval, the recording procedure (400 sentences from ModelTalker database), and quality screening steps, thus documenting the dataset creation thoroughly within the paper."
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10413,
      "completion_tokens": 201,
      "total_tokens": 10614
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1, Abstract, and Sections 2.1 and 2.2",
          "Reasoning": "The new dataset 'J-MAC' is constructed from audiobooks read by professional speakers. These audiobooks are recordings produced by humans, as indicated by 'audiobooks read by professional speakers' and the use of commercial audiobook products. Hence, the audio data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data collection",
          "Reasoning": "The text data of J-MAC is derived from out-of-copyright novels available on the web (Aozora project) that serve as reference texts for alignment to the audio. This text is human generated as it is authored literary text, used as ground-truth reference texts for alignment in the corpus construction."
        }
      ]
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11265,
      "completion_tokens": 180,
      "total_tokens": 11445
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2, 2.3",
            "reasoning": "The paper describes the corpus construction method involving automatic vocal-instrumental separation, CTC segmentation with pre-trained models for alignment, and voice activity detection for refinement. No manual human annotation is indicated for these processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not describe any instructions given to annotators since the annotation processes were fully automatic."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No scoring rubrics are described for the automatic annotation processes."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No examples are provided in the context of annotation guidelines because the annotation is performed automatically."
          }
        }
      ]
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12395,
      "completion_tokens": 306,
      "total_tokens": 12701
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple expert human annotators conducting quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is given about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that an AI model was used as a judge for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Sections 2.2 and 3.2",
          "reasoning": "The dataset quality assurance relies on automatic processes including vocal-instrumental source separation (Spleeter), signal processing-based voice activity detection (VAD), and CTC segmentation using a pre-trained end-to-end alignment model. These automatic and algorithmic methods are used to cleanse and align the data without human annotators, as explained in sections 2.2 and 3.2."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is documented and performed using automatic methods as described; hence, this label does not apply."
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12013,
      "completion_tokens": 348,
      "total_tokens": 12361
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data collection",
          "reasoning": "The corpus 'J-MAC' was constructed by collecting existing audiobook recordings of out-of-copyright books along with their reference texts available on the Web. The paper states that the audiobooks are existing commercial audiobook products, and the method extracts and aligns these existing audios and texts without creating new content, indicating the data is collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 SNR-based cleansing and Alignment procedures",
          "reasoning": "The collected audiobook data underwent processing steps including vocal-instrumental separation, connectionist temporal classification (CTC) for alignment between text and audio, and voice activity detection to refine alignment. These steps modify and transform the original collected data, thus deriving a corpus from existing materials with adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12531,
      "completion_tokens": 474,
      "total_tokens": 13005
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 Evaluation in speech synthesis",
          "reasoning": "J-MAC dataset was used to train text-to-speech synthesis models from scratch, as described in the speech synthesis evaluation where the training and validation sets were created from J-MAC data and used to train multi-speaker TTS systems."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention fine-tuning of pre-trained models using J-MAC; the dataset is primarily used for training models from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or usage of reinforcement learning based post-training techniques, such as RLHF, using the J-MAC dataset is described in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.4 Evaluation in speech synthesis",
          "reasoning": "The J-MAC corpus was used for evaluation by conducting Mean Opinion Score (MOS) tests to assess naturalness of audiobook speech synthesis, thus serving an evaluation purpose."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 and accompanying analysis",
          "reasoning": "The dataset was analysed to study influences of synthesis methods, speakers, and books on speech naturalness, indicating use for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the J-MAC dataset as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly demonstrates multiple practical usages of J-MAC including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13254,
      "completion_tokens": 510,
      "total_tokens": 13764
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced (J-MAC) is described as a Japanese audiobook speech corpus, with no mention of inclusion of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; it focuses solely on Japanese audiobooks."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not English; it is a Japanese audiobook corpus."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction and Section 2 Corpus construction",
          "reasoning": "The introduced dataset, J-MAC, is explicitly described as a Japanese audiobook corpus (see Section 1 and Table 2). The data is collected from Japanese audiobooks and uses Japanese texts and speech exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of recorded speech and aligned text; there is no inclusion of programming or code-based entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of mathematical or logical notation within the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human spoken language audio and text; biological or non-human communication systems are not part of the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned in the dataset; only Japanese natural language is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language is clearly documented as Japanese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language data; therefore, it is not inapplicable to language coverage."
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10472,
      "completion_tokens": 168,
      "total_tokens": 10640
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.1 Data collection and introduction paragraph",
          "reasoning": "The paper states that J-MAC is open-sourced on their project page (https://sites.google.com/site/shinnosuketakamichi/research-topics/j-mac_corpus), indicating that the code for dataset construction including data collection, cleansing, and alignment is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3 (Corpus construction and evaluations)",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process including data collection criteria, SNR-based cleansing with vocal-instrumental separation, CTC-based alignment and iterative realignment, and VAD refinement. Multiple figures and tables illustrate these processes, evidencing thorough documentation."
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6813,
      "completion_tokens": 296,
      "total_tokens": 7109
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 3.2",
          "Reasoning": "SaSLaW contains spontaneous dialogue speech recordings captured from participants using close-talking microphones and ear-mounted binaural microphones, directly recording what participants speak and listen to, hence human-generated audio data."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 3.2",
          "Reasoning": "Participants wore head-mounted cameras recording egocentric video at 30 fps during conversations, capturing what they watch; these are human-recorded video data from first-person perspective."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Annotation as TTS corpus",
          "Reasoning": "The close-talking microphone audio was segmented and transcribed into text via automatic speech recognition followed by manual correction, creating human-generated text transcripts of the spontaneous speech dialogue."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Figure 2",
          "Reasoning": "Head-mounted cameras and microphones are human-operated devices recording signals; impulses responses and ambient noise audio recorded via microphones and sensors are human-collected data capturing environmental sound characteristics during the recordings."
        }
      ]
    }
  },
  {
    "id": "take24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7665,
      "completion_tokens": 226,
      "total_tokens": 7891
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.2",
            "reasoning": "Transcription of close-talking microphone voices into text was performed using the Whisper model followed by manual correction. However, the initial segmentation and transcription steps are done by AI models (pyannote.audio for segmentation and Whisper for transcription). The manual correction is minimal and thus the core annotation process is primarily AI Model-based."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper describes the transcription and segmentation process as automatic followed by manual correction but does not mention any detailed annotation instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no indication of scoring rubrics or evaluation rubrics used for the transcription or annotation process in the description of dataset creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not include or mention any annotation examples or annotated samples provided as guidance during annotation."
          }
        }
      ]
    }
  },
  {
    "id": "take24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8795,
      "completion_tokens": 322,
      "total_tokens": 9117
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human annotator who is an expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts or multiple human annotators with subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance done by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2, Annotation as TTS corpus",
          "reasoning": "Quality assurance includes automatic transcription by an AI model (whisper) followed by manual correction. The initial annotation step is performed by an AI model acting as a judge in the transcription process."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2, Annotation as TTS corpus",
          "reasoning": "Automatic segmentation of utterances is done using pyannote.audio, which is an algorithmic technique for segmenting speech. This automated verification is part of the annotation quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an explicit QA process involving AI-based transcription and segmentation followed by manual correction, so quality assurance is present."
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8413,
      "completion_tokens": 428,
      "total_tokens": 8841
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 (Overview of SaSLaW) and Section 3.2 (Recording Configuration and Procedure)",
          "reasoning": "SaSLaW is constructed by recording spontaneous dialogues between human participants engaging in improvisational conversation under controlled noisy environments. The paper clearly states that two participants were recorded speaking real spontaneous dialogue, capturing what they speak, listen to, and watch synchronously with first-person devices. This is original content created entirely by human contributors, not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated solely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No human translation from other languages is mentioned as part of the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used in the data generation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not merely collected or aggregated from existing sources without modification. SaSLaW was newly recorded by authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset uses environmental noise samples from DEMAND, the speech data itself is original human recordings and not derived or adapted from other speech corpora. The noise signals are external but do not constitute derived data in the corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is explicitly documented in the paper."
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8931,
      "completion_tokens": 474,
      "total_tokens": 9405
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.2",
          "reasoning": "The SaSLaW dataset is used to fine-tune pre-trained text-to-speech models in a supervised manner to enable environment-adaptive TTS (EA-TTS) that incorporates auditory perception of the target speaker. Section 5.1 details the training conditions where models are fine-tuned on SaSLaW recordings after pre-training on other data, and Section 5.2 shows objective evaluation of these fine-tuned models using the SaSLaW dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper performs objective and subjective evaluations of TTS models trained on SaSLaW, these evaluations use SaSLaW-derived samples, so the dataset is used for training rather than exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (4.1 and 4.2)",
          "reasoning": "The authors analyze the SaSLaW dataset to examine how spontaneous speech features vary depending on environmental noise and interlocutor coordination. This analysis aims to understand signal characteristics and human speech adaptations, indicating primary usage for analyzing trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes practical usages of SaSLaW dataset in analysis and supervised fine-tuning of TTS models, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9654,
      "completion_tokens": 417,
      "total_tokens": 10071
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Recording Configuration and Procedure), Section 3.1 (Overview of SaSLaW), Abstract, and multiple mentions throughout the paper",
          "reasoning": "The SaSLaW corpus is constructed from spontaneous dialogues between human participants, and all examples and usage described in the paper indicate the use of only English language content. The paper mentions using tools such as Whisper for transcription and references a Japanese corpus JSUT only for pretraining, not for SaSLaW. No other languages are mentioned as part of the SaSLaW corpus recordings or transcriptions, implying the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6872,
      "completion_tokens": 155,
      "total_tokens": 7027
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and footnote with GitHub link",
          "reasoning": "The paper states that the SaSLaW corpus is constructed and published, and it provides a URL link to a GitHub repository (https://github.com/sarulab-speech/SaSLaW) in the abstract area, indicating that the code related to the dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Corpus Construction Methodology)",
          "reasoning": "Section 3 thoroughly describes the recording configurations, equipment used, procedures followed for recording, variation in noise environments, data annotation, and additional data collection for evaluation reproducibility. This detailed description serves as documentation for dataset creation."
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10742,
      "completion_tokens": 154,
      "total_tokens": 10896
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Audio Pair with Difference Dataset, paragraphs describing synthesis of APwD-Dataset",
          "Reasoning": "The paper introduces the APwD-Dataset, which consists of pairs of audio clips synthesized from existing labeled audio data (FSD50K and ESC-50). The dataset includes audio pairs with differences generated by the Scaiper tool, which synthesizes audio by mixing and modifying existing human-recorded samples. Thus, the audio data modality is audio, and it is both human generated (the original audio data recorded by humans) and model generated (the new paired data synthesized algorithmically by Scaiper)."
        }
      ]
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11594,
      "completion_tokens": 247,
      "total_tokens": 11841
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 (Audio Pair with Difference Dataset)",
            "reasoning": "The APwD-Dataset was synthesized using existing audio data from FSD50K and ESC-50 with a soundscape synthesis tool 'Scaper' to create paired audio samples with controlled differences and corresponding descriptive text. This indicates an automatic process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No explicit mention of any annotation instructions or guidelines provided to human annotators, as data was synthesized automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "There is no indication of scoring rubrics or criteria used in the annotation process since no human annotators were involved in creating labels; annotations were generated via synthesis rules."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.1",
            "reasoning": "Examples of the difference descriptions are explicitly given (e.g., 'increase the sound of rain', 'make car horn lower and add dog bark'), serving as examples for how auxiliary text describing differences was constructed."
          }
        }
      ]
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12724,
      "completion_tokens": 230,
      "total_tokens": 12954
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset (APwD-Dataset) was synthesized automatically using existing datasets (FSD50K and ESC-50) and an audio synthesis tool (Scaiper), followed by manual exclusion of noisy or impure audio samples. However, the paper does not document any quality assurance process conducted by human annotators or experts, nor does it mention any automated verification or AI-based quality checks besides the synthesis process itself. No information about annotators or quality validation procedures is provided. Therefore, no quality assurance process is described or performed."
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12342,
      "completion_tokens": 281,
      "total_tokens": 12623
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Audio Pair with Difference Dataset",
          "reasoning": "The authors constructed the APwD-Dataset by synthesizing pairs of audio clips from existing audio datasets FSD50K and ESC-50. They applied various modifications such as adding or decreasing background sounds and audio events to generate paired audio with differences, along with textual descriptions. Thus, the data is based on existing audio sources with transformations and adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12860,
      "completion_tokens": 380,
      "total_tokens": 13240
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The APwD-Dataset is used to train and test retrieval models using supervised contrastive learning with classification loss, where the dataset pairs audio clips with textual difference descriptions. This is a form of supervised fine-tuning to learn cross-modal representations."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The dataset is used explicitly for evaluating retrieval accuracy (Recall@K) of the proposed method and baseline to benchmark performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3.2",
          "reasoning": "The dataset is used to analyze the learned embeddings via visualization (UMAP) to confirm the relationship between audio differences and text representations."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used for supervised fine-tuning, evaluation, and analysis as documented in multiple sections."
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13583,
      "completion_tokens": 566,
      "total_tokens": 14149
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's newly constructed dataset, APwD-Dataset, contains textual descriptions that are in English only, with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; only English text descriptions are used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 'Audio Pair with Difference Dataset'",
          "reasoning": "The APwD-Dataset includes text descriptions in the form of imperative sentences in English that describe differences between paired audio clips, as illustrated by examples like 'increase the sound of rain' or 'make car horn lower and add dog bark.' There is no indication of any other languages being present in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-related content is included in the dataset entries. The dataset consists of audio clips and natural language text descriptions."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical formulas or formal logical notation are included in the dataset entries; the dataset is audio-text pairs with natural language text."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the audio contains sounds like 'dog barking' and 'chirping birds,' the dataset entries are audio files with textual descriptions, not biological sequences or symbolic representations of non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificially created languages; only English text is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language content is explicitly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language text entries describing audio differences, so it cannot be classified as not containing any language."
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10801,
      "completion_tokens": 197,
      "total_tokens": 10998
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 (Audio Pair with Difference Dataset)",
          "reasoning": "The paper describes in Section 4.1 the procedure for synthesizing the Audio Pair with Difference Dataset (APwD-Dataset), including the use of FSD50K, ESC-50 datasets and the Scaper tool for synthesis. However, it does not provide any URLs, links, or references to a code repository for dataset creation or code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Audio Pair with Difference Dataset)",
          "reasoning": "Section 4.1 provides a detailed explanation of the dataset creation process, including how audio pairs were synthesized, the labels used for synthesis, the procedure of data splitting, manual exclusion of noisy data, types of audio differences applied, and how descriptive text was assigned. Thus, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5995,
      "completion_tokens": 209,
      "total_tokens": 6204
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Protocol; Section 2.3 Acoustic low-level feature extraction; Section 2.1 Participants",
          "Reasoning": "The paper describes collecting voice recordings (audio) from participants performing speech tasks. These audio data were recorded from human participants using a built-in microphone in a controlled clinical environment as part of the newly implemented protocol designed by the authors for this study."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Protocol; Section 2.1 Participants",
          "Reasoning": "The paper reports collecting handwriting data (drawing trajectory) via a tablet's pen position sampled at 62.5 Hz using a custom MATLAB script during concurrent drawing tasks. These sensor signals are human-generated, as they arise from participant manual drawing movements captured by the tablet sensor system introduced by the authors in this new dataset."
        }
      ]
    }
  },
  {
    "id": "talkar22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6847,
      "completion_tokens": 225,
      "total_tokens": 7072
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 Participants",
            "reasoning": "The cognitive status classification (NCI vs MCI) was determined by a consensus conference based on extensive cognitive battery data, which implies evaluation by expert clinicians or neuropsychologists rather than non-expert annotators or automated methods."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Participants",
            "reasoning": "The cognitive assessments and classification followed a validated battery and consensus conference procedure, indicating detailed instructions and protocols guiding the annotation/classification process by experts."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.1 Participants",
            "reasoning": "The process used a validated battery of neuropsychological tests and MoCA scores to determine cognitive status, suggesting a rubric or criteria-based scoring system was applied during expert consensus for the classification."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not stated",
            "reasoning": "The paper does not explicitly mention provision of annotation examples or sample cases for training or clarification in the cognitive status classification."
          }
        }
      ]
    }
  },
  {
    "id": "talkar22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7977,
      "completion_tokens": 283,
      "total_tokens": 8260
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Participants",
          "reasoning": "Cognitive assessment and classification of participants as NCI or MCI was performed using a validated battery of neuropsychological tests and a consensus conference involving experts, indicating multiple human experts were involved in validating the cognitive status labels supporting the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information describing any quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance is not described as being performed or validated by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that automated verification or algorithmic rule-based quality assurance was applied to validate dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented through expert neuropsychological consensus for participant categorization, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7595,
      "completion_tokens": 421,
      "total_tokens": 8016
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Protocol and Section 2.1 Participants",
          "reasoning": "The paper reports new data collected from human participants diagnosed with Parkinson's Disease (PD). The authors designed a new speech and drawing protocol which was executed by seventeen participants (nine NCI and eight MCI), wherein simultaneous speech and drawing tasks were recorded in a clinical setting. These recordings and participant responses constitute original content created by human subjects specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as generated by AI or machine learning models; rather, models are applied to the collected data for classification purposes."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation of data from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that machine translation was used or that data originated from content in other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not gathered by aggregating pre-existing sources but collected anew via controlled experimental protocols."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While features are derived from raw audio (e.g., acoustic features extracted from recordings), the underlying dataset of participant speech is original human-collected data, not based on existing datasets or sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as original data collection from human participants following a newly designed protocol."
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8113,
      "completion_tokens": 306,
      "total_tokens": 8419
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.5 and 3.1",
          "reasoning": "The collected speech and drawing dataset was used to train Gaussian mixture models (GMMs) to classify participants as NCI or MCI. The dataset served as labeled training data for supervised learning to develop models discriminating cognitive impairment status."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4",
          "reasoning": "The dataset was used to evaluate model performance in discriminating NCI from MCI, reporting area under the curve (AUC) metrics and analyzing which features and tasks produce better discrimination, thus serving an evaluation purpose as well."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4",
          "reasoning": "The dataset was used primarily to analyze acoustic features and motor coordination patterns associated with cognitive impairment, to explore the physiological implications of these features, and to study differences in performance under single and dual task conditions."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8836,
      "completion_tokens": 505,
      "total_tokens": 9341
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Participants",
          "reasoning": "The participants were explicitly selected as English speakers (Section 2.1 Participants). All speech data collected involved reading and listing words in English as part of the protocol (Sections 2.2 Protocol, Tables 1 and 2). There is no mention of any other language used or recorded within the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of acoustic speech recordings and drawing data; no dataset entries involving programming language or code are present. Although computational methods and scripts (e.g., MATLAB) are mentioned, these do not constitute dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical methods for feature extraction and modeling, the dataset itself consists of speech and drawing data, not mathematical or logical symbolic expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech and drawing tasks data, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is based on natural human language (English) and there is no use of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly stated as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data clearly involves speech recordings in English, so it contains language data."
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6054,
      "completion_tokens": 143,
      "total_tokens": 6197
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link in the paper",
          "reasoning": "The paper does not mention or provide any links to publicly available code repositories related to the data collection, preprocessing, or dataset generation. There is no indication that code for dataset creation or processing is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper documents the dataset creation process including participant recruitment, inclusion and exclusion criteria, experimental protocols involving speech and drawing tasks, recording details, and acoustic feature extraction methods. There is a detailed description of the data collection and processing steps, enabling transparency and completeness regarding dataset creation."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7014,
      "completion_tokens": 196,
      "total_tokens": 7210
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Dataset construction and Table 1 Real samples distribution",
          "Reasoning": "The authentic speech samples are audio data collected from 162 distinct speakers of five Latin American Spanish accents. These are recorded samples sourced from a previous study [21], representing human-generated natural speech in audio format."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3 Dataset construction and Table 2 Spoof samples distribution",
          "Reasoning": "The spoof samples in the dataset are audio data generated using six different spoofing strategies, including three voice conversion (VC) algorithms (StarGAN, CycleGAN, diffusion model), one text-to-speech (TTS) system (Microsoft Azure TTS), and two combined TTS-VC approaches. These are model-generated synthetic audio samples created algorithmically."
        }
      ]
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7866,
      "completion_tokens": 198,
      "total_tokens": 8064
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3 Dataset construction",
            "reasoning": "The dataset authentic samples were sourced from another study with speakers from different Latin American nations; the selection of voice-conversion models was done by two human judges who rated the sample quality, implying expert human involvement in data selection and evaluation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 Dataset construction",
            "reasoning": "There is no explicit mention of detailed instructions provided to annotators for data annotation or labeling beyond the selection process for voice conversion samples."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 Dataset construction",
            "reasoning": "No evidence or description of scoring rubrics or formal criteria used for annotation is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 Dataset construction",
            "reasoning": "The paper does not mention inclusion of annotation examples within guidelines or documentation."
          }
        }
      ]
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8996,
      "completion_tokens": 299,
      "total_tokens": 9295
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert or target demographic member."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation indicating that multiple experts conducted quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of quality assurance by a single non-expert human annotator is described in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3, Dataset construction",
          "reasoning": "In Section 3, it is described that the selection of voice conversion architectures was performed by two human judges who rated speech conversion quality. Although their expertise level is not specified, they served as evaluators to select spoof methods, indicating quality assurance involved multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used as judges for quality assurance of the dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance process applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is some quality assurance conducted as noted by human judges; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8614,
      "completion_tokens": 518,
      "total_tokens": 9132
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset construction), paragraph 1",
          "reasoning": "The dataset's authentic speech samples were sourced from [21], which consists of recordings of 162 distinct speakers from various Latin American Spanish accents. These samples were originally recorded by human speakers and subsequently collected for this dataset, thus representing original human-produced speech data rather than adapted or translated content."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset construction), paragraph 3 and Table 2",
          "reasoning": "The spoof samples were generated by applying six different speech synthesis strategies, including voice conversion algorithms (StarGAN, CycleGAN, diffusion model) and Microsoft Azure TTS, as well as combinations of TTS and VC methods. These synthetic samples were generated entirely by AI models using the text and voice data, constituting new data generated from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation being used to produce data in the dataset is made in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset construction), paragraph 1",
          "reasoning": "The authentic speech samples were sourced from an existing dataset [21]. The authors aggregated these existing authentic samples from multiple Latin American accents into the HABLA dataset without significant modification to the original recordings, indicating a collated data component."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset construction), paragraphs 2-4 and Table 2",
          "reasoning": "The spoofed samples are derived from existing authentic samples through modification by speech synthesis algorithms (VC, TTS). This constitutes data based on existing sources with transformations applied, qualifying as derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and generation methods of the dataset; thus, the data origin is known and documented."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9132,
      "completion_tokens": 518,
      "total_tokens": 9650
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or mention the dataset being used exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Experiments",
          "reasoning": "The paper describes training anti-spoofing models from scratch using the HABLA dataset (Spanish dataset) to evaluate their performance (Section 4). The dataset is split into training, validation, and test sets, and models trained from scratch showed improved performance on the Spanish data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4, Experiments",
          "reasoning": "The paper discusses fine-tuning pre-trained English models using the HABLA Spanish dataset to create further-pretrained (FT) models, showing improved cross-language performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence of reinforcement learning or RL-based post-training methods using this dataset is provided."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4 and 4.1, Experiments and Experiments results",
          "reasoning": "The HABLA dataset is used for benchmarking and measuring the performance of pre-existing and newly trained anti-spoofing models, comparing their error rates and accuracies."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1, Experiments results; Section 5, Conclusions",
          "reasoning": "The paper uses the dataset to analyze performance differences across accents, spoofing strategies, and languages, providing insights on model effectiveness and challenges."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper fully documents usage of the HABLA dataset in training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9855,
      "completion_tokens": 551,
      "total_tokens": 10406
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The HABLA dataset includes only Spanish language entries across different Latin American regional accents but does not include multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is not composed of exactly two languages; it only contains Spanish."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is explicitly described as consisting of Spanish language samples only, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 3 Dataset construction",
          "reasoning": "The paper introduces the HABLA dataset as the first voice anti-spoofing dataset in the Spanish language with samples of various Latin American Spanish accents including Argentinian, Colombian, Peruvian, Venezuelan, and Chilean. All spoken content is in Spanish; hence, the dataset is monolingual in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication in the paper that the dataset includes programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of audio speech samples; no mathematical or logical expressions are included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No fictional or constructed languages are included in the dataset; all speech is natural Spanish."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the samples is clearly documented as Spanish; therefore, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language speech data and so it is not without language content."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7073,
      "completion_tokens": 158,
      "total_tokens": 7231
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 Dataset construction",
          "reasoning": "The paper provides a public URL for downloading the dataset itself (https://zenodo.org/record/7370805) but does not mention the release or availability of the source code used for dataset construction, data processing, or spoof sample generation scripts."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Dataset construction",
          "reasoning": "The paper contains a detailed description of the dataset creation process including the data sources, selection criteria, audio format and sampling rates, distribution across accents and genders, details on spoof sample generation using multiple voice conversion and TTS methods, and the resulting dataset statistics. This thorough description provides sufficient documentation of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7639,
      "completion_tokens": 147,
      "total_tokens": 7786
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Speech dataset",
          "Reasoning": "The new dataset used to train the TTS module is constructed using clean speech data from the VCTK dataset (manually recorded human speech) of multiple speakers combined with measured room impulse responses (RIRs) from the REVERB challenge database and Aachen impulse response database. This results in reverberant speech data created by convolving human-recorded clean speech with human-measured RIRs, where each speaker is paired with a unique environment. Thus, the dataset comprises audio modality speech data that is originally human generated and captured."
        }
      ]
    }
  },
  {
    "id": "tan22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8491,
      "completion_tokens": 225,
      "total_tokens": 8716
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2, Section 3.1",
            "reasoning": "The environment embedding extractor is trained using synthesized reverberant speech generated by convolving clean speech with measured room impulse responses (RIRs), with explicit environment labels from RIRs. The labeling is done programmatically according to the known RIR used, constituting an automatic annotation process without human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention any detailed instructions provided to annotators since the environment labels are derived automatically from the known applied RIRs."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or grading criteria are described for annotation, as labels are deterministic and assigned from environment conditions tied to RIRs."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples are provided or necessary since environment labels are directly assigned from the RIR used for reverberation synthesis."
          }
        }
      ]
    }
  },
  {
    "id": "tan22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9621,
      "completion_tokens": 346,
      "total_tokens": 9967
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any quality assurance was conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that a single non-expert human annotated or quality assured the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert humans performing quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models (embedding extractors) are used in the system, they are not described as performing quality assurance on the datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3.1",
          "reasoning": "The environment embedding extractor training involves the automatic generation of reverberant speech by convolving clean speech datasets with measured room impulse responses (RIRs), producing labeled data for environment conditions; this procedure is an automated data preparation and labeling process. Moreover, the embeddings are trained using algorithmic loss functions (softmax losses) to ensure embedding quality. There is no mention of manual verification or human annotation, implying the dataset construction and validation rely on automated, reproducible processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents and describes the automated procedures used to construct and verify datasets, thus quality assurance is not absent."
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9239,
      "completion_tokens": 364,
      "total_tokens": 9603
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The datasets used include multiple public datasets such as the VCTK dataset (clean speech data), measured RIRs from REVERB challenge database and Aachen impulse response database. These are existing sources aggregated for the purpose of this study without significant modification. The authors combine these existing datasets to create specific speaker-environment combinations for TTS training."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 2.2 and 3.1",
          "reasoning": "Derived data is created by convolving clean speech utterances from multiple speakers (from LibriTTS or VCTK) with measured room impulse responses (RIRs) from existing RIR datasets (BUT ReverbDB, REVERB challenge database, Aachen impulse response database). This process produces reverberant speech data that are transformations of existing clean speech data, thus constituting derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9757,
      "completion_tokens": 265,
      "total_tokens": 10022
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3",
          "reasoning": "The paper describes the training of the speaker embedding extractor, environment embedding extractor, and the TTS module using the specially constructed datasets (Sections 2.1, 2.2 and 3.1). These datasets are used to train models from scratch, as indicated by the neural network architectures and loss functions described without mention of pre-training or fine-tuning from pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the use of newly constructed datasets to train embedding extractors and the TTS model, demonstrating practical utility in training for environment-aware speech synthesis."
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10480,
      "completion_tokens": 595,
      "total_tokens": 11075
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses clean speech data from the VCTK dataset and LibriTTS 'train-clean-100' subset, both of which are English speech datasets. There is no mention of any other human languages involved."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that exactly two human languages were present in the newly proposed datasets; only English data are used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Speech dataset), Section 2.2 (Environment embedding extractor)",
          "reasoning": "The newly constructed datasets for training embedding extractors and the TTS module are based on LibriTTS 'train-clean-100', VCTK, and reverberant speech generated from clean English utterances. It is explicitly stated that clean speech data are used from VCTK and LibriTTS datasets, which contain English speech only. No other languages are mentioned or used."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language datasets are introduced or used in the new datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of speech utterances and reverberation impulse responses, no programming or structured code data are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas to describe loss functions and model architectures, the datasets themselves do not include mathematical or logical symbolic representations as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve human speech audio and reverberation impulse responses only; no biological or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or usage of fictional or artificially constructed languages in any proposed dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets are clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The proposed datasets contain speech utterances in English language; hence they contain language data."
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7698,
      "completion_tokens": 234,
      "total_tokens": 7932
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any link, URL, or archival location for the code related to the construction of the datasets used for training the speaker and environment embedding extractors or the TTS module, nor for the process of generating the constructed datasets from existing speech and RIR data."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 and Section 3.1",
          "reasoning": "The paper thoroughly documents the dataset construction process for the new datasets used to train the environment embedding extractor and the TTS module. It specifies the source datasets used (e.g., LibriTTS, BUT ReverbDB, VCTK, REVERB challenge, Aachen impulse response database), the method of dataset construction (e.g., generating reverberant speech by convolving clean speech with measured RIRs), the number of speakers, environments, and utterances per pair, and how the training/test splits are done. This level of documentation is sufficient to reproduce the dataset construction process based on the information provided."
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7179,
      "completion_tokens": 176,
      "total_tokens": 7355
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Datasets, paragraph 'We created our TED-EN-JP dataset as follows.'",
          "Reasoning": "The TED-EN-JP dataset is created from 628 hours of TED English speech recorded from human presentations, as stated in the data download step, indicating human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Datasets, paragraph 'We created our TED-EN-JP dataset as follows.'",
          "Reasoning": "The TED-EN-JP dataset includes human-created transcription texts (English transcriptions) and their Japanese translations, both originally produced by humans as indicated by the data download and alignment steps."
        }
      ]
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8031,
      "completion_tokens": 228,
      "total_tokens": 8259
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "The TED-EN-JP dataset was created through an automatic process involving paragraph and sentence alignment using the Vecalign tool, audio-to-text alignment using the Gentle forced aligner, and filtering based on word accuracy. No human annotators are explicitly mentioned for these tasks, indicating the annotation was automated."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any annotation guidelines or instructions provided to annotators because the data preparation involved automatic alignment tools rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria given to annotators, as the annotation was conducted via automatic alignment and filtering methods."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No examples of manual annotation or guidance samples are provided since the dataset creation was done through automated alignment and filtering processes."
          }
        }
      ]
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9161,
      "completion_tokens": 371,
      "total_tokens": 9532
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by a single human expert for the newly introduced TED-EN-JP dataset or others."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance or annotation validation for the TED-EN-JP dataset or any other dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA performed by a single non-expert annotator in relation to the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance conducted by multiple non-expert annotators is given."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA process does not involve any AI model as a judge or validator for dataset annotation or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets, paragraphs on Text alignment and Audio to text alignment",
          "reasoning": "The creation of the TED-EN-JP dataset includes automated alignment steps using Vecalign for sentence alignment between transcription and translation, and Gentle forced aligner for alignment of speech data with transcription paragraphs. Additionally, filtering was applied based on automated metrics such as word accuracy (filtering out speech data with less than 96% word accuracy). These steps constitute an automatic verification process to validate and clean the dataset, without mention of human annotation or manual validation in quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes automated processes for aligning and filtering the data for the TED-EN-JP dataset, thus some quality assurance is applied and documented."
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8779,
      "completion_tokens": 468,
      "total_tokens": 9247
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was created entirely from scratch by human contributors without reliance on existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any datasets were generated solely by AI or machine learning models without reference to existing sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that the translations used in the original TED-EN-JP dataset were produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that any translations were produced using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The TED-EN-JP dataset was created by downloading existing TED English speech data with transcriptions and Japanese translations, then aligning these texts and aligning audio to text using existing tools. This process involves collecting and aggregating existing data sources without data generation or translation indicated, thus the dataset is collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The TED-EN-JP dataset was derived from existing corpora (TED talks with English transcriptions and Japanese translations) by applying processing steps including paragraph and sentence alignment, forced alignment of audio to text, filtering based on alignment quality, and merging data to produce a dataset suitable for speech recognition tasks. This constitutes a derived dataset based on existing data with some modifications and processing."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and creation process for the new TED-EN-JP dataset is documented in detail in the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9297,
      "completion_tokens": 289,
      "total_tokens": 9586
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.2 Training and Section 3 Experiments",
          "reasoning": "The paper describes training the proposed Transformer-based multimodal ASR model from scratch with labeled datasets consisting of speech and source language text to output correct transcriptions, optimized in an end-to-end supervised manner."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Experiments (3.1 Datasets and 3.3 Experimental Results)",
          "reasoning": "The newly created TED-EN-JP dataset is used as a test set to evaluate and benchmark the proposed model's performance compared to the baseline model."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 Experimental Results",
          "reasoning": "The dataset is used to analyze and understand model performance improvements, as seen in the reporting of word error rates, attention weight visualization, and examples of corrected transcription outputs."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10020,
      "completion_tokens": 595,
      "total_tokens": 10615
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The authors introduced their original TED-EN-JP dataset consisting of English speech, its English transcription, and Japanese translation. Along with the TED-EN-JP dataset, they also used MuST-C datasets that include English speech and translations in Dutch and German, and CoVoST 2 dataset with English speech and Japanese translation. Thus, the proposed datasets contain entries involving more than two human languages: English, Japanese, Dutch, and German."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although each dataset pair involves two languages, the authors introduced multiple new datasets (including TED-EN-JP) that collectively cover more than two languages. Therefore, the dataset collection as a whole is considered multilingual rather than strictly bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All introduced datasets involve at least two languages (source speech in English and target text in Japanese, German, or Dutch). There is no dataset introduced that contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced contains entries solely in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of natural language speech and text (transcriptions and translations); there is no indication of programming or structured code content in the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are composed of speech and text data without any mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets include human speech and text, with no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages is made regarding the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the datasets are clearly specified: English, Japanese, Dutch, and German."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language data including speech and textual transcriptions and translations."
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7238,
      "completion_tokens": 155,
      "total_tokens": 7393
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Datasets, paragraph about original TED-EN-JP dataset creation",
          "reasoning": "The paper describes the process of creating the original TED-EN-JP dataset in section 3.1 but does not provide any links or references to publicly available code repositories for the data collection, preprocessing, or alignment processes."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The paper provides detailed documentation on the original TED-EN-JP dataset creation process, including data download, text alignment using Vecalign, audio-to-text alignment using Gentle forced aligner, and filtering criteria, all described in Section 3.1."
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8347,
      "completion_tokens": 173,
      "total_tokens": 8520
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Griots",
          "Reasoning": "The Griots corpus consists of audio and video recordings of live performances by griots, collected on location in Mali by a professional audio technician and videographer. This data is human-generated in the sense that it was recorded by humans capturing traditional oral performances, not synthesized or generated by models."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Griots",
          "Reasoning": "Transcriptions of the Griots audio recordings were produced by Malian doctoral candidates in linguistics using ELAN and internal transcription guidelines. These transcriptions are manually created by humans, not model-generated."
        }
      ]
    }
  },
  {
    "id": "tapo24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9199,
      "completion_tokens": 248,
      "total_tokens": 9447
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The fieldwork corpus was transcribed by several Bambara-speaking linguists at CNRS, described as linguists (experts). The griots corpus transcriptions were performed by five Malian doctoral candidates in linguistics, indicating multiple experts with relevant domain expertise."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The griots transcribers were trained to use ELAN and followed internally defined guidelines for rendering Bambara using the Latin alphabet with supplemental IPA characters, indicating provided instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Sections 3.1 and 3.2",
            "reasoning": "No mention was made of scoring rubrics or detailed standardized criteria used to score or rate the quality of annotations; transcription quality variability is mentioned but no rubric details provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Sections 3.1 and 3.2",
            "reasoning": "The paper does not mention providing transcription examples or annotated exemplars as part of the guidelines for transcribers."
          }
        }
      ]
    }
  },
  {
    "id": "tapo24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10329,
      "completion_tokens": 564,
      "total_tokens": 10893
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 Fieldwork",
          "reasoning": "The transcriptions of the fieldwork corpus were done by several Bambara-speaking linguists at CNRS, who are subject matter experts in the language and domain, indicating QA was conducted by multiple experts. However, the QA process for these transcripts is implied through the involvement of experts, but it does not explicitly state multiple QA rounds or multiple experts jointly performing QA on the same data. Given they are linguists with expertise, this applies as expert annotators providing QA."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Fieldwork and Section 3.2 Griots",
          "reasoning": "The fieldwork corpus transcripts were created by several Bambara-speaking linguists at CNRS, indicating multiple experts were involved. Likewise, the griots corpus was transcribed by five Malian doctoral candidates in linguistics, who, by institutional qualification and training, qualify as experts or at least near-experts in the subject matter. This indicates multiple human experts performed the transcription and QA process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA or annotation conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Griots",
          "reasoning": "In the griots corpus transcription, the paper states that the five transcribers often did not have extensive prior experience writing or reading Bambara, which qualifies them as non-experts. Multiple such non-experts contributed to the transcription process. The paper mentions some variability in transcription quality among these transcribers, suggesting multiple human non-experts were responsible for QA and annotation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models being used as judges or for quality assurance of dataset annotations is made."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of any automated verification or algorithmic checking as part of the dataset annotation quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents human transcription processes involving experts and non-experts indicating that some form of quality assurance was applied and described."
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9947,
      "completion_tokens": 497,
      "total_tokens": 10444
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3, specifically 3.1 Fieldwork and 3.2 Griots",
          "reasoning": "The two corpora were collected by human effort: the fieldwork corpus consists of archival recordings collected by linguists, journalists, and anthropologists in the 1970s-1990s and later transcribed by Bambara-speaking linguists; the griots corpus was recorded recently with thirty griots performing, with transcription performed by trained human transcribers. Both datasets constitute original, culturally significant speech data created or collected by humans and transcribed by human contributors without indication of being translated or derived from other data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention in the paper indicates that any of the datasets were generated by AI or machine learning models. All data comes from human-produced speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of recordings and transcriptions of Bambara speech; there is no indication that the data was translated from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of machine translation for producing the datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Fieldwork",
          "reasoning": "The fieldwork corpus consists of archival recordings from the 1970s-1990s which were digitized and transcribed recently. Thus, it is a collation of existing recordings without significant modification, serving as a gathered resource."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence suggests the datasets were derived from existing sources with modifications; while the fieldwork corpus is old recordings aggregated, there is no mention of significant transformation or adaptation applied to create the new dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper adequately documents the origin of the datasets."
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10465,
      "completion_tokens": 292,
      "total_tokens": 10757
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 Method, Section 5 Results",
          "reasoning": "The newly introduced Bambara speech corpora (fieldwork and griots) are used to fine-tune pre-trained multilingual ASR models (Wav2Vec2 XLSR and Whisper) using supervised learning methods to improve transcription accuracy."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Results",
          "reasoning": "The datasets are used for testing and evaluating ASR model performance, measuring word error rates (WER) and character error rates (CER) to benchmark accuracy across different training configurations and domains."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Results, Section 6 Discussion & Future Work",
          "reasoning": "The datasets are analyzed to understand the impact of domain differences, audio quality, and training configurations on ASR performance, providing insights into challenges and opportunities in low-resource language documentation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11188,
      "completion_tokens": 492,
      "total_tokens": 11680
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves primarily Bambara with occasional code-switching with French but does not contain more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Fieldwork, and Section 3 Data overview",
          "reasoning": "The fieldwork corpus includes recordings with Bambara and code-switching with French, meaning exactly two human languages are present in the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described contain no entries in English; the language focus is on Bambara and French."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although Bambara is the primary language, the dataset contains code-switching with French in the fieldwork corpus, so it is not exclusively monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's datasets consist of speech recordings and transcriptions and contain no programming or code-related entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of math or logical notation is present in the datasets."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that constructed or fictional languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the datasets are explicitly identified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain language data, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8406,
      "completion_tokens": 160,
      "total_tokens": 8566
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention found in the paper.",
          "reasoning": "The paper does not contain any explicit links, references, or mentions of public availability of code for dataset construction, data collection, or preprocessing. No repositories or URLs are provided related to code for dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Data), subsections 3.1 Fieldwork and 3.2 Griots",
          "reasoning": "The paper provides detailed descriptions of the datasets, including the data collection context, transcription process, speaker information, domains, audio characteristics, transcription protocols, and ethical approvals. These descriptions serve as documentation of the dataset creation process, supporting transparency and reproducibility to an extent."
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7515,
      "completion_tokens": 152,
      "total_tokens": 7667
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (ViCapPunc Dataset)",
          "Reasoning": "The ViCapPunc dataset is a newly introduced large-scale public dataset by the authors for Vietnamese capitalization and punctuation recovery. It consists of text data from 50,000 question and answer pairs crawled from reputable Vietnamese healthcare websites (Vinmec, Alobacsi, ISOFHCARE). This crawling of human-generated content from medical domain websites constitutes human-generated text data. There is no indication that this text data is generated or simulated by models. The source is explicitly stated as crawled from online healthcare sites, confirming it is human-generated text."
        }
      ]
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8367,
      "completion_tokens": 238,
      "total_tokens": 8605
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3",
            "reasoning": "Dataset ViCapPunc is automatically constructed by crawling 50,000 question and answer pairs from reputable Vietnamese healthcare websites online, followed by preprocessing steps such as text lowercasing, special character removal, punctuation retention, and automatic labeling according to a defined labeling scheme."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "Section 3 describes the labeling format and label definitions (capitalization labels and punctuation labels) used to annotate the dataset, effectively serving as annotation instructions for the labeling process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "There is no mention of scoring rubrics or criteria to differentiate annotation quality; the dataset labels appear to be assigned automatically based on predefined rules rather than manual annotation with rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 1 in Section 3",
            "reasoning": "Table 1 presents concrete examples of the dataset annotation format showing text, capitalization labels, and punctuation labels, thus illustrating annotation examples in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9497,
      "completion_tokens": 337,
      "total_tokens": 9834
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance carried out by a single expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information that multiple expert annotators performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by AI models is described."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automatic verification or rule-based quality assurance process."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not provide any description or mention of a quality assurance process applied to validate or verify the dataset annotations or contents. There is no documentation of annotation methods, human annotators, adjudication, or verification protocols."
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9115,
      "completion_tokens": 537,
      "total_tokens": 9652
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 ViCapPunc Dataset",
          "reasoning": "The dataset was created by crawling 50,000 question and answer pairs from reputable Vietnamese healthcare websites such as Vinmec, Alobacsi, and ISOFHCARE. This data was collected from real human-generated content (questions and answers) from medical domain websites, reflecting spoken language context. The paper describes pre-processing and labeling steps performed on the raw text, which aligns with human effort producing original content distinct from existing public datasets. Thus, the dataset is original content created by human contributors from scratch, not translated or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was generated entirely by AI or models. The dataset is based on crawled human-written text rather than synthetic data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information or indication in the paper suggests that the dataset is derived by translating data from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of dataset content being produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although data was collected from multiple medical question-answer websites, these sources are original human-generated content rather than existing datasets or corpora. The paper describes crawling data rather than aggregating existing datasets, and the data was transformed and labeled, indicating more than simple collation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 ViCapPunc Dataset",
          "reasoning": "The collected raw text was pre-processed by converting to lowercase, removing special characters except certain punctuations, and labeled with capitalization and punctuation tags. These transformations and the labeling process constitute modifications and adaptations from the original raw web data, making the dataset derived in the sense that it is not raw data directly used but processed and annotated for the task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and source of the dataset is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9633,
      "completion_tokens": 319,
      "total_tokens": 9952
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5 Experiments",
          "reasoning": "The ViCapPunc dataset introduced by the authors is used to fine-tune pre-trained language models (e.g., XLM-R, vELECTRA) with supervised learning methods. The training procedure and results reported in Section 5 show the dataset is used for supervised fine-tuning of these models for capitalization and punctuation recovery tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Experiments",
          "reasoning": "The ViCapPunc dataset is also used for evaluation and benchmarking the proposed JointCapPunc model performance by measuring precision, recall, and F1 scores on validation and test splits as detailed in Section 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Experiments (Ablation study)",
          "reasoning": "The authors conduct error analysis and ablation studies on their dataset, analyzing the impact of different model components on performance. This shows the dataset's use for analyzing trends and characteristics related to model effectiveness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10356,
      "completion_tokens": 562,
      "total_tokens": 10918
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset ViCapPunc is described as containing only Vietnamese language data. There is no indication of more than two human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset ViCapPunc is only in Vietnamese language; there is no mention of inclusion of exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not in English; it is specifically Vietnamese medical domain spoken language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3, ViCapPunc Dataset",
          "reasoning": "The dataset ViCapPunc is a large-scale public dataset for Vietnamese capitalization and punctuation recovery, containing spoken language from the Vietnamese medical domain. The dataset is explicitly stated to be in Vietnamese. For example, it contains 50,000 question and answer pairs crawled from Vietnamese healthcare websites and is lowercased Vietnamese text."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content is natural language text (Vietnamese), with capitalization and punctuation labels. There is no indication or description of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the model section describes mathematical notations to represent model components, the dataset itself consists purely of natural language sentences without mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human Vietnamese language text from medical conversation transcripts. There is no mention of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains naturally occurring Vietnamese language and does not include fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly documented as Vietnamese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language text in Vietnamese, so language is present."
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7574,
      "completion_tokens": 175,
      "total_tokens": 7749
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 1 Introduction",
          "reasoning": "The paper explicitly states in the Abstract and Introduction that they publicly release their dataset and the implementation of their model at https://github.com/anhtunguyen98/JointCapPunc, indicating that code related to data collection, preprocessing, and generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 ViCapPunc Dataset",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 3, including data source descriptions (crawling from Vietnamese healthcare websites), dataset statistics, preprocessing steps (lowercasing, removing special characters, segment length considerations), label definitions for capitalization and punctuation, and dataset splits into train, validation, and test sets, demonstrating transparent and complete dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10637,
      "completion_tokens": 108,
      "total_tokens": 10745
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2, 4.1",
          "Reasoning": "The new dataset consists of recordings of 28 participants reading aloud oral passages and performing shadowing tasks. These recordings are human-produced audio data obtained by recording the participants reading and shadowing passages aloud. The passages include various accents and are recorded specifically for this study, as detailed in Sections 3.2 and 4.1."
        }
      ]
    }
  },
  {
    "id": "tomita24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11489,
      "completion_tokens": 269,
      "total_tokens": 11758
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 3.1 and Section 3.2",
            "reasoning": "The data collection involved recruiting 28 university students from diverse language backgrounds who performed the speaking and shadowing tasks themselves. The paper describes participants performing shadowing and reading aloud tasks to generate the new World Englishes dataset. These participants are not described as experts annotating data but rather as subjects providing recordings."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 details that participants were given passages to read and shadow, with instructions such as practicing script-shadowing before reading aloud without audio input, indicating that instructions were provided for the shadowing tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not describe any scoring rubrics or formal criteria provided to participants for annotation or rating; the participants simply performed reading and shadowing tasks rather than rating data, so rubrics are not applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No examples or example annotations are provided to participants in the paper. The participants performed tasks rather than judgments, so providing annotation examples is not relevant here."
          }
        }
      ]
    }
  },
  {
    "id": "tomita24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12619,
      "completion_tokens": 348,
      "total_tokens": 12967
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert to validate the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence in the paper supports that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe a quality assurance process involving a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert annotators conducted quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models (e.g., Kaldi configurations) were used for processing and analysis, the paper does not indicate that AI models performed quality assurance or judgment of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 'Recording of World Englishes samples' and Section 4 'Results'",
          "reasoning": "Quality assurance appears to be conducted through automated processing methods such as Dynamic Time Warping (DTW) applied via Kaldi configurations to measure pronunciation gap (PG) and listening disfluency (LD). This automated verification ensures consistency and objectivity in the measurement of disfluency and pronunciation variation rather than manual annotation validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a clear procedure for automated measurement of listening disfluency and pronunciation gaps using established computational methods, indicating that some quality assurance in data analysis was performed."
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12237,
      "completion_tokens": 455,
      "total_tokens": 12692
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2, Data collection",
          "reasoning": "The paper describes that 28 participants read aloud selected passages and their oral passages were recorded. These recordings of World Englishes samples consist of original speech data created by human participants reading passages aloud. The data are not translated, adapted, or derived from pre-existing speech data but are newly recorded from scratch by human speakers."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models without human involvement in data creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the datasets being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by translating content using machine translation systems as indicated in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions existing corpora and publicly available passages, the new dataset consists of newly recorded speech samples, not aggregated existing data sets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Recording of World Englishes samples",
          "reasoning": "The passages read aloud by participants were originally from existing test materials (Eiken Grade-2 Tests) with GA accent, and were converted to RP versions using text-to-speech synthesis. The participants then read these passages aloud to create new data. Therefore, the data is derived from existing sources with modifications (e.g., choice of passages, conversion between accents) and human speech recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data collection and origin are clearly described."
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12755,
      "completion_tokens": 222,
      "total_tokens": 12977
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The newly introduced dataset consists of recordings of World Englishes speakers and their shadowing performances, which are used primarily to analyze the directional listening fluency and communicability among speakers. The dataset supports visualization techniques (communicability charts) and analysis of patterns in listening disfluency and pronunciation gap rather than training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13478,
      "completion_tokens": 533,
      "total_tokens": 14011
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Table 1 and Section 3.1",
          "reasoning": "The newly introduced dataset consists of audio recordings from 28 participants whose first languages span multiple language families and subfamilies, including Chinese (Sino-Tibetan), Japanese and Korean (Trans-Eurasian), French, Italian, Spanish, Hindi, Serbian, Ukrainian (Indo-European subfamilies), Hungarian (Uralic), and Malay (Dravidian). This indicates that the dataset entries involve more than two human languages represented by different speakers' accents and backgrounds."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although all passages recorded are English texts (from Eiken tests), the dataset focuses on speakers from diverse L1 backgrounds; since the entries represent various first languages influencing English pronunciation, the dataset is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises audio recordings and their analysis, but does not include mathematical or formal logical expressions as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human speech and listening behaviors; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages and accents of speakers are specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language speech data."
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10696,
      "completion_tokens": 146,
      "total_tokens": 10842
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention any code repository or provide links to code used for data collection, preprocessing, or analysis. No statements about publicly available code are found in the document."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 Data collection and 4 Analysis and visualization",
          "reasoning": "The paper documents the dataset creation process in detail, including participant recruitment and screening (Section 3.1), recording procedures of World Englishes samples (Section 3.2), and subsequent analysis and visualization methods (Section 4). This provides transparency on how the data were collected and processed, enabling reproducibility to some extent."
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7585,
      "completion_tokens": 161,
      "total_tokens": 7746
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The authors introduce a new in-house Vietnamese dataset named VBSF001 consisting of approximately 10 hours of audio recordings from a professional Southern female speaker, explicitly described as human-recorded speech data used for training and evaluation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The VBSF001 dataset includes normalized text corresponding to the audio utterances, which are manually collected and normalized by a rule-based TTS front-end module, implying the text originates from human-generated scripted sentences for TTS training."
        }
      ]
    }
  },
  {
    "id": "tran22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8437,
      "completion_tokens": 202,
      "total_tokens": 8639
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.3",
            "reasoning": "The MOS evaluation was conducted on 100 samples, each listened to by 20 testers who are Vietnamese native speakers, indicating multiple human non-expert annotators for quality evaluation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "The testers participated in a MOS test, a subjective evaluation requiring defined listening instructions to ensure consistency, though exact instructions are not detailed, the standard MOS methodology implies instructions were given."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "MOS testing uses a standardized rubric for rating speech quality on a scale, indicating the presence of a scoring rubric for the human listeners."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention providing examples or calibration samples to annotators for MOS evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "tran22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9567,
      "completion_tokens": 356,
      "total_tokens": 9923
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the dataset was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple subject matter experts or members of the target demographic performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based techniques used as a quality assurance process for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the new Vietnamese single accent dataset VBSF001 but does not describe any quality assurance process applied or documented for this dataset."
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9185,
      "completion_tokens": 389,
      "total_tokens": 9574
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper states that they use an in-house Vietnamese dataset named VBSF001 with a professional Southern female accent, consisting of approximately 7000 utterances (~10 hours). It is described as a dataset recorded by humans and not derived from any existing resource or translated data, indicating original content created entirely by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any datasets entirely using AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was created by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as in-house and recorded rather than collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and generation method of the dataset is explicitly described, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9703,
      "completion_tokens": 258,
      "total_tokens": 9961
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper explicitly states that their in-house Vietnamese dataset VBSF001 is used for training their speech synthesis model from scratch, as described in the training details in Section 3.1. The dataset is split into training, validation, and test sets and used for training their TTS models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 Latency and Quality",
          "reasoning": "The dataset is also used for evaluation purposes where the authors conduct Mean Opinion Score (MOS) tests and latency measurements on samples from the test set to benchmark the proposed model's performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10426,
      "completion_tokens": 529,
      "total_tokens": 10955
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only introduces a single dataset, VBSF001, which is described as a Vietnamese dataset with a professional Southern female accent. No other languages are mentioned."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains entries in exactly two human languages. The dataset is specified only to contain Vietnamese."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The released dataset, VBSF001, is specifically stated to be Vietnamese speech data, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper states that the in-house dataset VBSF001 is Vietnamese, with a professional Southern female accent. It consists of Vietnamese utterances only and no other languages are noted."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described purely as a speech dataset (audio and corresponding text) with no programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas describing training losses and models, these are not part of the dataset but part of the methodology."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech in Vietnamese; no biological sequences or non-human communication data is mentioned."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificial languages in the dataset. It is a natural language Vietnamese dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as Vietnamese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data in Vietnamese; therefore, it does contain language."
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7644,
      "completion_tokens": 195,
      "total_tokens": 7839
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Data and Abstract",
          "reasoning": "The paper mentions that the authors release a Vietnamese single accent dataset used in their experiments and provides a URL to the dataset on HuggingFace, but does not provide any link or mention of releasing any code associated with the dataset construction or preprocessing. There is no indication that the code for dataset collection or preprocessing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper provides a detailed description of the dataset used (VBSF001), including number of utterances (about 7000), total duration (~10 hours), speaker accent (professional Southern female), sampling frequency (22 kHz), train/validation/test splits, and a brief description of text normalization and the data preparation process. Therefore, the dataset creation process is documented in sufficient detail for reproducibility evaluation."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7799,
      "completion_tokens": 92,
      "total_tokens": 7891
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper introduces a private dataset named ASound, a crowdsourced dataset collected by the authors for respiratory illness detection recorded using mobile phones, which inherently involves human participation in recording their respiratory sounds. The data modality is audio as it contains respiratory sound samples from human participants."
        }
      ]
    }
  },
  {
    "id": "tran23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8651,
      "completion_tokens": 212,
      "total_tokens": 8863
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 4.1 Datasets",
            "reasoning": "The authors collected a crowdsourced dataset named ASound with samples recorded via mobile phones and assigned anonymized identifiers if consent was given. The annotation likely involves non-expert participants providing data, with no indication of involvement of experts annotating labels."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided to annotators for label collection in ASound dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "No information in the paper indicates the presence of scoring rubrics or detailed criteria guides for annotation in the ASound dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire Paper",
            "reasoning": "The paper does not provide or mention annotation examples or illustrative guidelines for annotators for the ASound dataset."
          }
        }
      ]
    }
  },
  {
    "id": "tran23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9781,
      "completion_tokens": 297,
      "total_tokens": 10078
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description about quality assurance by multiple non-expert human annotators in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used to perform quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification or algorithmic QA processes applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not provide any explicit information on a quality assurance process for the new dataset (ASound) that was collected, including its variants ASound-P and ASound-NP. There is no mention of human or automated quality checks, nor annotation validation methods, thus indicating no quality assurance is documented."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9399,
      "completion_tokens": 433,
      "total_tokens": 9832
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets, paragraphs discussing the ASound dataset",
          "reasoning": "The authors collected a new crowdsourced respiratory sound dataset named ASound with 4495 samples recorded using mobile phones. Additionally, they conducted a second collection phase with unique anonymized identifiers (ASound-P), indicating original data created entirely from scratch by human contributors, not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models. The datasets consist of real-world human recorded sound samples."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets generated by machine translation exists in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses several existing public datasets (Coswara, COUGHVID, ICBHI), these are not new datasets introduced by the authors; therefore, collated is not applicable to datasets introduced in this work."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating new datasets based on modifications or transformations of existing datasets. The ASound dataset is described as newly collected, not derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin of their newly introduced dataset (ASound) through crowdsourced human collection, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9917,
      "completion_tokens": 713,
      "total_tokens": 10630
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1, Algorithm 1, and Section 4.3.1",
          "reasoning": "The authors use their new dataset ASound-P in the pre-training phase where a transformer-based model is pre-trained via masked contrastive learning to learn personalized profile embeddings. Specifically, ASound-P samples with profile information are used to pre-train the profile extractor model as detailed in the training pipeline (Algorithm 1 lines 5 to 7). This pre-training is confirmed to improve performance in downstream tasks (Table 1) and is a core aspect of their framework (Section 3.1 and 4.3.1)."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training the model solely from randomly initialized parameters using the new dataset alone without pre-training. Instead, training involves pre-training and fine-tuning with personalization."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.3.1",
          "reasoning": "After the pre-training (profile extraction), the ASound dataset variants are used to fine-tune downstream classification models in a supervised manner. Section 3.2 describes end-to-end training with the extracted profile features concatenated to classification features, optimizing cross-entropy loss. The performance improvements shown in Table 1 demonstrate supervised fine-tuning on ASound and its subsets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper makes no mention of reinforcement learning or RL-based post-training using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.3 (Tables 1, 2, 3, 4)",
          "reasoning": "The ASound dataset, particularly its variants ASound-P and ASound-NP, is used for extensive evaluation and benchmarking of the proposed RoPADet method. Performance metrics including AUC on ASound-P and ASound-NP (Table 1) and cross-dataset evaluation (Table 3) confirm its usage for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3.3 and Table 4",
          "reasoning": "The dataset is used for analysis of performance fairness across user sub-groups (age and gender) in Table 4 and discussed in Section 4.3.3 to show fair classification outcomes. This analysis helps understand the impact of personalization."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that ASound or its variants are used as a knowledge base for retrieval-augmented generation or similar."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The ASound dataset, a new private dataset introduced by the authors, is heavily utilized throughout the paper for pre-training, supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10640,
      "completion_tokens": 476,
      "total_tokens": 11116
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that the new dataset contains samples in more than two human languages. There is no mention of multilingual linguistic coverage for the ASound dataset introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the new dataset has samples in exactly two human languages. The linguistic coverage of the dataset is not described as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper presents the ASound dataset collected for respiratory illness detection. The dataset is described in the context of respiratory sound recordings with no explicit mention of multiple languages or multilingual content. Since speech samples are from participants likely speaking English (or at least the paper does not specify otherwise), and no other languages are mentioned, it is reasonable to interpret the language coverage as monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the dataset entries are in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries described are sound recordings, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are part of the dataset entries. Mathematical notation appears only in model formulation, not the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of human respiratory sound recordings, not biological sequences like DNA or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly describes the dataset as containing respiratory sound recordings collected from human subjects, thus the language and nature of entries are known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio recordings of human speech and cough sounds, which are linguistic or paralinguistic data; thus, the dataset entries contain language."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7858,
      "completion_tokens": 159,
      "total_tokens": 8017
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The abstract explicitly states that source codes, reproducible baselines, and their new dataset are made publicly available at https://github.com/ReML-AI/RoPADet, indicating that code related to data collection, preprocessing, and dataset generation is accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "Section 4.1 provides a description of the newly collected ASound dataset, including the total number of recordings, collection method (crowdsourced via mobile phones), presence of profile information such as anonymized identifiers, and the reasoning behind the data collection. This constitutes documentation on dataset creation process."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8555,
      "completion_tokens": 116,
      "total_tokens": 8671
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 1, and Section 3.1",
          "Reasoning": "The HEBDB dataset is described as roughly 2500 hours of natural and spontaneous speech recordings, collected from testimonies and podcasts in Hebrew. The data is raw recordings of in-the-wild audio from human speakers, explicitly stated as natural speech and spontaneous. Thus, the audio modality is human generated, as it is directly recorded from human speech."
        }
      ]
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9407,
      "completion_tokens": 255,
      "total_tokens": 9662
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The transcriptions in the pre-processed version of the HEBDB dataset are generated automatically using a pre-trained Whisper large-v2 model. Voice Activity Detection (VAD) and Speech Segmentation use the silero-vad model, and transcript quality filtering uses a forced aligner based on the MMS model. No mention is made of human annotation for transcription or segmentation; thus, the annotations are produced by AI models."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The paper does not describe any human annotation guidelines, including instructions, because the annotation is done automatically via models rather than humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no mention of scoring rubrics for annotators since annotations are generated by AI models and automatic confidence scores are used for filtering."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Sections 3.1 and 3.2",
            "reasoning": "No annotation examples or manual instances are provided since annotation is automatic."
          }
        }
      ]
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10537,
      "completion_tokens": 397,
      "total_tokens": 10934
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human annotators with subject matter expertise or belonging to the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human conducted quality assurance on the dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (Pre-processing)",
          "reasoning": "The dataset transcriptions were generated automatically using a pre-trained Whisper large-v2 ASR model, constituting quality assurance performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 (Data filtering)",
          "reasoning": "An automatic verification step was performed using a forced aligner (MMS model) which provided confidence scores for utterances that were used to filter lower quality data, representing an automated, algorithmic quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents quality assurance procedures via automatic methods and AI models; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10155,
      "completion_tokens": 449,
      "total_tokens": 10604
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of recordings sourced from existing podcasts and testimonies; it does not represent original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data itself is not generated by AI or machine learning models; rather, models are used for transcription and preprocessing but not to generate the speech data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human translation involved in creating the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation of data; the transcription is done with an ASR model and transliteration tools are used only for alignment."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 (HEBDB dataset), Table 1",
          "reasoning": "The dataset is aggregated from multiple existing recorded sources such as podcasts and testimonies, without indication of the recordings being created by the authors. Thus, it is collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Pre-processing), Section 3.2 (Data filtering)",
          "reasoning": "The authors apply resampling, VAD-based segmentation, automatic transcription using a pre-trained ASR model, normalization (e.g., removing irrelevant tokens), diacritization, transliteration, and filtering based on confidence scores. These steps indicate the dataset is transformed and adapted from the original raw collected audio, making it a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper sufficiently documents and describes the data sources and generation method."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10673,
      "completion_tokens": 499,
      "total_tokens": 11172
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1 Baseline system, HuBERT model training description",
          "reasoning": "The dataset is used in self-supervised pre-training of the HuBERT-base model, where the authors train the model from the HEBDB dataset in an unsupervised manner before fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters without pre-training on this dataset; both provided baselines use pre-training or supervised fine-tuning instead."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Baseline system and Section 4.2 Results",
          "reasoning": "The dataset is used for supervised fine-tuning of both the HuBERT model (after pre-training) and the Conformer model, using the pre-processed and filtered version of HEBDB for ASR training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using the dataset for any reinforcement learning based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset itself is not used exclusively for evaluation; evaluation is done on the Fleurs benchmark, a separate dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or patterns; the paper focuses on releasing the dataset for model training and demonstrating baseline results."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used as a knowledge base to augment models via retrieval or similar mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has documented usage in pre-training and supervised fine-tuning as described in the paper."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11396,
      "completion_tokens": 565,
      "total_tokens": 11961
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 and Abstract",
          "reasoning": "The HEBDB dataset is composed solely of Hebrew language speech recordings; there is no indication that it includes more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 and Abstract",
          "reasoning": "The dataset entries are described only as Hebrew speech with no mention of any second human language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3 and Abstract",
          "reasoning": "The dataset focuses on Hebrew speech processing; English content is not described as part of the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 3; Section 3.1",
          "reasoning": "The HEBDB dataset contains only Hebrew language spoken data as per multiple mentions in the abstract and dataset description (Section 3). The text transcriptions are in Hebrew, and the speech recordings are Hebrew, confirming the dataset is monolingual in Hebrew, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains natural speech audio and accompanying transcriptions, without any indication of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or indication of mathematical or logical notation or symbolic representations in the dataset contents."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is focused on human speech in Hebrew; no non-human or biological communication data is described."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes Hebrew only and does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is explicitly stated as Hebrew and well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains spoken and transcribed Hebrew language, thus it includes language."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8614,
      "completion_tokens": 195,
      "total_tokens": 8809
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3 (Pre-processing)",
          "reasoning": "The abstract states that dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/. The paper describes in detail the preprocessing pipeline and mentions usage of open-source packages and models. This indicates that code related to preprocessing and dataset creation is available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (HEBDB dataset) and Subsections 3.1 (Pre-processing) and 3.2 (Data filtering)",
          "reasoning": "The paper provides detailed documentation on dataset collection, composition, preprocessing steps including resampling, segmentation using VAD, transcription methods, and data filtering using forced alignment with confidence thresholds. Statistical summaries of raw and preprocessed data and rationale for preprocessing decisions are documented, demonstrating transparency and completeness in dataset creation."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9137,
      "completion_tokens": 721,
      "total_tokens": 9858
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, Tables 1 and description in 'Aivaliot' subsection",
          "Reasoning": "The Aivaliot corpus is a newly introduced dataset compiled by the authors from oral narratives elicited from 18 elderly native speakers in 2002-2003. The audio recordings are human speech captured by researchers, explicitly stated as part of the Asia Minor Archive compiled within research projects. This confirms audio data modality and human-generated origin."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, Tables 1 and 'Aivaliot' subsection",
          "Reasoning": "The Aivaliot corpus was transcribed and annotated manually by two native speakers of the dialect, with detailed orthographic transcription. This indicates the presence of text data modality created by human annotators, confirming human-generated origin."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Cretan' subsection and Table 1",
          "Reasoning": "The Cretan corpus was compiled from 32 tapes of radio broadcasts recorded and aired between 1998-2001. The broadcasts contain radio narratives by a named speaker (Ioannis Anagnostakis). These are human speech audio recordings collected with permission from the library. Thus, this is audio data of human-generated origin."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Cretan' subsection",
          "Reasoning": "Initial transcripts for the Cretan dataset were produced using the Whisper Large-v2 model but were then manually corrected in collaboration with a native speaker/transcriber. The finalized transcriptions therefore include human-generated text data, despite initial model-generated drafts."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Griko' subsection and Table 1",
          "Reasoning": "The Griko corpus was collected during a field trip by linguists in 2013. The linguistic content is audio recordings of nine native speakers. These are human speech audio recordings recorded digitally in the field, hence audio modality and human-generated nature."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Griko' subsection",
          "Reasoning": "The Griko audio files were manually segmented, transcribed, glossed in Italian and annotated by trained linguists. This makes the text transcripts human-generated annotations accompanying the audio data."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Messenian' subsection and Table 1",
          "Reasoning": "The Messenian corpus was collected by new interviews conducted in 2023-2024 with residents from Kalamata and nearby villages resulting in collected narratives audio of six speakers. This new data is human-generated speech audio recordings."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Messenian' subsection",
          "Reasoning": "Initial transcripts of Messenian audio were produced using the Whisper Large-v3 model but these transcripts were then manually corrected by a native speaker of the dialect. This yields human-generated transcription text for the dataset."
        }
      ]
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9989,
      "completion_tokens": 286,
      "total_tokens": 10275
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3: Resources (Aivaliot, Cretan, Griko, Messenian)",
            "reasoning": "Transcriptions and annotations were performed by multiple individuals: for Aivaliot, two native speakers transcribed and annotated; Cretan transcripts were manually corrected in collaboration with experts; Griko corpus was transcribed and annotated by trained linguists; Messenian transcripts were manually corrected by a native speaker. This indicates multiple human experts (linguists and native speakers) were involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3: Resources and Data Segmentation",
            "reasoning": "The paper provides no explicit mention or detail of detailed annotation instructions provided to annotators for the transcriptions. Transcription methods and tools are mentioned but no description of instructions or guidelines given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3: Resources and Data Segmentation",
            "reasoning": "No mention or description of scoring rubrics or formal rubric-based evaluation criteria for annotations or transcriptions is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3: Resources and Data Segmentation",
            "reasoning": "The paper does not describe or provide actual annotation or transcription examples as part of annotation guidelines or documentation within the experimental or resource description sections."
          }
        }
      ]
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11119,
      "completion_tokens": 526,
      "total_tokens": 11645
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3 (Resources), paragraphs describing each dataset's transcription and correction",
          "reasoning": "The transcriptions and annotations for the datasets were performed or manually corrected by native speakers or trained linguists, who are either subject matter experts or members of the dialect-speaking community. For example, the Aivaliot corpus was transcribed and annotated by two native speakers of the dialect (Section 3, Aivaliot). Similarly, the Griko corpus was transcribed and annotated by trained linguists during fieldwork. Also, the Cretan and Messenian datasets were manually corrected by native speakers of the dialects after initial transcription. This indicates quality assurance was performed by single human experts or subject-matter experts."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human experts performed quality assurance collectively or that multiple experts cross-validated the annotations. Individual mentions of two native speakers annotating Aivaliot data do not explicitly indicate a multi-expert QA process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single non-expert annotator; all annotators are described as native speakers or trained linguists, implying subject matter expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA process does not report involvement of multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models (Whisper Large models) were used for initial transcriptions in some cases, the manual correction by native speakers or linguists indicates that automated outputs were not the final QA step. No AI-only QA process is described."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based QA processes are documented for annotation quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides explicit descriptions of manual transcription and correction by native speakers or trained linguists, indicating that quality assurance processes are documented and applied."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10737,
      "completion_tokens": 605,
      "total_tokens": 11342
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Resources), especially 3.1 Aivaliot, 3.2 Cretan, 3.3 Griko, 3.4 Messenian",
          "reasoning": "The paper describes that the Aivaliot corpus was compiled from narratives elicited from elderly speakers, collected with consent and transcribed by native speakers. The Cretan corpus was gathered from radio broadcasts and manually corrected transcripts from native speakers. The Griko corpus was collected during field trips by linguists who manually segmented and transcribed the audio. The Messenian corpus was recently collected via interviews, with transcription manually corrected by a native speaker. These processes indicate original data creation through human recording, transcription, and annotation, not derived or adapted from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists in the paper that any of the datasets were generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets were not produced by translating content from another language by human translators; they are original speech recordings and transcriptions of dialect speakers."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any datasets were created by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 (Resources), especially 3.1 Aivaliot and 3.2 Cretan",
          "reasoning": "Some datasets, like the Aivaliot corpus, were compiled from existing narratives collected during previous projects and stored in archives, thus representing collected or aggregated data from existing sources without significant modification. Similarly, the Cretan corpus was gathered from existing radio broadcasts collected by an audiovisual department, indicating collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (Benchmark Description), Data Segmentation and Dataset Creation",
          "reasoning": "For Cretan and Messenian corpora, initial transcriptions were generated using the Whisper automatic speech recognition models and then manually corrected by native speakers. This process involves modification and adaptation of model-generated transcriptions rather than purely original human transcription. Also, audio recordings were segmented and normalized, indicating transformations applied to the original audio and transcripts to prepare the datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear descriptions and provenance of the data sources and generation methods; thus, data origin is documented."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11255,
      "completion_tokens": 480,
      "total_tokens": 11735
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not used for pre-training models; the paper describes fine-tuning and evaluation but no pre-training on these datasets."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report training models from random initialization on these datasets; only fine-tuning pre-trained models is discussed."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Benchmark Description) and Section 5 (Results)",
          "reasoning": "The paper fine-tunes pre-trained ASR models (XLS-R-greek and Whisper-medium) on the new dialect datasets with supervised learning to adapt to the specific dialects, as explicitly described in Sections 4 and 5."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training or RLHF techniques being applied using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Results)",
          "reasoning": "All four new dialect datasets are used for benchmarking and performance measurement of ASR models, including zero-shot inference and fine-tuned evaluation, as shown in Section 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6 (Discussion)",
          "reasoning": "The datasets are used to analyze the challenges posed by dialectal variation on ASR performance and to discuss linguistic characteristics affecting results, as detailed in the discussion section."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used to augment models via retrieval or as an external knowledge base."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clear usage of the datasets for fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11978,
      "completion_tokens": 596,
      "total_tokens": 12574
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset contains dialects of Greek only; each dataset corresponds to a single non-English dialect. Although the paper mentions influences from other languages (e.g., Turkish and Italian), the dataset entries themselves are not mixed or annotated with multiple languages concurrently within entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Each dataset entry is focused on one dialect (e.g., Aivaliot, Cretan, Griko, or Messenian). There is no indication that the data contains exactly two human languages per entry."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain Greek dialects and no English-only content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3 Resources (especially 3.1 Aivaliot, 3.2 Cretan, 3.3 Griko, 3.4 Messenian)",
          "reasoning": "The datasets are transcriptions and audio recordings of four varieties of Greek dialects: Aivaliot, Cretan, Griko, and Messenian. Each dataset consists entirely of one dialect, all non-English, mostly using Greek alphabet for transcription (except Griko uses Latin alphabet). Therefore, each dataset is monolingual in a single non-English language variety."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code-related content is present in the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain speech and transcription data only, no mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of natural Greek dialects, not constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the datasets are explicitly documented and specified as Greek dialects."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Since the datasets contain transcribed speech in Greek dialects, they all contain human language."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9196,
      "completion_tokens": 183,
      "total_tokens": 9379
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 (Resources) and Section 4 (Benchmark Description)",
          "reasoning": "The paper describes the datasets' origins, collection methods, and preprocessing steps in detail, but it does not mention the availability of any associated code repository or provide links to code used for data collection, preprocessing, segmentation, or dataset creation. No URLs or references to public code are included."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Resources), Section 4 (Benchmark Description)",
          "reasoning": "The paper provides comprehensive descriptions of each new dataset's origin, collection procedures, transcription methods, ethical considerations (e.g., speaker consent, ethics committee approval), metadata annotations, and data segmentation techniques. It documents the process of converting audio to text segments and creating training/dev/test splits, which supports reproducibility of dataset construction."
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6545,
      "completion_tokens": 232,
      "total_tokens": 6777
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3; Section 3.1; Section 3.2",
          "Reasoning": "The Fair-Speech dataset consists of approximately 26.5K utterances in recorded speech by 593 people in the United States who self-recorded and submitted audio commands. The speech data was collected from paid participants speaking prompted commands related to voice assistant use cases. This data is explicitly recorded from humans and is thus human-generated audio."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The dataset includes verbatim transcriptions of the utterances, produced through multi-pass human transcription and curation by external vendors and internal linguistic experts. These transcriptions are human-generated textual data corresponding to the audio utterances."
        }
      ]
    }
  },
  {
    "id": "veliche24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7397,
      "completion_tokens": 230,
      "total_tokens": 7627
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 specifies that transcription was produced by multi-pass human annotation performed by external vendors reaching three-way agreement, with unresolved cases adjudicated by internal linguistic engineers, indicating multiple human experts involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The transcription process included detailed guidelines such as verbatim transcription, keeping colloquial and repeated words as spoken, spelling out numbers, capitalizing entities, and lowercasing other text without punctuation, indicating clear transcription instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No mention of formal scoring rubrics or guidelines for annotation quality scoring beyond inter-annotator agreement and auditing; the scoring is a measure of annotation quality rather than a rubric for annotation itself."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "The paper does not describe providing specific annotation examples as part of the guidelines for human annotators."
          }
        }
      ]
    }
  },
  {
    "id": "veliche24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8527,
      "completion_tokens": 328,
      "total_tokens": 8855
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single human annotator who is an expert or a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Data transcription",
          "reasoning": "The multi-pass human transcription was performed by external vendors until a three-way agreement was obtained. The 10% without agreement was resolved by internal linguistic engineers, and the audited data was reviewed by multiple internal teams including linguistic experts, indicating the involvement of multiple human experts in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that quality assurance was conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the annotators as external vendors and internal linguistic engineers, implying expertise rather than non-expert status. Thus, multiple non-expert human annotators do not apply."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of AI models as judges for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification techniques or rule-based quality assurance processes used for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a thorough multi-pass human transcription and multiple expert reviews, indicating that quality assurance was performed and documented."
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8145,
      "completion_tokens": 416,
      "total_tokens": 8561
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 3 Corpus contents",
          "reasoning": "The paper clearly states that the Fair-Speech dataset consists of audio recordings of approximately 26.5K utterances from 593 people in the United States, who were paid to record and submit audio commands. These are original audio recordings produced by human speakers responding to prompts, not derived from existing datasets. Participants self-identified their demographics and the data collection process involved participants producing their own content, not reading given text prompts, to ensure natural speech patterns."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human recorded audio; no mention is made of data generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translated data or materials translated by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation systems for dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Fair-Speech dataset is not aggregated from existing datasets but rather collected through new recordings made by participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications or adaptations; the data is original speech recordings from participants."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of data generation is explicitly described; thus, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8663,
      "completion_tokens": 285,
      "total_tokens": 8948
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Corpus contents; Section 4 Speech recognition experiments; Section 5 Results",
          "reasoning": "The paper explicitly states that the Fair-Speech dataset is intended to be used as an evaluation tool to uncover gaps or biases in ASR models. Multiple models are evaluated on this dataset in Section 4 and results demonstrating performance differences across demographic groups are analyzed in Section 5, confirming its role in benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Results",
          "reasoning": "Beyond evaluation, the dataset is used primarily for analyzing trends and patterns in ASR performance across demographic sub-groups, as detailed in Section 5, where the authors conduct statistical and model-based analyses including mixed-effects Poisson regression to understand fairness and disparity factors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9386,
      "completion_tokens": 508,
      "total_tokens": 9894
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3 Corpus contents, Linguistic variation and Table 1",
          "reasoning": "The dataset includes utterances from speakers whose native languages include English, Spanish, Mandarin, and other languages in smaller percentages. This indicates that the dataset contains entries in more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two languages, not exactly two, so it is not classified as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the majority of the data is from native English speakers, the dataset also includes other languages, thus it is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Since the dataset contains English plus other languages, it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains transcribed spoken voice commands, with no mention of mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human speech data and demographics, with no biological sequences or non-human communication present."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages being part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Language information is clearly documented in the dataset, including native languages of speakers."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly includes human language speech data."
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6604,
      "completion_tokens": 121,
      "total_tokens": 6725
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention any publicly available code repository or links related to data collection, preprocessing, or generation for the Fair-Speech dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 and subsections",
          "reasoning": "The paper provides detailed documentation about the dataset creation, including participant demographics, data collection methods with paid participants consenting to use of their data, recording conditions, transcription process, and demographic distributions. This documentation appears comprehensive and supports transparency and reproducibility in dataset creation."
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8017,
      "completion_tokens": 510,
      "total_tokens": 8527
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The NOTSOFAR recorded meeting dataset consists of 280 unique English meetings recorded in 30 different rooms with 4-8 attendees each. The recordings were captured using multiple devices with both single-channel and multi-channel microphone setups. These are real acoustic recordings involving human speakers in natural meeting scenarios, thus the audio data originates from human involvement and human recording tools."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The meeting dataset includes high-quality, multi-judge human transcriptions of the recorded meetings. The transcription process was conducted without machine aid and involved multiple professional human transcribers to ensure accuracy and minimize bias. Hence, the textual transcriptions are human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The NOTSOFAR simulated dataset is a 1000-hour training dataset synthesized using 15,000 real acoustic transfer functions convolved with clean speech from an external corpus. It is generated through simulation methods involving mixing up to three speakers, applying room impulse responses, and adding noise, i.e., it is model generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The simulated dataset uses clean speech drawn from the Librivox corpus, filtered for quality through an algorithmic Mean Opinion Score estimation. This clean speech originates from a human-generated corpus but is filtered and augmented by automated processing steps for simulation, which is part of the model-generated pipeline. However, since the final data is audio, and no textual content is directly presented or annotated as part of the simulated dataset, textual data is not directly introduced here. Therefore, no new text data is explicitly introduced as part of the simulated dataset besides that implied from the audio."
        }
      ]
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8869,
      "completion_tokens": 219,
      "total_tokens": 9088
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that transcription was performed independently by two professional transcribers, with a third judge resolving disagreements, indicating multiple human experts performed annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The use of a multi-judge annotation process with professional transcribers implies detailed annotation instructions were provided to ensure transcription accuracy and consistency, as described in the bias-free transcription and multi-judge annotation paragraphs."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any specific scoring rubrics or criteria provided to annotators for the transcription or segmentation tasks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no indication in the text that annotation guideline examples were provided to the annotators; the description focuses on the process and multiple judges rather than examples."
          }
        }
      ]
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9999,
      "completion_tokens": 329,
      "total_tokens": 10328
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 (NOTSOFAR Recorded Meeting Dataset)",
          "reasoning": "The quality assurance for the recorded meeting dataset includes transcription carried out independently by two professional transcribers with a third judge resolving non-consensus cases. Professional transcribers are considered experts in transcription, and the use of multiple transcribers ensures expert review."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert annotator. The transcribers are described as professional, which implies expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that non-expert annotators participated in quality assurance for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description that an AI model was used as a judge for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process or algorithmic rule-based quality assurance is described for the dataset annotation quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents and describes a multi-judge annotation process involving multiple human experts for quality assurance; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9617,
      "completion_tokens": 462,
      "total_tokens": 10079
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The NOTSOFAR Recorded Meeting Dataset consists of 280 unique meetings involving real human participants speaking English in various conference rooms, with multi-judge human annotation processes for transcription and segmentation performed without machine bias. The data is original and created entirely from scratch by human contributors, not derived or translated from pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced do not contain content generated entirely by AI or machine learning models; the simulated data uses recorded real acoustic transfer functions convolved with human speech but does not generate new speech content autonomously."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset recordings and transcripts are English and were generated natively; no indication is given that any content was created by translating from another language via humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the datasets was produced by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The recorded meeting dataset and simulated dataset are not merely aggregated from existing sources; they were collected and created with specific processes for this challenge."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The simulated training dataset is derived from existing clean speech recordings (Librivox corpus) selected by MOS filtering and real measured acoustic transfer functions (ATFs), which are convolved and processed to generate mixtures simulating realistic environments. Thus, it is adapted and transformed from pre-existing speech and measured data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods for both datasets are well-documented."
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10135,
      "completion_tokens": 352,
      "total_tokens": 10487
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The NOTSOFAR simulated training dataset, consisting of approximately 1000 hours of simulated multi-channel audio, is designed for training data-driven speech separation and enhancement methods from scratch with clean supervision signals."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The recorded meeting dataset of 280 distinct meetings with high-quality transcriptions and speaker labels is used to fine-tune or supervise training of DASR models, providing supervised learning data for speech recognition and diarization."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the datasets for reinforcement learning based post-training methods like RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 2",
          "reasoning": "The recorded meeting dataset is explicitly described as a benchmark dataset for evaluation and benchmarking of DASR systems, enabling reliable performance measurement with a large number of distinct meetings and associated metrics like tcpWER."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While metadata enables error analysis, the dataset is primarily intended for training and evaluation, and explicit use solely for analysis is not described."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as serving as a knowledge base for retrieval-augmented generation or related purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10858,
      "completion_tokens": 544,
      "total_tokens": 11402
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states that the recordings are exclusively in English (Section 3.1), with no mention of other languages; hence, the dataset is not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English as specified (Section 3.1), with no indication of a second human language; hence, it is not bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1, 'Oriented toward far-field' paragraph",
          "reasoning": "The recorded meeting dataset is explicitly stated to contain English language meetings spoken by native or near-native speakers, with no other languages recorded or mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses English exclusively; no non-English monolingual data is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes entries with programming or structured code-related content; the paper focuses on speech meeting data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes some mathematical notation to describe methods, the dataset itself does not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech in meeting scenarios only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages; only natural human English speech is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the dataset are explicitly stated and documented as English; no ambiguity or unknown language presence."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech data in English, so it does contain language."
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8076,
      "completion_tokens": 195,
      "total_tokens": 8271
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 1 and Section 4",
          "reasoning": "The paper mentions that an open-source baseline system including inference, training, data handling, and evaluation code is provided (Section 1), but there is no explicit mention or link to the release of the code used to construct or collect the datasets themselves, such as data collection scripts or preprocessing used to create the recorded and simulated datasets."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (3.1 and 3.2)",
          "reasoning": "The paper provides extensive documentation on the dataset creation processes for both the recorded meeting dataset and the simulated training dataset in Section 3. This includes detailed descriptions of recording conditions, participant details, acoustic complexities, annotation procedures, metadata, simulation methodologies including the real-room acoustic transfer functions, clean speech selection, and mixing process, all contributing to a thorough and transparent dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7505,
      "completion_tokens": 123,
      "total_tokens": 7628
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 and 3 (Experiment design and Data processing and analysis)",
          "Reasoning": "The new dataset is a corpus of radar data recorded using three different types of antennas positioned on speakers' cheeks while two German native speakers uttered 25 phonemes. The radar signals, representing transmission spectra as complex vectors sampled at 100 Hz, constitute sensor data captured via human subjects performing tasks. The dataset was recorded specifically for this study to compare antenna types and positions."
        }
      ]
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8357,
      "completion_tokens": 212,
      "total_tokens": 8569
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 (Experiment design) and Section 3 (Data processing and analysis)",
            "reasoning": "The annotation (phoneme segmentation) was done by a human using Praat software, specifically guided by audio recorded simultaneously. Given the phoneme-level detailed corpus and usage of Praat for segmentation, this implies expert knowledge in phonetic annotation to produce accurate phoneme boundaries."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not describe providing specific annotation instructions or guidelines for the segmentation or labeling process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No mention or description of scoring rubrics or formal evaluation rubrics for the annotation process is given in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not include or reference example annotation instances or exemplar guides for the human annotator."
          }
        }
      ]
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9487,
      "completion_tokens": 300,
      "total_tokens": 9787
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance by multiple human non-expert annotators is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for classification, the paper does not indicate that AI models perform quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 Data processing and analysis",
          "reasoning": "The segmentation of recorded data into target phonemes was performed using the Praat software, an automated tool. The identification of phoneme midpoints for feature extraction was conducted automatically, relying on audio segmentation. This constitutes an automated verification process applied to the data annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is evidence of an automatic segmentation and verification process; therefore, it is not accurate to say that no quality assurance or documentation exists."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9105,
      "completion_tokens": 395,
      "total_tokens": 9500
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Experiment design",
          "reasoning": "The dataset consists of recordings of phoneme utterances produced by two native German speakers in two sessions for each configuration, using different antenna types and positions. The corpus is described as being the same as in prior work [14], but the actual radar data recordings were newly acquired by human speakers performing the utterances in the experiment. Thus, the data is original human-produced speech-related radar signals recorded from scratch by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models without reference to raw experimental data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not produced by translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation for data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not aggregated from existing sources; it was newly recorded for this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though processed with feature extraction and segmentation, the core data is original radar recordings, not based on existing datasets with modification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is clearly documented: human speakers produced the utterances recorded through radar in the described experimental setup."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9623,
      "completion_tokens": 440,
      "total_tokens": 10063
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 (Data processing and analysis) and Section 4 (Results and discussion)",
          "reasoning": "The newly recorded dataset consisting of 8280 tokens across nine antenna setups, speakers, and sessions was used exclusively for phoneme recognition classification tasks to evaluate the performance of different antenna types and positions. The dataset serves as the benchmark to measure recognition rates and assess the effectiveness of the radar-based SSI under various configurations. The paper uses nested cross-validation and statistical tests to analyze recognition rates, confirming that this dataset is used for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results and discussion)",
          "reasoning": "Beyond evaluation, the dataset is used to analyze patterns and trends in recognition performance related to antenna type, antenna position, speaker differences, and feature sets. Linear mixed effect models and likelihood ratio tests are applied to the recognition rates derived from this dataset to study the significance of these variables, indicating its role in analyzing characteristics of the radar-based SSI system."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the newly recorded dataset for evaluation and analysis of the radar-based silent speech interfaces."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10346,
      "completion_tokens": 540,
      "total_tokens": 10886
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists solely of phoneme utterances by native German speakers; there is no indication of multiple languages used."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only German language phonemes, not two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The corpus and recorded phonemes are from native German speakers; the dataset is not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 (Experiment design), Abstract",
          "reasoning": "The proposed dataset is a corpus of 25 German phonemes uttered by native German speakers; the paper explicitly mentions use of German phonemes and native German speakers, indicating a monolingual dataset in the non-English German language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or code-related content is included as dataset entries; the dataset consists of radar recordings representing phoneme utterances."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper includes some mathematical models and analyses, the dataset itself contains no mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech-related radar data, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves natural language phonemes (German), not constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset's language is clearly specified as German phonemes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries of spoken human language (German phonemes) represented via radar signals, so language is present."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7564,
      "completion_tokens": 173,
      "total_tokens": 7737
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any links, URLs, or references to publicly available code or repositories related to the dataset creation or preprocessing steps. No indication that the code used for data collection, segmentation, feature extraction, or classification is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 (Experiment design) and 3 (Data processing and analysis)",
          "reasoning": "The paper provides detailed information about the dataset creation process. Section 2 describes the antenna types, positions, and the speech corpus recorded, including the number of subjects, sessions, and total tokens recorded. Section 3 describes how data was processed, segmented, and features extracted. This constitutes thorough documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5828,
      "completion_tokens": 193,
      "total_tokens": 6021
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Data",
          "Reasoning": "The paper explicitly states that the experiments use the Switchboard Telephone Corpus and the DailyDialog corpus as data sources. However, these are existing datasets, not introduced by the authors in this paper. Therefore, they do not qualify as new datasets introduced by the authors."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Language model surprisal",
          "Reasoning": "The authors trained a TurnGPT language model from scratch on equal amounts of DailyDialog and Switchboard data to obtain surprisal estimates. This trained model and its output surprisals are model-generated data, but these surprisal estimations are not described as new datasets. Therefore, no new dataset of this modality introduced by the authors is reported."
        }
      ]
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6680,
      "completion_tokens": 243,
      "total_tokens": 6923
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.4",
            "reasoning": "Section 3.4 describes recruiting 52 native English speakers from Prolific Academic to perform the acceptability rating task. These participants were not indicated to be experts, and represent multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 explains the acceptability rating task instructions where participants rate plausibility of upcoming turns on a 1-5 scale, indicating the presence of detailed instructions guiding their annotations."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The rating scale from 1 to 5 with defined labels (\"Very Unlikely\" to \"Very Likely\") serves as a scoring rubric for participant judgements as described in Section 3.1."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention provision of annotation examples or example stimuli to the participants in any section, so there is no evidence of examples in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7810,
      "completion_tokens": 335,
      "total_tokens": 8145
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about QA conducted by multiple human experts for the dataset annotations or validations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No single human non-expert is mentioned as performing quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.4 Behavioural study and Participants",
          "reasoning": "The human judgement task involved 52 native English speakers recruited from Prolific Academic, who performed acceptability rating tasks on dialogue turns. These participants are members of the target demographic (native speakers) but are not described as experts. Multiple non-expert human annotators made plausibility judgements as part of the quality assurance of the dataset stimuli."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper employs a language model (TurnGPT) to estimate surprisal, it does not use an AI model for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification processes for quality assurance of annotations; any computational procedures described relate to analysis rather than QA of dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance was applied via responses from multiple human non-expert participants, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7428,
      "completion_tokens": 463,
      "total_tokens": 7891
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.4",
          "reasoning": "The paper describes a novel human judgement task where participants rated the acceptability of dialogue turns. The stimuli for this task were generated by sampling context-response pairs from existing dialogue corpora, and human participants provided new, original ratings for these stimuli during the study, thereby creating new data originating from human contributions."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data are described as being generated entirely by AI or machine learning models. The language models were used to estimate surprisal on existing dialogue data, but did not generate new dialogue data or new experimental stimuli."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that data were generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The dialogue data used for generating stimuli were derived from existing corpora: the Switchboard Telephone Corpus and the DailyDialog corpus. These datasets were compiled and curated previously and reused here without significant modification, qualifying as collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4",
          "reasoning": "Stimuli for the human judgement task were created by deriving context-response pairs from existing dialogues, including the generation of negative samples by sampling turns with a range of conditional surprisals. This constitutes adaptation and transformation of existing data to create experimental stimuli."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The methods and origins of the datasets used and created are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7946,
      "completion_tokens": 304,
      "total_tokens": 8250
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The paper describes training the TurnGPT language model from scratch on combined data from the DailyDialog and Switchboard corpora, using this new training dataset to obtain surprisal estimates for investigating dialogue acceptability judgments."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.4 and Section 4.1",
          "reasoning": "The stimuli generated from the new sampling of dialogue turns in the DailyDialog and Switchboard corpora were used in a human behavioral judgment task to evaluate acceptability and to analyze the correlation between surprisal and human perception, thereby serving an evaluation role."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and Section 5",
          "reasoning": "The paper uses the collected human acceptability judgments and surprisal estimates from the newly processed dataset to analyze relationships between different surprisal operationalizations and human perception, as well as differences between spoken and written dialogues."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8669,
      "completion_tokens": 413,
      "total_tokens": 9082
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 Data",
          "reasoning": "The datasets used are the Switchboard Telephone Corpus and the DailyDialog corpus, both of which contain English dialogue data only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 Data",
          "reasoning": "The datasets do not contain exactly two human languages; both are monolingual English corpora."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 Data",
          "reasoning": "The Switchboard Telephone Corpus (spoken dialogue) and DailyDialog corpus (written dialogue) are both described as English language datasets representing spoken and written English dialogues respectively."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.2 Data",
          "reasoning": "No indication is given that any non-English language data is included in the datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "There is no mention or indication that the datasets include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "Although the paper discusses mathematical formulas and metrics, the datasets themselves do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The datasets contain human language dialogue, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "There is no mention or evidence that constructed or fictional languages are included in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2 Data",
          "reasoning": "The language of the datasets is explicitly stated as English and documented; no unspecified languages are present."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are composed of natural language dialogues and thus do contain language."
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5887,
      "completion_tokens": 228,
      "total_tokens": 6115
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not provide any explicit reference, link, or mention of code repositories related to the dataset creation, data preprocessing, or stimulus generation used for their novel acceptability judgement task or TurnGPT training. Therefore, it appears that code is not publicly shared or described for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 (Data), Section 3.4 (Behavioural study and Participants) and Section 3.3 (Language model surprisal)",
          "reasoning": "The paper documents the dataset sources clearly, describing the use of pre-existing datasets (Switchboard, DailyDialog), and the process of stimulus generation for the human judgement task (e.g., sampling contexts by cumulative surprisal, selection of true and negative turns). The preprocessing steps (removal of punctuation except turn-segmentation, vocabulary choices) and details of the TurnGPT model training are described. These descriptions provide transparency about dataset creation and experimental method, supporting reproducibility, even though no code is shared."
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7653,
      "completion_tokens": 201,
      "total_tokens": 7854
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2 (paragraphs 1-3) and Figure 1",
          "Reasoning": "The novel dataset SONYC-FSD-SED introduced in this paper is composed of 10-second audio soundscapes simulating environmental sound events. Background audio clips are human-generated data recorded by the Sound of New York City (SONYC) acoustic sensor network (real acoustic sensor recordings). Foreground sound events are sampled from the FSD50K dataset, which contains human-labeled sound events. These foreground sounds are mixed with the background clips to create simulated soundscapes using Scaper, a soundscape synthesis library. Thus, the dataset contains audio data that originate both from human-generated recordings and human-labeled examples, but the final dataset is partially model-generated (simulated via algorithmic sound mixing and manipulation) to create the long-term temporal environmental soundscape scenarios."
        }
      ]
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8505,
      "completion_tokens": 227,
      "total_tokens": 8732
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2",
            "reasoning": "The SONYC-FSD-SED dataset is synthesized by simulating occurrences of sound events and mixing background recordings with foreground sound events using automated tools such as Scaper. There is no explicit mention of human annotators labeling data; instead, the dataset is generated automatically using pre-labeled clips and probabilistic occurrence models."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention any annotation instructions provided to annotators since the dataset is created through simulation and automatic mixing, not human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No scoring rubrics or detailed guideline criteria for annotation are mentioned, as the labels come from existing datasets and are used in an automated mixing process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No annotation examples are provided or required, since the dataset construction is via automated processes combining existing labeled audio clips and background sounds."
          }
        }
      ]
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9635,
      "completion_tokens": 394,
      "total_tokens": 10029
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of QA being conducted by multiple human experts or annotators."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of QA involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset is used with AI models and active learning, the QA of dataset annotations is not performed by AI models acting as judges."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset description)",
          "reasoning": "The dataset SONYC-FSD-SED is constructed by algorithmic and rule-based simulation of soundscape data, including sampling background clips with lowest sound pressure level, running a pre-trained urban sound event classifier to filter clips, and simulating occurrence probabilities with von Mises distributions, which are formula based. The dataset annotation is generated according to these automated procedures without mention of human validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper does describe automatic processes used to generate and verify the dataset content."
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9253,
      "completion_tokens": 445,
      "total_tokens": 9698
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by human contributors. Instead, it uses existing recordings from the SONYC acoustic sensor network and the FSD50K dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated purely by AI or machine learning models without reference to existing data. The dataset involves soundscape synthesis by mixing existing audio clips, but not fully AI-generated data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no use of machine translation in data generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2, Dataset description",
          "reasoning": "The dataset uses existing audio recordings from the SONYC sensor network and the FSD50K dataset, which are collected from real-world environments. These data are aggregated and used as background and foreground sounds respectively."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2, Dataset description",
          "reasoning": "The authors simulate sound event temporal characteristics by modeling occurrence probabilities with von Mises distributions and sample sound events accordingly. They generate new soundscapes by mixing background audio clips from SONYC with foreground audio from FSD50K clips using pitch-shifting, time-stretching, and noise mixing (SNR adjustments). This results in a dataset derived from existing sources with transformations and adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is explicitly described and documented in Section 2."
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9771,
      "completion_tokens": 345,
      "total_tokens": 10116
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1, 4.1",
          "reasoning": "The SONYC-FSD-SED dataset is used to train the few-shot prototypical network models via episodic training (Section 3.1) for few-shot sound event detection. Experiments (Section 4.1) demonstrate training and evaluation of models on this dataset, indicating supervised fine-tuning use."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2, 4.3, 4.4",
          "reasoning": "The dataset is used extensively for evaluation and benchmarking of proposed few-shot learning and active learning methods in Sections 4.1 through 4.4, where performance metrics are reported and sampling strategies compared."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2, 4.4, 4.5",
          "reasoning": "The dataset is used to analyze model behavior, support set diversity, temporal sampling windows effects, and generalization trends of few-shot learning combined with active learning (Sections 4.2, 4.4, 4.5)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10494,
      "completion_tokens": 624,
      "total_tokens": 11118
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset SONYC-FSD-SED is composed of environmental and urban soundscapes created by mixing background recordings from the SONYC sensor network and foreground sound events from the FSD50K dataset. These sound event recordings do not contain spoken language content or any indication that they are English-language speech. Therefore, the dataset entries are not English content but audio sound recordings of various environmental sounds."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2 (Dataset description)",
          "reasoning": "The dataset consists of environmental acoustic recordings created from background environmental sounds and foreground sound events from FSD50K. There is no indication these are natural human language speech recordings in any language, so not considered as monolingual (non-English) language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical formulas and describes data sampling processes, the dataset itself does not contain programming or structured code-related content as entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 2, paragraph describing the use of von Mises probability density function",
          "reasoning": "The paper describes sampling occurrence probabilities using mathematical formulas such as the von Mises probability density function with explicit equations. These mathematical representations are part of the dataset modeling and generation process."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains environmental audio recordings and sound events, but no biological sequences (e.g., DNA) or non-human communication systems such as animal calls explicitly represented symbolically or in sequence data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and its contents are clearly described; there is no ambiguity about the nature of language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The SONYC-FSD-SED dataset consists of 10-second environmental and urban soundscape audio clips, created by mixing background sounds and foreground events. There is no textual or human language content contained within the dataset entries."
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7712,
      "completion_tokens": 177,
      "total_tokens": 7889
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2, Dataset description",
          "reasoning": "The paper states that the dataset SONYC-FSD-SED will be freely available online, but it does not provide any direct link, nor does it mention publicly available code repositories for data generation or construction. There is detailed description of the dataset creation process, but no explicit mention of code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, Dataset description",
          "reasoning": "The paper provides a comprehensive and detailed description of how the SONYC-FSD-SED dataset was created, including the selection and filtering of background and foreground sound clips, the simulation of occurrence probabilities using von Mises distributions, the mixing process using the Scaper library, and the final dataset statistics. This documentation is sufficient for understanding how the dataset was constructed."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8786,
      "completion_tokens": 338,
      "total_tokens": 9124
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 (COSPRO-2mix), Section 2.1 (COSPRO)",
          "Reasoning": "COSPRO-2mix is synthesized by mixing segmented utterances from the COSPRO corpus, which is collected and annotated by the Institute of Linguistics, Academia Sinica. Thus, the original audio recordings are human-recorded speech, i.e., human generated audio data, and then mixed according to WSJ0-2mix procedures to form the new dataset COSPRO-2mix."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 (EAT & EAT-2mix)",
          "Reasoning": "The EAT-2mix test sets are created by mixing English utterances and code-mixing utterances recorded from college students in Taiwan using microphone recordings. This data is collected from human speakers and then mixed to form test sets; thus, the audio data is human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 (TAT), Section 2.5 (TAT-2mix)",
          "Reasoning": "TAT-2mix is created by mixing utterances from the Taiwanese Across Taiwan (TAT) corpus which includes recordings on six different channels using microphones and smartphones from 200 speakers. This dataset was specifically tailored and synthesized by the authors for their experiments, thus constituting new dataset creation with human-generated audio modality."
        }
      ]
    }
  },
  {
    "id": "wang22c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9638,
      "completion_tokens": 276,
      "total_tokens": 9914
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2, subsections 2.1 and 2.4",
            "reasoning": "The COSPRO and TAT datasets were designed, collected, and originally annotated by experts at Academia Sinica, as stated in Section 2.1 (COSPRO) and 2.4 (TAT). The annotation processes involve linguistic expertise, such as prosody annotation for COSPRO and speech recognition oriented data collection for TAT."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 and 2.5",
            "reasoning": "The paper describes segmentation procedures using forced alignment with the Kaldi toolkit and detailed protocols to segment and prepare utterances for mixing (e.g., word boundary cuts, selection of speakers with sufficient data), indicating the presence of detailed instructions for annotation and preparation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe any scoring rubrics or scoring criteria for annotation tasks related to the datasets; annotations are more related to segmentation and alignment rather than subjective evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No examples of annotation outputs or annotated samples are provided or described in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "wang22c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10768,
      "completion_tokens": 409,
      "total_tokens": 11177
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being conducted by a single human expert for any of the new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper regarding quality assurance performed by multiple human experts on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA conducted by a single non-expert human annotator for the newly introduced datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of QA by multiple non-expert human annotators on the datasets created by the authors."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that AI models were used as judges for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although algorithmic processes (e.g., forced alignment with Kaldi toolkit for segmentation) are mentioned for processing speech segments, this serves dataset preparation rather than explicit quality assurance or verification of annotations. No automated QA process is explicitly described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any explicit quality assurance process conducted to validate the annotations or content of the new datasets COSPRO-2mix and TAT-2mix. The datasets were created by segmentation and mixing procedures, but no QA protocol or evaluator is documented."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10386,
      "completion_tokens": 471,
      "total_tokens": 10857
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 2.4",
          "reasoning": "COSPRO and TAT are human-collected corpora designed and recorded entirely by human contributors; COSPRO was designed, collected, and annotated by the Institute of Linguistics, Academia Sinica (Section 2.1), and TAT is a speech corpus recorded by 200 speakers across Taiwan with multiple channels (Section 2.4). These corpora consist of original speech recordings not derived or adapted from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated purely by AI or model synthesis."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any datasets are produced through human translation from one language to another."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Datasets like COSPRO and TAT were recorded and collected explicitly for this research purpose and are not described as aggregates of existing datasets without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2, 2.3, 2.5",
          "reasoning": "COSPRO-2mix, EAT-2mix, and TAT-2mix datasets were derived by mixing utterances from the original corpora (COSPRO, EAT, and TAT) following the mixing procedure of WSJ0-2mix, producing new mixed datasets. These are modifications or transformations of existing raw recordings to create speech separation test sets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and generation methods of all discussed datasets are explicitly described."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10904,
      "completion_tokens": 411,
      "total_tokens": 11315
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The newly constructed datasets COSPRO-2mix and TAT-2mix (including several channel-specific variants) are used as training sets to train speech separation models from scratch, e.g., the DPT-Net, Conv-TasNet, and DPRNN. This is explicitly described in Section 4 where models are trained on these datasets and evaluated on various test sets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The authors use the newly created COSPRO-2mix and TAT-2mix datasets for evaluation purposes to measure the performance impact of language and channel variability on speech separation models. Multiple test sets derived from these datasets serve as benchmarks in their experiments, as shown in Tables 2 and 3."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The paper analyzes trends and characteristics of speech separation performance across different channels using TAT-2mix. They perform pairwise PCA to visualize and understand relationships between channels and their impact on model performance, which is an analytical use of the datasets rather than purely training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents how the newly introduced datasets COSPRO-2mix and TAT-2mix are utilized in training, evaluation, and analysis of speech separation models; therefore, no category applies that indicates lack of practical usage."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11627,
      "completion_tokens": 732,
      "total_tokens": 12359
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.2 (COSPRO-2mix), Section 2.3 (EAT & EAT-2mix), and Section 2.5 (TAT-2mix)",
          "reasoning": "The paper introduces COSPRO-2mix derived from COSPRO which contains Mandarin Chinese speech, EAT-2mix which includes English speech and English/Mandarin code-mixing utterances, and TAT-2mix which contains Taiwanese speech. The paper specifically mentions using these datasets to investigate language impact, including bilingual cases like English/Mandarin code-mixing. Hence, the proposed datasets contain exactly two human languages in entries where code-mixing occurs (English and Mandarin)."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.3 (EAT-2mix English test set), Section 4.1 (Impact of Language experiments)",
          "reasoning": "The EAT-2mix English test set consists of English-only utterances from microphone recordings as stated in Section 2.3. Additionally, WSJ0-2mix (not newly created but used as baseline) is English-only. The EAT-2mix English test set is used to evaluate the models, indicating monolingual English data is included."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (COSPRO), Section 2.5 (TAT-2mix)",
          "reasoning": "COSPRO contains Mandarin Chinese speech exclusively, and TAT contains Taiwanese (Southern Min) speech exclusively, making these newly created datasets monolingual non-English datasets. This is explicitly stated in the paper that COSPRO is Mandarin and TAT is Taiwanese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced contain natural spoken language audio data; no programming or structured code content is included within the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe metrics and models, the datasets themselves contain only speech audio data without mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets focus exclusively on human speech and do not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention in the paper that the datasets contain any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the proposed datasets are specifically identified and documented (Mandarin, Taiwanese, English), so language origin is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "All proposed datasets contain speech in human languages, so it is not applicable that they contain no language."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8845,
      "completion_tokens": 205,
      "total_tokens": 9050
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "End of Section 1 (Introduction) and Section 2 (Datasets for Speech Separation)",
          "reasoning": "The paper states in the Introduction that the details and source codes are open-sourced, indicating that code related to data synthesis is publicly available. This is confirmed in Section 2 where datasets such as COSPRO-2mix and TAT-2mix are described as synthesized, implying the use of shared scripts for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Datasets for Speech Separation), subsections 2.2 COSPRO-2mix and 2.5 TAT-2mix",
          "reasoning": "The paper provides detailed explanations on how new datasets were constructed, including selection of speakers, segmentation procedures using forced alignment, mixing procedures aligned with WSJ0-2mix, and description of channels for the TAT-2mix dataset. These constitute thorough documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7491,
      "completion_tokens": 117,
      "total_tokens": 7608
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Depression corpus",
          "Reasoning": "The paper introduces a new Chinese clinical depression corpus collected from 131 participants during Hamilton Rating Scale for Depression (HAMD) interviews. The data consists of recorded speech audio from face-to-face clinical interviews, recorded by a professional audiologist using a dual channel recording device, saved as WAV files at 16kHz. This is explicitly described in Section 2 as newly collected data for this study."
        }
      ]
    }
  },
  {
    "id": "wang22q_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8343,
      "completion_tokens": 242,
      "total_tokens": 8585
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2, 'Depression corpus'",
            "reasoning": "The corpus was collected from clinical HAMD interviews conducted by physicians (experts) at Beijing Huilongguan Hospital, implying that multiple human experts performed or oversaw the annotation related to depression diagnosis."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2, 'Depression corpus' and Table 1",
            "reasoning": "The annotation involved asking a set of frequently asked questions listed in Table 1 during HAMD interviews, indicating a structured set of instructions for interviewers to follow."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2, 'Depression corpus'",
            "reasoning": "HAMD score thresholds were used to define depression status (scores higher than eight indicate depressive symptoms), indicating scoring rubrics guiding annotation to classify depression severity."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2, 'Depression corpus' and Table 1",
            "reasoning": "Table 1 lists example questions used during the interviews, serving as examples of the annotation process and guidelines for interviewers."
          }
        }
      ]
    }
  },
  {
    "id": "wang22q_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9473,
      "completion_tokens": 191,
      "total_tokens": 9664
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2, Depression corpus",
          "reasoning": "The dataset was collected from clinical HAMD interviews conducted face-to-face by physicians (clinicians, i.e., subject matter experts) with participants. These clinical interviews and the corresponding HAMD ratings were performed by medical professionals, ensuring that the annotations (diagnoses and depression severity scores) are expert-validated."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9091,
      "completion_tokens": 411,
      "total_tokens": 9502
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Depression corpus",
          "reasoning": "The paper clearly states that the authors collected a new Chinese clinical depression corpus from real clinical practice at Beijing Huilongguan Hospital. The dataset consists of 131 participants' speech recorded during face-to-face Hamilton Rating Scale for Depression (HAMD) interviews conducted by physicians. This data was generated entirely by human contributors (patients and clinicians) in clinical settings and was not translated, adapted, or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes any data generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected natively in Mandarin Chinese; there is no mention of data being produced by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation being used to produce any part of the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being collected or aggregated from existing sources without significant modification; rather, it was newly collected by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is explicitly described, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9609,
      "completion_tokens": 482,
      "total_tokens": 10091
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training a model from randomly initialized parameters on the new dataset; instead, it uses the dataset for fine-tuning a TDNN-based model."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.5 and 4.2",
          "reasoning": "The proposed new Chinese clinical depression corpus is used to train a TDNN-based depression detection model. The paper describes extracting features (MFCC) from the corpus and training the neural network in a supervised manner, utilizing cross-entropy loss. This indicates the dataset is used for supervised fine-tuning or supervised training of the model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of reinforcement learning or RL-based post-training techniques using this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The dataset is also used for evaluating the performance of the proposed depression detection system by conducting five-fold cross-validation and reporting accuracy, precision, recall, and F1 scores both at utterance and speaker level."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The dataset is used for analyzing the verbal expressions differences between depressed and healthy subjects through transcription and statistical comparison of word frequency and part-of-speech categories."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates multiple practical uses of the dataset, including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10332,
      "completion_tokens": 350,
      "total_tokens": 10682
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 Depression corpus",
          "reasoning": "The dataset is a newly collected Chinese clinical depression corpus with speech recordings from participants who are fluent Mandarin speakers. The interviews were conducted in Mandarin, as stated in Section 2. Therefore, the dataset entries contain exactly one non-English language, Mandarin Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7550,
      "completion_tokens": 172,
      "total_tokens": 7722
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or statement in the paper provides a link or mentions availability of code for dataset construction or processing.",
          "reasoning": "The paper describes the dataset collection process and preprocessing steps in the sections '2. Depression corpus' and '3. Methods', but does not provide any URL, link, or mention of publicly available code repositories related to data collection or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Depression corpus) and Section 3 (Methods)",
          "reasoning": "The paper documents the dataset creation process including participant recruitment, recording conditions, data preprocessing steps such as voice activity detection, segmentation into 3-second clips, and feature extraction with MFCC. Detailed demographic information and interview procedures are described, providing sufficient transparency about dataset construction."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7609,
      "completion_tokens": 219,
      "total_tokens": 7828
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.3.1 Recording Preparation",
          "Reasoning": "The audio data was recorded directly from human speakers using the microphone array of the Azure Kinect device, capturing natural speech uttered by 64 native Mandarin speakers."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.3.1 Recording Preparation, Figure 4",
          "Reasoning": "The dataset includes color images (RGB) captured by the Azure Kinect during recording sessions of human speakers reading sentences, thus the images directly originate from human recording."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.3.1 Recording Preparation, Figure 4(b)",
          "Reasoning": "Depth images were captured by the Azure Kinect device while speakers read the text; these depth images represent sensor data produced by human performance during recording."
        }
      ]
    }
  },
  {
    "id": "wang23o_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8461,
      "completion_tokens": 253,
      "total_tokens": 8714
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.4",
            "reasoning": "Section 3.4 Data Annotation describes using the open-source dictionary generation model grapheme-to-phoneme (G2P) and the Montreal Forced Aligner (MFA), an automatic forced alignment tool, to generate phoneme-level annotations and alignments. This indicates that the annotation process was conducted through automatic tools rather than manual human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.4",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators, likely because the annotation was performed automatically with tools, so specific instruction guidelines for human annotators are not applicable."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.4",
            "reasoning": "No information about scoring rubrics or criteria for annotation quality is described, as annotation is automated, so rubric-based manual annotation quality control does not apply."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.4",
            "reasoning": "No annotation examples are provided in the paper; given the automatic annotation process, no illustrative examples for annotators are needed or mentioned."
          }
        }
      ]
    }
  },
  {
    "id": "wang23o_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9591,
      "completion_tokens": 310,
      "total_tokens": 9901
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information indicating that multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance being performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's quality assurance process does not include AI models acting as judges for annotation validation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1.1 and Section 3.4",
          "reasoning": "The text acquisition pipeline includes algorithmic scoring combining di-phone and tri-phone coverage, tone combinations, etc., to filter sentences (Section 3.1.1). Additionally, forced alignment using the Montreal Forced Aligner (MFA), an automatic tool, is used for phoneme-level annotation alignment (Section 3.4). These automated processes constitute a form of quality assurance through automated verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents automated pipeline verification and manual filtering steps, so quality assurance is present."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9209,
      "completion_tokens": 488,
      "total_tokens": 9697
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Text Acquisition Pipeline",
          "reasoning": "The dataset's raw text was collected from authentic Chinese sources such as social media, news, novels, and poetry, then manually filtered and cleaned by human contributors to select suitable reading materials. The audio-visual data was recorded by native Mandarin speakers reading these texts, thus the content and recordings are original and created by humans without adapting or deriving from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any portion of the dataset was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the data was produced by translating from another language into Mandarin by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or statement suggests any machine translation was used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Text Acquisition Pipeline",
          "reasoning": "The raw text material was collated from various existing Chinese media sources such as Weibo, People's Daily, novels, and poetry. While cleaned and filtered, this collection process aggregates existing texts without fundamental modification to the source content."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Text Acquisition Pipeline and Section 3.4 Data Annotation",
          "reasoning": "Although the raw texts are collated from existing sources, the dataset applies filtering, sentence segmentation, and scoring algorithms to select and balance the material. Furthermore, detailed phoneme-level annotations, including tone sequences and phonetic segmentation, are derived using language dictionaries and forced alignment tools, representing derived and adapted data based on the original texts."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are sufficiently described in the paper."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9727,
      "completion_tokens": 464,
      "total_tokens": 10191
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the MA VD dataset for pre-training large models in an unsupervised or self-supervised manner. Instead, it is used in supervised settings."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training any models from scratch using the MA VD dataset; the baseline experiments utilize pre-trained models for feature extraction."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Baseline Experiment)",
          "reasoning": "In Section 4, the authors fine-tune a pre-trained Mandarin-Wav2Vec2 model and ResNet-18 model using supervised learning on the MA VD dataset to evaluate audio-visual speech recognition performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning based post-training techniques such as RLHF are mentioned or used with the MA VD dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 (Result and Analysis)",
          "reasoning": "The MA VD dataset is used for evaluation in baseline experiments to measure character error rate (CER) to benchmark the effectiveness of different modalities on AVSR tasks."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the dataset for analyzing trends or patterns beyond evaluation metrics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models via retrieval or other mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used in supervised fine-tuning and evaluation as demonstrated in Section 4."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10450,
      "completion_tokens": 448,
      "total_tokens": 10898
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only Mandarin language materials and there is no indication of more than two human languages present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is explicitly described as Mandarin only, without presence of a second human language in its entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains Mandarin data, not English; thus it is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 3 Our Mandarin Audio-Visual Dataset with Depth Information; Table 1",
          "reasoning": "The dataset is clearly presented as a large-scale Mandarin audio-visual dataset containing utterances all recorded in Mandarin by native Chinese speakers (Section 1). Table 1 also shows the dataset is 'Mandarin' language only. The raw text sources and annotations confirm only the use of Mandarin language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that code or programming language content is part of the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is linguistic audio-visual data; mathematical or logical notation content is not mentioned as part of the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Dataset involves human speech data; there is no biological sequence or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain fictional or constructed languages; it uses natural Mandarin language only."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language used in the dataset is clearly identified as Mandarin and well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language (Mandarin) audio and text, so language data is present."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7668,
      "completion_tokens": 212,
      "total_tokens": 7880
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and last paragraph of Section 1 Introduction",
          "reasoning": "The paper states explicitly in the abstract that \"The dataset and code will be released at https://github.com/SpringHuo/MA VD.\" and reiterates the intention to make the corpus open-source and free for research use in Section 1. This indicates that the code for dataset related tasks will be publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Our Mandarin Audio-Visual Dataset with Depth Information) and subsections 3.1 to 3.4",
          "reasoning": "The paper provides detailed documentation about the dataset creation process, including text acquisition pipeline (3.1), manual selection of text (3.1.2), speaker description (3.2), recording setup and environment (3.3.1 and 3.3.2), and data annotation process (3.4). This extensive information demonstrates transparency and completeness in documenting the dataset creation."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7739,
      "completion_tokens": 347,
      "total_tokens": 8086
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1; Section 3; Table 1",
          "Reasoning": "The GLOBE dataset consists of 535 hours of high-quality English speech audio recorded from 23,519 speakers worldwide with 164 accents. The data originates from human voice recordings collected in the Common Voice dataset and then filtered, enhanced, and curated by the authors to remove low-quality samples and improve transcription accuracy, thus the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1; Section 3.2; Table 1",
          "Reasoning": "Each audio utterance in the GLOBE corpus is associated with corresponding text transcriptions. The text data originates from the original Common Voice transcriptions, which are then validated and improved by the authors using Whisper ASR model and alignment filtering to ensure transcription accuracy. This text metadata is human-generated as it corresponds to human speech content."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 1; Section 3.3; Table 1",
          "Reasoning": "The GLOBE dataset includes detailed speaker metadata in tabular form such as speaker accent, age, and gender. Some of this metadata is completed or predicted by the authors using trained speaker information prediction models, thus a portion of this tabular data is human generated (original speaker metadata) and some are model generated (filled in by prediction models). Therefore, both human- and model-generated labels are present in this modality."
        }
      ]
    }
  },
  {
    "id": "wang24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8591,
      "completion_tokens": 202,
      "total_tokens": 8793
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.3",
            "reasoning": "The speaker metadata (accent, age, gender) missing in the original data were populated by trained speaker information prediction models developed by the authors, as described in Section 3.3. These models were used to automatically annotate missing speaker information."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not provide any description of detailed annotation instructions given to annotators. Instead, metadata prediction was automated using trained models."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No mention of scoring rubrics or human evaluation criteria for annotations. The annotation process is automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No examples or guidelines for annotation are presented; the annotation was done by automated models rather than human annotators requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "wang24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9721,
      "completion_tokens": 458,
      "total_tokens": 10179
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in the quality assurance process for the GLOBE dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Speaker Information Refinement and Speech Post-processing",
          "reasoning": "The authors trained three speaker information prediction models (for accent, age, and gender) to predict missing speaker metadata with high accuracies, which they used to populate missing speaker metadata. This constitutes quality assurance performed by AI models acting as judges for annotation completeness and accuracy."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Speech Sample Pre-processing and Filtering and Section 3.2 Speech Text Alignment",
          "reasoning": "Automated filtering was performed based on metrics such as signal-to-noise ratio, signal bandwidth, and internal silence using algorithmic methods. Speech transcript alignment and verification used automated tools like Whisper ASR and edit distance computation to ensure transcription accuracy. Additionally, post-processing to remove silences and enhance speech used automated algorithms. These automated verifications qualify as QA via automatic processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple quality assurance methods applied to the GLOBE dataset; thus, QA is present."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9339,
      "completion_tokens": 443,
      "total_tokens": 9782
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the data as being created entirely from scratch by human contributors. The GLOBE corpus is based on an existing dataset (Common Voice) which was collected from volunteer contributors, but was then processed and refined by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the data was generated entirely by AI or machine learning models. The authors used models for metadata prediction but did not generate new speech data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to any machine translation used for generating dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 1 (Abstract), Section 2, Section 3",
          "reasoning": "The GLOBE dataset is created by collecting and aggregating existing speech data from the Common Voice dataset, which contains recordings from volunteer contributors worldwide."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (Data Processing Pipeline)",
          "reasoning": "The authors applied extensive filtering, quality improvement, metadata correction, and post-processing (e.g., filtering low-quality samples, re-aligning text and utterance, predicting missing metadata, speech enhancement) to the Common Voice dataset, thus creating a dataset derived from an existing source with modifications and enhancements."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and processing of the dataset, so the data source is documented."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9857,
      "completion_tokens": 508,
      "total_tokens": 10365
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), especially 4.1.2 Experimental results and descriptions",
          "reasoning": "The GLOBE dataset is used to train speaker adaptive TTS models from scratch as demonstrated by training the modified YourTTS model on the GLOBE dataset. The experiments show that models trained on GLOBE achieve better speaker similarity and comparable naturalness to those trained on other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate fine-tuning a pre-trained model using the GLOBE dataset. Training described is from scratch rather than fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or demonstration of reinforcement learning based post-training methods like RLHF using the GLOBE dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experiments for Ground-Truth Speech Samples), Table 2",
          "reasoning": "The GLOBE dataset is used in evaluation, benchmarking model performance in speaker adaptive TTS systems, to assess speech naturalness, word error rates, speaker similarity, and diversity."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 (Data Processing Pipeline) and 4 (Experiments), including analysis of speaker metadata and diversity",
          "reasoning": "The authors analyze speaker accent distribution, speaker metadata quality, and perform speaker embedding distribution analysis to assess accent diversity and dataset quality."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not indicated to serve as a knowledge base for retrieval-augmented generation or similar augmentations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset's practical usages are clearly described for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10580,
      "completion_tokens": 536,
      "total_tokens": 11116
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The GLOBE dataset contains only English speech data from speakers worldwide with various English accents, but does not include multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper clearly states that GLOBE is an English corpus and does not contain entries in exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 2 Relevant English Multi-speaker Corpus; Table 1",
          "reasoning": "The paper explicitly defines GLOBE as a high-quality English corpus containing speech data with various English accents from around the world. All data entries are in English only, as per the descriptions and statistics provided (Table 1)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains English content only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of speech utterances and transcriptions in English without mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of fictional or artificially created languages; dataset only covers real-world English accents."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language content (English) and accents are clearly specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken English utterances and their transcriptions, thus contains language."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7798,
      "completion_tokens": 211,
      "total_tokens": 8009
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section describes code availability for dataset construction; no code repository link provided in the paper.",
          "reasoning": "The paper mentions the dataset will be publicly released and provides a website link for the GLOBE dataset, but there is no explicit mention or link to code repositories for the data processing pipeline or dataset construction code. The description of the data processing pipeline is detailed but does not indicate that the code is publicly shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 'Data Processing Pipeline' and subsections 3.1, 3.2, 3.3 describe dataset creation process.",
          "reasoning": "The paper provides a thorough description of the dataset construction process, including detailed procedures for speech sample pre-processing and filtering, speech text alignment, and speaker metadata refinement and post-processing. These are documented in Section 3 and its subsections, outlining the filtering criteria, transcription techniques, and metadata prediction models used to refine and curate the dataset."
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8104,
      "completion_tokens": 119,
      "total_tokens": 8223
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. Data and Task",
          "Reasoning": "The authors collected a new dataset consisting of thousands of human judgments of the pragmatic similarity between pairs of utterances, where the utterance pairs originate from unstructured conversations among students, task-oriented dialogs, and toddler utterances. These are recorded audio clips, thus the modality is audio. The data clearly comes from human speech recordings, and the human judges provided similarity ratings, indicating human involvement in data creation."
        }
      ]
    }
  },
  {
    "id": "ward24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8956,
      "completion_tokens": 204,
      "total_tokens": 9160
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3",
            "reasoning": "The dataset comprises human judgments of pragmatic similarity collected from 6 to 9 judges per utterance pair, described as hand-picked for sensitivity and adeptness with language nuances, indicating expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "Judges were asked to rate 'How pragmatically similar are the two clips, in terms of the overall feeling, tone, and intent?', which constitutes an explicit annotation instruction."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "Judgments were made on a scale from 1 to 5, implying a scoring rubric defining levels of similarity."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No examples of annotation instances or detailed exemplars are provided or mentioned in the paper's description of the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "ward24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10086,
      "completion_tokens": 330,
      "total_tokens": 10416
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes that multiple human judges (6 to 9 per utterance pair) provided judgments, but there is no indication that any single human acted alone as an expert annotator for quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3: Data and Task",
          "reasoning": "The dataset involved multiple human judges (6 to 9 per utterance pair) who were hand-picked for their sensitivity and adeptness with language nuances and were well-compensated and supervised, indicating they were experts or members of the target demographic providing quality assurance through multiple expert annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance conducted by a single human annotator without subject matter expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators were described as hand-picked for adeptness with language nuances and were supervised and compensated well, identifying them as experts rather than non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models acting as judges or performing quality assurance on annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that automated processes were used for quality assurance of the human annotation data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents a rigorous annotation and quality assurance process involving multiple expert human annotators, so no lack of QA is reported."
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9704,
      "completion_tokens": 410,
      "total_tokens": 10114
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3: Data and Task",
          "reasoning": "The paper describes a new collection of data consisting of human judgments of pragmatic similarity between pairs of utterances. These judgments were obtained in controlled sessions, with human judges rating hundreds of utterance pairs created from original spoken conversations and dialogs. The utterances themselves are from natural conversations, not pre-existing datasets, and the similarity judgments were made directly by human annotators without reference to prior data, indicating original, human-created data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data generated entirely by AI or machine learning models is presented or introduced in the paper."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No human translation of data from other languages is indicated for this new dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data from other languages is indicated for this new dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The utterance pairs and corresponding human judgments are not merely collected or aggregated from existing sources; rather, they were specifically created and collected for this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not described as derived from existing sources with modifications; it appears to be original, not transformations or adaptations of pre-existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly documented and specified."
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10222,
      "completion_tokens": 304,
      "total_tokens": 10526
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The new dataset of human judgments of pragmatic similarity between utterance pairs is used to train models predicting these judgments from audio features. Specifically, feature selection and model training to predict similarity scores involve supervised learning using this collected dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 5.1, and Table 1",
          "reasoning": "The dataset is used for evaluating and benchmarking various models (e.g., HuBert-based models) by correlating predicted similarity scores with human judgments, serving as ground truth for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2, Section 5, and Section 6",
          "reasoning": "The dataset facilitates qualitative and quantitative analyses of model errors, strengths, weaknesses, and generality across languages and dialects, supporting deeper understanding of pragmatic similarity."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10945,
      "completion_tokens": 670,
      "total_tokens": 11615
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 Data and Task",
          "reasoning": "The paper introduces a new dataset of human judgments of pragmatic similarity between utterance pairs in English and Spanish only, specifically American English and Mexican/American-border Spanish. No mention or inclusion of more than two languages is present, thus it is not multilingual in the sense of more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3 Data and Task",
          "reasoning": "The new dataset consists of utterance pairs judged for pragmatic similarity in two human languages: American English and Mexican/American-border Spanish. This is explicitly stated with data collected in three sessions: two English sessions and one Spanish session, representing exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3 Data and Task",
          "reasoning": "While partial data is for English only, the dataset as a whole includes Spanish data as well. The paper treats data from both languages as part of the new dataset, so it is not exclusively monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3 Data and Task",
          "reasoning": "The dataset includes Spanish utterances, which are non-English, but the dataset also contains English utterances. Therefore it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset comprises human spoken utterances and human similarity judgments. There is no mention or indication of any entries containing programming or structured code-related contents."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset contains audio utterance pairs and human judgments of pragmatic similarity, with no indication of entries including mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset is of human spoken language utterances only, with no mention of biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No mention or use of fictional or artificially created languages is made in the dataset. All languages are natural human languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The languages of the dataset entries are explicitly stated as English and Spanish. There is no ambiguity or lack of documentation about the languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains utterances with human language (English and Spanish), so language is present."
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8163,
      "completion_tokens": 238,
      "total_tokens": 8401
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 'Data and Task' and Section 7 'Summary, Implications, and Future Work'",
          "reasoning": "The paper mentions the dataset of thousands of human judgments collected by the authors, describing the data collection conditions and details in Section 3. However, the paper does not provide any indication or links to code repositories for the dataset construction or annotation processes. The available code link at https://github.com/andysegura89/Pragmatic_Similarity_ISG pertains to model code, not the dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 'Data and Task'",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 3, including the number of utterance pairs, the number of human judges, the nature of the utterances (English and Spanish, from student conversations and toddler speech), the judgment scale used, and properties such as task-agnostic judgments, judge training and quality, and the mix of lexically identical versus different pairs. This comprehensive description serves as documentation of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6974,
      "completion_tokens": 219,
      "total_tokens": 7193
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 5, and 7",
          "Reasoning": "The dataset includes recorded speech from three human narrators (Perec Zylberberg, Sara Blacher-Retter, and Leib Rubinov), whose audiobooks and narrated short stories were originally recorded by humans and later digitized. The authors collected and segmented this found audio data to form the speech component of the dataset."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 3.3, 4.4, and 5",
          "Reasoning": "The text transcripts in the dataset are derived from OCR scans of books and corresponding manual transcriptions. These texts were hand-corrected by a human expert for transcription accuracy and to match exactly what the narrators produced, including disfluencies. The text was therefore human generated or corrected to serve as accurate speech-text pairs."
        }
      ]
    }
  },
  {
    "id": "webber22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7826,
      "completion_tokens": 222,
      "total_tokens": 8048
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.4, Section 5",
            "reasoning": "The paper states that the final hand-corrected texts were all corrected manually by the third author, a specialist in Yiddish, indicating a single human expert performed the manual annotation and correction of text in the dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to the annotator for manual text correction beyond the fact that it was done by a specialist; no formal instructions are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not specify any scoring rubrics or criteria used to evaluate the annotations; only the correction itself by the expert is mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No examples of annotation or correction guidelines are provided in the paper; the process appears to rely solely on expert manual correction without published examples."
          }
        }
      ]
    }
  },
  {
    "id": "webber22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8956,
      "completion_tokens": 293,
      "total_tokens": 9249
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4.4, Section 5",
          "reasoning": "The final published version of the dataset includes texts manually corrected by the third author, who is described as a specialist in Yiddish. This individual conducted the hand corrections to ensure transcription accuracy, including matching the exact speech content and disfluencies, indicating a single human expert performed quality assurance on text annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.4, Section 3.5",
          "reasoning": "Automated tools were employed for utterance-level segmentation using Aeneas (which involved forced alignment via dynamic time warping and low-quality synthetic speech) and for forced alignment using the Montreal Forced Aligner. These automatic processes served to verify and segment the dataset content at the utterance and phoneme level, representing automated QA processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple QA steps both manual by an expert and automated by software; thus, QA was applied and documented."
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8574,
      "completion_tokens": 448,
      "total_tokens": 9022
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not stated to contain any content created entirely from scratch by human contributors. The content originates from pre-existing audiobooks and texts, not newly created original material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset content was generated entirely by AI or machine learning models. The models were trained on the dataset but did not generate the source data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset including content produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate any dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset was created by collecting found data: audiobooks and OCR digitized books from existing sources such as the Yiddish Book Center and University of Haifa resources. These materials were aggregated without being originally created for this purpose."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.2, 3.3, 3.4, 4.4, and 5",
          "reasoning": "The collected data was hand-corrected and processed (e.g., text hand-corrections, orthographic normalization, manually adjusted segmentation, and respelling) to produce a cleaned and consistent dataset suitable for TTS training. These modifications constitute derivations from the original collated materials."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and the preprocessing steps, so the data origin is specified."
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9092,
      "completion_tokens": 273,
      "total_tokens": 9365
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The dataset was used to train FastSpeech2 TTS models from scratch, as described in Section 4 where the authors build single-speaker TTS models using the dataset and report evaluation results on these models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset was used to evaluate the resulting TTS system, the dataset itself is not exclusively used for evaluation or benchmarking, but primarily for training the system."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 and 4.4",
          "reasoning": "The dataset and results from TTS models trained on it were analyzed to understand the effect of transcription quality and orthography on speech naturalness and intelligibility, guiding iterative improvements to the dataset itself."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9815,
      "completion_tokens": 603,
      "total_tokens": 10418
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are all in Yiddish, which is a single language. Although Yiddish contains loanwords from other languages, the dataset is focused on Yiddish text and speech only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages. It only contains Yiddish language entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content; the dataset is exclusively Yiddish speech and text."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3.1; Section 5",
          "reasoning": "The dataset is a Yiddish text-to-speech dataset containing speech and aligned text in Yiddish exclusively. Although Yiddish includes words of Hebrew, Aramaic, Germanic and Slavic origin, the language itself is singular and non-English. The paper explicitly states it is the first Yiddish TTS dataset and system (Abstract) and details that all recordings and texts are in Yiddish (Section 3.1). The final dataset contains utterances in Yiddish from three speakers (Section 5)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Section 3.2 (mention of Python library)",
          "reasoning": "While the paper mentions a Python library for text preprocessing, the dataset entries themselves are not code or programming language content but are Yiddish text and speech."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset contains mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human language data only, no biological or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Yiddish is not a constructed language; the dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified and documented as Yiddish."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken and written language data (Yiddish), so it does contain language."
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7033,
      "completion_tokens": 186,
      "total_tokens": 7219
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 5",
          "reasoning": "The paper explicitly states in Section 5 that along with the dataset, the authors have published corrected transcripts, scripts for downloading and segmenting audio, scripts for normalizing and respelling text, and model files accessible through the GitHub page https://github.com/REYD-TTS. This indicates that all code related to data collection, preprocessing, and generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 5",
          "reasoning": "The paper provides a detailed description of the dataset creation process in multiple sections: Section 3 describes the source material, automated and manual text preprocessing, utterance-level segmentation, and forced alignment; Section 5 describes the final dataset contents and the accompanying resources released for reproducibility. This offers transparent and comprehensive documentation of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6299,
      "completion_tokens": 80,
      "total_tokens": 6379
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Data",
          "Reasoning": "The paper states that they recruited 63 UK-based speakers to read scripted utterances over the telephone line, resulting in a dataset of 12.7 hours of telephone audio, which is thus human recorded speech data."
        }
      ]
    }
  },
  {
    "id": "wei24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7151,
      "completion_tokens": 192,
      "total_tokens": 7343
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3",
            "reasoning": "Section 3 describes recruiting 63 speakers to read scripted utterances and each utterance was transcribed, accounting for deviations from scripts; this indicates multiple humans performed transcriptions, likely non-expert transcribers given no mention of expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "There is no explicit mention of detailed annotation instructions or guidelines provided to transcribers in Section 3 or elsewhere."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not mention any scoring rubrics or criteria used for the transcription annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No examples of annotation or transcription guidelines are included in the paper or appendix."
          }
        }
      ]
    }
  },
  {
    "id": "wei24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8281,
      "completion_tokens": 415,
      "total_tokens": 8696
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance was conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not detail that multiple human experts or domain experts performed the quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single non-expert human annotator performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "Section 3 states that 63 speakers recorded scripted utterances and that each utterance was transcribed taking into account deviations from the script. The paper does not mention the expertise level of transcribers, but given the number of speakers involved and that transcriptions consider deviations, it is reasonable to conclude that multiple human annotators (likely non-experts) provided transcriptions for quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated or algorithmic verification techniques applied for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper does describe a manual transcription process and does not indicate the absence of quality assurance."
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7899,
      "completion_tokens": 409,
      "total_tokens": 8308
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3, Data",
          "reasoning": "The authors explicitly state that they recruited 63 human speakers to read scripted utterances created for this study, resulting in 7,031 unique utterances. These utterances were crafted to represent commercial banking enquiries involving account names, reflecting original content specifically created for this research. There is no indication that this data was translated, adapted, or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources without significant modification; it was expressly designed and created by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data is based on existing sources with modifications or adaptations; rather, it is newly created original scripted utterances and recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation method is fully documented in Section 3."
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8417,
      "completion_tokens": 327,
      "total_tokens": 8744
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4, 5",
          "reasoning": "The dataset is used to fine-tune the Whisper model with in-domain data for better recognition of named entities, especially in scenarios involving novel or previously unheard names. The paper describes fine-tuning with a learning rate of 1e-5 and batch size 8 on this dataset and evaluates the impact on recognition performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3, 5",
          "reasoning": "The dataset is used for evaluation and benchmarking of various models and tuning strategies across different scenarios (seen-and-heard, seen-but-unheard, unseen-and-unheard). This includes cross-evaluation data splits and unscripted data to assess real-world performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2, 6",
          "reasoning": "The paper uses the dataset to analyze model performance trends such as recall and WER under different prompt conditions, prompt list lengths and positions, and strategies for prompt tuning. This analysis provides insight into model behavior and attention mechanisms."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9140,
      "completion_tokens": 555,
      "total_tokens": 9695
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's proposed dataset consists of utterances recorded by 63 UK-based speakers in a commercial banking context with scripted and unscripted data. There is no mention of multiple languages; only English utterances are discussed."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English language utterances. There is no indication that two languages are included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 Data",
          "reasoning": "The dataset consists of 7,031 unique utterances spoken by UK-based speakers, all in English, focused on commercial banking queries. The examples cited, such as 'search the overdue invoices for Alpha Limited,' are English language utterances. There is no indication or mention of any other human languages in the data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is English, not non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken utterances and transcriptions related to commercial banking tasks. There is no mention or presence of code, programming language, or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the dataset or examples of mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human spoken English utterances; there is no mention of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that constructed or fictional languages are used in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language is clearly stated and documented as English; thus, the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are all spoken utterances with transcriptions and thus contain language."
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6358,
      "completion_tokens": 148,
      "total_tokens": 6506
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention of code availability",
          "reasoning": "The paper does not mention any link or repository to access code for dataset collection, preprocessing, or generation. No URLs or references to code release are given anywhere in the text, suggesting the code is not publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Data",
          "reasoning": "The paper provides a detailed description of the new dataset, including the number of utterances, speaker demographics, acoustic conditions, stratification of data into clusters, data collection procedure (recorded over telephone lines), and transcription approach. This documentation supports understanding and potential reproduction of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9765,
      "completion_tokens": 107,
      "total_tokens": 9872
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.1 Smartphone app Plapper; Section 2.1.3 Speech material",
          "Reasoning": "The new dataset is the Plapper corpus introduced by the authors, consisting of self-recorded read speech audio recordings collected via the smartphone app Plapper from 1,522 German speakers. This data was recorded by human participants speaking, thus originates from human generated audio data."
        }
      ]
    }
  },
  {
    "id": "weirich24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10617,
      "completion_tokens": 272,
      "total_tokens": 10889
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1.2, 2.1.3",
            "reasoning": "Participants self-recorded their own speech using the Plapper smartphone app; the recordings were then semi-automatically labeled and specific segments hand-corrected. This setup involves multiple non-expert participants providing speech data via self-recordings rather than expert annotators performing direct annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1.1, 2.1.3",
            "reasoning": "Participants were instructed via the app to read a list of seven sentences for data collection, indicating presence of instructions for the recording task in the digital interface."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any explicit scoring rubrics or standardized rating scales used by annotators for labeling or evaluating the recordings; ratings mentioned pertain to self-assessed scales (e.g., masculinity-femininity) self-reported by participants rather than annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of annotation or labeling are provided in the paper or supplementary materials; the annotation process involved semi-automatic labeling and hand corrections without examples being detailed."
          }
        }
      ]
    }
  },
  {
    "id": "weirich24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11747,
      "completion_tokens": 324,
      "total_tokens": 12071
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert humans involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by AI models."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and 2.1.4",
          "reasoning": "The paper states that audio recordings are semi-automatically labeled using WebMa and specific segments are hand-corrected. Moreover, f0 values are extracted using a custom Praat script with defined parameters, indicating automated processes combined with some manual correction. This suggests quality assurance is largely based on automated verification of audio labeling and parameter extraction supported by some manual checking, but no mention of expert human annotation reviews or formal QA protocols."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A form of quality assurance is described as semi-automatic labeling, manual corrections, and scripted f0 extraction, so it is not accurate to say no QA process is applied or documented."
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11365,
      "completion_tokens": 410,
      "total_tokens": 11775
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1.2 Participants; 2.1.3 Speech material",
          "reasoning": "The dataset consists of speech recordings collected directly from human participants who self-recorded read speech via the smartphone app Plapper. The participants are from Germany and recorded themselves reading specific sentences, generating new, original human-spoken data created from scratch through human contribution. This is explicitly stated as data collected via voluntary self-recordings and not adapted or derived from previously existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models. The data originates from human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data by humans from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate or modify the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not taken from existing sources or aggregated from other datasets; it was newly collected from participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on existing sources with modifications or adaptations; it is original speech data recorded by participants for the study."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of data generation are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11883,
      "completion_tokens": 317,
      "total_tokens": 12200
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The Plapper corpus dataset is used primarily for analyzing trends and patterns in fundamental frequency (f0) variation as a function of gender, age, self-ascribed gender spectrum, sexual orientation, and region in German read speech. The paper presents statistical models and discusses phonetic and sociolinguistic analyses based on this new corpus to investigate social and biological influences on speech characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12606,
      "completion_tokens": 286,
      "total_tokens": 12892
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and 2.1.3",
          "reasoning": "The Plapper corpus contains recordings of spoken German only. Participants from Germany recorded themselves reading sentences in German, as stated in Sections 2.1.1 (smartphone app designed to sample the German-speaking population in Germany) and 2.1.3 (speech material consists of self-recorded read speech). No other languages are mentioned as part of the dataset, confirming that it is monolingual and non-English (German)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9824,
      "completion_tokens": 174,
      "total_tokens": 9998
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided",
          "reasoning": "The paper does not mention or provide any link to code repositories or explicit code used for data collection, preprocessing, or dataset generation. While methods and processing steps are described, no source code is shared publicly or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method), especially 2.1 (Smartphone app Plapper), 2.1.1 to 2.1.5",
          "reasoning": "The paper provides a detailed description of the data collection process using the Plapper smartphone app, participant recruitment, the speech material, acoustic analysis procedures including extraction of fundamental frequency, and the statistical analysis approach. This coverage documents the dataset creation process including recording, labeling, segmentation, preprocessing, and analysis."
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8154,
      "completion_tokens": 155,
      "total_tokens": 8309
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The dataset comprises over 85 hours of segmented speech utterances from long-form audio recordings found online (the 'Letters' recordings). These recordings represent natural human speech collected from publicly available sources and were manually verified and processed for TTS training."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "Corresponding text transcripts for the found speech recordings were used. These transcripts were human-generated as they represent the original textual data aligned to the speech and were lightly normalized but not algorithmically generated."
        }
      ]
    }
  },
  {
    "id": "wells23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9006,
      "completion_tokens": 224,
      "total_tokens": 9230
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The paper describes a fully automated alignment and segmentation pipeline using a character-based HMM-GMM acoustic model trained with Kaldi and ASR decoded output; this process segments and aligns over 85 hours of found Scottish Gaelic recordings without mention of human annotators performing the alignment or segmentation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators, likely because the segmentation and alignment were performed automatically rather than by human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No scoring rubrics or grading guidelines are described in relation to the automatic data segmentation or alignment process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not provide annotation examples for the segmentation or alignment process; the procedure is algorithmic and automatic rather than manual annotation with examples."
          }
        }
      ]
    }
  },
  {
    "id": "wells23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10136,
      "completion_tokens": 315,
      "total_tokens": 10451
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human experts or native speakers on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert humans performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for forced alignment and model training, these are not described as quality assurance or verification of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset segmentation and alignment are performed via automated methods including forced alignment using a character-based Kaldi model and ASR decoding combined with sequence alignment algorithms (e.g., Smith-Waterman). These automated, algorithmic processes constitute a form of automatic verification and quality assurance of the dataset segmentations without explicit human review reported."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper does describe automated quality assurance processes for dataset segmentation and alignment, so QA is documented."
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9754,
      "completion_tokens": 380,
      "total_tokens": 10134
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created new speech or text data from scratch by human contributors. Instead, they use 'found' data, specifically publicly available online Scottish Gaelic recordings and transcripts."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No original data was generated by AI or machine learning models themselves independent of existing data; models are trained based on the found data but do not generate new foundational speech or text data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is used or mentioned to produce any portion of the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Corpus Preparation), especially Sections 2 and 2.2.",
          "reasoning": "The corpus is created by collecting and segmenting over 85 hours of Scottish Gaelic recordings found online (the Letters recordings), along with their transcripts. This data is existing material publicly available but collated and processed by the authors to create the segmented corpus for TTS training."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.2, and 2.3.",
          "reasoning": "The authors apply transformations and adaptations such as speech segmentation, forced alignment, selection of subsets based on acoustic unit coverage, and acoustic unit extraction via HuBERT features and k-means clustering on the found data to derive training subsets. This indicates that the data is modified and processed from its original source."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and processing of the data are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10272,
      "completion_tokens": 253,
      "total_tokens": 10525
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Model specification) and Section 5 (Results and discussion)",
          "reasoning": "The datasets of segmented Scottish Gaelic speech utterances (2h and 8h subsets selected) are used to train FastPitch acoustic models from scratch using character, phone, or acoustic unit inputs."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Listening test design) and Section 5 (Results and discussion)",
          "reasoning": "Held-out test sets derived from the discovered and segmented Scottish Gaelic speech corpus are used for subjective evaluation and listening tests to assess the performance of the trained TTS models."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10995,
      "completion_tokens": 402,
      "total_tokens": 11397
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The dataset primarily contains Scottish Gaelic speech and text, but the texts also include some non-Gaelic words, mostly snippets of English (Section 3.1). They apply an English G2P model to these non-Gaelic words (Section 3.2). Therefore, the dataset entries contain two human languages: Scottish Gaelic and English."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although Scottish Gaelic is the main language, the presence of English words in the dataset prevents it from being purely monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8213,
      "completion_tokens": 186,
      "total_tokens": 8399
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Conclusion",
          "reasoning": "The abstract explicitly states, 'We release our corpus building recipe so that others may easily apply our work to new languages.' The conclusion repeats that they 'release our corpus building and model code so that others may easily apply our work to new languages.' This indicates that the code used for constructing the dataset is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Recording corpus, Segmentation, and Data selection)",
          "reasoning": "Section 2 details the process of dataset creation, including recording data sourcing, segmentation, alignment, and acoustic unit-based data selection methods. The documentation is quite comprehensive, describing the iterative alignment procedure, acoustic model training, forced-alignment steps, and data set selection using discrete acoustic units. Therefore, the paper provides thorough documentation on the dataset creation process."
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11017,
      "completion_tokens": 170,
      "total_tokens": 11187
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Data Collection), especially 3.3 (Speech Elicitation and Collection), and Section 3.4 (Ethics)",
          "Reasoning": "The paper introduces a new dataset comprising audio recordings collected from adult human subjects undergoing acute pain inducement via a Cold Pressor Task. Speech was elicited by having participants read out loud randomized sentences, with audio recorded via microphones. This implies that the data modality is audio and the data was generated through human recording of natural speech during an experiment conducted by the authors. The dataset is explicitly described as newly collected and curated by the authors. No mention is made of model-generated or simulated audio. Hence, the dataset is audio modality, human generated."
        }
      ]
    }
  },
  {
    "id": "williams24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11869,
      "completion_tokens": 279,
      "total_tokens": 12148
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 4",
            "reasoning": "The pain levels were labeled by manually extrapolating participant self-reported pain statements to previous utterances. The paper states that authors manually reviewed and labeled the utterances according to reported pain levels. There is no indication that experts (e.g., clinicians) performed the annotations; labeling was based directly on participants' own reported levels, implying a manual but non-expert process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not describe any specific annotation instructions or guidelines given to annotators. Since annotation was based on directly extrapolating participant self-reports, no detailed instructions are mentioned."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "No mention of scoring rubrics or annotation rubrics is found in the paper. Pain labels came directly from participant ratings on a 1-10 scale and were rounded or grouped for classification tasks, with no rubric described for annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not provide any examples of annotation or sample labeled instances given to annotators during the annotation process. The labeling derived automatically from participant self-reports without further examples."
          }
        }
      ]
    }
  },
  {
    "id": "williams24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12999,
      "completion_tokens": 338,
      "total_tokens": 13337
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was performed by a single human annotator with expertise or membership in the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple human experts or expert annotators for data annotation or validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that a single non-expert conducted quality assurance on the dataset annotations or labels."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators conducting quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by any AI model as a judge of annotation quality or correctness."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4. Speech Data Pre-Processing",
          "reasoning": "The authors manually reviewed all utterances for quality control and removed utterances with quality issues, such as noise interference. They also used an automated voice activity detection (VAD) algorithm (Python webrtcvad toolkit) to trim recordings to remove leading and trailing silence and discarded utterances below a certain length threshold. These processes suggest automated verification techniques were applied as part of quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance steps are described, including manual review and automated trimming, so it is incorrect to say no quality assurance was performed or documented."
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12617,
      "completion_tokens": 411,
      "total_tokens": 13028
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Data Collection",
          "reasoning": "The dataset was created by recruiting 15 human participants who were subjected to a cold pressor task to induce acute pain. The participants read out aloud sentences specifically selected for this study, with recordings made under controlled conditions. The speech data is original and collected from scratch by human subjects performing the task and reading the sentences during the experiment."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated or synthesized by any AI or machine learning models; rather, it consists of direct human speech recordings during the experiment."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any data was produced by human translation from another language is provided; the dataset consists of English speech recorded from participants."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of machine translation being used to generate the dataset is mentioned."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected from existing sources or aggregated; it was directly recorded from a novel experimental protocol."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While acoustic features were extracted from the recordings, the underlying data (speech audio) is not derived from existing datasets; no modifications or transformations were applied to produce the dataset itself."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the dataset is clearly documented and described in detail in the paper."
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13135,
      "completion_tokens": 335,
      "total_tokens": 13470
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.2, 5.3",
          "reasoning": "The dataset is used to train machine learning models such as SVM, Logistic Regression, and MLP classifiers from scratch to predict pain levels from speech features as described in Sections 5.2 and 5.3. There is no mention of pretrained models being fine-tuned; models are trained using this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3",
          "reasoning": "The dataset is used for evaluation purposes, with performance metrics such as F1 scores reported for both binary and three-class pain classification tasks (Section 5.3). The dataset serves as the evaluation benchmark for the classifiers trained."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3 (Feature Analysis)",
          "reasoning": "The dataset is used extensively for analyzing acoustic feature contributions to pain classification using SHAP values and PCA, to understand the relationship between speech features and pain levels (Section 5.3 Feature Analysis). This analysis clarifies how different features affect model predictions, contributing to explainability."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13858,
      "completion_tokens": 501,
      "total_tokens": 14359
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described is collected exclusively in English. There is no mention or indication of more than two human languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains entries only in English and does not include a second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.3 Speech Elicitation and Collection",
          "reasoning": "The paper explicitly states that the dataset consists of English speech only, where participants read aloud sentences from the Harvard Sentences, which are English sentences. The dataset is an English speech-only pain dataset as mentioned in the abstract and Section 3.3."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is in English, not a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech utterances only; no entries contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset entries contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human speech recordings; it does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language is clearly documented as English; thus, the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries do contain language in the form of English speech."
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11076,
      "completion_tokens": 145,
      "total_tokens": 11221
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not specified",
          "reasoning": "The paper does not mention or provide any links, references, or supplemental materials indicating that the code for data collection, preprocessing, or dataset construction is publicly available or shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: Data Collection and Section 4: Speech Data Pre-Processing",
          "reasoning": "The paper provides detailed documentation about the dataset creation process, including participant recruitment criteria, pain inducement protocol (Cold Pressor Task), speech elicitation methods, audio recording setup, data preprocessing steps such as quality control and labeling, and ethical considerations. This information adequately documents the dataset construction process."
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8182,
      "completion_tokens": 80,
      "total_tokens": 8262
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The paper states that they used an existing cognitive and physical load dataset corpus presented in [7], so no new dataset was created by the authors. No new dataset was introduced; the datasets used are pre-existing."
        }
      ]
    }
  },
  {
    "id": "wu23i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9034,
      "completion_tokens": 253,
      "total_tokens": 9287
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that the dataset used is a corpus consisting of five existing datasets with predefined splits into training, validation, and testing sets based on speaker identities. These datasets originate from previous works cited in [7], indicating that the data was collected and annotated by multiple human experts in prior studies. As the authors reused this existing combined dataset, the underlying annotations involve multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not describe or provide any annotation instructions for the dataset labels. The labels correspond to whether speech was produced under cognitive or physical load tasks, presumably annotated in the original datasets, but no instructions are described in this paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No mention of scoring rubrics or detailed annotation criteria is provided in the paper regarding the labeling of cognitive or physical load in the speech data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide annotation examples or guidelines illustrating the labeling process for the datasets used."
          }
        }
      ]
    }
  },
  {
    "id": "wu23i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10164,
      "completion_tokens": 282,
      "total_tokens": 10446
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for any newly introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple human experts. The paper uses existing datasets without specifying expert annotation or validation processes."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by a single non-expert annotator for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided in the paper regarding quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models for quality assurance or validation of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification processes applied for quality assurance of the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing voice stress detection datasets but does not introduce any new datasets. Moreover, it does not provide details about any quality assurance or validation processes for these datasets. Therefore, no quality assurance process is documented for the data used."
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9782,
      "completion_tokens": 437,
      "total_tokens": 10219
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses an existing cognitive and physical load dataset corpus as presented in [7], consisting of five datasets with 111 speakers and nine languages. There is no indication that the authors collected or created a new dataset entirely from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is derived from existing speech recordings; no data was artificially generated or created solely by models as original data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data acquired through human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data acquired through machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The authors used an existing corpus consisting of five separate datasets collected from multiple languages and speakers, combined for their experiments. The data is aggregated from these existing sources without creating new raw data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4 Decreasing input audio length, Section 3.5 Utterance-level speaker embeddings",
          "reasoning": "The authors derived new data representations by chunking original audio files into shorter segments (3-5 seconds) and by computing speaker embeddings (using ECAPA and Resemblyzer) from full-length audio. These processes involve transformation and adaptation of existing data rather than completely new data generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as existing datasets combined and processed as described."
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10300,
      "completion_tokens": 316,
      "total_tokens": 10616
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2, 3.3, and 3.5",
          "reasoning": "The combined voice stress datasets are used to fine-tune pre-trained audio embedding models by training downstream classifiers (SVM, MLP, Transformer) for binary classification tasks of voice stress detection. The paper describes training and evaluation procedures on combined and chunked datasets using supervised methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3, Table 1",
          "reasoning": "The datasets are explicitly split into training, validation, and test sets to evaluate cross-dataset voice stress detection performance and generalization. Performance metrics (UAR) are reported to benchmark and validate the models' effectiveness."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.5 and 3.6",
          "reasoning": "The paper conducts detailed analyses on how speaker embeddings and different types of audio embeddings (utterance-level, framewise) affect voice stress detection performance, including individual variability analysis across combined datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11023,
      "completion_tokens": 241,
      "total_tokens": 11264
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states in Section 3.1 that the dataset corpus consists of five existing datasets with a total of 111 speakers from nine languages. This indicates that the combined dataset used by the authors contains entries from more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8241,
      "completion_tokens": 157,
      "total_tokens": 8398
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to data collection, preprocessing, or dataset construction. The focus is on models and evaluation using existing datasets, with no indication that code for dataset construction is released."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section",
          "reasoning": "The paper states that it uses an existing voice stress dataset corpus from prior work [7], consisting of five datasets, 111 speakers, nine languages, and 12 hours duration. There is no detailed description of dataset construction or data collection within this paper, only reference to prior datasets. Therefore, documentation about dataset creation is not provided in this paper."
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9679,
      "completion_tokens": 141,
      "total_tokens": 9820
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Dataset generation pipeline",
          "Reasoning": "The CodecFake dataset is newly introduced in the paper and is created by re-synthesizing speech from the VCTK corpus using 15 pre-trained neural audio codec models. The original VCTK speech data is human-generated (recorded from human speakers), and the speech is then model-generated through codec-based resynthesis. Therefore, the audio data modality is a mixture of human generated source recordings and model generated synthesized speech, specifically designed to comprise deepfake audios from codec-based speech synthesis systems."
        }
      ]
    }
  },
  {
    "id": "wu24p_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10531,
      "completion_tokens": 226,
      "total_tokens": 10757
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.3 Dataset generation pipeline",
            "reasoning": "The CodecFake dataset was created by encoding real speech from VCTK corpus into discrete codes using pre-trained codec encoders and then re-synthesizing speech using the corresponding codec decoders following an automated pipeline, with no mention of human annotators involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2 and Section 3",
            "reasoning": "The paper does not describe any detailed annotation instructions given to human annotators as the dataset creation is an automatic re-synthesis process without human labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 and Section 3",
            "reasoning": "No scoring rubrics or evaluation criteria designed for human annotation are mentioned, since the process is automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not provide annotation examples or guidelines since the dataset generation is an automatic codec-based re-synthesis; thus no human annotation examples are applicable."
          }
        }
      ]
    }
  },
  {
    "id": "wu24p_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11661,
      "completion_tokens": 299,
      "total_tokens": 11960
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the CodecFake dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the CodecFake dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert human annotators performed any quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using AI models to perform QA on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Dataset generation pipeline",
          "reasoning": "The CodecFake dataset is generated by a fully automatic process of encoding and decoding speech using pre-trained neural audio codec models, ensuring consistent synthesis without human intervention. The data generation uses codec resynthesis from VCTK corpus samples with different codecs, effectively functioning as an automated verification of the dataset correctness by construction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the dataset generation procedure as an automated process, so QA has been applied via automatic verification."
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11279,
      "completion_tokens": 463,
      "total_tokens": 11742
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not created entirely from scratch by humans; rather, it is derived from existing speech data (VCTK corpus) processed by codecs."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2, CodecFake Dataset Generation Pipeline",
          "reasoning": "The CodecFake dataset is generated by resynthesizing existing speech data (VCTK corpus) through 15 pre-trained neural audio codec models. This involves encoding real speech into discrete codes and decoding it back into speech using neural audio codec models, which are machine learning models. Hence, the resulting data is generated by models transforming existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of any translation activity performed by human translators in the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or claim that the dataset involves translating content from one language to another via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset uses existing VCTK speech data, it does not simply aggregate or collect existing recordings; it transforms them via codec resynthesis, indicating modification rather than direct collation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1, Codec-based speech generation; Section 2.3, Dataset generation pipeline",
          "reasoning": "The CodecFake dataset is derived from the original VCTK corpus by applying transformations using pre-trained neural audio codec models to produce resynthesized speech. This process represents modifications and transformations of existing data, creating derived content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11797,
      "completion_tokens": 370,
      "total_tokens": 12167
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 Implementation details; Section 2.3 Dataset generation pipeline",
          "reasoning": "The CodecFake dataset is used to train anti-spoofing models by fine-tuning the AASIST-L pre-trained backbone model with a supervised learning setup, as described in Section 3.1 and 2.3. Models are trained using the CodecFake subsets combined with original VCTK utterances for supervised anti-spoofing training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 Evaluation details; Section 2.3 Dataset generation pipeline",
          "reasoning": "The CodecFake dataset is also used as an evaluation benchmark, particularly its test subsets, to measure the performance of anti-spoofing models against deepfake audios generated by codec-based speech synthesis systems. This usage is described in Section 3.2 and Section 2.3."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 2 CodecFake",
          "reasoning": "The paper performs a comprehensive analysis using the CodecFake dataset to assess the effectiveness of anti-spoofing models when presented with codec-based synthesized speech. This analytical use is indicated in the introduction and in the description of the dataset in Section 2."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12520,
      "completion_tokens": 649,
      "total_tokens": 13169
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.2 and Table 1",
          "reasoning": "The CodecFake dataset is created by resynthesizing speech from the VCTK corpus using 15 different pre-trained neural audio codec models. The training data for these codec models include multiple languages: English (e.g., Librispeech, LibriTTS), Chinese (e.g., AISHELL, zh-en datasets), and potentially others such as Common Voice (which supports multiple languages) and MUSDB, AudioSet, Jamendo, which may include multilingual content. Specifically, codec models F5 and F6 are trained on both English and Chinese (zh-en) datasets, indicating bilingual data, and other datasets used for training codec models include multiple languages. Therefore, the resulting dataset contains entries with more than two human languages. Hence, it is appropriate to classify CodecFake as multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CodecFake dataset contains more than two languages due to the diverse training data of the codecs used, including English, Chinese, and possibly others; thus, the dataset is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset includes English speech from VCTK, the use of codec models trained on Chinese and other language data means the dataset is not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains English and other languages, so it is not exclusively a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio waveforms synthesized from codec-based speech generation, not code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech audio data only."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages of the dataset are documented through the data origins of the codecs and VCTK corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language audio data derived from human languages; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9738,
      "completion_tokens": 192,
      "total_tokens": 9930
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 2; Section 3.1",
          "reasoning": "The paper states in the abstract: 'We will release all code and data,' indicating intent to make the code publicly available. Additionally, in Section 2 and 3.1, the dataset generation and training procedures mention usage of pre-trained codecs and detailed pipeline, implying code for dataset generation exists. Also, a project URL is given as https://codecfake.github.io suggesting code/data availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (CodecFake)",
          "reasoning": "Section 2 describes in detail the dataset creation procedure, design principles, codec adoption, and the dataset generation pipeline, including the choice of base corpus (VCTK), the splitting strategy, and the use of 15 codec models. Hence the dataset creation process is well documented within the paper."
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5892,
      "completion_tokens": 169,
      "total_tokens": 6061
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The new dataset is constructed by the authors, consisting of synthesized single-channel noisy audio samples generated by convolving clean speech from the TIMIT corpus with 108 real-world Room Impulse Responses (RIRs) (32 recorded by the authors and 76 from the EchoThief Impulse Response Library), and adding noise including environmental sounds and pink/babble noise at various Signal-to-Noise Ratios. Hence, the audio modality is present, and its origin is both human generated (speech recordings, recorded RIRs, real noise recordings) and model generated (synthesized noisy audio samples created by convolution and noise addition as part of data synthesis)."
        }
      ]
    }
  },
  {
    "id": "xiang23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6744,
      "completion_tokens": 221,
      "total_tokens": 6965
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The dataset is constructed by synthesizing noisy audio samples using clean speech from TIMIT, various real-world RIRs, and different noise types with added noise at set SNR levels. This process is computational and automatic using existing datasets and simulated noise blends without manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not describe any annotation instructions provided to annotators, as dataset creation was an automatic synthesis process rather than human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No scoring rubrics or rating criteria are described or needed since the ground truth STI values are synthetically computed and directly used as labels."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No annotation examples are provided or necessary due to the dataset being generated via algorithmic synthesis."
          }
        }
      ]
    }
  },
  {
    "id": "xiang23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7874,
      "completion_tokens": 311,
      "total_tokens": 8185
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert annotating or validating the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any single non-expert human annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert humans conducting quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not detail any AI model used specifically for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset is synthetically generated by convolving clean speech with real room impulse responses and adding noise at specified SNR levels. The ground truth STI values are calculated using established acoustic formulas and standard methods such as Schroeder's method, which amounts to an automated computational verification of labels rather than manual annotation. This indicates that the quality assurance is conducted through automatic verification based on formulas and synthesis procedures."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes the dataset generation process clearly with ground truth values computed algorithmically, indicating some form of quality assurance rather than none."
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7492,
      "completion_tokens": 407,
      "total_tokens": 7899
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not created entirely from scratch by human contributors. Instead, it was synthesized using existing resources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset uses 108 real-world room impulse responses (RIRs) collected from offices and the EchoThief Impulse Response Library, along with the TIMIT dataset for clean speech and TUT Acoustic Scenes dataset for noise, all existing sources. These existing components were collected and aggregated for use in the new dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The authors generated the noisy audio samples by convolving clean speech with RIRs and adding noise at various SNR levels, thus transforming and combining existing data to produce a synthesized dataset covering the full range of STI values."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sourcing and generation process of the dataset is clearly specified in Section 4.1."
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8010,
      "completion_tokens": 289,
      "total_tokens": 8299
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset; Section 4.2 Experimental Details",
          "reasoning": "The constructed dataset consisting of over 590,000 synthesized noisy audio samples with corresponding STI ground truth and clean audio is used to train the proposed eSTImate model from scratch. The training is conducted with this dataset, as described in the experimental setup."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 STI Evaluation; Section 4.4 Ablation Study; Section 4.5 Framework Comparison",
          "reasoning": "The dataset is used as a test set with known STI ground truth values to evaluate and benchmark the performance of the proposed model and compare it against previous methods. The test results demonstrate low MAE and RMSE, showing the dataset's role in evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8733,
      "completion_tokens": 416,
      "total_tokens": 9149
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is constructed using speech from the TIMIT dataset, which contains only English sentences from speakers of American English dialects; no mention of any other human languages is given."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as containing two human languages; only English speech from TIMIT is used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset is synthesized using the TIMIT dataset as the clean speech corpus, which consists of English speech samples; no other languages are mentioned. Hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly one non-English language; it exclusively uses English speech from TIMIT."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio recordings with associated STI labels, not programming or structured code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical equations to describe methods and formulations, the dataset itself contains audio data, not mathematical or logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain biological or non-human communication data; it contains human speech audio data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or fictional languages in the dataset; only natural spoken English is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset are clearly specified\u2014the TIMIT dataset contains English speech; thus, language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of human speech audio, so it contains language data."
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5951,
      "completion_tokens": 143,
      "total_tokens": 6094
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper describes the dataset construction process in Section 4.1 but does not provide any links or references to publicly available code repositories for dataset construction or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "Section 4.1 provides a detailed description of how the dataset was constructed, including source datasets (TIMIT for clean speech, TUT Acoustic Scenes for noise), RIRs used, noise types and SNR levels, and the overall size and distribution of the synthesized dataset, thus documenting the dataset creation process adequately."
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7482,
      "completion_tokens": 104,
      "total_tokens": 7586
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection",
          "Reasoning": "The SSBPR dataset consists of 7570 snoring audio recordings collected from 20 adult patients during overnight polysomnography at a sleep medicine center, recorded using a subminiature lavalier microphone placed 3cm from the mouth. This indicates the audio data were directly recorded from human patients using human-operated equipment."
        }
      ]
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8334,
      "completion_tokens": 229,
      "total_tokens": 8563
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "Annotations of snoring sounds with sleep body position were performed by three experienced experts: a certified technician and two certified doctors with RPSGT certification, scoring PSG signals including sleep stages, body position, and video signals."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The annotation process followed standard scoring rules from the AASM Manual V2.6 for sleep stages and combined multiple PSG signals and video for body position labeling, indicating the presence of detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The paper does not mention any specific scoring rubrics or numerical rating systems used for annotation beyond following AASM standards and sensor data interpretation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "No explicit annotation examples or detailed annotated samples are provided within the paper to illustrate the annotation guidelines or process."
          }
        }
      ]
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9464,
      "completion_tokens": 337,
      "total_tokens": 9801
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Annotation",
          "reasoning": "The dataset annotation process involves sleep stages scored by three experienced experts according to the AASM Manual V2.6. Specifically, a certified technician performed the initial scoring, and two certified doctors (all RPSGT certified) verified the scoring of PSG data, indicating multiple human experts were involved in the quality assurance of annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The AI model (AST) is used for baseline classification experiments, not for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an automated verification or algorithmic process used for quality assurance of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a detailed human expert annotation and verification process; thus, quality assurance is present and documented."
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9082,
      "completion_tokens": 435,
      "total_tokens": 9517
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection and Section 2.2 Data Annotation",
          "reasoning": "The SSBPR dataset was collected from 20 adult patients undergoing overnight polysomnography (PSG) at a sleep medicine center. Snoring audio was recorded directly via a subminiature lavalier microphone placed near patients' mouths during natural sleep. The data was annotated by experienced certified technicians and doctors based on simultaneous PSG signals including EEG, EOG, EMG, body position sensors, and video monitoring. This indicates the data is originally collected and annotated by human experts from scratch, not adapted or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any part of the dataset was generated by AI or machine learning models. The dataset is based entirely on real clinical recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation from other languages in the creation of the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used in the dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely an aggregation of pre-existing data sources; it involves original data collection and annotation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not based on existing datasets with modifications; instead it is newly collected clinical data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the source and data collection methods of the dataset."
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9600,
      "completion_tokens": 273,
      "total_tokens": 9873
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3. Baseline Method",
          "reasoning": "The paper describes training a baseline model (AST) from scratch on the SSBPR dataset for sleep body position classification using supervised learning with snoring sound spectrograms as inputs."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 Results",
          "reasoning": "The dataset is used to evaluate the accuracy of the baseline model in classifying sleep positions based on snoring sounds assessed by accuracy metrics reported in Table 2 and confusion matrix in Figure 3."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 Results and Section 4 Conclusions",
          "reasoning": "The authors analyze classification accuracy differences across genders and sleep positions, and discuss potential pathophysiological reasons and dataset characteristics affecting performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10323,
      "completion_tokens": 457,
      "total_tokens": 10780
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of snoring audio recordings and associated labels; the paper does not mention the dataset containing any programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical symbolic expressions are present in the dataset entries; the data are audio recordings labeled by sleep body position."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection and Abstract",
          "reasoning": "The dataset contains recorded snoring sounds, which are biological signals produced by human physiology during sleep. These are sounds generated by human biological processes, thus qualifying as biological communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes no fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's content and nature are clearly described as snoring sounds collected from human subjects; language categorization is not ambiguous."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset entries consist of audio recordings of snoring sounds representing biological signals, not language content or text. Hence, the dataset does not contain any human language."
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7541,
      "completion_tokens": 185,
      "total_tokens": 7726
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4 Conclusions, final paragraph",
          "reasoning": "The paper states that the dataset will be released at https://github.com/xiaoli1996/SSBPR but does not mention any release of the code used for data collection, preprocessing, or dataset construction. There is no explicit indication or link provided for the code repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Datasets (particularly subsections 2.1 Data Collection and 2.2 Data Annotation)",
          "reasoning": "The paper provides detailed documentation about the dataset creation process, including information on subject recruitment (20 adult patients), ethics approval and consent, recording setup (microphone type, placement, sampling rate), annotation methodology using synchronized PSG signals and expert scorers, and data distribution details. These details offer transparency and completeness regarding dataset creation."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6926,
      "completion_tokens": 153,
      "total_tokens": 7079
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3, Abstract",
          "Reasoning": "The FakeSound dataset is a new dataset introduced by the authors in this paper for the deepfake general audio detection task. It consists of general audio clips derived from the AudioCaps dataset (human recorded audio) which are then manipulated via an automated pipeline. The pipeline uses models such as AudioLDM1/2 and AudioSR to generate deepfake audio segments by inpainting masked regions. Thus, the dataset comprises both human-generated original audio and model-generated manipulated audio segments, explicitly created for model training and evaluation as detailed throughout Section 2 and Section 2.3."
        }
      ]
    }
  },
  {
    "id": "xie24d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7778,
      "completion_tokens": 226,
      "total_tokens": 8004
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2. FakeSound: Deepfake General Audio Dataset",
            "reasoning": "The dataset FakeSound is constructed using an automated manipulation pipeline consisting of grounding, masking, regeneration, and replacement steps, which generate deepfake audio without human annotation involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2. FakeSound: Deepfake General Audio Dataset",
            "reasoning": "The paper describes the automated pipeline for dataset construction but does not mention any manual annotation instructions provided for humans annotating the data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 and 5.2 Subjective Evaluation",
            "reasoning": "No scoring rubrics or formal evaluation guidelines for annotators are described in the paper for dataset annotation, only subjective human evaluation for model testing is mentioned separately."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 and elsewhere",
            "reasoning": "No annotation examples or guidelines for dataset construction are provided; the dataset is algorithmically generated via the manipulation pipeline without human labeling examples."
          }
        }
      ]
    }
  },
  {
    "id": "xie24d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8908,
      "completion_tokens": 383,
      "total_tokens": 9291
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset. The process is automated and no expert review is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any single non-expert human involvement for quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although 10 human evaluators performed subjective evaluation on the dataset, this was for testing detection difficulty rather than quality assurance of dataset annotations or content. Hence it does not count as QA of dataset annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify an AI model being used as a judge or reviewer for quality assurance of the dataset itself."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 (FakeSound: Deepfake General Audio Dataset) and Section 2.1 and 2.2",
          "reasoning": "The FakeSound dataset is created using an automated manipulation pipeline consisting of an audio-text grounding model to locate key segments, masking of those segments, and generative models (AudioLDM1/2 and AudioSR) to regenerate and replace those segments. This pipeline largely avoids human involvement and relies on algorithmic and model-based generation for dataset creation, implying the dataset quality is ensured through automated processes rather than human annotation verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described as an automatic pipeline for generating and manipulating audio samples for the dataset, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8526,
      "completion_tokens": 449,
      "total_tokens": 8975
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by human contributors. Instead, it leverages an existing dataset (AudioCaps) as a base."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 2.3",
          "reasoning": "The FakeSound dataset is generated by applying an automated manipulation pipeline that uses AI models, including AudioLDM1/2 (inpainting generative models) and AudioSR (super-resolution), to regenerate masked audio segments. Thus, significant portions of the final dataset are newly generated by AI models rather than being purely sourced from existing recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of translation from another language involving human translators in the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of use of machine translation systems to generate or transform data for this dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the original data used for manipulation is from AudioCaps, the resulting dataset is not simply collated but significantly manipulated by generation models."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 2.2, Section 2.3",
          "reasoning": "The dataset is derived from existing audio data (AudioCaps). The key segments are detected and masked based on grounding models, then replaced with model-generated inpainted audio segments, effectively modifying and transforming the original audio data to create deepfake samples."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and generation method are clearly documented throughout the paper."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9044,
      "completion_tokens": 510,
      "total_tokens": 9554
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2, Section 5.1",
          "reasoning": "The FakeSound dataset is used as the primary training data for developing a deepfake general audio detection model. Section 2 describes dataset construction and manipulation for training, and Section 5.1 details the training setup using the dataset, indicating it is used to train the detection model from its initialized state (except for the frozen feature extractor)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the use of the dataset for fine-tuning a pre-trained model in a supervised manner. The proposed model uses frozen pre-trained feature extractor but is trained with FakeSound from scratch for the detection layers."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training methods using the FakeSound dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.3, Section 6",
          "reasoning": "FakeSound includes several test sets (Test-Easy, Test-Hard, Test-Zeroshot) explicitly designed for comprehensive evaluation of deepfake detection models, as described in Sections 2.3 and 6. The model and human evaluations use these test sets to benchmark performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analyzing trends or characteristics outside of training and evaluation contexts."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation in this work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The FakeSound dataset is clearly used for training and evaluation of a deepfake detection model, thus it has practical utility described in the paper."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9767,
      "completion_tokens": 567,
      "total_tokens": 10334
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper describes the FakeSound dataset as manipulated general audio samples from AudioCaps, which contains audio captions in English. There is no indication of multiple human languages involved."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention in the paper of the FakeSound dataset including exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.3 Dataset Metadata",
          "reasoning": "The dataset uses AudioCaps as the source dataset, which contains English captions and associated English language descriptions. No indication of other spoken human languages is given. Hence, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No non-English monolingual language content is mentioned in the paper with respect to the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of audio data and includes no entries of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper contains mathematical formulas in the methodology and evaluation metrics, the dataset entries themselves do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset comprises general audio manipulated for deepfake detection, not biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The linguistic content of the dataset entries is clearly documented as based on English audio captions and audio clips. Language is not unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are audio content which includes human language (English) and other environmental sounds. Therefore, it is not without any language."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6985,
      "completion_tokens": 227,
      "total_tokens": 7212
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.2 and 2.3 (paragraphs about the manipulation pipeline and dataset metadata) and footnote with GitHub URL",
          "reasoning": "The paper states in Section 2.2 that the FakeSound dataset along with the training and evaluation code is available at https://github.com/FakeSoundData/FakeSound, indicating that the code used for data collection, preprocessing, and generation is publicly accessible. Hence, the code repository is linked explicitly for dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (FakeSound: Deepfake General Audio Dataset)",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 2, including an explanation of the manipulation pipeline (grounding, masking, regeneration, and replacement), the models used (AudioLDM1/2, AudioSR), dataset metadata (Table 1 with training and test splits), and how the manipulated segments are generated and limited. This thorough description covers dataset generation methodology, enabling reproducibility and comprehension of dataset construction."
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11468,
      "completion_tokens": 131,
      "total_tokens": 11599
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Laughter data collection",
          "Reasoning": "The paper introduces a new large-scale in-the-wild Japanese laughter corpus composed of about 3.5 hours of laughter utterances collected from online videos featuring human speakers (casters and YouTubers). The data consists of single-speaker laughter recordings obtained by manual segmentation and denoising from these videos, thus it originates from human recorded audio."
        }
      ]
    }
  },
  {
    "id": "xin23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12320,
      "completion_tokens": 214,
      "total_tokens": 12534
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2",
            "reasoning": "Section 2 describes that about 1500 crowd-sourcing workers labeled videos into categories (single-speaker laughter, multi-speaker laughter, others). This indicates multiple human non-expert annotators performed the labeling task through crowdsourcing."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "The labeling task involved providing workers with three categories to label videos: single-speaker laughter, multi-speaker laughter, and others. This implies instructions were given to workers for consistent annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention any scoring rubrics or criteria beyond the three category labels for the crowd-sourcing annotation tasks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not describe providing annotation examples or exemplar videos to guide annotators during the labeling process."
          }
        }
      ]
    }
  },
  {
    "id": "xin23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13450,
      "completion_tokens": 366,
      "total_tokens": 13816
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple expert human annotators performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided that a single non-expert human annotator conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2: Laughter data collection",
          "reasoning": "The paper states that about 1500 crowd-sourcing workers labeled detected videos for single-speaker laughter, multi-speaker laughter, or others. These workers are likely non-experts, and multiple annotators were involved in labeling for quality assurance of the laughter data."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2: Laughter data collection",
          "reasoning": "The paper uses an open-sourced pretrained laughter detection model to automatically discover videos potentially containing laughter as an initial step; this constitutes AI model based quality assurance to filter candidate videos."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2: Laughter data collection",
          "reasoning": "They used the pretrained Demucs v3 source separation model to extract vocals and reduce noise in the utterances, representing an automatic process applied to the data to improve quality. Additionally, automatic crawling and filtering steps used algorithmic approaches."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents quality assurance steps including crowd-sourced labeling, AI model detection, and automatic source separation."
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 13068,
      "completion_tokens": 463,
      "total_tokens": 13531
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Laughter data collection",
          "reasoning": "The paper describes the creation of a large-scale in-the-wild Japanese laughter corpus collected from videos on YouTube. The laughter utterances were manually segmented after crowdsourced human labeling to identify single-speaker laughter. This process indicates that the laughter data is original content created by human contributors, not derived from existing datasets or translations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The laughter corpus itself is not generated by AI or machine learning models; they used models only for processing and representation, not for creating the original laughter data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the laughter data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the laughter data was generated through machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 Laughter data collection",
          "reasoning": "The authors collected data by crawling existing videos from YouTube, which is an aggregation of existing public content. They used an open-sourced laughter detection model and human labeling to select relevant utterances but did not modify the original laughter utterances extensively beyond segmentation and noise reduction."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Laughter data collection",
          "reasoning": "The laughter utterances underwent processing such as source separation (using Demucs model) to reduce noise and isolate vocals. This processing represents a transformation of the collected original videos, thus the final corpus is derived from existing source videos with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly described and documented."
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13586,
      "completion_tokens": 280,
      "total_tokens": 13866
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The proposed large-scale in-the-wild Japanese laughter corpus is used to train the text-to-speech model from scratch using pseudo phonetic tokens (PPTs) derived from the corpus. The TTS model is trained on this dataset (Section 4.1) to synthesize laughter, as described in Section 3.2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The corpus is split into train, validation, and test sets, with the test set used exclusively for evaluation of the laughter synthesis models (Section 4.2). Objective and subjective evaluations are conducted on the test utterances from the corpus."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14309,
      "completion_tokens": 496,
      "total_tokens": 14805
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset contains only Japanese laughter utterances collected from Japanese speakers, and there is no indication that multiple languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only one language (Japanese), so it is not bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not English-only; it consists of Japanese laughter utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 Laughter data collection",
          "reasoning": "The proposed corpus is described as an in-the-wild Japanese laughter corpus collected from YouTube videos, with non-Japanese videos discarded during preprocessing. Thus, all entries are Japanese laughter utterances."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio laughter utterances without code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio laughter data, no mathematical or logical symbolic notations are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human laughter audio; there is no biological sequence data or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included; dataset is human Japanese laughter."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly stated and documented as Japanese laughter."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human laughter speech recordings in the Japanese language, so it contains language."
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11527,
      "completion_tokens": 241,
      "total_tokens": 11768
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 2",
          "reasoning": "The paper states in the abstract: 'We publicate the proposed corpus and the code implementation of the proposed method.' Also, at the bottom of page 2, footnotes provide the URLs (sites.google.com and github.com) where both the corpus and code are publicly available. Although the code repository likely includes the code for the proposed laughter synthesis method, it is reasonable to conclude the code related to dataset collection and preprocessing is publicly provided to enable reproducibility given the explicit mention and links."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Laughter data collection",
          "reasoning": "Section 2 provides a detailed description of the dataset creation process, including data sources (YouTube videos of casters and YouTubers), crawling procedures, use of a pretrained laughter detection model, crowd-source labeling to filter for single-speaker laughter, manual segmentation criteria, denoising with a pretrained Demucs v3 model, and statistics on the final corpus size and speaker count. This documentation is sufficiently transparent and detailed to enable understanding and reproduction of the data collection protocol."
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8776,
      "completion_tokens": 127,
      "total_tokens": 8903
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Datasets",
          "Reasoning": "The paper introduces a self-recorded dataset consisting of 1,097 crying sounds collected by recruiting new mothers as volunteers who recorded and annotated babies' cries using mobile phones in indoor environments. This data is clearly audio modality, human-recorded and annotated, thus Human Generated is true, Model Generated is false, and origin is known."
        }
      ]
    }
  },
  {
    "id": "xu22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9628,
      "completion_tokens": 226,
      "total_tokens": 9854
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "The self-recorded dataset was collected and annotated by new moms who volunteered to record and label the babies' cries based on their own experience. The paper notes these moms lack experience, indicating they are non-experts, and multiple moms contributed, indicating multiple annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper states that moms labeled the sounds based on their own experience and mentions a lack of experience, but does not describe providing any detailed annotation instructions or protocols."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no indication in the paper that rubrics or specific scoring criteria were provided to the annotators for labeling the self-recorded dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention providing examples to support the annotation process for this self-recorded dataset."
          }
        }
      ]
    }
  },
  {
    "id": "xu22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10758,
      "completion_tokens": 370,
      "total_tokens": 11128
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the annotations for the self-recorded dataset were done by a single expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The CRIED database annotations were performed by two experts in the field of early speech-language development, indicating quality assurance from multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a single non-expert annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The self-recorded dataset was annotated by several new moms as volunteers without specific expertise, implying multiple human non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic quality assurance or verification process applied to the datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are clearly described for both datasets; thus, this label does not apply."
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10376,
      "completion_tokens": 414,
      "total_tokens": 10790
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 (Datasets)",
          "reasoning": "The paper states that the authors recruited new mothers as volunteers to record and annotate new babies' cry sounds using mobile phones in indoor environments, resulting in a self-recorded dataset consisting of 1097 crying sounds. This indicates original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any synthetic data or data generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any machine translation used in producing any datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses the existing CRIED database, it is a pre-existing dataset and not newly introduced by the authors. There is no indication the authors aggregated or collected data from multiple existing sources to create a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The differential time-frequency log-Mel spectrogram features are derived transformations of audio signals, but these are feature extractions and transformations applied to datasets rather than new datasets themselves."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and generation method of the self-recorded dataset are explicitly described; therefore, the origin is documented and does not qualify for N/A."
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10894,
      "completion_tokens": 333,
      "total_tokens": 11227
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3 Results, Table 2",
          "reasoning": "The self-recorded dataset is used to train the infant cry recognition model from scratch without indication of pre-training on this dataset. The experimental results in Section 3.3 show training and evaluating the model on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 Results, Table 2",
          "reasoning": "The paper reports that fine-tuning the vision transformer model pretrained on ImageNet using the CRIED and self-recorded datasets achieves better performance, indicating these datasets are used for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 Results, Table 2 and Table 3",
          "reasoning": "The proposed self-recorded dataset is used to evaluate and validate the effectiveness of the proposed infant cry recognition method alongside the standard CRIED dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 Results, Figure 2 and accompanying text",
          "reasoning": "The self-recorded dataset is used for analyzing classification performance patterns such as confusion among classes, which informs understanding of infant cry recognition challenges."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11617,
      "completion_tokens": 623,
      "total_tokens": 12240
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The proposed self-recorded dataset and the CRIED dataset contain infant cry vocalizations without mention or indication of multiple human languages. The labeling is based on cry types or needs, not language."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not involve exactly two human languages; the cry data is unlabeled by language, focusing on infant cries and not spoken language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The datasets are related to infant cries, which do not contain spoken English content or words; thus, they are not monolingual English datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The datasets contain infant cry sound recordings without linguistic content or spoken non-English language. The babies' cries are not linguistically classified."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets consist of audio recordings of infant cries with labels about crying type or need, with no programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the paper uses mathematical notation to describe features and models, the datasets themselves do not contain mathematical or logical symbolic data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The proposed self-recorded dataset and the CRIED dataset contain infant cry vocalizations, which are biological vocal signals, representing non-linguistic communication by infants, classified based on different expressed needs or emotions."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not include any fictional or artificially created languages; only natural infant cry sounds are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The nature of the dataset is clearly specified as infant cry recordings; the language content is inherently non-linguistic infant vocalization, so it is not unknown but non-linguistic."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains vocalizations that represent infant communication via cries, which are forms of biological communication, thus not language-less."
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8835,
      "completion_tokens": 171,
      "total_tokens": 9006
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Datasets and elsewhere",
          "reasoning": "The paper mentions a self-recorded infant cry dataset collected from volunteers recording babies' cries with mobile phones, but no code repository or link is provided for data collection, preprocessing, or annotation code related to this dataset. No details about sharing the code or tools used for data collection are given."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The paper describes the self-recorded dataset including number of samples, classes, recording conditions (mobile phones in indoor environments), sample rates, and annotation process based on moms' labeling experience, which provides some documentation on dataset creation. However, details about exact collection protocol, consent, or ethical approvals are not mentioned."
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6529,
      "completion_tokens": 127,
      "total_tokens": 6656
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.2",
          "Reasoning": "The paper introduces the VoxTube dataset consisting of over 4 million utterances from more than 5,000 speakers extracted from Creative Commons licensed YouTube videos. The data consists exclusively of audio segments extracted from these videos. These audios originate from human speech recorded in videos, thus are human generated recordings. The paper clearly states the audio extraction and processing pipeline, emphasizing audio only data and no use of visual information from videos."
        }
      ]
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7381,
      "completion_tokens": 199,
      "total_tokens": 7580
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2",
            "reasoning": "The labeling of the VoxTube dataset is performed by a fully automated unsupervised approach that uses pre-trained speaker recognition models and clustering methods (Section 3.2). There is no mention of human annotators being involved in labeling the dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper describes an automated pipeline for data collection and labeling without mentioning any instructions for human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "Since the annotation is performed automatically without human assessors, there are no scoring rubrics provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not provide any annotation examples since the data labeling is automated without human guidelines or examples."
          }
        }
      ]
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8511,
      "completion_tokens": 346,
      "total_tokens": 8857
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "Quality assurance is performed through an automated, unsupervised method that uses a pre-trained speaker recognition AI model to extract embeddings for audio segments. The filtering and clustering procedures to decide the predominant speaker per channel are algorithmic and leverage the AI model's outputs as the basis for selecting valid utterances, effectively replacing human annotation or verification."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The filtration process involves automatic, rule-based clustering techniques (Hierarchical Agglomerative Clustering) applied to the embeddings extracted by the AI model. This automated verification helps ensure the dataset contains segments from predominant speakers without manual intervention, constituting an automated quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes an automated quality assurance process involving AI model-based embedding extraction and clustering; hence quality assurance is documented and performed."
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8129,
      "completion_tokens": 436,
      "total_tokens": 8565
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The VoxTube dataset is collected from existing YouTube videos with CC BY licenses and not created entirely from scratch by human contributors. The data is not newly recorded or manually crafted speech content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not represent content generated by AI or machine learning models; rather, it consists of speech recordings from real-world sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data involves human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation process used to create the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.2",
          "reasoning": "The VoxTube dataset is collected by aggregating audio data from multiple existing YouTube videos under CC BY licenses. The collection pipeline extracts audio and associated embeddings but does not create new speech data; the data is collected from existing open-source media."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2, 3.3",
          "reasoning": "The dataset undergoes an automated unsupervised labeling process involving clustering, embedding extraction with a pre-trained speaker recognition model, filtering, and duplicate removal. These are transformations and adaptations applied on the collected raw audio data, making the dataset derived from the original YouTube sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly described in the paper sections detailing collection and filtration."
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8647,
      "completion_tokens": 238,
      "total_tokens": 8885
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.3.3 (Training), Section 5 (Results)",
          "reasoning": "The VoxTube dataset is used to train speaker recognition models from scratch with a ResNet48 architecture, as described in Section 4.3.3 where training setups are given, and Section 5 where the trained models using VoxTube data show performance results. The models are trained from scratch using VoxTube data alone as well as in combination with VoxCeleb2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9370,
      "completion_tokens": 439,
      "total_tokens": 9809
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Description",
          "reasoning": "The VoxTube dataset contains utterances pronounced in more than 10 languages, including English, Russian, Spanish, and Portuguese among others. This is explicitly mentioned in the abstract and Section 3.1, indicating the dataset includes multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech audio data and does not include programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on audio utterances for speaker recognition and does not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human speech audio and does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly reported and detailed."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain spoken human languages."
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6588,
      "completion_tokens": 194,
      "total_tokens": 6782
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3 (Description and Collection pipeline) and Footnote 1",
          "reasoning": "The paper provides a URL (https://github.com/IDRnD/VoxTube/) where the VoxTube dataset can be downloaded, indicating availability of data and possibly the related code. The description in Section 3 outlines the data collection and filtering pipeline in sufficient detail, suggesting that code related to data collection and preprocessing is available at the given project webpage/repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Description and Collection pipeline and Filtration subsections)",
          "reasoning": "The paper thoroughly documents the dataset creation process, including metadata filtering, audio extraction, embedding extraction using a pre-trained model, hierarchical clustering filtration steps, duplicates removal, and statistics of the dataset collected. The detailed step-wise description provides transparency and completeness about the dataset creation, which supports reproducibility."
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7422,
      "completion_tokens": 157,
      "total_tokens": 7579
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The new dataset's audio involves speech segments narrated by two standard Mandarin speakers (human recorded) and two synthesized speech streams generated by shifting the fundamental frequency of the original speech using Adobe Audition software, thus comprising both human generated and model generated audio data."
        },
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The dataset includes EEG recordings collected from human subjects via 64-channel EEG amplifier during the auditory attention task, resulting in time series data that are human generated (recorded from human participants)."
        }
      ]
    }
  },
  {
    "id": "yan24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8274,
      "completion_tokens": 267,
      "total_tokens": 8541
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.2, 2.3",
            "reasoning": "The EEG data and auditory attention labels are collected from human subjects participating in the experiment, with focus instructed to a target speaker among four talkers. The subjects are clearly human experimental participants serving as annotators of attention labels during EEG recording."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Section 2.2 describes that before stimulus presentation, subjects were cued which talker to attend, and received visual feedback and relevant instructions on a monitor, which indicates the presence of detailed annotation instructions for the attentive focus during data collection."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit rubric description found",
            "reasoning": "The paper does not provide any information about scoring rubrics or explicit criteria used by annotators beyond instructing subjects to attend one talker. The attention labels derive from experimental conditions rather than subjective human annotation scored by rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation guidelines described",
            "reasoning": "The paper does not mention providing annotation examples or illustrative cases for subjects performing attention tasks; the attention selection is guided by instructions rather than annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "yan24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9404,
      "completion_tokens": 328,
      "total_tokens": 9732
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted specifically by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving multiple human experts or annotators for validating the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that a single non-expert conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert humans performing quality assurance on the newly introduced dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for decoding tasks on the dataset, the paper does not describe AI models performing quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.5 (TRFs estimation), Section 2.6 (Speech stimulus reconstruction)",
          "reasoning": "The data preprocessing and analysis involved automated computational methods such as temporal response functions estimation using existing toolboxes (mTRF toolbox) and stimulus-reconstruction with ridge regression and cross-validation, indicating an automated verification process rather than manual annotation or review. This constitutes an automated quality assurance process verifying dataset content through algorithmic techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents automated processing and analysis methods applied as quality assurance; thus, quality assurance was indeed described."
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9022,
      "completion_tokens": 466,
      "total_tokens": 9488
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper states that the new AAD database was created by recording EEG data from human subjects attending to four-talker speech scenarios constructed using speech segments narrated by human speakers and some pitch-shifted versions. The stimuli were based on narrations from the book 'Twenty Thousand Leagues under the Sea' read by human speakers, with pitch shifting applied to create variations. The EEG data was recorded from human listeners under controlled experimental conditions. Thus, the data (EEG responses and stimuli combinations) were newly created by human contributors from scratch, not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not simply aggregated or collected from existing sources without modifications; it involved recording new EEG data under experimental conditions."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "Although the speech stimuli were based on original narrations, two of the four talkers' speeches were created by shifting the fundamental frequency (F0) of the original speech with speech synthesis software, which is a form of data derived by transformation of existing speech data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9540,
      "completion_tokens": 443,
      "total_tokens": 9983
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.8 and 3.3",
          "reasoning": "The newly introduced four-talker EEG database is used to train deep neural network models (e.g., DenseNet-3D) from scratch for auditory spatial attention detection (ASAD). Section 2.8 describes the training of models on the new dataset using 5-fold cross-validation, and Section 3.3 reports the performance results, indicating the dataset's use for model training from random initialization."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.8 and 3.3",
          "reasoning": "The DenseNet-3D model is initialized (fine-tuned) by bootstrapping weights from a pre-trained DenseNet-2D model prior to training on the new four-talker EEG dataset. This supervised fine-tuning approach uses the dataset to adapt a pre-trained model to the four-talker scenario, as described in Sections 2.8 and 3.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.2",
          "reasoning": "The new four-talker EEG dataset is used to evaluate and benchmark the performance of multiple ASAD models and stimulus reconstruction methods. Section 3.2 reports decoding accuracy for stimulus reconstruction, while Section 3.3 presents ASAD model accuracies, indicating the dataset's role in performance evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is used to analyze neural responses via temporal response functions (TRFs), comparing attended and unattended speech representations. Section 3.1 details this analysis, highlighting the dataset\u2019s role in investigating cortical processing characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10263,
      "completion_tokens": 547,
      "total_tokens": 10810
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset only involves speech stimuli in one language, Mandarin Chinese, with multiple talkers but no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech stimuli from speakers all speaking in Mandarin Chinese, not two different languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The speech corpus used is a Chinese translation of the book 'Twenty Thousand Leagues under the Sea', and the speakers are standard Mandarin speakers, indicating the dataset is not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The dataset uses speech stimuli from four talkers all speaking Mandarin Chinese (a non-English language). This is explicitly stated in the description of audio materials."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the implementation code is available on Github, the dataset itself does not contain entries of programming languages but rather EEG and audio data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Mathematical expressions and notation appear in the paper describing methods, but the dataset entries themselves consist of speech and EEG signals, not symbolic mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech and EEG signals, rather than biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language and speech content are clearly specified as Mandarin Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes natural human language speech content."
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7481,
      "completion_tokens": 128,
      "total_tokens": 7609
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "End of Section 1 (Introduction)",
          "reasoning": "The paper explicitly states that both the implementation code and the new four-talker database are available on public repositories, providing URLs to GitHub and Zenodo."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper provides a thorough description of the data collection environment, experimental procedure, stimuli, EEG data acquisition and processing, speech envelope extraction, and stimulus reconstruction. The detailed explanation of the experimental setup and preprocessing procedures indicates comprehensive documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10412,
      "completion_tokens": 177,
      "total_tokens": 10589
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 4 Experimental results; Section 5 Conclusions",
          "Reasoning": "The authors explicitly introduce a new large-scale dataset called Audioset-TSD based on the Audioset dataset for target sound detection (TSD). Audioset is a human-curated audio event dataset, and the TSD version involves audio clips annotated for target sounds within mixtures. The data modality is audio since it deals with sound event detection and reference/mix audio clips. The dataset is human generated because it is derived from Audioset, which is based on human-labeled audio data. The paper clearly states that they establish a new dataset called Audioset-TSD in addition to using the pre-existing URBAN-TSD."
        }
      ]
    }
  },
  {
    "id": "yang22e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11264,
      "completion_tokens": 202,
      "total_tokens": 11466
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2, Conclusions",
            "reasoning": "The paper states that the Audioset-TSD dataset is established based on Audioset, with no indication of manual annotation by human labelers. The dataset construction appears to be automatic or semi-automatic, relying on existing labels or heuristics; no human annotation process is described."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not describe any annotation guidelines or instructions for human annotators for the Audioset-TSD dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention of scoring rubrics or criteria for annotation in the paper for the Audioset-TSD dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide any annotation examples or detailed annotation guidelines for the dataset construction."
          }
        }
      ]
    }
  },
  {
    "id": "yang22e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12394,
      "completion_tokens": 301,
      "total_tokens": 12695
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human expert conducted quality assurance on the newly introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human experts performed quality assurance on the datasets UrbanSound-TSD or Audioset-TSD."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that a single human non-expert performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model to perform quality assurance on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process applied to the new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces two datasets (UrbanSound-TSD and Audioset-TSD) but does not describe any quality assurance process for the annotations or content of these datasets. No information is provided about human or machine verification, expert involvement, or annotation validation. Hence, no documented QA process is present."
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12012,
      "completion_tokens": 396,
      "total_tokens": 12408
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental results and Abstract",
          "reasoning": "The paper establishes a new dataset named Audioset-TSD based on the existing Audioset dataset (reference [3]). This indicates that the dataset is collected or aggregated from an existing source (Audioset) without evidence of original content creation from scratch. Similarly, the URBAN-TSD dataset is based on UrbanSound dataset [18] which is also a prior dataset. Hence, the datasets introduced are collated from pre-existing datasets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental results and Abstract",
          "reasoning": "The new datasets (URBAN-TSD and Audioset-TSD) are built based on existing datasets (UrbanSound and Audioset). Though they are based on existing sources, the paper mentions that they are established (implying some modifications, such as labeling for target sound detection task). Therefore, these datasets are derived from existing datasets with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and preparation of the datasets are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12530,
      "completion_tokens": 339,
      "total_tokens": 12869
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 (Experimental results)",
          "reasoning": "The authors use the new Audioset-TSD dataset to train their RaDur model from scratch for target sound detection, as shown in the experimental comparisons with TSDNet. The dataset is used directly to train the detection model with no indication of prior pre-training on other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Experimental results)",
          "reasoning": "The new datasets URBAN-TSD and Audioset-TSD are used to evaluate and benchmark the performance of the proposed RaDur method against previous approaches by reporting segment-based and event-based F-scores and average precision. This confirms their role in evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Experimental results) and Section 4.3 (Ablation studies)",
          "reasoning": "The URBAN-TSD and Audioset-TSD datasets are used to analyze the effect of event duration on detection performance and to conduct ablation studies on hyper-parameters, demonstrating the datasets' use for detailed analytical purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13253,
      "completion_tokens": 588,
      "total_tokens": 13841
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces two new datasets: URBAN-TSD and Audioset-TSD, both based on existing UrbanSound and Audioset datasets, respectively. There is no mention or indication that these datasets contain data in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the proposed datasets have exactly two human languages. The datasets pertain to sound event detection, not multilingual language data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental setup and evaluation metrics",
          "reasoning": "The datasets are based on UrbanSound and Audioset, which are large-scale audio event datasets primarily collected and labeled in English contexts. Given no mention of other human languages and the use of English references (e.g., references to UrbanSound, Audioset, and sound labels like 'chop', 'bouncing'), the datasets can be considered monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any non-English language exclusive datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of audio recordings and labels related to sound events; no code or programming language data entries are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets themselves contain audio data and annotations, not mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the datasets focus on environmental sound events, they do not specifically include biological sequences or non-human communication such as DNA or chemical signaling."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages are involved in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets entries are explicitly known or implied to be English in context."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language content in the form of sound event labels and annotations, thus not applicable to 'N/A' category."
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10471,
      "completion_tokens": 193,
      "total_tokens": 10664
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 5, Conclusions; Footnote 1 at the end of the paper",
          "reasoning": "The paper explicitly states in the Conclusions section that the source code and dataset of this work have been released, and provides a GitHub URL (https://github.com/yangdongchao/RaDur) indicating public availability of the code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset and Experimental Setup paragraphs and throughout Section 4 (Experiments)",
          "reasoning": "The paper documents the dataset creation process by explaining that they built two datasets: URBAN-TSD (based on UrbanSound) and Audioset-TSD (based on Audioset). They describe the datasets' construction and refer to prior datasets upon which these were based. Though details are relatively brief, the dataset curation and processing are described in the Experiments section."
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7097,
      "completion_tokens": 136,
      "total_tokens": 7233
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4 Experiments; Footnote 1",
          "Reasoning": "The paper introduces a new ASR error correction dataset by applying internal ASR and Text-to-Speech (TTS) engines to a single-turn dialogue dataset called TOP. This process involves human-generated original text data (the TOP dataset) and model-generated ASR outputs (via internal ASR and TTS systems). Thus, the dataset consists of text modality data combining human-generated original transcriptions and model-generated ASR transcriptions for ASR error correction tasks."
        }
      ]
    }
  },
  {
    "id": "yang22g_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7949,
      "completion_tokens": 255,
      "total_tokens": 8204
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 (Experiments)",
            "reasoning": "The paper states that they use their own commercial ASR and TTS engines to synthesize the new ASR error correction dataset from the existing TOP single-turn dialogue dataset. This data generation involves automated TTS and ASR processes to convert text to audio and back to text, simulating ASR errors automatically rather than using human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4 (Experiments)",
            "reasoning": "No explicit description or mention of detailed instructions or annotation guidelines for any human annotators is provided, as data is generated automatically via TTS and ASR pipelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4 (Experiments)",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria for manual annotation related to the new dataset construction. The data is synthesized automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4 (Experiments)",
            "reasoning": "No examples or annotation samples are provided for human annotation guidelines because the dataset is generated via automated ASR and TTS pipelines without manual human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "yang22g_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9079,
      "completion_tokens": 203,
      "total_tokens": 9282
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance procedures or validation steps taken to ensure the correctness or quality of the newly introduced ASR error correction dataset derived from the TOP dataset with internal ASR and TTS engines. There is no mention of human annotation, expert review, multiple annotators, automated verification, or AI-based validation. Thus, no QA process is documented for the dataset introduced by the authors."
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8697,
      "completion_tokens": 378,
      "total_tokens": 9075
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset was created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the dataset was created by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset produced by machine translation is mentioned."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is not described as merely collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4, paragraph 1 and 5 Conclusions",
          "reasoning": "The paper states that the authors apply internal ASR and Text-to-Speech (TTS) engines to a single-turn dialogue dataset (TOP) to produce a benchmark ASR error correction dataset. This indicates that the new dataset is derived by transforming existing data (TOP dataset) through ASR and TTS systems to simulate errors and create paired data for correction tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly stated and documented in the paper."
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9215,
      "completion_tokens": 286,
      "total_tokens": 9501
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The newly produced ASR error correction dataset based on the TOP single-turn dialogue dataset is used for supervised fine-tuning of the proposed constrained decoder models to evaluate their performance for ASR error correction tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4 and 4.2 (Experiments and Results Analysis)",
          "reasoning": "The dataset is also used as a public benchmark dataset to evaluate and compare the effectiveness and inference speed of the proposed models against existing state-of-the-art methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Results Analysis) and Table 5",
          "reasoning": "The dataset is used for analyzing trends in error types and the effectiveness of corrections over various error categories such as grammatical errors, similar sound errors, and entity errors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9938,
      "completion_tokens": 480,
      "total_tokens": 10418
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4, Dataset description and Table 2",
          "reasoning": "The newly created dataset is based on the TOP dataset, described in Section 4, which focuses on navigation and events in English. The ASR output and reference texts are in English as evidenced by the example sentences (e.g., 'cheapest airfare from tacoma to orlando'). There is no mention of other languages covered in the new dataset, indicating it is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains transcribed spoken language text data; no code or programming language data entries are involved."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content is related to ASR error correction on natural language text and does not include mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data such as DNA sequences or animal signals are described in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset definitely contains human language text."
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7156,
      "completion_tokens": 199,
      "total_tokens": 7355
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1",
          "reasoning": "The abstract states that the code and datasets are made publicly available, providing a GitHub link (https://github.com/yangjingyuan/ConstDecoder). This implies that code related to the dataset creation, including the process of generating the ASR error correction dataset from a single-turn dialogue dataset using internal ASR and TTS engines, is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 1 and Section 4",
          "reasoning": "The paper documents the dataset creation process in Section 1 and Section 4, where it details the methodology to produce the ASR error correction dataset from the TOP dataset using internal TTS and ASR engines. Section 4 describes the datasets, specifically mentioning the creation process of the new TOP dataset variant with the applied ASR and TTS. The paper thus provides sufficient documentation on the dataset creation process."
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11406,
      "completion_tokens": 195,
      "total_tokens": 11601
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.1; Section 2. Dataset Description",
          "Reasoning": "The MagicData-RAMC dataset consists of 180 hours of Mandarin conversational speech data recorded from native speakers using mainstream smartphones at 16 kHz sampling rate, as explained in Section 2.1 and the Abstract. Since recordings are made by human participants speaking and using human-operated mobile devices, the audio data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 Dataset Description",
          "Reasoning": "The dataset includes accurate transcriptions and annotations such as voice activity timestamps and speaker demographic information, all manually labeled and proofed. The transcriptions are created by human annotators as described in Section 2 and the Abstract, indicating human-generated text data."
        }
      ]
    }
  },
  {
    "id": "yang22h_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12258,
      "completion_tokens": 227,
      "total_tokens": 12485
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 2.1",
            "reasoning": "The dataset's transcriptions and segmentations are manually labeled by annotators using a platform developed by Magic Data Technology Co., Ltd. Each recording is labeled by one annotator and then proofed by professional inspectors, indicating annotation was performed by human annotators but no indication they are subject matter experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1",
            "reasoning": "The paper states that sound segments without semantic information and various spontaneous speech phenomena are annotated and transcribed in detail, implying annotators had instructions to handle these cases carefully to ensure accuracy."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not mention any scoring rubrics or formal rubric-based evaluation criteria used during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no mention of annotation examples or guideline samples provided for annotators in the dataset description."
          }
        }
      ]
    }
  },
  {
    "id": "yang22h_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13388,
      "completion_tokens": 350,
      "total_tokens": 13738
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the quality assurance was performed by a single human annotator who is a subject matter expert or member of the target demographic. Therefore, this label is not applicable."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Collection setup",
          "reasoning": "The paper states that all speech data are manually labeled, each recording is labeled by one annotator, and the transcriptions are proofed by professional inspectors to ensure labeling and segmentation quality. This indicates that multiple human experts (annotators and inspectors) were involved in the quality assurance process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was done by a single non-expert annotator. The annotators and inspectors are implied to be professional, so this label doesn't apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions professional inspectors and manual labeling by annotators but does not specify that annotators or proofers lack expertise; thus, this label does not apply."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models as judges for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based techniques for quality assurance of annotations are described for the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is clearly described involving manual labeling and professional proofing; thus, lack of QA does not apply."
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 13006,
      "completion_tokens": 376,
      "total_tokens": 13382
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Collection setup",
          "reasoning": "The dataset MagicData-RAMC consists of 180 hours of conversational speech data manually recorded from native Mandarin speakers using smartphones. The speech and transcriptions are manually labeled and proofed, indicating that the data are original human-produced conversational speech content created from scratch and not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any AI or machine learning model generated or synthesized the speech data or transcripts."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the data were created by translating content from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated or collected from existing sources. Instead, it was recorded freshly from human speakers."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data were adapted or modified from existing datasets; the content is original human conversational recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the data origin and collection process."
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13524,
      "completion_tokens": 263,
      "total_tokens": 13787
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is used as part of the training data combined with MAGICDATA-READ to train an end-to-end automatic speech recognition (ASR) model from scratch, demonstrating supervised training use."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The dataset is used to evaluate baseline systems on ASR, speaker diarization, and keyword search tasks to benchmark performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.4 and 2.5",
          "reasoning": "The paper provides detailed analysis and statistics of the dataset's structure, speaker information, utterance statistics, and comparison with other datasets, indicating use for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14247,
      "completion_tokens": 588,
      "total_tokens": 14835
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2 and throughout the paper",
          "reasoning": "The MagicData-RAMC dataset contains only Mandarin Chinese conversational speech data; there is no indication of the presence of more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2 and dataset description",
          "reasoning": "The dataset consists solely of Mandarin Chinese speech data, with no mention of exactly two languages involved."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2 and dataset description",
          "reasoning": "The dataset is in Mandarin Chinese and does not contain English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 2 Dataset Description; Section 2.2 Speaker information",
          "reasoning": "MagicData-RAMC is described as a high-quality rich annotated Mandarin conversational speech dataset, collected from native Mandarin Chinese speakers, and all transcriptions and annotations are in Mandarin Chinese only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section indicates presence of code",
          "reasoning": "The dataset pertains to human conversational speech and transcripts; there is no mention of programming languages or code snippets included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No mention within the dataset description",
          "reasoning": "The dataset contains conversational speech in Mandarin, with manual transcriptions, voice activity timestamps, topics, and speaker information but no mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No mention of biological or non-human communication data",
          "reasoning": "The dataset involves only human Mandarin speech and transcripts, with no non-human communication or biological sequences included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No indication in any section",
          "reasoning": "The dataset consists of spontaneous Mandarin Chinese dialogs; no constructed or fictional languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Dataset description clearly states Mandarin language",
          "reasoning": "The language of the dataset is explicitly specified as Mandarin Chinese; it is not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains conversational speech data and transcriptions, thus includes language data."
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11465,
      "completion_tokens": 168,
      "total_tokens": 11633
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any link, URL, or reference to publicly available code repositories or scripts related to the data collection, preprocessing, or generation processes for the MagicData-RAMC dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Dataset Description) and subsections 2.1 to 2.5",
          "reasoning": "The paper provides a thorough description of the dataset creation process including collection setup (devices, environments, sampling rate), labeling procedures (manual annotation, proofreading, segmentation), speaker information, partitioning details, utterance statistics, and topic labels. This comprehensive documentation is contained mainly in Section 2 and its subsections, providing transparency about the dataset construction."
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7801,
      "completion_tokens": 172,
      "total_tokens": 7973
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Mandarin Lombard Corpus)",
          "Reasoning": "The corpus contains speech recordings of 28 native Mandarin speakers reading 'Grid'-like sentences at four noise levels. This data is audio modality as it consists of audio recordings, and it is human generated since it was recorded from human speakers under controlled conditions as described in the paper."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Mandarin Lombard Corpus)",
          "Reasoning": "The corpus includes word and phoneme level transcriptions obtained via forced alignment based on human-generated speech data. These transcriptions are text modality and originate from human speech content, thus human generated."
        }
      ]
    }
  },
  {
    "id": "yang22i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8653,
      "completion_tokens": 222,
      "total_tokens": 8875
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3, Mandarin Lombard Corpus",
            "reasoning": "The paper states that 28 native Mandarin speakers were recruited to produce the speech recordings for the new Mandarin Lombard Corpus. These speakers were likely non-experts in annotation but provided speech data as part of corpus collection."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3, Mandarin Lombard Corpus",
            "reasoning": "There is no mention of instructions provided to the speakers that would qualify as annotation instructions; the focus was on speech production not annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3, Mandarin Lombard Corpus",
            "reasoning": "The paper does not describe any rubric-based scoring or labeled ratings by annotators for the collected speech data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3, Mandarin Lombard Corpus",
            "reasoning": "No annotation examples or guidelines were provided for the speech collection since this was a recording task, not a manual annotation with label examples."
          }
        }
      ]
    }
  },
  {
    "id": "yang22i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9783,
      "completion_tokens": 350,
      "total_tokens": 10133
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that multiple human experts performed quality assurance on the dataset or annotations is provided in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of AI models used for judging or validating the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 (Mandarin Lombard Corpus) and Section 2.3 (STOI-based significance test)",
          "reasoning": "The paper mentions the use of the Montreal Forced Aligner to produce phonetic transcriptions (word and phoneme boundaries) for each sentence. Forced alignment tools perform automated forced phonetic alignment, which is an automated verification process rather than manual annotation. This suggests that data labeling was performed automatically. There is no mention of manual validation of these transcriptions. Therefore, automatic verification via algorithmic techniques was used for quality assurance of the transcriptions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides details about automated forced alignment and statistical significance testing for Lombard style classification, indicating some form of quality assurance is applied."
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9401,
      "completion_tokens": 415,
      "total_tokens": 9816
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 3",
          "reasoning": "The paper clearly states that they recruited native Mandarin speakers who read newly created 'Grid'-like sentences in different background noise conditions and recorded these utterances. The sentences were structured and randomly generated as new content for this corpus, and the speech data was collected fresh from human contributors, not adapted or derived from existing datasets. This process is described in Section 2.1 (Sentence selection, Speaker recruitment, Recording setup) and Section 3 (Mandarin Lombard Corpus)."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected from human speakers reading sentences; there is no indication that data was generated or synthesized by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the data was produced by translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the data was produced by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus was not collected or aggregated from existing sources; it was recorded from scratch."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not adapted or derived from an existing dataset; it is original recordings, not transformations of prior data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are explicitly documented."
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9919,
      "completion_tokens": 230,
      "total_tokens": 10149
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Analysis of Lombard Effect)",
          "reasoning": "The newly introduced Mandarin Lombard corpus is primarily used for analyzing the Lombard effect characteristics, such as vowel duration, formants, and acoustic parameters, comparing the effects in Mandarin versus English. The paper includes detailed statistical analysis (Section 4) to characterize speech under different noise levels, without any mention of using the dataset for training or evaluation of ML models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10642,
      "completion_tokens": 590,
      "total_tokens": 11232
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced, the Mandarin Lombard Grid, contains speech data only in Mandarin Chinese and no other human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; it is solely Mandarin Chinese speech data."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content; it is focused on Mandarin Chinese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3, \"Mandarin Lombard Corpus\", and throughout the paper notably in the Abstract and Introduction.",
          "reasoning": "The newly introduced dataset is a corpus of Mandarin Chinese speech containing Lombard speech styles induced by noise at different SPLs. The paper explicitly states it is a Mandarin Lombard corpus and details the speakers as native Mandarin speakers. Thus, the dataset is monolingual with the language being Mandarin Chinese, which is a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain any programming or structured code-related content. While tools like Montreal Forced Aligner and Praat are used for processing, the dataset itself is audio and transcriptions of Mandarin speech."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are speech recordings and their transcriptions; although statistical tests and acoustic parameters are analyzed in the paper, the dataset does not contain mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human speech in Mandarin. It does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of natural Mandarin Chinese speech and does not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is specified clearly as Mandarin Chinese, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data in the form of Mandarin speech and transcriptions."
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7860,
      "completion_tokens": 210,
      "total_tokens": 8070
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3 (Mandarin Lombard Corpus)",
          "reasoning": "The paper explicitly states that the corpus is being made freely available for download at https://github.com/ASP-WHU/Mandarin-Lombard-Grid, which is a public GitHub repository hosting the dataset. Although the paper does not detail all code related to data collection and preprocessing, the availability of the corpus and the linked repository suggests that at least data and possibly related materials are openly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 (Lombard Style Classification) and 3 (Mandarin Lombard Corpus)",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including sentence selection, speaker recruitment, recording setup, noise conditions, number of speakers and utterances, and transcription methods (e.g., forced alignment). It also describes the noise stimulation and recording environment, and dataset contents. This comprehensive documentation supports reproducibility."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8590,
      "completion_tokens": 173,
      "total_tokens": 8763
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Corpus Details",
          "Reasoning": "The authors introduce a new speech command recognition dataset collected for 41 Mandarin commands targeting air conditioner control. The training set consists of approximately 95k utterances recorded manually on smartphones in near-field quiet conditions from 1744 different speakers (human generated). Additionally, the dataset is augmented by applying Room Impulse Response simulation using pyroomacoustics to increase data by 10 times via simulated reverberation and added noise, which is model generated data. Furthermore, 400k negative samples from human voice sources and environment noise are added. Thus, the dataset consists of both human recorded audio and simulated augmented audio. This is explicitly described in Section 5.1."
        }
      ]
    }
  },
  {
    "id": "yang22m_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9442,
      "completion_tokens": 238,
      "total_tokens": 9680
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 5.1",
            "reasoning": "The new dataset introduced is the authors' collected Mandarin speech command recognition dataset with 41 commands. The dataset consists of speech utterances recorded from 1744 speakers and various data augmentation applied. The paper does not specify expert annotation, but since it is speech commands presumably labeled by multiple speakers (likely non-expert annotators) or derived from command recording and corresponding labels, the annotations are performed by multiple human non-experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5.1",
            "reasoning": "The paper does not mention any detailed annotation instructions or guidelines used for labeling the dataset utterances."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5.1",
            "reasoning": "There is no mention of scoring rubrics or criteria for annotations in the dataset descriptive section or elsewhere."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5.1",
            "reasoning": "No annotation examples or sample labeled utterances are provided or referenced in the paper regarding the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "yang22m_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10572,
      "completion_tokens": 260,
      "total_tokens": 10832
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information describing QA by multiple human experts in the dataset creation or annotation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by multiple non-expert human annotators is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification processes for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new Mandarin speech command dataset but does not provide any description or documentation of quality assurance processes for the dataset annotations or content. There is no indication of human annotation verification or any other QA procedure applied or reported."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10190,
      "completion_tokens": 450,
      "total_tokens": 10640
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1 Corpus Details",
          "reasoning": "The authors collected a new Mandarin speech command dataset specifically for their experiments, containing 41 commands with recordings from 1744 speakers and about 95k utterances. The dataset is described as recorded on smart phones in near-field quiet environments, indicating human contributors recorded original speech data specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by any AI or machine learning models; it consists of human-recorded utterances and augmented data created using room impulse responses and noise addition, not synthetic speech generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by translating speech or text from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources without modification. The authors specifically state that the dataset is collected for the SCR task, not compiled from other data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 Corpus Details",
          "reasoning": "The training corpus is augmented by applying room impulse response simulation and adding noise of various kinds to the original human-recorded utterances, which is a common data augmentation practice that transforms the original data to enhance robustness."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data collection and augmentation process is clearly documented in Section 5.1, so the origin of the dataset is specified."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10708,
      "completion_tokens": 436,
      "total_tokens": 11144
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly collected dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.2",
          "reasoning": "The authors collected a sizable speech command dataset specifically for the SCR task and trained their acoustic models from scratch using this dataset, as described in 'Corpus Details' and 'Experimental Setups'."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using a pre-trained model or fine-tuning models supervised using the collected dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RLHF methods leveraging this dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.5",
          "reasoning": "The collected evaluation set is explicitly used for evaluating and benchmarking the SCR system's performance and confusion errors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used just to analyze trends or patterns but rather for training and evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as serving as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the practical usage of the newly collected dataset for training and evaluation."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11431,
      "completion_tokens": 499,
      "total_tokens": 11930
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 5.1",
          "reasoning": "The dataset introduced contains only Mandarin commands; no indication of multiple languages is given."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 5.1",
          "reasoning": "No evidence or mention of exactly two languages in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 5.1",
          "reasoning": "The dataset is described as consisting of Mandarin commands; no English-only dataset is introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The dataset contains 41 Mandarin commands only, indicating a monolingual dataset in a single non-English language, Mandarin."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech utterances representing Mandarin commands, no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper contains mathematical formulas describing the method, the dataset itself does not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly specified as Mandarin; thus, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken Mandarin command utterances, so it does contain language content."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8649,
      "completion_tokens": 141,
      "total_tokens": 8790
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No code repository or link mentioned in the paper",
          "reasoning": "The paper does not provide any link or mention of code availability related to the dataset construction, data preprocessing, or generation. There is no indication that such code is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 Corpus Details",
          "reasoning": "The paper dedicates Section 5.1 to describe the dataset collected by the authors, detailing the corpus size, number of commands, recording environment, speaker variation, data augmentation techniques, and negative sample sources. This documentation provides transparency about the dataset creation process used for experiments."
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7329,
      "completion_tokens": 154,
      "total_tokens": 7483
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1, especially 3.1.1 and 3.1.2 (Proposed Method: Relation-based Counterfactual Data Augmentation)",
          "Reasoning": "The new dataset entries are generated by the authors using automatic data augmentation methods. Token-level augmentation modifies nouns with substitutions based on WordNet to create counterfactual sentence pairs of different classes, and sentence-level augmentation generates sentences via a generator model iteratively trained and filtered by model confidence. The resulting augmented sentence pairs for NLI tasks are thus model-generated synthetic data, not manually written by humans, as explicitly described in Section 3.1."
        }
      ]
    }
  },
  {
    "id": "yang23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8181,
      "completion_tokens": 230,
      "total_tokens": 8411
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1, Section 3.1.1",
            "reasoning": "The newly introduced dataset consists of automatically generated counterfactual sentence pairs created via token-level and sentence-level augmentation methods described in Section 3.1.1 and 3.1. The authors use spaCy and WordNet to replace nouns and a generator model to produce sentence-level augmentations, without involving human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions described",
            "reasoning": "Since the augmentation is done automatically via models and lexical resources, there are no annotation instructions provided for human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric described",
            "reasoning": "No scoring rubric or quality assessment guidelines are described as the annotation process is automated."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 2, Figure 1",
            "reasoning": "Examples of generated sentences and augmentation process are provided in Table 2 and Figures 1a and 1b to illustrate the data generation."
          }
        }
      ]
    }
  },
  {
    "id": "yang23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9311,
      "completion_tokens": 427,
      "total_tokens": 9738
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the datasets introduced or used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the newly generated datasets. No such process is described in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although CF-SNLI dataset used for evaluation was originally human-annotated (by crowdworkers as cited), this dataset is pre-existing and not introduced by this paper. For the newly generated counterfactual augmented data introduced by the authors, there is no description of human annotators or non-expert annotators performing QA."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1.2 (Sentence-level augmentation)",
          "reasoning": "The paper explains that in sentence-level augmentation, a generator model is trained to generate sentences, and a classifier model is used to filter samples based on confidence (threshold tau). This indicates that AI models are used as judges for quality assurance of generated augmented data."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1.1 (Token-level Data Augmentation) and Section 3.1.2 (Sentence-level augmentation)",
          "reasoning": "The token-level augmentation uses algorithmic techniques (WordNet-based substitutions with defined rules for synonym, hypernym, antonym, etc.) for generating counterfactual data. Sentence-level augmentation applies filtering using confidence thresholds automatically. These systematic procedures constitute automated verification of generated data quality without human intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes an AI model-based filtering and automated rule-based substitution process for dataset augmentation. Hence, quality assurance processes are present and documented."
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8929,
      "completion_tokens": 509,
      "total_tokens": 9438
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created entirely by human annotators from scratch. Instead, it uses existing datasets such as CF-SNLI and SNLI for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (Proposed Method), specifically Sections 3.1.1 (Token-level Data Augmentation) and 3.1.2 (Sentence-level augmentation process described in Figure 1b and text)",
          "reasoning": "The authors generate new counterfactual sentence pairs using automatic augmentation methods. Token-level augmentation applies substitutions based on WordNet to generate sentences with altered relations, while sentence-level augmentation uses a generator model trained on the original data to generate new sentences for each class, filtered by model confidence. This data is newly generated by AI/machine learning models without human annotation, making it 'New Data from Model'."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using machine translation to generate or adapt data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The augmented data is not merely aggregated or collected from existing sources; it is generated or derived through augmentation methods."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1 (Token-level Data Augmentation)",
          "reasoning": "The token-level generated data is derived from existing sentence pairs by selectively substituting words (nouns) with synonyms, hypernyms, antonyms, hyponyms, or co-hyponyms according to predefined rules to produce counterfactual examples. This is a transformation of existing data and thus fits 'Derived'."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is clearly documented and described; thus N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9447,
      "completion_tokens": 350,
      "total_tokens": 9797
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4.1, Section 4.2",
          "reasoning": "The paper describes that the newly generated counterfactual sentence pair datasets (via token-level and sentence-level augmentation) are used to fine-tune pre-trained language models such as BERT and RoBERTa. Specifically, these augmented datasets serve as the training data for supervised fine-tuning of models with both contrastive learning and cross-entropy objectives. This is explicitly detailed in Section 3.2 (Relation-based Contrastive Learning) and evaluated in Section 4.1 (Experiment Setup) and Section 4.2 (Results)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2",
          "reasoning": "The paper uses the proposed counterfactually augmented dataset to evaluate model robustness and performance, for example using CF-SNLI test sets and other general NLI datasets to benchmark different models and training methods, as described in Sections 4.1 and 4.2. Hence, the dataset is also used for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10170,
      "completion_tokens": 500,
      "total_tokens": 10670
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment Setup",
          "reasoning": "The authors use the CF-SNLI dataset, which is based on SNLI, a large annotated corpus for natural language inference in English (Section 4.1). The augmentation methods such as token substitutions rely on English lexical resources like WordNet, and examples throughout the paper are all in English. There is no mention of any other natural language being present in the dataset or generated during augmentation; thus, the dataset contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and augmentation involve natural language sentence pairs, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas explaining loss functions, the dataset entries are natural language sentence pairs without mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human natural language sentence pairs with no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or include fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) used in the dataset are clearly specified and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language sentences, specifically English, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7388,
      "completion_tokens": 169,
      "total_tokens": 7557
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or statement provides code availability information.",
          "reasoning": "The paper does not mention any code release, repository, or link for the dataset creation or augmentation code. There is no explicit indication that the code used for the relation-based counterfactual data augmentation or contrastive learning is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Proposed Method - Relation-based Counterfactual Data Augmentation)",
          "reasoning": "The paper thoroughly describes the dataset creation process related to their new augmented data, including token-level and sentence-level augmentation methods, substitution rules with WordNet relations, the sentence generation and filtering process, and how data pairs are constructed for contrastive learning. This provides transparency and completeness of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6550,
      "completion_tokens": 97,
      "total_tokens": 6647
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3 (implied from methodology)",
          "Reasoning": "The paper describes EEG signals collected by a wireless mobile EEG system from subjects in real-life scenarios (walking and sitting). EEG data is a sensor signal modality recorded from human participants in experimental conditions designed by the authors, indicating human-generated sensor data collected for this study."
        }
      ]
    }
  },
  {
    "id": "yang23r_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7402,
      "completion_tokens": 213,
      "total_tokens": 7615
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 and the Abstract",
            "reasoning": "The paper introduces a new EEG dataset collected using a wireless mobile EEG system from subjects in real-life scenarios (walking and sitting). Data collection involving EEG typically requires expert involvement, including setting up and supervising experiments. The paper does not mention multiple annotators or crowdsourcing; rather, it implies controlled data collection with expert supervision."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not mention or describe any instructions given for annotation or data labeling, nor does it detail annotation guidelines related to the dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "There is no indication of scoring rubrics or evaluation criteria for annotations related to the dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "No examples or annotation samples are provided or discussed in the paper regarding the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "yang23r_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8532,
      "completion_tokens": 371,
      "total_tokens": 8903
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided in the paper about quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by any single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance performed by multiple non-expert human annotators is found in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any AI model was used specifically as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses algorithms such as CSP, CNN, and related signal processing methods, it does not describe these algorithms as a quality assurance process for the dataset annotations or validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process applied to the dataset annotations or content. No explicit or implicit QA methods or verification steps for the dataset are mentioned."
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8150,
      "completion_tokens": 448,
      "total_tokens": 8598
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data collection and preprocessing, and Section 3 Experiments and results",
          "reasoning": "The paper describes EEG data collected from human subjects using a wireless mobile EEG system in real-life scenarios involving sitting and walking. This data consists of human physiological signals recorded during auditory attention tasks, and is original content created by human participants specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study does not introduce any datasets generated entirely by AI or machine learning models without reference to existing data. All data corresponds to EEG signals recorded from human subjects."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data generated by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset created by aggregation or collection from existing sources without modification. The data used is newly collected EEG data from their own experiments."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper applies various signal processing and filtering methods (e.g., bandpass filtering into sub-bands) on the raw EEG data for analysis, these processed signals are not presented as separate new datasets but rather as analysis results. The new dataset is the raw EEG recordings themselves."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly specified as newly recorded EEG signals from human subjects, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8668,
      "completion_tokens": 316,
      "total_tokens": 8984
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, 4.1",
          "reasoning": "The paper describes training auditory attention detection (AAD) models from EEG data captured in real-life scenarios to decode directional focus of attention. The dataset is used to train models such as CSP-based classifiers and CNNs, as indicated by the performance results in Table 1 and method descriptions."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Table 1",
          "reasoning": "The dataset collected by the authors is used to evaluate and benchmark the performance of different AAD methods (SR, CCA, CNN, CSP) for auditory attention detection in real-life scenarios, as shown in Table 1 and comparative analyses."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The authors use the dataset to analyze the contribution of different EEG frequency bands (\u03b4, \u03b8, \u03b1, \u03b2) to AAD accuracy in sitting and walking states, demonstrating characteristic patterns in attention decoding."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9391,
      "completion_tokens": 582,
      "total_tokens": 9973
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the proposed dataset contains entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the proposed dataset contains entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "General description of experimental setting and stimuli, Sections 1 and 4",
          "reasoning": "The dataset involves EEG signals recorded while subjects attended to auditory stimuli in real-life scenarios. The paper is written in English and does not mention multiple languages or any non-English language content in the stimuli. The auditory stimuli and attention decoding experiments likely use English speech or speech envelopes, which implies the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only one non-English language; the paper uses English as the language for description and the experiments imply English speech."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithms and mathematical models, it does not present a dataset containing entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 2.2 and 2.3 Mathematical descriptions of CSP and classification methods",
          "reasoning": "The paper contains mathematical formulations describing CSP and classifiers but does not introduce a dataset that contains entries in mathematical or formal logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains EEG recordings related to human cognitive processes and attention, but does not involve biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is known and implied to be English in the auditory stimuli and experimental context."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes EEG data recorded during auditory linguistic stimulation, so it contains human language content."
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6609,
      "completion_tokens": 154,
      "total_tokens": 6763
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code for data collection, preprocessing, or dataset generation. There is no indication of any code repository or supplementary material for code provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Data collection; Section 3 Methodology",
          "reasoning": "The paper describes the EEG data collection process in real-life scenarios with a wireless mobile EEG system, including details about experimental conditions (sitting and walking), frequency bands explored, and the methods (CSP) applied. This documentation allows understanding of how the dataset was created and processed, although no explicit mention of dataset release is found."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8001,
      "completion_tokens": 104,
      "total_tokens": 8105
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset",
          "Reasoning": "The paper introduces a new 16,600-hour in-house dataset of de-identified audio recordings for the Alexa keyword detection task. These recordings were obtained from real user interactions (human involvement) and processed into audio features (LFBE spectrograms). The dataset is explicitly collected by the authors and used for both training and evaluation."
        }
      ]
    }
  },
  {
    "id": "yang23y_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8853,
      "completion_tokens": 190,
      "total_tokens": 9043
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The dataset is based on human annotation of keywords, as stated in Section 3.1: \"The keyword labels were based on human annotation and underwent a quality check inspection\". This implies involvement of multiple human experts to ensure annotation quality."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of scoring rubrics or formal annotation guidelines used during the annotation process in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No example annotations or annotation guidelines examples are presented or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "yang23y_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9983,
      "completion_tokens": 320,
      "total_tokens": 10303
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper states that the keyword labels in the in-house 16,600 hours Alexa keyword detection dataset were based on human annotation and underwent a quality check inspection. Although it does not explicitly describe the number or expertise of annotators, the annotation involves human labelers and a quality check process, indicating multiple human annotators were involved. There is no indication these annotators were experts, so this label best fits."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly mentions human annotation and a subsequent quality check inspection for labels in the in-house dataset, indicating some form of QA was conducted."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9601,
      "completion_tokens": 397,
      "total_tokens": 9998
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper states in Section 3.1 that the authors collected a 16,600-hour de-identified in-house dataset specifically for the Alexa keyword detection task, with human annotation and quality check inspection. This indicates original content created entirely from scratch by human contributors rather than sourced from existing public datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated purely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any of the data was produced by human translators translating from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The in-house dataset was collected specifically for this task and not described as aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as based on existing sources with modifications; it appears to be newly collected data specific to the task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is clearly specified in Section 3.1; therefore, this category does not apply."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10119,
      "completion_tokens": 552,
      "total_tokens": 10671
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset; Section 3.2 Implementation",
          "reasoning": "The 16,600 hours in-house Alexa keyword spotting dataset is used for pre-training the student models in a self-supervised manner with knowledge distillation, as described in the methodology and implementation sections. The dataset is leveraged for pre-training models using self-supervised objectives (Wav2vec 2.0 style) along with proposed distillation techniques."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters on this dataset without pre-training or knowledge distillation. The student models are trained with knowledge distilled from teacher models, not strictly from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Implementation; Section 4 Results",
          "reasoning": "The same dataset is used for supervised fine-tuning of the student models after pre-training and distillation, with the addition of a linear classifier for keyword spotting, using human-annotated keyword labels. This is explicitly described in Section 3.2 and evidenced in evaluation in Section 4."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of reinforcement learning or RL-based post-training techniques such as RLHF on the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset; Section 3.3 Evaluation Metrics; Section 4 Results",
          "reasoning": "A portion of the dataset (85 hours for validation and 85 hours for testing) is explicitly held out and used for evaluation and benchmarking of the models on normal and noisy conditions. Evaluation metrics such as false acceptance rate at fixed false rejection rate are computed on this data."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Single-View vs Dual-View; Section 4.3 Teacher Codebook vs Codebook from Scratch; Section 4.4 Layer Selection from Teacher Model",
          "reasoning": "The dataset is further used for extended analysis and ablation studies to investigate the impact of different distillation techniques and layer selection, as detailed in the analysis sections of the Results."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models in retrieval or similar frameworks in this work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced is actively used for pre-training, fine-tuning, evaluation, and analysis of models as documented in the paper."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10842,
      "completion_tokens": 537,
      "total_tokens": 11379
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing multiple human languages; it only references an in-house Alexa keyword spotting dataset without details of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains entries in exactly two human languages; no such bilingual data description is provided."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is described as an in-house Alexa keyword dataset. The teacher model is trained on LibriSpeech (a primarily English corpus), and there is no mention of other languages in the dataset. The context and use in Alexa keyword spotting strongly imply the data is English speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that the dataset contains a single non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is audio recordings processed into feature representations; no programming languages or code snippets are included as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes equations and formulas describing methods, these do not pertain to dataset entries but to modeling techniques."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio recordings; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language is explicitly implied to be English; the dataset language is specified and not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of human speech utterances, so it includes language data."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8060,
      "completion_tokens": 196,
      "total_tokens": 8256
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper describes the dataset as 16,600 hours of de-identified audio recordings collected in-house for Alexa keyword detection. There is no mention of any released code or link to code repositories for data collection, preprocessing, or dataset construction. No code is publicly provided or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper provides a description of the dataset including the size (16,600 hours), data source (Alexa keyword detection task), processing steps (64-D LFBE spectrogram with specified window and shift size), train-validation-test split details, and evaluation conditions (normal and playback). Although details on the data collection process are limited to stating 'de-identified audio recordings in various front-end conditions,' the dataset creation and characteristics are documented in sufficient detail relevant to reproducibility."
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8179,
      "completion_tokens": 157,
      "total_tokens": 8336
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data recording and annotation",
          "Reasoning": "The MSceneSpeech dataset consists of real audio recordings performed and recorded by professional recording artists according to daily life scenarios, using interpretative performances rather than synthetic generation, indicating human involvement in audio creation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data recording and annotation",
          "Reasoning": "The accompanying text scripts for each scenario were carefully selected and curated by humans to align with the contextual backdrop, and text annotations were verified and manually corrected, implying the text data is human generated and carefully prepared."
        }
      ]
    }
  },
  {
    "id": "yang24d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9031,
      "completion_tokens": 190,
      "total_tokens": 9221
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that professional recording artists performed and recorded the audio according to selected texts and scenarios, indicating annotation by expert human annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 describes that scripts were carefully selected to match scenarios and recording artists were instructed to perform interpretatively, implying detailed instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any scoring rubrics or criteria for annotation evaluations in the dataset creation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No examples of annotation or recording guidelines or sample annotations are provided or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "yang24d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10161,
      "completion_tokens": 243,
      "total_tokens": 10404
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 Data recording and annotation",
          "reasoning": "The dataset recordings were performed by professional recording artists who interpreted the texts according to the scenarios, implying that experts conducted the recording and quality assurance during production."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1 Data recording and annotation",
          "reasoning": "The use of Whisper, an AI-based automatic speech recognition system, was employed to transcribe the recordings and help verify text annotations, indicating AI involvement in quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Processing",
          "reasoning": "An ASR proofreading step was applied to remove audio segments where text similarity fell below 80%, which constitutes an automated verification process as part of quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9779,
      "completion_tokens": 375,
      "total_tokens": 10154
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data recording and annotation",
          "reasoning": "The MSceneSpeech dataset was created by professional recording artists who performed and recorded original scripts curated specifically for various daily life scenarios. The data is not translated or derived from pre-existing material; it is original content meticulously recorded with expressive nuance to capture prosodic variety relevant to the intended scenarios."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any portion of the dataset was generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was created via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing datasets or sources; it was newly recorded as described."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the data is based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and method of generation of the dataset."
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10297,
      "completion_tokens": 476,
      "total_tokens": 10773
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MSceneSpeech dataset for pre-training large models. Pre-training is conducted on a separate aggregated public dataset."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MSceneSpeech dataset is not used to train models from scratch; it is used for fine-tuning pre-trained models as stated in Section 5.1."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The paper explicitly states that after pre-training on a large mixed dataset, supervised fine-tuning is performed on the MSceneSpeech dataset (around 15 hours) to enable style-related speech synthesis and prosody transfer."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning based post-training methods such as RLHF using the MSceneSpeech dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MSceneSpeech dataset is not solely used for evaluation or benchmarking; it is primarily used for supervised fine-tuning."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The paper performs analysis of prosodic features across scenes, comparing pitch variance and prosody richness in MSceneSpeech versus other datasets to highlight the advantages of their dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence that MSceneSpeech is used as a knowledge base for retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has multiple practical uses in the paper, including supervised fine-tuning and analysis."
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11020,
      "completion_tokens": 235,
      "total_tokens": 11255
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3, Dataset Description",
          "reasoning": "The MSceneSpeech dataset is explicitly described as a Mandarin speech dataset in Section 3, with recordings by professional speakers in Mandarin for various daily life scenarios. No other human languages are included in this dataset according to the description."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8238,
      "completion_tokens": 296,
      "total_tokens": 8534
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 5.1 (Experimental setup)",
          "reasoning": "The paper states explicitly in the abstract that the MSceneSpeech dataset and audio samples of their baseline model are available at a public URL (https://speechai-demo.github.io/MSceneSpeech/). Additionally, in Section 5.1, it mentions the code and models are available on GitHub for the baseline, indicating related code resources are publicly accessible. While it mainly refers to code for the baseline model, the availability of the dataset and audio samples at the public site implies data preparation or access code is provided, supporting reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (MSceneSpeech Dataset), specifically Sections 3.1 (Data recording and annotation) and 3.2 (Data Processing)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the selection of scenes, text script preparation, recording by professional artists with interpretative performances rather than mere recitation, verification and correction of text annotations (including use of the Whisper ASR system and manual corrections). It describes data processing steps such as segmenting audio clips by punctuation and duration constraints, automatic ASR proofreading to filter low-quality alignments, and manual refinements. The dataset split strategy into training and testing subsets is also explained. This level of detail provides transparency and completeness in dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8989,
      "completion_tokens": 201,
      "total_tokens": 9190
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and Table 3",
          "Reasoning": "The dataset is built upon the original GigaSpeech ASR corpus, which consists of 10,000 hours of English speech audio recorded by humans. The paper explicitly mentions using the original speech audio from GigaSpeech, which is human-generated through recorded English speech."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.2",
          "Reasoning": "The training set's translated text target side is obtained by translating GigaSpeech transcripts using a strong machine translation (MT) model developed by the authors. This translation is model generated. The test set translations, however, are produced by human translators, but since the test set is separate, only the training set's text translations are considered model generated here."
        }
      ]
    }
  },
  {
    "id": "ye23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9841,
      "completion_tokens": 302,
      "total_tokens": 10143
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 (Test Sets) and Section 2.1 (Training Set Human Evaluation)",
            "reasoning": "The test set translations were produced by human translators who looked at the transcriptions ('Section 2.2'). Additionally, for human evaluation of the pseudo labels on the training set, 2 professional translators produced ground truth translations and multiple evaluators (3 per language) rated translation quality ('Section 2.1'). This suggests multiple expert human annotators were involved in validating and annotating the test set translations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2 and 2.1",
            "reasoning": "The paper does not describe providing any explicit instructions or annotation guidelines given to the human translators who produced the test set translations or to the evaluators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.1 (Human Evaluation)",
            "reasoning": "For human evaluation of training set translations, evaluators rated translations on a scale from 0 to 6 with a specific rubric where 6 means fluent and faithful translation and 0 means incomprehensible, indicating the presence of scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 and 2.2",
            "reasoning": "There is no mention of providing annotation examples or specific examples for translators or evaluators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "ye23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10971,
      "completion_tokens": 329,
      "total_tokens": 11300
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Test Sets and Section 2.1 Training Set",
          "reasoning": "The test sets in GigaST are produced by human translators who are likely professional translators, indicating multiple human experts annotated the test data. Additionally, for human evaluation of the training set translations, 2 professional translators produced ground truth translations, and 6 evaluators (assumed experts) rated the translations, all indicating multiple human experts involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide evidence that multiple non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance described involves human evaluation and does not involve AI models acting as judges."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated metrics (BLEU) are used for evaluation and filtering, this is not described as a quality assurance process for annotation validity in the dataset creation context."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents quality assurance processes involving human experts for the test sets and human evaluation of training set translations."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10589,
      "completion_tokens": 500,
      "total_tokens": 11089
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Test Sets",
          "reasoning": "The paper states that the test sets for English-Chinese and English-German were produced by human translators translating the original English transcriptions. This original translation was done by humans viewing the transcripts to produce target language translations, as described in Section 2.2."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by AI or machine learning models from scratch without referencing or transforming existing data."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 2.2 Test Sets",
          "reasoning": "The test sets are explicitly described as human translations of English transcripts into German and Chinese, designed to ensure high quality and avoid alignment errors. This fits the category of human translations."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 2.1 Training Set",
          "reasoning": "The training set target translations were obtained by translating the English transcripts of GigaSpeech using strong neural machine translation models that were trained and refined via data filtering, knowledge distillation and back translation. This indicates the training labels are machine translated, as explicitly stated."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not simply collected or aggregated from existing sources without modification. It is constructed by translation, either machine or human, of existing transcripts."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Dataset Creation",
          "reasoning": "The new GigaST speech-to-text translation corpus is derived from the existing GigaSpeech English ASR corpus by applying machine translation to the transcripts for training data and human translations for test data. Hence, the dataset is based on an existing source with modifications (translations) applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data creation pipeline and origins for both training and test sets, so the origin is clearly specified."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11107,
      "completion_tokens": 492,
      "total_tokens": 11599
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4 (Conclusion)",
          "reasoning": "The GigaST dataset is explicitly used as a training dataset for training speech-to-text translation models from scratch, as described in Section 3.2 where various Speech-Transformer and SSL-Transformer models are trained on GigaST subsets of increasing size. The conclusion also states that the corpus is suitable for training ST systems."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using GigaST for fine-tuning pre-trained models; rather, it is used directly as a training corpus."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based fine-tuning using the GigaST dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2 (Test Sets), Section 3.1 (Evaluation), Tables 4 and 5",
          "reasoning": "GigaST includes human-translated test sets for En-Zh and En-De used exclusively for evaluation and benchmarking of ST models, demonstrated in multiple results tables."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 (Analysis on Speech Representations)",
          "reasoning": "The dataset is used to analyze the effects of pre-trained speech encoders and model size on performance, contributing to understanding model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used as a knowledge base to augment models through retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used for multiple practical purposes including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11830,
      "completion_tokens": 586,
      "total_tokens": 12416
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The GigaST dataset contains translations from English into two separate target languages, Chinese and German, but the dataset entries are bilingual pairs, not containing more than two languages in a single entry."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2 Dataset Creation (especially 2.1 Training Set and 2.2 Test Sets)",
          "reasoning": "The GigaST dataset is created by translating English speech transcripts into two different languages: German and Chinese. Each entry consists of source English speech and a corresponding translated text either in German or Chinese. Therefore, each dataset entry pairs exactly two human languages (English source and one target language), making it bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains translated text into German and Chinese, so it is not limited to English-only content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pairs English speech with either German or Chinese translation, hence it is bilingual, not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech and natural language translation texts, with no indication of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of mathematical or formal logical expressions being part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech and translations, no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are real human natural languages (English, German, Chinese), no constructed or fictional languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved in the dataset entries are clearly documented as English for the source speech and German or Chinese for the translations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language content in English, German, and Chinese, so 'no language' does not apply."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9048,
      "completion_tokens": 148,
      "total_tokens": 9196
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 4 Conclusion",
          "reasoning": "The paper states in the abstract and conclusion that they release the training scripts on NeurST and provide the dataset publicly at https://st-benchmark.github.io/resources/GigaST, indicating that code related to dataset creation and training is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Dataset Creation",
          "reasoning": "Section 2 provides a detailed description of the translation process used to create the pseudo speech-to-text translation corpus, including the machine translation system, training data, data filtering, multi-resolution training, and human evaluation steps, which documents the dataset creation process comprehensively."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7549,
      "completion_tokens": 450,
      "total_tokens": 7999
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "Tencent Corpus is a newly introduced dataset by the authors, containing speech clips with various simulated impairments and live recordings. The data includes artificially degraded speech based on publicly available clean speech datasets (Magic Data, ST Mandarin, AiShell 100h) which are human-generated audio recordings. These clean speech clips are further processed with algorithmic simulations to add impairments, noise suppression, packet loss concealment, and reverberation effects (both simulated and live room recordings). Therefore, the final data includes both human-generated audio and model-generated (simulated) audio samples."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.4",
          "Reasoning": "PSTN Corpus is introduced as a new dataset in this challenge. It uses clean reference files from human-generated audiobook recordings (Librivox) and adds background noise clips from Audioset, Freesound, and DEMAND corpora. Noise is added algorithmically with varying SNR to simulate real phone call scenarios. The clean speech is human-generated while the noisy versions are model-generated via data processing."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 and 4.4 combined with Section 4 introduction",
          "Reasoning": "The paper explicitly states only NISQA Corpus is publicly available prior, thus it is not a new dataset introduced by the authors. IU Bloomington Corpus is also not created by the authors but included as additional data. Therefore, only the Tencent Corpus, PSTN Corpus, and the newly created TUB Corpus are new datasets introduced or created by the authors. TUB Corpus is described in Section 4.2 as newly created: clips from unimpaired signals are selected and synthetic degradations applied (codecs, bandwidth, background noises, packet loss), constituting model-generated augmentations applied on human-generated audio recordings. Hence, TUB Corpus includes human-recorded audio that is model-processed to simulate impairments."
        }
      ]
    }
  },
  {
    "id": "yi22b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8401,
      "completion_tokens": 208,
      "total_tokens": 8609
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1",
            "reasoning": "The subjective scoring procedure for the Tencent Corpus was conducted via crowdsourcing with more than 24 listeners rating each clip, and after cleaning, more than 20 subjective scores were averaged per clip, indicating multiple non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1",
            "reasoning": "The scoring procedure was conducted via crowdsourcing similar to ITU-T P.808, which provides standardized instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1",
            "reasoning": "Since the procedure followed ITU-T P.808 standard, which defines detailed scoring rubrics for subjective speech quality evaluation, rubrics were provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "--",
            "reasoning": "The paper does not mention providing annotation examples in the guidelines for the Tencent Corpus subjective labeling."
          }
        }
      ]
    }
  },
  {
    "id": "yi22b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9531,
      "completion_tokens": 479,
      "total_tokens": 10010
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by any single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no specific indication that multiple human experts performed quality assurance on the datasets. The paper notes subjective rating collection but annotator expertise is not described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that a single non-expert performed quality assurance tasks."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.3, 4.4",
          "reasoning": "The subjective quality scores (MOS) were obtained through crowdsourcing following ITU-T Rec. P.808, which involves multiple human annotators who are assumed to be non-experts. For example, in Tencent Corpus (Section 4.1), each clip was rated by over 24 listeners (likely crowd workers). Similarly, IU Bloomington Corpus ratings were crowdsourced on Amazon Mechanical Turk (Section 4.3), and PSTN Corpus ratings were collected on AMT following P.808 (Section 4.4). This collectively shows multiple human non-experts performed quality assurance via subjective ratings."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models performing quality assurance or validation of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses automated data processing (e.g., speech degradation simulation), it does not describe automated verification or algorithmic QA of annotations or subjective scores."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described through subjective ratings collected by multiple crowd annotators as per ITU-T P.808 standards, so this is not applicable."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9149,
      "completion_tokens": 571,
      "total_tokens": 9720
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.4",
          "reasoning": "The Tencent Corpus and PSTN Corpus introduced by the authors include speech clips that were created by recording live speech or selecting clean speech from existing datasets and then subjected to various impairments and processing such as noise addition, reverberation, and packet loss concealment. The speech clips were rated by human listeners through crowdsourcing following ITU-T P.808, which involves human subjective evaluation, thus embodying newly created and scored data contributed by humans. The datasets include live recordings and controlled recordings combined with human subjective MOS ratings."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain entirely AI-generated speech or content; impairments are simulated but the speech signals originate from human speech recordings, not generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any data was produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that machine translation was used to generate any data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.3, 4.4",
          "reasoning": "The Tencent Corpus includes speech clips sourced from publicly available datasets such as Magic Data, ST Mandarin, and AIshell 100h. The IU Bloomington Corpus includes speech clips extracted from COSINE and VOiCES datasets. The PSTN Corpus uses clean reference files derived from the public Librivox audiobook dataset. These data were collected and aggregated from existing sources without solely being created from scratch by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.4",
          "reasoning": "Several datasets, especially the Tencent and PSTN Corpora, are based on existing clean speech but modified through simulated degradations such as noise addition, reverberation, codec impairments, noise suppression, and packet loss concealment to simulate realistic impairments experienced in online conferencing. This demonstrates the data is derived from existing sources with modifications and transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins and generation methods are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9667,
      "completion_tokens": 495,
      "total_tokens": 10162
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the new datasets for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Task Description) and Section 5 (Baseline System)",
          "reasoning": "The training datasets provided (Tencent Corpus, PSTN Corpus, NISQA Corpus, and IU Bloomington Corpus) are used to train models from scratch or to develop models for the non-intrusive speech quality assessment task, as described in the challenge. Baseline systems were trained on these datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe the use of the new datasets for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the datasets being used for reinforcement learning post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 (Task Description), Section 4.5 (Dataset Division), Section 6 (Challenge Results)",
          "reasoning": "The datasets, including newly created test datasets such as TUB and portions of Tencent and PSTN Corpora, are explicitly used for evaluation, benchmarking, and performance measurement of submitted models in the challenge."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss using the new datasets primarily for trend or pattern analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases to augment models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes practical usages for the new datasets in training and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10390,
      "completion_tokens": 499,
      "total_tokens": 10889
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4.5 Dataset Division, last paragraph",
          "reasoning": "The challenge datasets include speech clips composed of Chinese, English, and German languages, which makes the dataset multilingual with more than two human languages represented."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than two languages (Chinese, English, German), so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages including Chinese and German, not only English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages including English, Chinese and German, so it is not monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech clips and subjective MOS scores without any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some mathematical notation is used for evaluation metrics and polynomials, the dataset content itself does not include mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are included in the dataset; it only contains human speech."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly specified as Chinese, English, and German."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of human speech recordings, which inherently contain language."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7608,
      "completion_tokens": 192,
      "total_tokens": 7800
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4 and elsewhere",
          "reasoning": "The paper does not provide any links or references to code repositories or any indication that the code used for constructing the new datasets (Tencent Corpus, IU Bloomington Corpus subset, PSTN Corpus subset) is made publicly available. There is no mention of code release or open-source repositories for the dataset construction or processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 Data Description",
          "reasoning": "The paper documents the dataset creation processes in detail in Section 4, describing the sources, impairment types, processing steps, subjective testing methodologies (crowdsourcing with ITU-T P.808 or ITU-R BS.1534), and dataset division strategies. The documentation includes proportions of different impairments applied, sampling frequencies, number of subjective ratings, and references to related datasets and methodologies, providing transparency and completeness about dataset construction."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6940,
      "completion_tokens": 351,
      "total_tokens": 7291
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection, Section 2.2 Data Annotation, and Section 6 Conclusion",
          "Reasoning": "The authors collected a novel dataset consisting of 11 televised political debate videos from the 2020 Democratic presidential primaries (Section 2.1). These are real-world videos recorded from TV broadcasts by human-operated cameras, making them human-generated data. The paper states this dataset was newly collected by the authors for their study (Section 2 and 6)."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3.1 Vocal Features and Section 2.1 Data Collection",
          "Reasoning": "The audio modality is extracted from the collected debate videos, which were recorded in real-world televised debates, a human-generated source. This data captures the vocal features from the original human speech audio tracks in the videos. The audio is not synthetically generated but directly extracted from human speech (Section 2.3.1)."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3.2 Facial Features and Section 2.3.3 Gestural Features",
          "Reasoning": "The facial and gestural features are extracted from the frames of the collected debate videos, which are human-generated video recordings of the candidates in debates. These features originate from human recordings captured in the videos. Although features are computationally extracted, the base data is human-generated video data (Section 2.3.2 and 2.3.3)."
        }
      ]
    }
  },
  {
    "id": "yoon22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7792,
      "completion_tokens": 206,
      "total_tokens": 7998
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The paper states that eight college students were recruited to annotate the emotional intensity scores for short video clips. College students are generally considered non-expert annotators, and multiple annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided to the annotators beyond the rating scale task."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "There is no mention of any scoring rubrics or guidelines specifying criteria on how to score beyond the 1 to 5 scale."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The paper does not indicate that specific annotation examples were provided to annotators during the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "yoon22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8922,
      "completion_tokens": 372,
      "total_tokens": 9294
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that annotators were subject matter experts or belonged to the target demographic. Therefore, annotators cannot be assumed to be experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Annotation",
          "reasoning": "The dataset's emotional intensity annotation was performed by recruiting eight college students who rated emotional intensity scores for short video clips. There is no indication that these annotators are subject matter experts or members of the target demographic. Multiple annotators (8) performed the annotations, thus 'Multiple Human Non-Experts' applies."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the proposed model is trained on the annotated data for emotional intensity prediction, it is not described as used for quality assurance of the annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic verification of the annotations or dataset quality is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented via multiple human annotators providing scores, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8540,
      "completion_tokens": 431,
      "total_tokens": 8971
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Annotation",
          "reasoning": "The authors collected video clips from televised debates and recruited eight college students to annotate the perceived emotional intensity scores on a scale from 1 to 5, generating original emotional intensity labels for 920 short clips. This annotation is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors train a model to predict emotional intensity, but they do not generate new data purely from models as a dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated by translating content from other languages by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper of any machine translation being used to generate dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The debate videos were collected from publicly available televised debate sources such as NBC, ABC, and CBS. These videos were aggregated and collated without significant modification for the purpose of analysis."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 Non-verbal Feature Extraction",
          "reasoning": "The authors extracted vocal, facial, and gestural features from the collected videos using tools such as Librosa, dlib, and OpenPose, thus deriving structured feature data from the raw videos to be used in the model."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins and generation methods are clearly documented."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9058,
      "completion_tokens": 474,
      "total_tokens": 9532
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Problem Statement), Section 4.1 (Experimental Settings)",
          "reasoning": "The novel debate dataset with annotated emotional intensity scores is used to train the proposed multimodal deep-learning model from randomly initialized weights to predict perceived emotional intensity."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fine-tuning a pre-trained model using the dataset; all model weights are randomly initialized and trained from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for any reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 (Experimental Results)",
          "reasoning": "The dataset is split into training and test sets; the test portion is used for evaluating model performance and benchmarking against baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Case Study: 2020 Democratic Party Presidential Primaries in the USA)",
          "reasoning": "The dataset is used to analyze correlations between the predicted emotional intensity and changes in candidates' favorability in public polls, thus for analyzing trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for training, evaluation, and analysis, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9781,
      "completion_tokens": 606,
      "total_tokens": 10387
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of political debate videos from the 2020 Democratic presidential primaries in the USA, which are conducted in English only, as there is no mention of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are from televised debates in English only, with no indication of a second language being included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection; Abstract; Section 5",
          "reasoning": "The dataset consists of televised debate videos from the 2020 Democratic presidential primaries in the USA, all in English as the candidates speak English. The paper explicitly mentions using timestamped transcripts with speaker information, and all source debates are from US news channels (NBC, ABC, CBS) which feature English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes methods that use code (e.g., Librosa, dlib, OpenPose) for feature extraction, the dataset itself consists of video clips of political debates, not programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of video clips and annotated emotional intensity scores. Mathematical notations appear only in the description of the prediction model and problem formulation, not as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human political debate videos and their annotations. There is no inclusion of biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or fictional languages such as Klingon or Esperanto is found in the dataset description."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the content source and language (English), so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language (English) in video form, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6999,
      "completion_tokens": 183,
      "total_tokens": 7182
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and 6",
          "reasoning": "The paper does not provide any link or mention that the code used for collecting, processing, or annotating the dataset is publicly available. While the paper describes the dataset collection and annotation procedures, there is no indication of a publicly accessible repository or code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1, 2.2, and 2.3",
          "reasoning": "The paper provides detailed documentation on dataset collection, annotation process, and feature extraction methods in Sections 2.1 (Data Collection), 2.2 (Data Annotation), and 2.3 (Non-verbal Feature Extraction). This includes specifics on video sources, annotator recruitment and rating scheme, and extraction tools and parameters for vocal, facial, and gestural features."
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8513,
      "completion_tokens": 129,
      "total_tokens": 8642
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, 2.2",
          "Reasoning": "The dataset includes bonafide singing vocals sourced from open-source singing datasets, which are recordings involving human singers. This counts as human generated audio data. Additionally, it has deepfake singing vocals generated by various state-of-the-art singing voice synthesis (SVS) and singing voice conversion (SVC) models, indicating parts of the data are model generated audio. The whole dataset consists of audio clips only, validating the modality as audio."
        }
      ]
    }
  },
  {
    "id": "zang24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9365,
      "completion_tokens": 242,
      "total_tokens": 9607
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 and 2.2.1, 2.2.2",
            "reasoning": "Deepfake singing voice clips are generated using multiple state-of-the-art open-source SVS and SVC systems, applied automatically to bonafide singing clips in our dataset creation pipeline, constituting an automatic annotation process rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified in paper",
            "reasoning": "The paper does not mention the presence of annotation instructions since the deepfake data generation is automated through synthesis systems, not manual annotation by humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in paper",
            "reasoning": "There is no mention of scoring rubrics or criteria because the labels (bonafide or deepfake) derive from the generation process itself rather than a subjective human labeling requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in paper",
            "reasoning": "No annotation examples or guidelines are provided because the dataset labeling is based on controlled synthesis methods, eliminating the need for examples in annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "zang24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10495,
      "completion_tokens": 296,
      "total_tokens": 10791
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset construction or annotation process involved using AI models as judges for quality assurance, according to the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification or rule-based algorithmic techniques used specifically for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any dedicated quality assurance process for validating the annotations or content of the introduced CtrSVDD dataset. The dataset is constructed by sourcing bona fide vocals from open-source datasets and generating deepfake vocals using known synthesis methods, but no explicit QA process is documented."
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10113,
      "completion_tokens": 461,
      "total_tokens": 10574
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that the bonafide singing vocals are sourced from existing open singing datasets (Section 2.1). There is no indication that original singing vocals were newly recorded or created entirely from scratch by human contributors specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The deepfake singing voice clips are generated by applying 14 state-of-the-art singing voice synthesis (SVS) and singing voice conversion (SVC) systems, most of which are open-source machine learning models trained on publicly available datasets. This constitutes original data generated entirely by AI or machine learning models as detailed in Section 2.2."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any translation of data through human translators in the dataset construction."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that machine translation was used to generate any of the dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The bonafide singing voice clips are collected from multiple existing public singing datasets without significant modification, as described in Section 2.1."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The deepfake data is derived from the bonafide singing clips by applying various singing voice synthesis and conversion models that transform the source recordings into deepfakes. This indicates transformations based on existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the sources and methods used to generate the dataset, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10631,
      "completion_tokens": 292,
      "total_tokens": 10923
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 and 4",
          "reasoning": "The paper describes the use of the CtrSVDD dataset to train baseline deepfake detection models from scratch using various front-end features and a fixed back-end module, with experimental details in Sections 3 and 4 indicating training procedures on the dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The dataset is used for benchmarking and performance evaluation of baseline deepfake detection systems, with specific evaluations, metrics (Equal Error Rate), and ablation studies reported in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 5",
          "reasoning": "The paper uses the dataset to analyze feature effectiveness, generalization challenges, and distribution characteristics of various deepfake methods, demonstrated through detailed discussions and t-SNE visualizations."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11354,
      "completion_tokens": 558,
      "total_tokens": 11912
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 Details of bonafide vocals",
          "reasoning": "The CtrSVDD dataset's bonafide vocals are sourced from various open singing datasets across multiple languages, explicitly Mandarin Chinese and Japanese (e.g., Opencpop, M4Singer, Kising for Mandarin; Ofuton-P5, OnikuKurumi6, Kiritan, and JVS-MuSiC for Japanese). The presence of multiple distinct human languages (Mandarin and Japanese) clearly indicates multilingual coverage."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than two languages and thus does not qualify as only bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content; rather, it includes Mandarin and Japanese singing vocals."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages (Mandarin and Japanese), so it is not monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains singing vocal audio data but does not include any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio clips and not mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human singing voice audio and deepfake singing voice; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no presence of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states the languages of the singing vocals in the dataset; thus the language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains singing voice audio clips in known human languages, so language coverage is applicable."
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8572,
      "completion_tokens": 185,
      "total_tokens": 8757
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2.2, Section 5",
          "reasoning": "The abstract states the CtrSVDD dataset and baselines are publicly accessible. Section 2.2 explains that models used for deepfake generation were trained with open-source toolkits and public recipes, indicating code availability. Section 5 (Conclusion) reiterates the release of dataset, baseline implementations, and pretrained model weights, implying code for data generation and processing is made public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (CtrSVDD dataset design)",
          "reasoning": "Section 2 provides detailed documentation on the dataset creation process including the source of bonafide singing data, detailed description of 14 deepfake generation methods, selection of datasets, segmentation methods, and dataset partitioning. This level of documentation ensures transparency and reproducibility."
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7363,
      "completion_tokens": 150,
      "total_tokens": 7513
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2 and Abstract",
          "Reasoning": "The CN-Spoof dataset introduced in this paper is created by applying five different vocoders (three neural vocoders and two DSP vocoders) to real mel-spectrograms extracted from the waveforms of the CN-Celeb1&2 datasets, which were collected via human speech recordings. Thus, the source real audio is human-generated, and the spoofed audio samples are model-generated due to vocoder synthesis. Therefore, the audio modality encompasses both human-generated (original) and model-generated (vocoder reconstructed) audio data explicitly described in the paper."
        }
      ]
    }
  },
  {
    "id": "zeng23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8215,
      "completion_tokens": 193,
      "total_tokens": 8408
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2",
            "reasoning": "The CN-Spoof dataset was created by applying five vocoders to real mel-spectrograms from CN-Celeb1&2 datasets, which is a deterministic copy-synthesis process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No mention of any annotation instructions is provided since the dataset generation involves vocoding operations, not human labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No rubric or scoring criteria are described for any annotation, as the dataset consists of vocoder-generated fake audio pairs without manual judgment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No annotation examples are provided because the generation process is automatic and does not involve manual annotation requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "zeng23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9345,
      "completion_tokens": 377,
      "total_tokens": 9722
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator with subject matter expertise or as a member of the target demographic for the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance or validation of the dataset annotations or fake data generation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert human annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for anti-spoofing in the experiments, the paper does not state that an AI model was used in quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 (CN-Spoof dataset and protocols)",
          "reasoning": "The CN-Spoof dataset was created by applying multiple pretrained neural vocoders and DSP vocoders to generate fake utterances from the real data\u2019s mel-spectrograms. This process is automated via known vocoding algorithms, effectively performing automatic generation and verification of spoofed data without human annotation verification. The procedure of dataset construction relies on automated vocoder-based synthesis methods, which is a form of automated process ensuring consistency of dataset generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes an automated process to generate the dataset using vocoders, which implies a quality assurance approach via automatic verification rather than absence of any QA."
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8963,
      "completion_tokens": 412,
      "total_tokens": 9375
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset CN-Spoof is not described as original content created entirely from scratch by human contributors. Instead, it is produced by processing existing data using vocoders."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "Section 2 (CN-Spoof dataset and protocols)",
          "reasoning": "While the CN-Spoof dataset is generated using neural vocoders (models), the data is derived from existing real mel-spectrograms. Thus, it is not generated entirely by AI without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely collected or aggregated without modification; it involves transformation using vocoders."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 (CN-Spoof dataset and protocols)",
          "reasoning": "The CN-Spoof dataset is created by applying multiple vocoders to reconstruct waveforms from mel-spectrograms extracted from the existing CN-Celeb1&2 datasets. This involves modifications and transformations applied to existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method for CN-Spoof is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9481,
      "completion_tokens": 414,
      "total_tokens": 9895
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the CN-Spoof dataset being used for pre-training large models on general patterns or in an unsupervised/self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The CN-Spoof dataset combined with CN-Celeb1&2 datasets is used to train multiple anti-spoofing models from scratch using supervised learning, as delineated in the experiments and training methodology sections."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used to fine-tune pre-trained models; the training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that the dataset is used for reinforcement learning or RL-based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.3",
          "reasoning": "A subset of the combined datasets including CN-Spoof is used as a held-out evaluation dataset for benchmarking the performance of anti-spoofing models under cross-genre mismatch protocols."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 1 and Section 4.3",
          "reasoning": "The dataset is used to analyze genre mismatch effects on generalization ability by training models under different cross-genre protocols and visualizing embeddings, thus characterizing trends and patterns related to genre."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models; rather, it is used for training and evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper extensively describes practical uses of the dataset in training, evaluation, and analysis, hence 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10204,
      "completion_tokens": 364,
      "total_tokens": 10568
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The CN-Spoof dataset is based on the CN-Celeb1&2 datasets, which are Chinese speaker recognition corpora consisting of multi-genre speech in the Chinese language. No other human languages are mentioned as part of this dataset, thus it contains entries with exactly one non-English human language, i.e., Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7422,
      "completion_tokens": 156,
      "total_tokens": 7578
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or explicit mention",
          "reasoning": "The paper does not provide any links, URLs, or explicit references to source code repositories for constructing or generating the CN-Spoof dataset or related code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (CN-Spoof dataset and protocols)",
          "reasoning": "The paper provides a detailed description of how the CN-Spoof dataset was created, including using five different vocoders on mel-spectrograms from CN-Celeb1&2 datasets, selection of numbers of utterances, total size of fake utterances, grouping of genres, and data splits for training and evaluation, constituting comprehensive documentation of dataset construction."
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8494,
      "completion_tokens": 201,
      "total_tokens": 8695
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Synthetic text generation",
          "Reasoning": "The synthetic texts are generated by the authors using a data augmentation method based on a seq2seq model which replaces words with others from the same semantic field for agglutinative languages. This is a model-generated dataset specifically created by the authors as part of their new data augmentation approach."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Synthetic audio generation",
          "Reasoning": "The synthetic audios are generated by the authors using a Tacotron 2 TTS model trained on 15 hours of single-speaker transcribed audio. This TTS model synthesizes 99 hours of audio from the synthetic texts created in the previous step, resulting in a new synthetic audio dataset generated entirely by the model without direct human recording."
        }
      ]
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9346,
      "completion_tokens": 258,
      "total_tokens": 9604
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.2, Section 4.3",
            "reasoning": "The synthetic text generation involved linguistic processing with a Quechua lexicon, semantic frame tagging, and bilingual dictionary usage, indicating expert-level human involvement for preparing and validating annotation steps. The TTS model was trained on data recorded by a single speaker, implying controlled human involvement but not an AI-only or automatic annotation process."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.2",
            "reasoning": "The paper details a modified delexicalisation algorithm and provides a methodology for semantic frame tagging and text preparation, implying that explicit instructions were followed for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no explicit mention of scoring rubrics or a grading system used for quality control of annotated data in the synthetic text or audio generation process."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.2, Table 3",
            "reasoning": "An example of a delexicalised Quechua sentence with semantic frame codes is provided (Table 3), showing that examples are given in annotation guidelines for text processing."
          }
        }
      ]
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10476,
      "completion_tokens": 326,
      "total_tokens": 10802
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert or annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human expert annotators performing quality assurance on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that multiple human non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance is not described as being performed by an AI model acting as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification processes to assure quality of the dataset or annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the creation and preprocessing of the Southern Quechua speech corpus and synthetic dataset generation, but does not document any explicit quality assurance process for validating dataset annotations or content, such as human review or automated checks. While the TTS synthetic audio quality was measured using a mean opinion score (MOS) evaluated by Quechua speakers, this assessment relates to the output audio quality rather than a quality assurance process for the dataset annotations or labeling. Therefore, no specific quality assurance procedures concerning dataset content or annotations are described."
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10094,
      "completion_tokens": 710,
      "total_tokens": 10804
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 The Corpora, Section 1 Introduction, and Section 7 Conclusions",
          "reasoning": "The authors have created a new speech corpus for Southern Quechua consisting of 99 transcribed and cleaned hours of audio data, collected from multiple sources including dictionaries and a book. They preprocessed and cleaned this corpus themselves, improving audio quality and segmenting as needed. This dataset is original content created by human contributors specifically for this research, as stated in the Introduction (contribution 1) and detailed in Section 4.1. It is not a translation or adaptation but an original collection and preparation of Quechua speech data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.2 Synthetic Text Generation, Section 4.3 Synthetic Audio Generation",
          "reasoning": "The paper describes synthetically generated text produced using a seq2seq model with a modified delexicalisation algorithm for agglutinative languages, generating 8,320 synthetic sentences (Section 4.2). These synthetic sentences were then used to generate 99 hours of synthetic audio using a Tacotron 2 text-to-speech model trained on 15 hours of single-speaker audio from the corpus (Section 4.3). This indicates that new data\u2014both synthetic text and synthetic audio\u2014was generated entirely by AI models based on the original data but representing novel generated content, not mere transformations. Thus, the synthetic text and synthetic audio data qualify as New Data from Model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation systems to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 The Corpora",
          "reasoning": "The initial corpora used include the Siminchik corpus and Lurin corpus, which contain audio and transcripts collected from existing multiple sources such as dictionaries and a book. These datasets were preprocessed and cleaned but essentially represent collated data aggregated from existing sources without significant fundamental modification. This collated data formed the base for further augmentation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2 Synthetic Text Generation, Section 4.3 Synthetic Audio Generation",
          "reasoning": "The synthetic texts were generated by applying a modified delexicalisation algorithm and seq2seq model on the original corpus, which is a transformation and adaptation of the existing textual data. Then, synthetic audio was derived by synthesizing these texts using a TTS model trained on a subset of original audio. Therefore, the synthetic audio is derived from the original data through a multi-step process involving model-based transformations. This indicates a derived data nature."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and methods of generation are explicitly documented in the paper."
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10612,
      "completion_tokens": 234,
      "total_tokens": 10846
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.4 (Training of Southern Quechua ASR system) and Section 5 (Results)",
          "reasoning": "The authors used the introduced augmented dataset, consisting of 99 hours of original transcribed audio plus 99 hours of synthetic audio generated via their data augmentation method, to train an ASR model from scratch with the wav2letter++ architecture, as described in Section 4.4 and evaluated in Section 5."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11335,
      "completion_tokens": 608,
      "total_tokens": 11943
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced in the paper contains only one language, Southern Quechua, as discussed throughout the paper. Although English is used for explanation and in bilingual dictionaries for linguistic processing, the dataset entries themselves are exclusively in Quechua."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not bilingual. While the paper mentions the use of Quechua-English bilingual dictionaries for semantic frame labeling in synthetic text generation (Section 4.2), the dataset entries are not presented as bilingual text pairs but only as Southern Quechua sentences."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content. English appears in translations and explanations but is not part of the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1, 'The Corpora'; Section 2, Quechua Language",
          "reasoning": "The new dataset introduced is a speech corpus in Southern Quechua, a non-English language. It contains 99 hours of transcribed audio solely in Southern Quechua, with synthetic text and audio also in the same language. There is no indication of presence of other human languages in the data entries themselves."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses the use of machine learning toolkits and models, the dataset itself does not contain code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical notations are present in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any constructed or fictional languages; it only contains Southern Quechua."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as Southern Quechua."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (Southern Quechua), so the 'N/A' category does not apply."
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8553,
      "completion_tokens": 163,
      "total_tokens": 8716
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.1 The Corpora",
          "reasoning": "The paper states that both the Siminchik corpus and the Lurin corpus can be found at https://github.com/Llamacha/quechua_resources, indicating that the associated data and code used for collection and preprocessing are publicly available in an accessible repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 The Corpora and Section 4 Methodology",
          "reasoning": "The paper provides detailed descriptions of the dataset creation and preprocessing steps, including cleaning audio files, segmentation, normalization to Southern Quechua standard, and the usage of tools like the Quechua morphological parser and normalizer. This documentation comprehensively covers the dataset construction process."
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7250,
      "completion_tokens": 183,
      "total_tokens": 7433
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The paper explicitly states that the authors collected speech recordings from 254 Chinese male patients who read a given Chinese text in sitting and lying positions, with audio recorded at 16kHz. This speech data is newly collected by the authors for their study, indicating the audio modality and human origin."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "Patient characteristic features such as age, BMI, and neck circumference were collected alongside the speech data from the patients. These features are manually gathered clinical data about the patients, representing tabular data with human origin, introduced newly by the authors for their OSA detection study."
        }
      ]
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8102,
      "completion_tokens": 187,
      "total_tokens": 8289
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1",
            "reasoning": "The dataset consists of patients hospitalized due to sleep snoring at Beijing Tongren Hospital, with speech data and patient characteristics collected likely by medical professionals or experts during clinical procedures."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide explicit annotation instructions for data collection, only mention that patients read a set text in sitting and lying positions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention of scoring rubrics or specific labeling guidelines beyond AHI threshold classification for OSA in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not include example annotations or guidelines illustrating the annotation process for the dataset."
          }
        }
      ]
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9232,
      "completion_tokens": 282,
      "total_tokens": 9514
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the collection of a new in-house dataset from 254 patients with speech data and patient characteristics; however, no mention is made of any quality assurance or annotation verification process performed on the dataset. There is no documentation of expert review, multiple annotators, non-expert annotators, AI model validation, or automated verification steps. Therefore, no quality assurance process is described or performed."
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8850,
      "completion_tokens": 387,
      "total_tokens": 9237
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper states that the authors collaborated with Beijing Tongren Hospital to collect data from 254 patients who read a set piece of text in both sitting and lying positions. This data was recorded specifically for this study, thus it is original human-contributed speech data generated from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources; it was collected anew from patients and thus not collated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the authors use pre-trained models, the actual dataset collected consists of original recordings and patient characteristics, not derived from existing datasets or transformed versions thereof."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is clearly documented as newly collected data from human patients in Section 4.1."
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9368,
      "completion_tokens": 471,
      "total_tokens": 9839
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The model uses a pre-trained XLSR encoder and fine-tunes it with the dataset; there is no mention of training a model from scratch with the dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Method), Section 4.1 (Dataset), Section 5 (Results)",
          "reasoning": "The in-house dataset collected by the authors is used to fine-tune the pre-trained XLSR model in a supervised manner for OSA detection. Section 3 describes using supervised training with audio-AHI paired data. Section 4.1 describes the dataset collection. Section 5 shows results based on fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training methods are described or used with the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Dataset), Section 5 (Results)",
          "reasoning": "The dataset is split into training, validation, and test sets, and the test set is used to evaluate and benchmark model performance as described in Sections 4.1 and 5."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is used for training and evaluation, not primarily for analysis of trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has clear and practical usage in both supervised fine-tuning and evaluation stages."
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10091,
      "completion_tokens": 356,
      "total_tokens": 10447
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The in-house dataset consists of speech recordings from 254 patients who read a piece of Chinese text consisting of 10 sentences. The paper explicitly states that all patients were Chinese males and that the text read was in Chinese, indicating the dataset is monolingual and non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7309,
      "completion_tokens": 118,
      "total_tokens": 7427
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1",
          "reasoning": "The paper describes the dataset collection procedure but does not include any mention of publicly available code repositories or links related to the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1",
          "reasoning": "The paper provides detailed documentation of the dataset collection including patient demographics, data acquisition procedure (reading 10 sentences in sitting and lying positions), recording specifications (sample rate), and ethical approval. This constitutes thorough documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7256,
      "completion_tokens": 229,
      "total_tokens": 7485
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data set",
          "Reasoning": "The newly built EEG dataset was collected from 12 healthy participants during EEG recording using a 64-channel electrode cap and NeuroScan Acquire 4.3 software, as described in Section 2.1. The EEG data comprises approximately 240 minutes of recordings evoked by speech emotional prosody stimuli, and was manually recorded in a controlled experimental protocol by human subjects."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data set",
          "Reasoning": "The emotional prosody speech stimuli used to evoke the EEG responses were selected from the Mandarin Chinese Speech Emotional Stimulation Database (MCAESD) built by natives with broadcasting experience. The stimuli consist of concatenated declarative Mandarin Chinese meaningless sentences spoken by human actors to evoke specific emotions. This dataset was manually recorded by human speakers and validated by human listeners, as explicitly stated in Section 2.1."
        }
      ]
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8108,
      "completion_tokens": 310,
      "total_tokens": 8418
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1",
            "reasoning": "The EEG emotion dataset involved 12 participants as subjects (decoders) who performed emotion annotation by self-assessment after listening to emotional prosody stimuli. The stimuli were originally encoded by six students with broadcasting experience (emotion encoders) who recorded the speech stimuli. The participants then annotated their emotional response among four emotion types after each stimulus. Thus, annotation was performed by multiple human experts as the annotators had relevant background and the paper refers to them as decoders and listeners with accuracy validation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not describe any explicit annotation instructions given to the subjects for their self-assessment annotations during the EEG experiment. It only mentions the annotation task and the selection of high-accuracy stimuli according to listeners' annotation but does not provide or refer to detailed annotation guidelines or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no mention of scoring rubrics or explicit emotion classification criteria provided to annotators in the annotation guidelines. The paper only indicates that subjects chose among four emotion types without detailing rubrics or graded scales."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not provide or mention examples included as part of the annotation guidelines for participants performing the emotion annotation task."
          }
        }
      ]
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9238,
      "completion_tokens": 398,
      "total_tokens": 9636
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention quality assurance performed by a single human expert annotator for the new EEG emotion dataset or the emotional prosody stimuli."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1, Data set",
          "reasoning": "The EEG emotion dataset was built using speech stimuli from the Mandarin Chinese Speech Emotional Stimulation Database (MCAESD), where recordings were made by six students with broadcasting experience (considered experts in speech encoding of emotion). Additionally, annotation results were obtained from 40 subjects (decoders), who responded to the speech stimuli in the EEG experiment and provided emotional self-assessments. This indicates multiple human experts and multiple human decoders participated in annotation and validation of the stimuli and induced emotions."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1, Data set",
          "reasoning": "The EEG experiment involved 12 native Mandarin speakers without reported specific expertise (non-expert participants) who provided self-assessment annotations after hearing the prosody stimuli, serving as multiple human non-expert annotators validating the emotional content of the stimuli and their emotional responses."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model used for quality assurance of the dataset or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process via code or rule-based techniques for annotation quality assurance is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset and annotations were validated by multiple human expert encoders and multiple human non-expert decoders, so quality assurance process is described."
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8856,
      "completion_tokens": 516,
      "total_tokens": 9372
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The EEG dataset was newly built by collecting EEG data from 12 healthy participants responding to specific speech stimuli selected from the Mandarin Chinese Speech Emotional Stimulation Database (MCAESD). The speech stimuli were recorded by human speakers with broadcast experience, and the EEG data was collected through human-conducted experiments. Therefore, both the EEG data and the speech stimuli inducing the EEG responses are original human-generated data collected via experimental procedures described in the paper."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was generated entirely by AI or machine learning models. All data discussed are collected from human participants or existing speech databases."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was generated via machine translation methods."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The speech stimuli used to evoke EEG responses were selected from an existing speech database (MCAESD). The authors aggregated and selected specific speech stimuli based on evaluation criteria to build the EEG experiment materials. Hence, the speech stimuli component can be considered collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The authors concatenated multiple speech sentences from the existing MCAESD database to create longer, approximately one-minute clips to induce stable EEG responses, which is a modification and adaptation of the original speech stimuli. Thus, the final speech stimuli can be considered derived data based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and methods of generation for the data are clearly described in the paper, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9374,
      "completion_tokens": 335,
      "total_tokens": 9709
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, Experiments",
          "reasoning": "The paper describes training the EEG-Prosody CRNN model from scratch on the newly constructed EEG dataset and prosody features using supervised learning, as detailed in the experiments section."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fine-tuning a pre-trained model using this dataset; the model is trained directly on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning based post-training (such as RLHF) is described or utilized on this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2, Results and Discussions",
          "reasoning": "The dataset is used as a benchmark for evaluating the performance of the proposed and baseline models, with classification accuracies reported using cross-validation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2, Results and Discussions and Section 4, Conclusion",
          "reasoning": "The dataset is used to analyze feature importances, frequency bands, and to assess the impact of emotional prosody features on EEG emotion classification, providing insights beyond just training and testing."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used or described as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10097,
      "completion_tokens": 542,
      "total_tokens": 10639
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described in the paper uses only Mandarin Chinese in the speech stimuli, so it does not contain more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only one language, Mandarin Chinese; therefore, it is not bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English speech stimuli; it is based on Mandarin Chinese meaningless sentences."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The paper states that the speech stimuli are selected from the Mandarin Chinese Speech Emotional Stimulation Database and consists of Chinese meaningless sentences. All subjects are native Mandarin Chinese speakers. Thus, the dataset entries contain exactly one non-English language: Mandarin Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include programming or structured code content; it consists of speech stimuli and EEG recordings."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although EEG recordings are biological signals, the dataset entries are EEG data and speech stimuli, the stimuli being human language speech, not non-human communication systems or biological sequences like DNA."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The speech stimuli use Mandarin Chinese meaningless sentences, not constructed languages such as Klingon or Esperanto."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as Mandarin Chinese, so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language stimuli in Mandarin Chinese and EEG recordings; therefore, it cannot be considered as having no language."
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7315,
      "completion_tokens": 161,
      "total_tokens": 7476
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention in the text",
          "reasoning": "The paper does not contain any mention or link to code repositories or supplementary materials related to code for constructing or processing the dataset. There is no indication that the code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Data set) and Section 2 (Material and Methods)",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including the selection criteria for speech stimuli from MCAESD, description of the EEG recording experiment, participant demographics, experiment protocol, equipment and preprocessing methods. This extensive documentation covers how the EEG data and stimuli were collected and processed for the new EEG emotional dataset built by the authors."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7287,
      "completion_tokens": 129,
      "total_tokens": 7416
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 CVTE-poly",
          "Reasoning": "The CVTE-poly dataset is a new dataset introduced by the authors, consisting of sentences containing target polyphone words crawled from \u2018Baidu Baike\u2019, a human-generated encyclopedia website. The labels for pronunciations were manually confirmed and corrected for polyphones with multiple pronunciations. Therefore, the data are text modality and originate from human-generated sources (webpages authored by people and manual annotation). No indication of model-generated or synthetic data is present."
        }
      ]
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8139,
      "completion_tokens": 211,
      "total_tokens": 8350
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "In Section 3.2, the authors describe a manual confirmation of some labels because polyphones in a word can have multiple pronunciations depending on context. This manual checking implies multiple human annotators with expertise verified the labels to ensure correctness."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not describe or provide any annotation instructions for the annotators who manually confirmed the labels, so no instructions can be confirmed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No scoring rubrics or formal evaluation criteria for annotation decisions are mentioned or described for the human annotators in the dataset creation or refinement process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not mention providing examples for annotation during manual confirmation or labeling processes."
          }
        }
      ]
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9269,
      "completion_tokens": 368,
      "total_tokens": 9637
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions manual confirmation of some labels but does not specify that a single human expert conducted quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human experts performing quality assurance; it only mentions manual confirmation without specifying number or expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information indicates that a single non-expert conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper states that for polyphones with multiple pronunciations, 'we check the sentences and labels manually' but does not specify number or expertise of annotators. Since this suggests manual annotation without explicit expert confirmation, it implies multiple human non-experts were likely involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used as judges for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification or rule-based checks as part of quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A manual label checking process is described, so quality assurance is documented."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8887,
      "completion_tokens": 433,
      "total_tokens": 9320
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset was created entirely from scratch by human contributors without relying on existing data sources. Instead, data was collected and then refined manually."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not generated by AI or machine learning models. The dataset was created by crawling web sources and manual annotation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating content from another language using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 CVTE-poly",
          "reasoning": "The dataset was created by collecting polyphone words from the latest official dictionary (MCD-7) and then crawling sentences containing these words from Baidu Baike, an existing web encyclopedia. This indicates data collection from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Refinements to CPP and Section 3.2 CVTE-poly",
          "reasoning": "The authors refined the existing CPP dataset by correcting wrong labels and removing certain samples. Additionally, for CVTE-poly, after crawling sentences from Baidu Baike, some labels were manually confirmed or corrected to ensure quality. Thus, the dataset is not purely raw collection but contains modifications derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies clearly the collection and refinement process; data origin is documented."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9405,
      "completion_tokens": 480,
      "total_tokens": 9885
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using CVTE-poly for pre-training large models in an unsupervised or self-supervised manner. Instead, it leverages pre-trained language models as backbones."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses pre-trained language models (ERNIE) as the backbone and does not describe training models from randomly initialized parameters using CVTE-poly."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 and 5 (Baseline configuration, Experiments)",
          "reasoning": "The CVTE-poly dataset is used to fine-tune pre-trained ERNIE models with supervised learning by training a fully connected layer and softmax layer on top to predict polyphone pronunciations."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of using CVTE-poly for reinforcement learning post-training techniques such as RLHF is provided in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CVTE-poly dataset is not used exclusively for evaluation or benchmarking. Instead, the paper uses the original CPP test set for evaluation; CVTE-poly is primarily a training dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While analysis is conducted regarding dataset balance and bias, CVTE-poly itself is not primarily used for analysis but for training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe CVTE-poly serving as a knowledge base for retrieval-augmented generation or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly uses CVTE-poly in training and demonstrates its utility in the modeling pipeline."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10128,
      "completion_tokens": 427,
      "total_tokens": 10555
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset CVTE-poly focuses exclusively on Chinese text with polyphone disambiguation, and there is no mention of multiple human languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only Chinese language data; there is no indication of exactly two languages being present."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are not in English; they are in Chinese characters with Pinyin annotations for pronunciation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2 (CVTE-poly description) and throughout the paper",
          "reasoning": "The dataset CVTE-poly is exclusively Chinese language data, consisting of Chinese polyphone characters and their correct Pinyin pronunciations based on context. There is no inclusion of other languages besides Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or code language content is present within the dataset entries; the dataset consists of natural language text in Chinese."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries do not include mathematical or formal logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No biological or non-human communication data is part of the dataset, which is strictly Chinese textual data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention or inclusion of fictional or constructed languages such as Klingon or Esperanto is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset explicitly documents its language as Chinese with detailed annotations; language is clearly specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain Chinese language text; therefore, language is present."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7346,
      "completion_tokens": 160,
      "total_tokens": 7506
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and footnote on page 1",
          "reasoning": "The paper states in the abstract and footnote that both codes and data are available to the public at a specific URL (https://github.com/NewZsh/polyphone). This indicates that the code related to data collection and processing is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3, especially 3.2 CVTE-poly",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 3, describing the pipeline for collecting polyphones from the dictionary, crawling sentences from Baidu Baike, manual checking of ambiguous labels, and data statistics. This constitutes transparent and complete documentation of the dataset construction."
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7903,
      "completion_tokens": 172,
      "total_tokens": 8075
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction, Section 4 Method, Section 5 Experiments",
          "Reasoning": "The paper explicitly states that they build and publish a new personalised speech enhancement (PSE) dataset to reflect on-device close-talk microphone scenarios with numerous types of environmental noise. This dataset includes corrupted input utterances paired with enrolment utterances from the same speaker. The clean speech is from LibriSpeech, but the new dataset is constructed by the authors by adding silence, overlapping and non-overlapping noise, and noise can start/end anywhere in the clip to better represent on-device scenarios. This data consists of audio recordings (speech and noise mixtures) captured with human involvement (human speech recordings from LibriSpeech and ambient noises)."
        }
      ]
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8755,
      "completion_tokens": 203,
      "total_tokens": 8958
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4, Dataset description in Section 5",
            "reasoning": "The dataset is constructed by mixing clean speech from LibriSpeech with various environmental and babble noises from datasets such as FreeSoundDataset and LibriMix, using a scripted and deterministic data synthesis process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not stated in paper",
            "reasoning": "There is no indication in the paper of annotation instructions being provided for data creation, as the dataset is synthesized automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not stated in paper",
            "reasoning": "No scoring rubrics or evaluation guidelines for annotation are mentioned, as no subjective annotations appear to be performed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not stated in paper",
            "reasoning": "The paper does not provide examples of manual annotations or guidelines; dataset formation is automated and no annotation examples exist."
          }
        }
      ]
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9885,
      "completion_tokens": 316,
      "total_tokens": 10201
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance (QA) process performed by a single human expert on the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts performed QA on the new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention QA done by a single non-expert human annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert humans performing QA on the dataset in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models to perform quality assurance on the new dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset construction involves simulated noisy audio generated through mixing clean speech and noise from existing datasets, the paper does not explicitly describe any automated verification or algorithmic QA process for dataset annotations or content quality."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new dataset for personalized speech enhancement reflecting on-device close-talk microphone scenarios, but does not document or describe any quality assurance process for validating the dataset annotations or contents. The dataset is generated by mixing clean speech with noise sources, but no QA process for correctness or consistency is described."
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9503,
      "completion_tokens": 448,
      "total_tokens": 9951
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of entirely original datasets made from scratch by human contributors. Instead, it constructs its dataset using existing data sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated purely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss translating data from another language, neither by humans nor by machines."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating data by machine translating from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section '4.1 Self-attention Encoder' and Abstract",
          "reasoning": "The paper states that the new dataset is built to reflect on-device close-talk microphone scenarios and uses speech samples from existing datasets like LibriSpeech and noise samples from FreeSoundDataset and LibriMix. Thus, the dataset is aggregated from pre-existing sources without creation of entirely new original recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section '4.1 Self-attention Encoder', Abstract",
          "reasoning": "The authors generate a new PSE dataset by applying modifications such as adding noise, periods of silence, overlapping/non-overlapping noises, and varying noise placement to existing speech data from LibriSpeech. These transformations constitute derivation since the data is based on existing sources but adapted to reflect real-world on-device use cases."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is documented in the paper; therefore, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10021,
      "completion_tokens": 261,
      "total_tokens": 10282
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Section 5",
          "reasoning": "The newly built dataset reflects the on-device close-talk microphone scenario and is used to train the proposed fully Transformer-based personalized speech enhancement (PSE) models from scratch. The paper details training experiments on this dataset, indicating it is used for training the models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5, Table 1 and 2",
          "reasoning": "The dataset is used to evaluate model performance in terms of signal-to-distortion ratio (SDR) and word error rate (WER) for downstream ASR tasks, benchmarking the proposed models against baselines."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10744,
      "completion_tokens": 566,
      "total_tokens": 11310
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's introduced dataset is based on LibriSpeech, which is an English speech corpus. There is no mention of including multiple languages; therefore, the dataset is not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages; it is based solely on English speech from LibriSpeech."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 and throughout the paper, e.g., 'For ground-truth clean speech, we use LibriSpeech [38]'",
          "reasoning": "The new dataset is constructed by using LibriSpeech as clean speech source, which contains only English language. The paper specifically states using LibriSpeech for training and testing, indicating dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on LibriSpeech, an English dataset, so non-English monolingual does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio data combined with noise; there is no programming or code content included in dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes mathematical expressions related to the model and methods, the dataset entries do not include mathematical or logical notation as content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset exclusively consists of human speech audio and environmental noise; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly documented as English from LibriSpeech, so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of speech audio data in English and thus contains language."
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7962,
      "completion_tokens": 187,
      "total_tokens": 8149
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 1 Introduction, Section 5 Evaluation",
          "reasoning": "The paper mentions that the authors design and publish a new PSE dataset for on-device close-talk microphone scenarios, but there is no explicit link, URL, or reference to any publicly available code repository for dataset construction, preprocessing, or generation provided in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 1 Introduction, Section 5 Evaluation",
          "reasoning": "The paper documents the dataset creation process in general terms: describing the use of LibriSpeech for clean speech, adding periods of silence, overlapping and non-overlapping noise, types of environment noise from FreeSoundDataset and LibriMix babble noise, and aligns the data splits for train, dev, and test sets. These details on dataset construction are sufficient to understand the dataset creation process even though not exhaustive."
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6700,
      "completion_tokens": 158,
      "total_tokens": 6858
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.4 Data collection",
          "Reasoning": "The DysArinVox dataset includes speech recordings from 173 participants including healthy individuals and those with vocal disorders. Recordings are collected in professional studio settings using human speakers who read carefully designed Mandarin scripts covering phonemes and various speech styles."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.4 Data collection",
          "Reasoning": "The dataset provides laryngoscopic images of patients, which are medical images captured via human-operated laryngoscopic equipment, thus constituting human generated data in image modality."
        }
      ]
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7552,
      "completion_tokens": 203,
      "total_tokens": 7755
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.3",
            "reasoning": "The paper states that MOS and GRBAS scores were provided by professional doctors, implying multiple speech pathology experts performed subjective evaluations and detailed pathological diagnosis."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not explicitly mention providing detailed annotation instructions for the experts performing subjective or objective evaluations."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "The use of standardized scoring scales like MOS and GRBAS, with detailed indices for laryngoscopic observations, indicates the presence of scoring rubrics guiding annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3, Table 4",
            "reasoning": "Table 4 presents illustrative examples of perceptual evaluation and laryngoscope indices with example subjects, demonstrating inclusion of annotation examples for guidance."
          }
        }
      ]
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8682,
      "completion_tokens": 190,
      "total_tokens": 8872
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The paper states that subjective evaluations (MOS and GRBAS scores) were provided by professional doctors, indicating that quality assurance of annotations was done by human experts (speech pathology experts). Although the number of experts is not specified, the involvement of subject matter experts implies at least a single human expert performed QA."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8300,
      "completion_tokens": 419,
      "total_tokens": 8719
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (The construction of DysArinVox)",
          "reasoning": "The DysArinVox dataset consists of speech recordings and laryngoscopic images collected directly from 173 participants, including 146 pathological speakers and 27 healthy controls. The recordings were designed and recorded specifically for this corpus, with original, carefully crafted scripts covering all Mandarin phonemes and prosody variations. The data were recorded in professional settings with human participants and involved expert clinical diagnostic labels and evaluations by speech pathology professionals. Therefore, the data is original content generated entirely from scratch by human subjects and clinical annotations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models. All data are collected from human participants and clinical evaluations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data produced by translating content from another language through human translators. The corpus is composed of original Chinese speech data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data included machine translated content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is newly recorded and does not aggregate or compile existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information shows that the data is based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of the data collection."
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8818,
      "completion_tokens": 258,
      "total_tokens": 9076
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 1, 3.3, and 4",
          "reasoning": "The paper primarily introduces DysArinVox as a resource for pathological speech research and analysis. The corpus provides detailed subjective and objective evaluations (e.g., MOS, GRBAS scores, laryngoscopic indices) and rich labels for diverse disorders, aimed at enhancing the understanding of physical bases of speech disorders, patterns, and characteristics. The dataset is emphasized as a foundation for further research and diagnostic framework development, rather than being explicitly described as used for model training or evaluation in the paper."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9541,
      "completion_tokens": 578,
      "total_tokens": 10119
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset DysArinVox only contains entries in Mandarin Chinese. No mention is made of any other human languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two languages; it only contains Mandarin Chinese."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English content; it is developed specifically for Mandarin Chinese pathological speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 3.1",
          "reasoning": "The DysArinVox corpus contains only Mandarin Chinese speech data. The paper explicitly states that the corpus is a 'Chinese pathological speech corpus' and that recording scripts were designed to cover phonemes in Mandarin Chinese, including modern and classical Chinese poetry (Section 3.1). Hence, it is considered monolingual with exactly one non-English language, Mandarin."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech and laryngoscopic image data related to Mandarin speech disorders. There is no indication of including programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical symbolic representations or formal notations are described as entries in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset includes laryngoscopic images showing the anatomical vocal folds, these visual data are medical images rather than biological sequences or non-human communication data such as DNA sequences or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed or fictional languages in the dataset; all speech data is in natural Mandarin Chinese."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language composition of the dataset is explicitly documented as Mandarin Chinese; it is not unspecified or unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data in Mandarin Chinese, so it does contain language entries."
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6759,
      "completion_tokens": 153,
      "total_tokens": 6912
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention any publicly available code or provide links to code repositories related to the construction, preprocessing, or generation of the DysArinVox dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 1, 2, and 3 (especially Section 3)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including recording script design (Section 3.1), diversity of disorders (Section 3.2), labeling methodology (Section 3.3), and data collection environment and procedures (Section 3.4). This comprehensive description of methods ensures transparency and allows understanding of dataset construction."
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8091,
      "completion_tokens": 330,
      "total_tokens": 8421
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2, Database description: The MISP2021-A VWWS database is described as a collection of recorded audio-visual data from conversations in a home TV scenario, including far-, mid-, and near-field audio data. Further, Section 4.1 mentions audio data simulation using pyroomacoustic and noise addition with MISP2021 tools.",
          "Reasoning": "The audio data in the MISP2021-A VWWS database includes real recorded human speech collected during sessions from native speakers (human generated). However, Section 4.1 states that data simulation for audio data was done using room impulse response generation and noise addition tools, indicating that part of the audio data was also model generated (simulated). Therefore, both human and model generated labels are true for audio modality."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 and 3.2 describe video data as the lip-region video of speakers captured by cameras in the recording sessions. The video data is collected from actual speakers in natural conversation scenarios. Section 3.2 details using video frames as input for lip feature extraction.",
          "Reasoning": "The video data modality consists of real human-recorded video sequences of speakers' lip regions during data collection sessions, implying human involvement during data capture. There is no mention of video data being generated or simulated by models. Thus, only Human Generated is true and Model Generated is false for video modality."
        }
      ]
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8943,
      "completion_tokens": 228,
      "total_tokens": 9171
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2",
            "reasoning": "Section 2.2 describes the MISP2021-A VWWS database as containing manually double-checked data, and removal of asynchronous samples implies that human experts reviewed and curated the data to ensure quality and synchronization. The annotation likely involved expert humans to identify wake word presence and confusing words for evaluation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "The paper mentions non-overlapping speakers and recommends various conversation topics during recording, implying guidelines and instructions were given to annotators or speakers to ensure consistent data collection and annotation quality."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "There is no explicit mention of scoring rubrics or formal annotation rubrics in the paper for labeling the wake word or confusing words."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not describe providing annotation examples or samples as part of the annotation process or guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10073,
      "completion_tokens": 335,
      "total_tokens": 10408
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance carried out by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information describing involvement of multiple human experts for quality assurance in the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about QA by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA being conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an AI model being used as a judge for dataset quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 The MISP2021-A VWWS database",
          "reasoning": "The paper states that the authors \"double-checked all data and deleted a few asynchronous samples\" in the training and development sets and added confusing word utterances in the evaluation set to increase challenge. However, it does not describe human annotators verifying data labels. The 'double-checking' suggests the use of automated or semi-automated verification processes rather than manual expert annotation. Thus, the quality assurance process appears to be primarily automatic or algorithmic verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper indicates some level of checking and correction of the data, so quality assurance is documented, therefore N/A does not apply."
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9691,
      "completion_tokens": 539,
      "total_tokens": 10230
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The MISP2021-A VWWS database is described as a newly designed and publicly released audio-visual wake word spotting database. It includes about 125 hours of audio-visual data collected from 327 native Mandarin Chinese speakers in various realistic conversational home TV scenarios. The data involve multi-microphone audio and multi-camera video recordings at multiple distances (near-, mid-, and far-field). The authors explicitly state that this database was designed to overcome the limitations of existing corpora and the recordings were made with human participants, indicating the data is original content created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any dataset content fully synthetically or automatically by models. While data augmentation is applied during training, the original dataset is recorded from human speakers in real environments."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves translated content from other languages via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation involvement in the dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not collected or aggregated from existing sources. The data were newly recorded by the authors' teams specifically for this challenge and study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "In Section 4.1, the authors describe audio data simulation involving Room Impulse Responses (RIR) generated by pyroomacoustic toolkit according to real room sizes and microphone positions, and adding noise with different SNR levels using official MISP2021 tools. This indicates that part of the data (simulated audio variations) is derived from original recorded data by applying modifications and transformations including reverberation and noise addition to augment the training data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the origin and methods of data generation for the introduced dataset; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10209,
      "completion_tokens": 340,
      "total_tokens": 10549
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), especially 4.1 (Data simulation and implementation details) and 4.2 - 4.4 (Results)",
          "reasoning": "The MISP2021-A VWWS database is used to train audio-only, video-only, and audio-visual fusion wake word spotting models from scratch, as described in Section 4. Data augmentation and training details confirm usage for training models starting from initial parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) and Section 3.4 (Evaluation)",
          "reasoning": "The database is used for evaluating system performance on metrics such as false alarm rate, false reject rate, and combined Score on development and evaluation subsets, as detailed in Sections 3.4 and 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.2 - 4.4 and the Conclusion",
          "reasoning": "The dataset is used for analyzing system behavior and the complementary benefit of visual information to acoustic data, including detailed result analysis and misclassification examination, indicating usage for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10932,
      "completion_tokens": 355,
      "total_tokens": 11287
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 - The MISP2021-A VWWS database",
          "reasoning": "The dataset consists of audio-visual data where the wake word is \u201cXiao T Xiao T\u201d. All speakers are native Chinese speaking Mandarin without strong accents, indicating the dataset contains exactly one non-English human language, Mandarin Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8150,
      "completion_tokens": 155,
      "total_tokens": 8305
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2.2",
          "reasoning": "The paper explicitly states in the abstract and in Section 2.2 that the MISP2021 audio-visual wake word spotting database and the code are released publicly online. This indicates that the code related to data collection, preprocessing, and generation is made available to the community."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 (Database)",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 2.2, including dataset features, speaker distribution, recording details, data segmentation, and specifications of data fields and modalities. This constitutes clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9441,
      "completion_tokens": 262,
      "total_tokens": 9703
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 4.1",
          "Reasoning": "The VCTK-Stutter dataset is created by extending the existing VCTK++ dataset via editing real acoustic waveforms based on forced alignment and word-level alignments acquired by WhisperX. It introduces dysfluencies such as word repetition and missing by editing audio to introduce silences or repetitions. Since it involves extensions on real human speech recordings from the VCTK corpus, the audio data are human generated recordings with manual or rule-based editing, not synthesized by a model."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2, especially 2.2.1 and 2.2.2",
          "Reasoning": "The VCTK-TTS dataset is a synthetic dysfluency dataset created by injecting dysfluencies in the text (phoneme) space using TTS rules and then generating corresponding speech via the VITS text-to-speech model. This process is fully automated where the dysfluent speech audio is synthesized by the VITS model, making the audio data model generated rather than human recorded."
        }
      ]
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10293,
      "completion_tokens": 291,
      "total_tokens": 10584
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The dysfluency annotations for VCTK-Stutter and VCTK-TTS are produced through automated simulation pipelines: VCTK-Stutter uses forced alignment and WhisperX for word-level alignment combined with rule-based insertion of dysfluencies, and VCTK-TTS uses text-based TTS rules applied to IPA sequences combined with the VITS model generating speech and alignments. Hence annotation is an automatic simulation process, not human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The datasets are generated via automated simulation pipelines without mention of human annotators or instructions to annotators; annotation is derived from rule-based or model-based manipulations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No scoring rubrics or manual scoring criteria are described since the annotations are generated automatically via simulation rules rather than scored by humans."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.1 and 2.2",
            "reasoning": "Examples of simulated dysfluencies and pipeline details are provided outlining types of dysfluencies injected (e.g., word repetition, missing, blocking). The paper provides concrete examples and statistical summaries illustrating the dysfluency types included in the datasets."
          }
        }
      ]
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11423,
      "completion_tokens": 375,
      "total_tokens": 11798
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human expert performing quality assurance on the newly introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that multiple human experts were involved in quality assurance of the newly introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about a single non-expert human annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators involved in quality assurance processes for the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model used as a judge or quality assurance method for the datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2. Naturalistic Dysfluency Simulation (especially 2.1 and 2.2)",
          "reasoning": "The datasets VCTK-Stutter and VCTK-TTS are created via automated simulation methods: VCTK-Stutter extends VCTK++ using forced and WhisperX alignments to automatically insert dysfluencies in acoustic space, and VCTK-TTS uses rule-based injections in text space followed by VITS model synthesis. The annotation is automatically generated from the simulation and alignment processes. There is no mention of manual annotation or human QA, indicating the QA relies mostly on automated processes and algorithmic verification of code and simulation rules."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The quality assurance process is documented via the automatic simulation and annotation pipeline, so it is not the case that no quality assurance process was applied or documented."
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11041,
      "completion_tokens": 480,
      "total_tokens": 11521
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The VCTK-Stutter dataset is an extension of the VCTK++ dataset by incorporating word-level dysfluencies using word alignments generated by WhisperX to insert artificial dysfluencies at precise regions. This process involves manually designing dysfluency annotations and utilizing human-understandable alignments, indicating original content created with human effort, not automated generation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The VCTK-TTS dataset is created by injecting dysfluencies in the text space using TTS rules applied to IPA phoneme sequences, then generating naturalistic dysfluent speech using the VITS text-to-speech synthesis model. This process automatically generates synthesized speech with dysfluencies, constituting new data generated by a machine learning model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data created by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation systems is found in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors use existing datasets like VCTK, their new datasets involve modifications or generation processes rather than mere aggregation without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "VCTK-Stutter is derived from the pre-existing VCTK++ dataset by modifying it with artificial dysfluencies introduced at word-level alignments, hence it is based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins and generation methods of the new datasets are explicitly stated; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11559,
      "completion_tokens": 272,
      "total_tokens": 11831
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Training",
          "reasoning": "The VCTK-Stutter and VCTK-TTS datasets are used to train the YOLO-Stutter detector from scratch as described in the training procedures where the model is trained over multiple epochs on these datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.3 Evaluation Metrics, 4.4 Validation, 4.5 Results and Discussions",
          "reasoning": "The datasets VCTK-Stutter and VCTK-TTS are used as test sets for benchmarking the performance of the dysfluency detection model as shown in evaluation metrics and validation sections. Their performance results on these datasets are reported to measure detection accuracy and bound loss."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12282,
      "completion_tokens": 588,
      "total_tokens": 12870
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are based solely on English speech data derived from the VCTK corpus, which consists of English speakers with different accents but only one language."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of exactly two human languages are present in the datasets. The datasets focus exclusively on English speech."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (Naturalistic Dysfluency Simulation), Section 2.1 (VCTK-Stutter), Section 2.2 (VCTK-TTS)",
          "reasoning": "The two new datasets, VCTK-Stutter and VCTK-TTS, are created from the VCTK corpus which contains English speech from 109 native English speakers with various accents. The simulation rules and injections also operate on English text and phonemes (IPA derived from English sentences). All evaluations and experiments were conducted on English speech data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with non-English language speech data are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of speech and aligned dysfluency annotations, without any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation to describe loss functions, the datasets themselves do not contain mathematical or logical symbolic representations as content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are human speech datasets focused on dysfluency; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets use natural (English) language and simulated dysfluencies but do not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language in the datasets is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets obviously contain human language data (English speech and transcriptions), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9500,
      "completion_tokens": 234,
      "total_tokens": 9734
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2.2, Footnote 1",
          "reasoning": "The paper states in the Abstract that \"Code and datasets are open-sourced at https://github.com/rorizzz/YOLO-Stutter,\" indicating public availability. The method pipeline and TTS rules for dataset creation are detailed in Section 2.2.1 and 2.2.2, and a footnote references https://github.com/timmahrt/pysle for tools related to data construction, supporting code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2, 2.1, 2.2, Tables 1 and 2",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 2 and its subsections (2.1 and 2.2), describing the methodology for simulating dysfluencies in both acoustic and text space, including dysfluency types, annotation procedures, and TTS rules. Tables 1 and 2 present dataset statistics and naturalness evaluation, demonstrating comprehensive documentation of the datasets."
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7740,
      "completion_tokens": 146,
      "total_tokens": 7886
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2, 'Creating pronunciation dictionaries' and 'Collecting existing dictionaries'",
          "Reasoning": "The new dataset is a curated G2P dictionary dataset combining multiple publicly available pronunciation dictionaries (human generated text data from linguistic resources) and newly created dictionaries generated by applying the espeak-NG model (model generated) on filtered human-created wordlists from Leipzig corpora. Thus, the resulting dataset includes both human-generated text data and model-generated phonetic transcriptions as text. The data modality is 'text' since the dataset consists of word-to-phoneme mappings represented as textual entries."
        }
      ]
    }
  },
  {
    "id": "zhu22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8592,
      "completion_tokens": 220,
      "total_tokens": 8812
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2, 'Creating pronunciation dictionaries' and Section 2, 'Collecting existing dictionaries'",
            "reasoning": "The G2P dataset for around 100 languages was created by aggregating and merging publicly available pronunciation dictionaries and by generating dictionaries using the espeak-NG system from wordlists. This annotation was carried out by an automatic process rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not mention providing any instructions as the annotation was performed by automatic merging and generation processes rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "There is no indication of scoring rubrics or quality grading applied during the automatic dictionary creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "No annotation examples or guidelines with annotated examples are provided in the paper for this automatic process."
          }
        }
      ]
    }
  },
  {
    "id": "zhu22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9722,
      "completion_tokens": 323,
      "total_tokens": 10045
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator for the pronunciation dictionary data."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance of the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of multiple non-expert human annotators performing QA is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used to verify or judge the quality of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2, \"Collecting existing dictionaries\" and \"Creating pronunciation dictionaries\"",
          "reasoning": "The dataset was constructed by manually selecting and merging publicly available pronunciation dictionaries from various sources. For some languages, pronunciation dictionaries were created automatically using the espeak-NG system applied to wordlists after filtering. Repeated entries were removed automatically. These processes indicate quality assurance through automated or rule-based merging and filtering rather than manual annotation verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some form of automated verification and filtering is described, hence it is not the case that no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9340,
      "completion_tokens": 460,
      "total_tokens": 9800
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any entirely new datasets were created from scratch by human contributors. Instead, data was aggregated from existing public sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset is described as being generated entirely by AI or machine learning models. The data used for training was gathered from existing pronunciation dictionaries or derived through existing tools (e.g., espeak-NG), but no new data was synthesized by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by human translators or by translating content from one language to another."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being generated by machine translation or derived through automated translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2, especially 'Collecting existing dictionaries'",
          "reasoning": "The authors aggregated pronunciation dictionaries from various publicly available sources including online dictionaries, dictionary collections like ipa-dict and Wikipron. They merged and cleaned these sources without significant modification other than removing duplicates and selecting among competing datasets, constituting collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2, 'Creating pronunciation dictionaries'",
          "reasoning": "For languages lacking existing dictionaries or with small Wikipron entries, the authors created pronunciation dictionaries by applying espeak-NG to wordlists obtained from the Leipzig corpora after filtering. This represents data derived by applying a pronunciation generation model to existing wordlists."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly specifies the data sources and their processing, so the origin of data is documented."
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9858,
      "completion_tokens": 378,
      "total_tokens": 10236
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Section 5 (Evaluations)",
          "reasoning": "The curated multilingual G2P dataset of around 100 languages was used to train ByT5-based multilingual and monolingual models from randomly initialized weights as baselines, demonstrating the effect of dataset size and architecture on performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 6 (Low resource G2P)",
          "reasoning": "The pretrained multilingual G2P models were fine-tuned with supervised learning on low-resource languages using the respective low-resource dataset splits, leading to improved phone error rate and word error rate."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Evaluations) and Section 6 (Low resource G2P)",
          "reasoning": "The dataset was partitioned into train, development, and test sets, then used to evaluate the G2P models' performance quantitatively by measuring phone error rate (PER) and word error rate (WER) across languages."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Evaluations) and Section 6 (Low resource G2P)",
          "reasoning": "The dataset was analyzed for distribution of dictionary sizes per language, orthographic characteristics impact on model performance, correlation between dataset size and error rates, and differences in zero-shot and fine-tuned performance on low-resource languages."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10581,
      "completion_tokens": 407,
      "total_tokens": 10988
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2: Multilingual Pronunciation Dictionaries",
          "reasoning": "The authors curated a dataset from various sources covering around 99 languages with more than 3000 entries each, combining publicly available G2P dictionaries and new dictionaries, resulting in a massive multilingual G2P dataset spanning approximately 100 languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset covers many languages (about 100), not exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to English only; it includes data from about 100 languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset covers multiple languages rather than exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are grapheme-to-phoneme mappings for human languages and do not contain code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of grapheme-to-phoneme conversion pairs in human languages and does not include mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication systems are part of the dataset."
        },
        "Constructed Language": {
          "is_applicable": true,
          "reference": "Section 5: Evaluations",
          "reasoning": "The dataset includes Esperanto, a constructed language, as noted where Esperanto (epo) is mentioned as a language with phonetically regular spelling system that is easily handled by G2P models."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are properly identified with codes and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains linguistic entries with multiple languages."
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7799,
      "completion_tokens": 185,
      "total_tokens": 7984
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 and Introduction",
          "reasoning": "The paper states in the Introduction and Section 2 that they have made their code publicly available at https://github.com/lingjzhu/CharsiuG2P. This repository includes the code and pretrained models related to dataset construction from various sources, preprocessing, and training. Therefore, code related to the dataset creation and processing is available for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "Section 2 provides a detailed description of how the dataset was constructed from a variety of existing pronunciation dictionaries and other resources. It specifies the data sources, merging strategies, filtering criteria, the use of espeak-NG to generate dictionaries for some languages, and data partitioning. This provides transparent and sufficient documentation of the dataset creation process in the paper."
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7124,
      "completion_tokens": 147,
      "total_tokens": 7271
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 4.1 Data split and training; Section 6 Conclusion",
          "Reasoning": "The PodcastFillers dataset introduced in this work consists of 145 hours of speech recordings from over 350 speakers, sourced from 199 public podcast episodes. The data consists of annotated filler words and common podcast sounds. Since podcasts are recordings of human speech, the modality is audio and the origin is human-generated speech recordings. The paper explicitly states that the dataset was acquired from publicly available podcasts and annotated by humans through crowdsourcing, confirming human involvement in the data generation process."
        }
      ]
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7976,
      "completion_tokens": 222,
      "total_tokens": 8198
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 Dataset",
            "reasoning": "The paper states that filler candidates generated by VAD and ASR were annotated via crowdsourcing with two annotators per candidate, and three when the first two disagreed, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 Dataset",
            "reasoning": "The annotation interface described in Section 2 provided annotators with instructions to determine whether candidates were filler words or not, and to select among specific filler and non-filler labels, implying the presence of detailed annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "The paper does not mention any scoring rubrics or standardized evaluation criteria provided to annotators during labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "There is no mention of annotation examples or example annotations provided to annotators in the annotation guidelines or the description of the annotation interface."
          }
        }
      ]
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9106,
      "completion_tokens": 311,
      "total_tokens": 9417
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was conducted by a single human annotator who is an expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human annotators with subject matter expertise or target demographic background performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2, paragraph describing annotation process.",
          "reasoning": "The paper states that each filler candidate was annotated by two people, or three when the first two disagreed, indicating multiple human annotators performed the annotation. There is no evidence these annotators had subject matter expertise, so they are considered non-experts. This demonstrates a quality assurance process with multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for classification in the detection pipeline, the paper does not describe any AI model used explicitly for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated verification or algorithmic/rule-based QA process for validating the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8724,
      "completion_tokens": 504,
      "total_tokens": 9228
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 2 Dataset",
          "reasoning": "The PodcastFillers dataset is a novel speech dataset created by the authors through annotation of public podcast episodes. Over 85K filler word candidates were annotated via crowdsourcing, resulting in 35K filler word annotations and 50K other event annotations. This indicates original creation of labeled data by human annotators, not derived from or translations of existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset content was generated entirely by AI or machine learning models without human involvement."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on English podcasts with annotations; there is no mention of translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 Dataset; Section 6 Conclusion",
          "reasoning": "The dataset audio is collected from existing public podcast episodes (199 episodes), which are aggregated to create the dataset. While the authors perform annotation and preprocessing, the raw audio data is collected from these existing sources without significant modification, hence it is collated from external sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Dataset; Section 3 Filler Detection Pipeline",
          "reasoning": "The authors used outputs from a Voice Activity Detection (VAD) model and Automatic Speech Recognition (ASR) system to generate filler candidates, which they then annotated. This process involves transformation and adaptation of existing audio data (podcast episodes) and model-generated candidate segments to produce the final annotated dataset, making the dataset derived from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of dataset creation are clearly documented as described above."
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9242,
      "completion_tokens": 275,
      "total_tokens": 9517
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4.1",
          "reasoning": "The PodcastFillers dataset is used to train classifiers for filler word detection and classification from scratch, as described in sections about the filler detection pipeline and data split and training where models are trained on the train split of PodcastFillers."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 and Section 4.3",
          "reasoning": "PodcastFillers is used as a test set for evaluating the proposed filler detection and classification pipelines with detailed metrics provided in the results section."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The paper conducts ablation studies and detailed analyses of the impact of pipeline components on performance using the PodcastFillers dataset, indicating its use for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9965,
      "completion_tokens": 399,
      "total_tokens": 10364
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 4 Experimental Design",
          "reasoning": "The PodcastFillers dataset consists of speech from public podcast episodes primarily in English, as indicated throughout the paper. It leverages a commercial ASR trained on English (e.g., Fisher English corpus is referenced for baseline comparisons). There is no mention of other languages being included, and all examples and baseline comparisons center on English speech data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken podcast audio with annotated filler words and other speech events, which inherently involves human language (English)."
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7183,
      "completion_tokens": 156,
      "total_tokens": 7339
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 6 Conclusion",
          "reasoning": "The paper states in the abstract and conclusion that the PodcastFillers dataset is made publicly available online at podcastfillers.github.io, which implies the associated code used for dataset creation and annotation pipeline is also available to support the dataset release for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 Filler Detection Pipeline and 4 Experimental Design",
          "reasoning": "The paper provides clear, detailed documentation of the dataset creation process, including the pipeline involving voice activity detection and automatic speech recognition to generate filler candidates, the crowdsourced annotation procedure, dataset statistics, and data splits, all described mainly in Sections 3 and 4."
        }
      }
    }
  }
]