[
  {
    "id": "adigwe24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7118,
      "completion_tokens": 183,
      "total_tokens": 7301
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5.1 and Section 5.2",
          "Reasoning": "The authors used a parallel corpus of natural speech consisting of utterances taken from spontaneous conversation and their read-aloud counterparts from the same British English female speaker. Since these are natural speech recordings, they are human generated audio data afforded by recorded human voices."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 6.1 and Section 6.2",
          "Reasoning": "The authors trained a modified FastSpeech2 text-to-speech model and used it to synthesise speech with targeted prosodic modifications on eight predefined sets of feature values. The resulting synthetic speech samples were generated by the model and are thus model generated audio data."
        }
      ]
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7918,
      "completion_tokens": 266,
      "total_tokens": 8184
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4 (Main Experimental Setup)",
            "reasoning": "Section 4 states that 16 native English speakers were recruited and interviewed individually for listening tasks, indicating multiple human non-expert annotators participated in evaluation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 5.2 (Experimental Setup) and Section 6.2 (Experimental Setup)",
            "reasoning": "Section 5.2 provides explicit instructions given to participants (e.g., distinguishing conversational speech in pairs); Section 6.2 also describes instructions for rating individual utterances as conversational or not, evidencing the presence of instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention of scoring rubrics in annotation guidelines found in the paper.",
            "reasoning": "The paper describes a forced-choice (Task 1) and rating task (Task 2) but does not provide detailed scoring rubrics or formal evaluation criteria beyond instructions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotated data or annotation examples provided in the paper.",
            "reasoning": "There are no annotation examples or guideline examples demonstrated within the content; the paper focuses on interview responses and qualitative descriptions rather than illustrating annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9108,
      "completion_tokens": 364,
      "total_tokens": 9472
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single human expert for validating new dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 4: Main Experimental Setup; Sections 5 and 6 describing Task 1 and Task 2",
          "reasoning": "The quality assurance in the new datasets (natural and synthetic speech stimuli for tasks) involved multiple human listeners who are native English speakers participating in semi-structured interviews and evaluation tasks. These participants belong to the target demographic (native English speakers) and are multiple in number (16 participants). Their perceptual judgments act as the validation for the dataset's quality and are treated as expert listeners given their direct involvement and focus on detailed prosody evaluation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The multiple human annotators are native speakers and presumably recruited as participants with a relevant demographic background rather than purely non-experts. The paper does not suggest the annotators lacked relevant expertise or demographic alignment."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model was used as a judge for quality assurance in dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic verification processes for dataset annotations or content quality assurance are described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a human listening evaluation protocol for validating the datasets used, so QA is present and documented."
        }
      }
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8718,
      "completion_tokens": 492,
      "total_tokens": 9210
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The paper uses utterance pairs selected randomly from a parallel corpus of spontaneous conversation and their read counterparts, taken from the same British English Female speaker. Furthermore, 16 native English speakers were recruited for interviews and evaluations specifically for this study, indicating original human data collection and novel experimental data created by human subjects."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 6.1",
          "reasoning": "The study trained a modified FastSpeech2 TTS model to generate synthetic speech with controlled prosodic variations. They generated 64 synthetic samples with predefined prosodic variations to test listener perception. These synthetic speech samples generated by the trained TTS model represent new data created entirely by a machine learning model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The natural speech stimuli are sourced from an existing parallel corpus (read and spontaneous utterances by the same speaker) that was collected previously (as cited from previous work [16]). The paper compiles this data for use in their Task 1 experiments without major modifications."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though some modifications are done via prosodic manipulation in synthesis, these do not constitute adapted versions of existing natural speech data. Instead, synthetic speech is created anew from text using the TTS model; thus, the data is not merely modified from existing natural recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods are clearly specified and documented."
        }
      }
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9236,
      "completion_tokens": 346,
      "total_tokens": 9582
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 5 and 6",
          "reasoning": "The authors use the natural speech data from a parallel corpus of spontaneous and read utterances (Section 5.1) and synthetically generated prosodic variations (Section 6.1) exclusively for evaluation purposes, assessing listener perception of conversationality and measuring responses to natural and synthetic speech stimuli."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.4",
          "reasoning": "The dataset from the natural speech pairs is used to analyze perceptual dimensions and listener explanations on conversational speaking style through thematic analysis of qualitative interview data, providing insights into factors influencing conversationality judgments."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9959,
      "completion_tokens": 514,
      "total_tokens": 10473
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 5.1, 6.1",
          "reasoning": "The dataset introduced consists of utterances from a British English female speaker, including both spontaneous conversational speech and read-aloud speech in English, as explicitly stated in Section 5.1. Additionally, the synthetic speech used in Task 2 is trained and generated using the LJ Speech dataset, which contains English content (Section 6.1). Therefore, the datasets are monolingual with English as the only language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the FastSpeech2 TTS model and WaveGlow vocoder are referenced (Section 6.1), the dataset itself consists of speech utterances, not programming codes or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves speech utterances; there is no indication that it contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural English speech, with no mention of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English; therefore, unknown does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data (English), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "adigwe24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7177,
      "completion_tokens": 215,
      "total_tokens": 7392
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in paper",
          "reasoning": "The paper does not provide any link or mention of publicly available code repositories related to the dataset creation, data collection, preprocessing, or generation. The datasets used in Task 1 and Task 2 rely on existing datasets or synthetic speech generated using published models, but no code for constructing or processing the datasets introduced in this paper is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 5.1 and 6.1",
          "reasoning": "The paper documents the datasets used: Task 1 uses a parallel corpus of conversational vs read utterances from a British English female speaker, described in Section 5.1, including selection criteria and control measures. Task 2 uses a modified FastSpeech2 model trained on LJ Speech with specified prosodic controls, with details provided in Section 6.1 including description of prosodic variation parameters. This documentation is sufficient to understand dataset construction and experiment design for reproducibility purposes."
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8369,
      "completion_tokens": 177,
      "total_tokens": 8546
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The new dataset introduced and used for evaluation is AfriSpeech-200, a 200-hour Pan-African accented English speech corpus containing clinical and general domain speech with audio recordings collected from over 2,300 unique speakers with 120 accents from over 10 African countries, indicating the audio data was human recorded speech."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The AfriSpeech-200 dataset includes ground truth transcripts associated with the audio clips. These transcripts are human created or human recorded text corresponding to the spoken audio, representing the human generated textual modality accompanying the audio."
        }
      ]
    }
  },
  {
    "id": "afonja24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9169,
      "completion_tokens": 234,
      "total_tokens": 9403
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.3.1",
            "reasoning": "The dataset used (AfriSpeech-200) was not annotated with medical named entities by humans; instead, a commercial medical NER model (Amazon Comprehend Medical) was used to automatically extract medical named entities from ground truth transcripts, resulting in 'silver' annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3.1",
            "reasoning": "There is no mention of detailed annotation instructions given to human annotators because the named entity annotations were automatically generated by an AI model, not manually annotated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3.1",
            "reasoning": "Since annotations were generated automatically by the AI model, there are no reported scoring rubrics or guidelines provided for manual annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3.1",
            "reasoning": "No annotation examples are provided because the extraction of medical named entities was performed automatically by a commercial model without manual annotation guidelines or examples."
          }
        }
      ]
    }
  },
  {
    "id": "afonja24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10359,
      "completion_tokens": 355,
      "total_tokens": 10714
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that dataset annotation or quality assurance was performed by a single human subject matter expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple human annotators with subject matter expertise for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by a single human without subject matter expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple non-expert humans were involved in dataset quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3.1",
          "reasoning": "The paper explicitly states in Section 3.3.1 that the dataset was not annotated with medical named entities; instead, they leveraged a commercially available medical NER model (Amazon Comprehend Medical) to automatically extract medical named entities from the ground truth transcripts. This constitutes quality assurance through an AI model's automated annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors developed an automated fuzzy string-matching algorithm (MedTextAlign) to align predicted entities with the ground truth, this is described as an evaluation method rather than a quality assurance process for the dataset annotations themselves. Hence, automatic verification of annotations is not explicitly reported."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is indeed applied, as the annotations are produced by an AI NER model (Amazon Comprehend Medical) and not left undocumented."
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9969,
      "completion_tokens": 327,
      "total_tokens": 10296
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset AfriSpeech-200 is described as a 200-hour Pan-African accented English speech corpus collected from over 2,300 unique human speakers across more than 10 African countries, containing clinical and general domain speech. This indicates that the speech data was generated by human speakers and recorded, making it original content created by humans from scratch, not derived or translated from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper suggests the dataset was generated by AI or machine learning models. The speech data originates from human speakers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources without modification; it is a newly recorded corpus from human speech."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications or adaptations; rather, it is newly recorded speech."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the origin and method of data generation as human speech corpus collection, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10487,
      "completion_tokens": 292,
      "total_tokens": 10779
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2, Section 5.5",
          "reasoning": "The AfriSpeech-200 dataset is used to fine-tune pretrained ASR models in supervised learning settings to improve performance on accented medical speech, as detailed in the fine-tuning experiments and results sections."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 5, Table 4",
          "reasoning": "The dataset is employed for benchmarking and evaluating various open-source and commercial ASR models, assessing metrics such as WER, medical WER, medical CER, and medical named entity recall on accented clinical speech."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 to 5.5",
          "reasoning": "The dataset facilitates detailed analysis of model performance trends, error types, and impact of fine-tuning across entity categories and different ASR architectures."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11210,
      "completion_tokens": 542,
      "total_tokens": 11752
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset consists of English speech with African accents, but there is no mention of multiple human languages within the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset contains only English speech, despite various African accents and dialects, no indication of exactly two human languages present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The AfriSpeech-200 dataset is described as a Pan-African accented English speech corpus, indicating all data is in English spoken with various accents, i.e., monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "There is no data in any single non-English language; the dataset only includes accented English speech."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of speech data with transcripts, no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are present in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech only; there is no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or indication of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset explicitly states it is English speech with African accents; the language is known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains linguistic content (English speech)."
        }
      }
    }
  },
  {
    "id": "afonja24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8428,
      "completion_tokens": 175,
      "total_tokens": 8603
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any link, repository, or mention of the code related to the collection, creation, or preprocessing of the AfriSpeech-200 dataset or any other new dataset component. The dataset is acknowledged as contributed by Intron Health, but no code repository is referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper provides a detailed description of the AfriSpeech-200 dataset in Section 3.1, including number of hours, number of accents, speakers, dataset splits, and types of medical named entities considered. While the dataset creation pipeline or recording setup is not exhaustively described, sufficient dataset statistics and composition are provided to offer insight into its contents."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9447,
      "completion_tokens": 176,
      "total_tokens": 9623
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. VoxSim Dataset - Data source, Listening test procedure, Quality control",
          "Reasoning": "The VoxSim dataset consists of approximately 70k human perceptual speaker similarity ratings for 41k pairs of audio utterances sampled from the VoxCeleb1 dataset. The VoxCeleb1 utterances are speech segments extracted from human recordings in YouTube videos. The similarity scores are collected from twelve human evaluators via a listening test, thus the data modality is audio and the origin involves human-generated utterances recorded by humans as well as human involvement in the subjective rating process. The paper explicitly states the evaluation of voice similarity through listening tests by human evaluators, confirming human involvement. It is not model-generated or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10247,
      "completion_tokens": 258,
      "total_tokens": 10505
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2, 'Listening test procedure'",
            "reasoning": "The paper states that twelve evaluators participated in the listening test to evaluate speaker similarity, implying multiple human experts were involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2, 'Listening test procedure'",
            "reasoning": "Evaluators were instructed to rate on a 6-point Likert scale with explicit descriptions for each score, were guided to rate evenly across scores from 1 to 6, and were told to focus solely on voice characteristics, ignoring content, language, and environment, indicating detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2, 'Listening test procedure'",
            "reasoning": "A 6-point Likert scale with defined semantic labels for each point (1: Definitely different speakers to 6: Definitely the same speaker) is provided, serving as a clear scoring rubric."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2, Figure 2 'Speaker similarity annotation page'",
            "reasoning": "Figure 2 presents an example annotation page used in the listening test, demonstrating an example provided to annotators for guidance."
          }
        }
      ]
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11437,
      "completion_tokens": 493,
      "total_tokens": 11930
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes that the listening test was conducted by twelve evaluators, but does not indicate that these evaluators were subject matter experts or members of the target demographic. Therefore, single human expert QA does not apply."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset was annotated by multiple human evaluators (twelve), the paper does not state that these annotators were subject matter experts. Thus, multiple human expert QA does not apply."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data quality assurance involved multiple human annotators (twelve evaluators), not a single annotator. Hence, single human non-expert QA does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2, 'Listening test procedure' and 'Quality control'",
          "reasoning": "The dataset's speaker similarity ratings were collected from twelve evaluators through a listening test. There is no indication that these evaluators were experts. The paper specifies quality control procedures such as monitoring the speaker verification rate and providing feedback to evaluators if their error rates deviated significantly, which reflects a quality assurance process conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model being utilized to perform quality assurance during the data annotation or validation process."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated metrics such as Equal Error Rate (EER) are used to monitor evaluator performance, the dataset annotations themselves are not verified through an automated process. The EER is used as a quality control measure but not as an automated verification of annotation correctness."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described and implemented via multiple human annotators and quality control measures."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11047,
      "completion_tokens": 492,
      "total_tokens": 11539
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (VoxSim Dataset), particularly sections 'Listening test procedure' and 'Quality control'",
          "reasoning": "The paper describes collecting nearly 70,000 speaker similarity scores through a listening test performed by twelve human evaluators who rated pairs of utterances sampled from the existing VoxCeleb1 dataset. This subjective evaluation by humans is original content created from scratch, not translated or adapted from prior material, as it involves new perceptual ratings obtained through controlled listening tests."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models; the voice similarity ratings are based on human listening tests, not model-generated content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any translation from another language was involved in producing the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine-based language translation is involved or mentioned in the data collection or generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (VoxSim Dataset) 'Data source' paragraph",
          "reasoning": "The utterances themselves are sourced from the existing VoxCeleb1 dataset, meaning the audio data is collected from an existing corpus without modification. The new dataset collates these utterances into pairs for evaluation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 (VoxSim Dataset) 'Listening test procedure', 'Quality control', and 'Data statistics'",
          "reasoning": "The dataset is derived by taking existing audio utterances from VoxCeleb1 and creating pairs, followed by collecting new speaker similarity ratings via human perception, which transforms and enriches the original data with new annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data source and method of generation, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11565,
      "completion_tokens": 471,
      "total_tokens": 12036
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 3.1",
          "reasoning": "The paper describes training the SVSNet model from scratch on the VoxSim dataset (Section 4.1). This shows that VoxSim is used to train at least one model from randomly initialized parameters, as SVSNet is not pre-trained and trained purely on VoxSim data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 3.1",
          "reasoning": "The paper clearly states that ECAPA-TDNN and WavLM-ECAPA models are fine-tuned from pre-trained speaker recognition models using the VoxSim dataset (Section 4.1). Thus, VoxSim is used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper includes no mention or evidence of using VoxSim for reinforcement learning based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Table 3",
          "reasoning": "The VoxSim dataset test set is used for evaluating the performance of speaker similarity prediction models, as shown in the results reported in Table 3 and discussed in Section 4.1. This demonstrates its use as an evaluation benchmark."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 and Figure 3",
          "reasoning": "The paper includes qualitative analysis such as t-SNE visualization of speaker embeddings extracted by the model trained using VoxSim to study characteristics of speaker similarity (Section 4.1). This evidences the use of VoxSim for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that VoxSim serves as a knowledge base or is used for retrieval-augmented generation or similar augmentation tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset VoxSim is actively used for training (both from scratch and fine-tuning), evaluation, and analysis, as explicitly detailed in the paper."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12288,
      "completion_tokens": 520,
      "total_tokens": 12808
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2: VoxSim Dataset - Data source",
          "reasoning": "The VoxSim dataset is derived from VoxCeleb1, which consists of utterances from speakers of a wide range of nationalities and languages. The paper explicitly states that the utterances contain various languages and diverse linguistic contexts, indicating that the dataset includes more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two languages, so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not restricted to only English; it contains multiple languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple non-English languages and English, so it is not monolingual in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech utterances and speaker similarity ratings, with no mention of programming or structured code-related content in the data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of mathematical or formal logical expressions or symbolic representations within the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only; biological or non-human communication is not involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language contents are documented and specified as diverse human languages from VoxCeleb1 speakers."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain human language utterances."
        }
      }
    }
  },
  {
    "id": "ahn24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9506,
      "completion_tokens": 223,
      "total_tokens": 9729
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not mention the availability of any code repository or provide links related to code for dataset construction or processing. It only mentions the data source (VoxCeleb1) and describes the data collection and annotation process, but there is no explicit statement or URL indicating that code used for generating or processing the VoxSim dataset is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: VoxSim Dataset, notably 'Data source', 'Listening test procedure', 'Quality control', and 'Data statistics'",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including the data source (VoxCeleb1), the method for creating utterance pairs, the listening test procedure with 12 evaluators and the 6-point Likert scale used, quality control steps such as evaluating the EER and outlier removal, and dataset statistics including train/test splits. This comprehensive information documents the dataset creation sufficiently for reproducibility and understanding."
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10571,
      "completion_tokens": 237,
      "total_tokens": 10808
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Dataset",
          "Reasoning": "The DEFCOMM-DB dataset is composed of video recordings collected from YouTube showing real-world English-speaking interactions featuring defensive communication types. The videos are genuine, non-acted dialogues captured by human-operated cameras and publicly available, thus making the video modality human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Dataset",
          "Reasoning": "From each collected video, the audio stream is extracted representing the speech of participants in natural defensive communication settings. The audio is directly derived from the videos recorded by humans, meaning the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Dataset",
          "Reasoning": "Textual modality consists of speech transcriptions obtained from the video recordings. Though not explicitly detailed, these transcriptions are clearly based on human speech content, thus originating from human-generated data rather than model-generated."
        }
      ]
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11371,
      "completion_tokens": 268,
      "total_tokens": 11639
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Annotation",
            "reasoning": "The paper states that 11 participants, including four AI researchers (experts), one master's student with psychology background, and six undergraduates from social sciences, psychology, computer science, economics, and management, performed annotation. Since the majority are non-expert undergraduates and the group includes multiple annotators, the overall category best fits 'Multiple Human Non-Experts'."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Annotation",
            "reasoning": "Before annotation, participants were given verbal and written instructions about the purpose and goals, detailed descriptions of the four classes, examples of videos featuring targeted reactions, and technical details about the labeling process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 and 2.2.1",
            "reasoning": "The paper does not mention any formal scoring rubric or numerical criteria for labeling. Instead, annotators assigned labels based on class definitions and instructions without explicit rubric details."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2 Annotation",
            "reasoning": "Annotators were provided with examples of videos featuring targeted reactions of the classes to guide the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12561,
      "completion_tokens": 352,
      "total_tokens": 12913
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset annotation process involved multiple annotators with varying expertise rather than a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "Section 2.2 Annotation",
          "reasoning": "Although among the 11 annotators there were some with backgrounds in psychology and AI research, the majority were undergraduates from various fields and there is no indication that all annotators were experts or belonged to the target demographic. Thus the annotators cannot be collectively considered multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the quality assurance was done by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotation",
          "reasoning": "The annotation was performed by 11 participants, including undergraduates and only a few with some expertise, but no clear indication that all were experts or part of the target demographic. Therefore, overall the annotation was done by multiple human non-experts as part of the quality assurance process."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of any AI model to perform quality assurance on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic or algorithmic verification used as quality assurance for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is documented through multiple human annotators performing annotations with majority voting and inter-annotator agreement analysis, indicating a quality assurance process is applied."
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12171,
      "completion_tokens": 360,
      "total_tokens": 12531
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of existing real-world video recordings sourced from YouTube, not created from scratch by humans specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data consists of real video recordings from online sources and is not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the data being translated from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate or process the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The DefComm-DB dataset was collected from existing YouTube video recordings of genuine, non-acted dialogues from various real-world settings without significant modifications to the original data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though processed and annotated, the dataset is not described as transformed or adapted data derived from existing sources in a significant manner; it mainly consists of collected videos."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the source and method of data generation."
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12689,
      "completion_tokens": 313,
      "total_tokens": 13002
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Settings)",
          "reasoning": "The DefComm-DB dataset is used to train Support Vector Machine models with features extracted from audio and textual modalities. The models are trained in a supervised manner to classify different forms of defensive communication, indicating supervised fine-tuning on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Settings and Results)",
          "reasoning": "The dataset is partitioned into train, development, and test sets, and the models' performance is evaluated on the unseen test set using metrics like Unweighted Average Recall (UAR). Thus, DefComm-DB is used for evaluation and benchmarking of classification methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset) and Section 2.2.1 (Inter-Annotator Agreement)",
          "reasoning": "The paper provides detailed analysis of the dataset including annotation procedure and inter-annotator agreement, reflecting use for analyzing patterns and annotation reliability."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13412,
      "completion_tokens": 552,
      "total_tokens": 13964
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset DefComm-DB consists of videos featuring English-speaking individuals only, indicating no more than one language present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset entries are exclusively in English, so exactly two human languages are not present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "DefComm-DB contains genuine non-acted dialogues between English-speaking individuals (Section 2: 'It comprises genuine non-acted dialogues between English-speaking individuals'), indicating that all dataset entries are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset is specified to include English-speaking individuals, so it does not contain single language data in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset only contains video, audio, and textual transcripts of spoken language; no programming or structured code is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no indication that the dataset includes mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset consists entirely of human communication and does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no mention or indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 2 Dataset",
          "reasoning": "The language of the dataset is explicitly stated as English-speaking dialogues, so the language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken English utterances and their transcriptions; thus, they do contain language."
        }
      }
    }
  },
  {
    "id": "amiriparian23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10630,
      "completion_tokens": 195,
      "total_tokens": 10825
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper about code availability for dataset construction.",
          "reasoning": "The paper does not provide any link, URL, or statement indicating that the code used for collecting, preprocessing, or generating the dataset DefComm-DB is publicly available. The dataset is mentioned to be available for academic researchers, but there is no indication that the associated code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Dataset) and subsections 2.1 (Search Parameters) and 2.2 (Annotation)",
          "reasoning": "The paper describes the dataset creation process in detail including the data sources (YouTube videos), criteria for selecting videos based on defensive communication markers, search queries used to find relevant videos, and the annotation procedure with annotator background and inter-annotator agreement analysis. This provides reasonably complete documentation of the dataset construction process for reproducibility."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7195,
      "completion_tokens": 122,
      "total_tokens": 7317
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: EmoSet++",
          "Reasoning": "EmoSet++ is introduced as a comprehensive multi-lingual, multi-cultural speech emotion corpus comprising 37 unique emotion datasets with 150,907 speech recordings totaling 119.5 hours. The datasets encompass human speech recordings that were collected in various conditions. Thus, the data is in audio modality and is human generated from human speech recordings as explicitly stated in the description of EmoSet++ in Section 2."
        }
      ]
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7995,
      "completion_tokens": 233,
      "total_tokens": 8228
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2",
            "reasoning": "The paper states that EmoSet++, which is newly introduced, integrates 37 unique emotion datasets with speaker-independent splits. Typically, emotion datasets in speech emotion recognition are annotated by multiple human experts, and the paper mentions using datasets with manually constructed speaker-independent splits, implying expert involvement in annotations and splitting."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not provide any explicit information about annotation instructions or guidelines for EmoSet++. It only mentions collecting and integrating datasets and creating speaker-independent splits, without details on instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No mention of scoring rubrics or annotation rubrics is found in the paper concerning EmoSet++ or its constituent datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire Paper",
            "reasoning": "The paper does not describe providing annotation examples or guidelines for EmoSet++. The focus is on dataset integration and model fine-tuning rather than annotation procedures or examples."
          }
        }
      ]
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9185,
      "completion_tokens": 330,
      "total_tokens": 9515
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator on the EmoSet++ dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or documentation in the paper that multiple human experts were involved in the quality assurance or annotation validation process for EmoSet++."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that quality assurance was done by a single human non-expert annotator for the EmoSet++ dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or documentation that multiple non-expert humans performed quality assurance on EmoSet++."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any AI model as a judge for quality assurance of the EmoSet++ datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated or algorithmic verification process used as quality assurance for EmoSet++ dataset annotations or labeling."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The EmoSet++ dataset is an aggregation of 37 existing speech emotion datasets and the paper does not document any specific quality assurance process applied to validate or verify the annotations or content of these datasets collectively. Quality assurance details for the underlying individual datasets are not described, and no new annotation or verification process is reported by the authors."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8795,
      "completion_tokens": 380,
      "total_tokens": 9175
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created entirely new speech emotion recordings or datasets that were recorded from scratch by human contributors. Instead, the data used is aggregated from existing speech emotion datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not claim the creation of any datasets that were generated by AI or machine learning models. The new data introduced is based on real speech recordings from existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any human translations applied to the datasets."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe data created via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (EmoSet++)",
          "reasoning": "The authors introduce EmoSet++, which is described as an integration of 37 unique emotion datasets comprising existing speech emotion corpora collected from various sources and languages. The data is aggregated (collated) from these existing datasets without indication of substantial modification, constituting a large multi-lingual corpus through collation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the data labels from the integrated datasets are mapped into six classes based on Russell's Circumplex of Affect (Section 2), this label mapping is a transformation of annotations rather than the data itself being modified or derived. The underlying audio data remains from existing sources. Therefore, the dataset is best characterized as collated rather than derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data for EmoSet++ is explicitly described: it is an aggregation of multiple existing speech emotion datasets, thus is not undocumented."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9313,
      "completion_tokens": 366,
      "total_tokens": 9679
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The EmoSet++ dataset is used for supervised fine-tuning rather than unsupervised or self-supervised pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using EmoSet++. The usage described is fine-tuning pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Section 3.1 explains that the authors fine-tune the HuBERT model on EmoSet++ in a supervised manner with a linear classification head added for emotion classification. This fine-tuning is conducted in a round-robin fashion over the 37 datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training techniques on EmoSet++ or other datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "EmoSet++ is explicitly used for training (fine-tuning) models rather than exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using EmoSet++ primarily for analysis of trends or characteristics; it is used mainly as a resource for fine-tuning."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that EmoSet++ is used as a knowledge base for retrieval or augmenting generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical use of the EmoSet++ dataset for supervised fine-tuning of speech emotion recognition models."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10036,
      "completion_tokens": 471,
      "total_tokens": 10507
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2 (EmoSet++)",
          "reasoning": "EmoSet++ is explicitly described as a comprehensive multi-lingual, multi-cultural speech emotion corpus integrating 37 unique emotion datasets spanning 15 languages, including commonly spoken languages such as English, German, Mandarin, and rarer languages like Persian and Urdu."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of speech emotion recordings, no indication of inclusion of programming or structured code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of speech recordings and emotion labels, not mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "EmoSet++ contains human speech data for emotion recognition, without any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificially constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the languages contained within EmoSet++, and thus the languages are not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of human language speech recordings with emotion annotations, so the dataset contains language."
        }
      }
    }
  },
  {
    "id": "amiriparian24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7254,
      "completion_tokens": 190,
      "total_tokens": 7444
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and 5",
          "reasoning": "The paper describes EmoSet++ as an aggregation of 37 existing speech emotion datasets. It does not provide any direct link or mention availability of code for collecting or assembling this aggregated dataset. The only related shared resource is the ExHuBERT model on HuggingFace, but no repository or code for the dataset construction is mentioned."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "The paper provides a description of the dataset assembly process in Section 2, detailing that EmoSet++ extends EmoSet, integrates 37 datasets totaling 150,907 samples and 119.5 hours, the languages covered, details on label mapping to six classes using Russel's Circumplex of Affect, and dataset split methods. This information provides documentation on how the dataset was constructed and processed."
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8213,
      "completion_tokens": 122,
      "total_tokens": 8335
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Table 1",
          "Reasoning": "The authors introduce a new dataset of speech audio recordings consisting of approximately 2000 OOV words per language (Hindi and Tamil) recorded by volunteers who are not professional voice artists. These recordings were made in acoustic pods with professional microphones, capturing human speech from volunteers. This dataset is newly created by the authors specifically for augmenting TTS training data to improve OOV performance, therefore it is human generated audio data."
        }
      ]
    }
  },
  {
    "id": "anand24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9013,
      "completion_tokens": 249,
      "total_tokens": 9262
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 (Creation of the INDI-COOV benchmark)",
            "reasoning": "The paper states that a single human language expert manually scanned and classified the list of OOV words into categories as part of the benchmark creation process (Section 2). This indicates the annotation was done by a single expert."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 (Creation of INDI-COOV benchmark)",
            "reasoning": "The language expert was tasked to manually classify words into specific categories and prioritize certain categories once a target level was met, indicating clear instructions to follow category definitions and prioritization during annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 (Creation of INDI-COOV benchmark)",
            "reasoning": "There is no mention of provided scoring rubrics or formal rating scales for classification; classification appears to be categorical without explicit scoring criteria."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 (Creation of INDI-COOV benchmark)",
            "reasoning": "There is no mention of example annotations or example classifications provided to the annotator for guidance."
          }
        }
      ]
    }
  },
  {
    "id": "anand24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10203,
      "completion_tokens": 346,
      "total_tokens": 10549
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.3 Recording Setup and Process",
          "reasoning": "An expert proficient in the language listened to all recordings live during the recording sessions and pointed out any pronunciation mistakes made by the speaker. This expert also filtered out words that were deemed inappropriate due to spelling mistakes, archaic nuances, profanity, or toxicity. This indicates that a single human expert was involved in quality assurance of the recorded data."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple human experts performing quality assurance on the dataset recordings or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not conducted by a single non-expert annotator; instead, an expert was involved as described."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 Benchmark Creation",
          "reasoning": "The identification and classification of words into categories involved programmatic identification of words containing high-frequency OOV character bigrams using a greedy algorithm and filtering steps. This implies an automated process was used as part of the quality assurance for dataset content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents and describes quality assurance processes involving human expert review and automated techniques for dataset creation and validation."
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9813,
      "completion_tokens": 505,
      "total_tokens": 10318
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 - Recording OOV Words with Volunteers",
          "reasoning": "The paper details the collection of new speech data recorded by volunteers who are non-professional voice artists. This data consists of recordings of out-of-vocabulary words prepared specifically for this study, involving human volunteers reading curated scripts. The data was thus created entirely from scratch by human contributors and not derived, translated, or generated by models."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models. All new speech data is recorded from human volunteers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper of data produced by translating content from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine-translated data in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 - Creation of INDI OOVB Benchmark",
          "reasoning": "The INDI OOVB benchmark text corpus was created by collating and aggregating existing text corpora from multiple diverse sources such as Sangraha, Bharat Parallel Corpus, IndicVoices transcriptions, Wikipedia, government websites, and others without significant modification apart from classification and filtering."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 - Creation of INDI OOVB Benchmark",
          "reasoning": "After collating the text corpora, the authors used a greedy algorithm and human expert intervention to select and classify out-of-vocabulary words based on missing character bigrams. This process involved filtering, categorizing, and creating some words especially when sufficient words were not found in categories, showing some derived adaptations applied to the collated data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The sources and methods of data generation are clearly specified for both the collated text benchmark and the newly recorded speech data."
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10331,
      "completion_tokens": 314,
      "total_tokens": 10645
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3, 4.1, 5.2",
          "reasoning": "The newly recorded dataset of OOV words with volunteer speakers is used to fine-tune pre-trained TTS models. This supervised fine-tuning using the new dataset significantly improves the intelligibility error rates on OOV words without degrading voice quality or perceptual quality."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2, 4.3, 5.1",
          "reasoning": "The paper introduces the INDI-COOV benchmark dataset containing OOV words across various real-world categories for Hindi and Tamil. This dataset is used exclusively for evaluation and benchmarking of TTS model performance on OOV words."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.5",
          "reasoning": "The dataset and evaluation results are analyzed to identify common pronunciation errors and patterns in TTS outputs for both OOV and IV words, helping understand error trends but not directly used for training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11054,
      "completion_tokens": 499,
      "total_tokens": 11553
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2 (Creation of INDI-COOV benchmark) and throughout the paper",
          "reasoning": "The proposed new datasets introduced in the paper, specifically the INDI-COOV benchmark and the volunteers' OOV word recordings, cover more than two human languages. The languages explicitly mentioned are Hindi and Tamil, and the data includes code-mixed examples involving English-Hindi and English-Tamil within the Codemixed (CM) category. Therefore, the data entries comprise at least three human languages: Hindi, Tamil, and English, making the dataset multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset entries contain programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not include any biological or non-human communication sequences in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural human language content in Hindi, Tamil, and English code-mixed forms."
        }
      }
    }
  },
  {
    "id": "anand24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8272,
      "completion_tokens": 165,
      "total_tokens": 8437
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.2",
          "reasoning": "The paper mentions that the recording scripts used during data collection are released to facilitate reproducibility, but there is no explicit mention or link to publicly available code or code repositories for dataset construction, preprocessing, or generation in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including the steps for curating the OOV benchmark (Section 2) and the process of recording OOV words with volunteers (Section 3). It describes corpus compilation, word selection, categorization, recording script preparation, volunteer recruitment, ethical considerations, and recording setup, thus offering comprehensive transparency about the dataset creation."
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7666,
      "completion_tokens": 270,
      "total_tokens": 7936
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Corpus Creation, Section 1 Introduction",
          "Reasoning": "The MuA ViC corpus includes audio tracks from TED and TEDx talk recordings, which are human recorded speeches from over 8000 speakers in 9 languages. This audio data is human generated as it is captured from real-world human speech events."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Corpus Creation, Section 1 Introduction",
          "Reasoning": "MuA ViC includes video tracks from TED and TEDx talk recordings capturing speakers on stage, recorded via cameras operated in these events. This video data is human generated as it originates from human-captured recordings of live talks."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3 Corpus Creation",
          "Reasoning": "The corpus contains human transcriptions of the speech, which are human generated. Additionally, translations are present that come either from human translations sourced from TED2020 or pseudo-translation labels generated by M2M-100 418M machine translation models. Therefore, the text modality includes both human generated and model generated data."
        }
      ]
    }
  },
  {
    "id": "anwar23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8466,
      "completion_tokens": 226,
      "total_tokens": 8692
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3: Corpus Creation",
            "reasoning": "The paper indicates that TED and TEDx talks are transcribed and translated by humans, which implies manual annotation. However, it does not specify expert annotators; the transcriptions and translations are from external sources like TED2020 and mTEDx datasets. Hence, likely multiple non-expert humans performed annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3: Corpus Creation",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators for transcriptions or translations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3: Corpus Creation",
            "reasoning": "There is no mention of scoring rubrics or formalized evaluation criteria in the annotation process for the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3: Corpus Creation",
            "reasoning": "The paper does not provide or reference any annotation guideline examples or exemplars used during annotation for the MuA ViC corpus."
          }
        }
      ]
    }
  },
  {
    "id": "anwar23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9656,
      "completion_tokens": 308,
      "total_tokens": 9964
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance done by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts for validating the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance involving a single non-expert human annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided about quality assurance involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that AI models were used for quality assurance or as judges for dataset validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automatic or algorithmic quality assurance processes applied to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied to the MuA ViC dataset during its creation or validation. Although the data source is from TED and TEDx talks with human transcriptions and translations sourced or pseudo-generated, no explicit QA steps are documented. Therefore, no quality assurance process is considered present based on the information provided in the paper."
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9266,
      "completion_tokens": 463,
      "total_tokens": 9729
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is sourced from existing TED and TEDx talk recordings; no indication that data was originally created from scratch by humans specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus contains natural audio-visual recordings; no mention of data generated originally by AI or ML models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "Section 3, Corpus Creation",
          "reasoning": "The paper states that text translations were obtained from the TED2020 corpus, but it does not specify that translations were done by humans for this dataset; thus no clear evidence of newly generated human translations."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 3, Corpus Creation",
          "reasoning": "The paper states that for untranslated train set examples, pseudo-translation labels were acquired from a machine translation model M2M-100 418M, indicating some data translations were generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3, Corpus Creation",
          "reasoning": "The dataset is collected by sourcing existing TED and TEDx talk recordings, reusing and aligning audio and video tracks with existing human transcriptions and text translations from publicly available corpora (LRS3-TED and TED2020)."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3, Corpus Creation",
          "reasoning": "The authors developed a fuzzy text matching strategy to align the TED2020 translations with LRS3-TED transcriptions, and generated pseudo-translation labels from machine translation models, which involves modifying and deriving new data from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation methods are described explicitly in the paper."
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9784,
      "completion_tokens": 425,
      "total_tokens": 10209
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.2, 4.3",
          "reasoning": "The paper describes fine-tuning pre-trained models on the MuA ViC dataset for supervised audio-visual speech recognition and speech-to-text translation tasks. Specifically, Sections 4.2 and 4.3 present baseline results using fine-tuned models on this dataset, showing improvements in performance for both recognition and translation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning techniques or RL-based post-training methods applied on this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.2, 4.3",
          "reasoning": "MuA ViC serves as a benchmark dataset for evaluating and benchmarking audio-visual speech recognition and speech-to-text translation models. The paper reports extensive evaluation metrics such as WER and BLEU scores on its test sets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specifically use the dataset for analyzing trends or patterns beyond reporting standard evaluation results."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for augmenting models via retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10507,
      "completion_tokens": 358,
      "total_tokens": 10865
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 1 Introduction, Table 1",
          "reasoning": "The dataset MuA ViC introduced in this paper contains audio-visual speech data in 9 human languages: English, Arabic, German, Greek, Spanish, French, Italian, Portuguese, and Russian. This is clearly stated in Section 1 and shown in detail in Table 1, confirming it covers more than two human languages, justifying the multilingual label."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any presence of programming or structured code-related content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of human spoken language speech and visual data, with no biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any use of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "anwar23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7725,
      "completion_tokens": 123,
      "total_tokens": 7848
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Conclusion",
          "reasoning": "The paper explicitly states that the corpus is available at https://github.com/facebookresearch/muavic, indicating that the code and data for dataset construction are publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 'Corpus Creation'",
          "reasoning": "The dataset creation process is documented in the paper's Section 3, describing data sourcing from TED and TEDx talks, alignment with transcriptions and translations, matching strategies, and handling of untranslated data with pseudo-translation labels."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6048,
      "completion_tokens": 234,
      "total_tokens": 6282
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1",
          "Reasoning": "The audio signals (S0, S1, S2) were synthesized using XKL for the vowels and natural utterance segments for consonants, controlled and manipulated programmatically as described, indicating model generation without direct human recording for the whole stimuli."
        },
        {
          "Modality": "video",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "Visual stimuli were produced using physical models of the human vocal tract attached to a miniature robot hand unit controlled by servo motors and Arduino, generating synchronized jaw movements, i.e., robot-generated visual data rather than recorded human video."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "Tactile stimuli were generated using the same robot hand unit controlled by servo motors and Arduino to provide tactile cues to participants, representing signals generated by the robotic apparatus without direct human capture."
        }
      ]
    }
  },
  {
    "id": "arai22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6848,
      "completion_tokens": 274,
      "total_tokens": 7122
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.2 Procedure; Section 3.1 Participants",
            "reasoning": "The annotations correspond to behavioral responses collected from 33 normal Japanese listeners who were participants in an experiment assessing their perception of audio and multimodal stimuli. The annotation corresponds to a human perceptual labeling (heard /atta/ or /a/+ta/) by individuals with expert linguistic background implicitly (native Japanese listeners) and the experiment was performed by human experts conducting and labeling participant responses."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Procedure",
            "reasoning": "The paper states that participants were asked to indicate what they heard (/atta/ or /a/+/ta/) for each trial, which represents simple instructions given to participants for annotation of perception data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 Experiment; no mention of scoring rubrics",
            "reasoning": "No detailed scoring rubrics or multi-level rating scales were described. The human annotations were binary forced-choice responses without rubric guidelines mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No examples or demonstration trials of labeling are described in the paper for the annotation procedure; only the instructions to indicate responses were given."
          }
        }
      ]
    }
  },
  {
    "id": "arai22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8038,
      "completion_tokens": 382,
      "total_tokens": 8420
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert related to dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper of multiple human experts performing quality assurance for dataset annotations or content validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human was involved in quality assurance of dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of multiple non-expert humans conducting quality assurance in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study does not describe any use of AI models as judges or for quality assurance of dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated processes or algorithmic verification used as quality assurance for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new stimuli synthesized and generated via physical models and robotic units, but no explicit quality assurance or validation process of these stimuli as dataset annotations is described. The experiments focus on perception tests rather than dataset construction or annotation verification; thus, no quality assurance process for a dataset is documented."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7648,
      "completion_tokens": 490,
      "total_tokens": 8138
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1, 2.2, and 2.3",
          "reasoning": "The audio stimuli S0, S1, and S2 were synthesized using XKL for vowels and natural recorded consonant sounds, prepared specifically for this study. The visual stimuli involved a physical model of the human vocal tract with a miniature robot hand unit controlled to produce articulatory jaw movements synchronously with the audio. The tactile stimuli used the same mechanical setup to provide tactile cues. These stimuli were created from scratch by the researchers for this study to investigate multimodal speech perception, thus constituting original content created by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of data generated entirely by AI or machine learning models independently creating the dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was produced by translating content from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was involved in generating the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not merely collected or aggregated from existing sources but was newly created for the experiment."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 4 Discussion",
          "reasoning": "The audio stimuli were synthesized by modifying formant transitions on a base vowel and natural consonant sounds, which constitutes a transformation of existing elements. Additionally, the stimuli are based on previously studied utterances (/atta/ and /a/+pause+/ta/) from prior work, indicating these data are derived through modifications and adaptations of pre-existing speech characteristics and recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper sufficiently documents the origin and method of generation of the datasets."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8166,
      "completion_tokens": 277,
      "total_tokens": 8443
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 Results and Section 4 Discussion",
          "reasoning": "The newly created stimuli (audio, audio+visual using vocal tract model, and audio+tactile) are used to measure listeners' perception responses. The data is collected to evaluate perception differences under different stimulus conditions, thus the dataset is used for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Discussion",
          "reasoning": "The paper analyzes patterns in perception responses (response rates) to determine how visual and tactile cues affect speech perception of geminate consonants in Japanese. The dataset is used primarily for analyzing perceptual trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the usage of the newly created stimuli dataset in evaluation experiments and perception analysis."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8889,
      "completion_tokens": 389,
      "total_tokens": 9278
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 3.1 Participants",
          "reasoning": "The dataset consists of speech stimuli and perception data focused on Japanese utterances such as /atta/ and /a/+pause+/ta/. The paper clearly states that the participants are native Japanese listeners and studies are conducted on Japanese geminate consonants. There is no mention of other languages in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken Japanese stimuli and human participants' perception responses, so it contains entries with language."
        }
      }
    }
  },
  {
    "id": "arai22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6107,
      "completion_tokens": 131,
      "total_tokens": 6238
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention any publicly available code or link to a repository containing the code used for generating the stimuli or conducting the experiments."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed descriptions in Section 2 about the preparation of audio, visual, and tactile stimuli, including how the physical vocal tract model and tactile devices were used and synchronized with audio signals. Section 3 details the experimental procedure and participant information. This documentation sufficiently describes the dataset creation process and experimental setup."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6675,
      "completion_tokens": 267,
      "total_tokens": 6942
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, 3.3, 3.5",
          "Reasoning": "The paper describes recordings of bilateral behind-the-ear (BTE) hearing-aid microphone audio from eight normal-hearing participants engaged in interactive conversational tasks. This audio data is captured from human participants during the experiments, making it audio modality data generated through human involvement."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, 3.5",
          "Reasoning": "Head movement was tracked using 9-DOF Razor inertial measurement units (IMUs) worn by participants to record yaw, pitch, and roll head movements. The IMU signals derived from human participants during the conversational tasks constitute sensor/signal data originated through human involvement."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.5",
          "Reasoning": "Four Logitech HD webcams recorded each participant during the recordings. This video data is captured through human-operated cameras during the study, thus it is human-generated video data."
        }
      ]
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7475,
      "completion_tokens": 201,
      "total_tokens": 7676
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.6 (Annotation)",
            "reasoning": "The annotations were performed manually using Wavesurfer software based on the first author listening to the audio and visually inspecting the waveform, indicating a single human expert performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.6",
            "reasoning": "The paper states that annotations were given by the first author based on audio listening and waveform inspection but does not mention detailed annotation instructions provided in written guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.6",
            "reasoning": "No mention of scoring rubrics or specific criteria for annotations is described in the annotation section or elsewhere in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.6",
            "reasoning": "The paper does not provide any examples of annotations or annotated data within the text or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8665,
      "completion_tokens": 273,
      "total_tokens": 8938
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": true,
          "reference": "Section 3.6 Annotation",
          "reasoning": "Annotations were done manually by the first author listening to the audio and visually inspecting the waveform to place utterance markers, which indicates a single human annotator performing quality assurance without explicit mention of them being an expert or having special qualification beyond author involvement."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance for the dataset annotations was performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic validation was described as quality assurance for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A manual annotation process is described (Section 3.6), thus QA is documented."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8275,
      "completion_tokens": 430,
      "total_tokens": 8705
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Methods), especially 3.1 Participants, 3.2 Experimental Set-up, 3.3 Acoustic conditions, 3.4 Tasks",
          "reasoning": "The paper describes novel data recorded from human participants during interactive conversational tasks designed and conducted by the authors. The speech and head movement data were collected from eight normal-hearing participants who performed two- and four-person conversational tasks under controlled acoustic conditions. This data is original content created entirely from scratch by human contributors and is not translated, adapted, or derived from any pre-existing dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any datasets were generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper mentions use of existing speech databases for babble noise and acoustic stimuli, the dataset introduced consists of newly recorded conversations and head movement data, not simply collected or aggregated existing data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data does not appear to be directly based on existing sources with modifications; rather, it is newly recorded experimental data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of data generation is explicitly documented and described."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8793,
      "completion_tokens": 376,
      "total_tokens": 9169
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses the dataset for analysis and testing of an existing DOA algorithm, but does not use it exclusively for evaluation or benchmarking of models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4 (Results), 5 (Discussion), and 6 (Conclusions)",
          "reasoning": "The new dataset of speech and head movements recorded during two- and four-person interactive conversational tasks in different acoustic conditions is used primarily to analyze trends and characteristics of head movements and conversational behavior. Detailed analyses of head positions, velocities, and conversational roles are presented. The dataset supports investigation of behavioral patterns and acoustic environment effects rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the dataset for behavioral analysis and algorithm testing, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9516,
      "completion_tokens": 470,
      "total_tokens": 9986
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Participants and throughout the paper",
          "reasoning": "The dataset consists of recordings of eight normal-hearing participants' conversations in English. The paper states that six participants were native English speakers and two were fluent non-native speakers, implying that the conversational content recorded in the dataset is exclusively English. The tasks (e.g., '20 questions' and NASA survival task) and recorded conversations involved only English language utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains conversational audio and head movement data, but no content with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human conversational speech and head movement data, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificial languages in the data."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language in the form of English conversational speech."
        }
      }
    }
  },
  {
    "id": "archerboyd23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6734,
      "completion_tokens": 169,
      "total_tokens": 6903
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any links to code repositories or software related to the data collection, preprocessing, or dataset construction. There is no indication that such code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Methods), including 3.1 Participants, 3.2 Experimental Set-up, 3.3 Acoustic conditions, 3.4 Tasks, and 3.5 Software and Hardware.",
          "reasoning": "The paper describes in detail the dataset creation process including participant recruitment and demographics, experimental set-up and conditions, tasks performed, equipment used for data acquisition, and annotation procedures. This provides sufficient documentation of the dataset creation process, enabling transparency and potential reproducibility of the data collection process."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 14424,
      "completion_tokens": 209,
      "total_tokens": 14633
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Abstract; Introduction; Conclusion",
          "Reasoning": "The paper introduces a new reverberant dataset used for training the proposed TRUNet network, generated from measured room impulse responses (RIRs) of an actual microphone array, meaning the acoustic signals were recorded with human involvement (via microphones) but simulated reverberation effects are modeled. The dataset involves multi-channel audio signals containing speech mixtures with reverberation, created realistically by combining human speech with measured RIRs. Hence, the modality is audio. The dataset is both human generated (the speech signals and recorded RIRs) and model generated (the dataset is simulated by convolving speech with RIRs to create reverberant mixtures). This is explicitly stated in the Abstract and Introduction, and confirmed in the Conclusion section where they mention training the network on a large reverberant dataset generated from measured RIRs of an actual microphone array."
        }
      ]
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 15224,
      "completion_tokens": 177,
      "total_tokens": 15401
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Abstract and Conclusion",
            "reasoning": "The paper describes the generation of a large reverberant dataset using measured room impulse responses and an acoustic simulator. This simulation-based method for data generation is an automatic process modeled computationally, not involving human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for the dataset generation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "There is no explicit mention of scoring rubrics or criteria for annotation provided in the paper for this dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not provide or mention annotation examples related to the dataset creation."
          }
        }
      ]
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 16414,
      "completion_tokens": 327,
      "total_tokens": 16741
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any manual quality assurance or validation performed by a single human expert for the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset. The dataset is generated from measured room impulse responses and simulated mixtures, with no mention of expert annotation or review."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No manual quality assurance by a single non-expert is reported in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No manual quality assurance by multiple non-expert annotators is described."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 1 and Abstract",
          "reasoning": "The new dataset is generated using measured room impulse responses (RIRs) from an actual microphone array and simulated reverberant mixtures. This process uses acoustic simulation techniques and algorithmic generation, which constitutes an automated process for dataset creation and quality control. There is no mention of manual annotation but the dataset is constructed and validated through automated simulation methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes that the dataset is generated from measured RIRs and simulation techniques, indicating a deliberate automated generation process rather than absence of quality assurance."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 16024,
      "completion_tokens": 424,
      "total_tokens": 16448
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the dataset used was created entirely from scratch by human contributors. Instead, the dataset was generated using measured room impulse responses and existing speech data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being translated from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being machine translated."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as simply collected or aggregated from existing sources without significant modification; rather it was generated in a specific manner for this research."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Abstract and Section 1 Introduction",
          "reasoning": "The dataset for training and testing is generated from measured room impulse responses (RIRs) of an actual microphone array and existing speech signals. This involves applying real measured RIRs to clean speech corpora to simulate reverberant speech mixtures. Thus the data is derived from existing sources (speech corpora) with transformations applied (convolution with measured RIRs) to create a realistic, reverberant, multi-channel dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides explicit descriptions on how the dataset was generated, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 16542,
      "completion_tokens": 351,
      "total_tokens": 16893
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Implicit) and Section 4.2",
          "reasoning": "The paper describes training the proposed TRUNet model end-to-end on a large reverberant dataset. The dataset is used directly to train the model from random initialization, without mention of prior pre-training. This is implied in the discussion of network training and evaluation in Section 4.2 and in the methods description."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) and Section 4.2",
          "reasoning": "Evaluation on a realistic and challenging reverberant dataset generated from measured room impulse responses is performed to benchmark and measure the performance of the proposed TRUNet and other methods, as described in Section 4.1 and 4.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and throughout the experimental results",
          "reasoning": "The dataset is used to analyze the effectiveness of the proposed TRUNet design and loss function variations on source separation performance, providing insight into spectral, temporal, and spatial processing components."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly shows practical usage of the proposed reverberant dataset for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 17265,
      "completion_tokens": 412,
      "total_tokens": 17677
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any dataset comprising exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 1 and Section 4",
          "reasoning": "The datasets used for training and evaluation consist of English speech signals only, as referenced by the usage of the CSTR VCTK corpus (English multi-speaker corpus) and other English speech data sources. The paper discusses speech signals with no mention of other languages, indicating monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of non-English language content in the datasets is given in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset itself does not contain programming or structured code content; rather the paper discusses deep learning models and architectures, but these are not datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain mathematical or logical notation as entries; such notation is used in the paper to describe methods and models only."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data like DNA sequences or animal signals are mentioned for the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially constructed languages such as Klingon or Esperanto in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved are clearly documented (English), so language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of spoken language data (English speech), so they do contain language entries."
        }
      }
    }
  },
  {
    "id": "aroudi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 14483,
      "completion_tokens": 149,
      "total_tokens": 14632
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link provided",
          "reasoning": "The paper does not mention or provide a link to any code repository for the dataset construction or generation process. There is no information about making the code publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 and throughout the text",
          "reasoning": "The paper describes the dataset as a large reverberant dataset generated from measured room impulse responses of an actual microphone array. It discusses the acoustic scenario, the reverberant conditions, and the generation process with references to measured room impulse responses to simulate realistic reverberant sound source separation environments, indicating some level of documentation regarding the dataset creation process."
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10634,
      "completion_tokens": 117,
      "total_tokens": 10751
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and 4.1",
          "Reasoning": "SpeechGLUE is the new dataset introduced in the paper as a speech version of the GLUE benchmark. It is created by converting GLUE's text sentences to speech utterances using a single-speaker TTS system (VITS model), which is a model-generated process. Therefore, the data modality is audio and the source is model generated, not human recorded or manual."
        }
      ]
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11434,
      "completion_tokens": 207,
      "total_tokens": 11641
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2, Section 4.1",
            "reasoning": "The SpeechGLUE dataset is constructed by converting existing GLUE text-based datasets into speech using a Text-to-Speech (TTS) system (VITS model), an automatic process. There is no indication of human annotation for the speech data itself."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any annotation instructions for the SpeechGLUE dataset since it is generated via an automatic TTS process rather than human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of scoring rubrics or human annotation guidelines related to SpeechGLUE data generation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any annotation examples for SpeechGLUE because no manual annotation process was involved."
          }
        }
      ]
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12624,
      "completion_tokens": 327,
      "total_tokens": 12951
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the SpeechGLUE dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance for the SpeechGLUE dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance involving multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models to perform quality assurance on the SpeechGLUE dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "SpeechGLUE dataset is created by converting GLUE text sentences to speech using a neural TTS system (VITS). This conversion is an automated process rather than manual annotation or labeling. The paper describes using TTS for this purpose, which inherently applies an automatic, reproducible transformation to generate the speech data, serving as a form of quality assurance by constrained synthesis reducing acoustic variability."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "An automatic process (TTS conversion) is described for dataset generation, so QA is not absent or undocumented."
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12234,
      "completion_tokens": 464,
      "total_tokens": 12698
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset created entirely from scratch by human contributors by recording humans reading new sentences or generating novel text data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The SpeechGLUE dataset is generated by synthesizing speech from existing GLUE text data using a text-to-speech (TTS) system (VITS model). This speech data is generated by a machine learning model without human recording, thus it is original data created by a model rather than collected from humans."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or mention of content translated by machine translation systems in the dataset generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4.1",
          "reasoning": "The SpeechGLUE dataset is based on the existing GLUE benchmark text data which is publicly available. The authors collected this existing dataset and converted it to speech via TTS without fundamentally altering the original textual data structure."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The original GLUE text dataset is derived into SpeechGLUE by applying a text-to-speech system to produce speech signals. This transformation from text to speech involves modifications and adaptations, thus the data is derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and generation process of the SpeechGLUE data is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12752,
      "completion_tokens": 238,
      "total_tokens": 12990
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4.1, Table 2",
          "reasoning": "SpeechGLUE is introduced as a speech version of the GLUE benchmark and is used exclusively for evaluating the linguistic knowledge captured by speech SSL models. The dataset is generated by converting GLUE text data into speech using TTS, and is used to extensively evaluate speech SSL models in various NLU tasks without updating the upstream model parameters, indicating its exclusive use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13475,
      "completion_tokens": 509,
      "total_tokens": 13984
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses datasets in English; no indication of multiple human languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are derived from English GLUE tasks, converted into speech via TTS. No mention of exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The SpeechGLUE dataset is created by converting English text sentences from the GLUE benchmark into corresponding speech utterances using text-to-speech (TTS) systems. All data entries are in English, as the original GLUE benchmark is English-only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly English-based; no non-English language data entries are described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-related content is described as part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic notations are part of the dataset; tasks are natural language-based."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that biological sequences or non-human communication data are included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses natural English language only, with no constructed or fictional language content."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) used are clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language data, so language is applicable."
        }
      }
    }
  },
  {
    "id": "ashihara23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10693,
      "completion_tokens": 170,
      "total_tokens": 10863
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3.2 and Introduction",
          "reasoning": "The paper explicitly states that they release SpeechGLUE for reproducibility at https://github.com/ashi-ta/speechGLUE, indicating that the code used for constructing their new dataset (SpeechGLUE) is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The paper provides a detailed description of the dataset creation process for SpeechGLUE, including how the original GLUE text datasets are converted to speech using a TTS system (VITS trained on LJSpeech), the resampling methods, data size details in Table 2, and text normalization steps. This level of detail clearly documents their dataset construction."
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5957,
      "completion_tokens": 98,
      "total_tokens": 6055
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Speech material",
          "Reasoning": "The authors introduced the PATAFreq recording database, collected recently during the 2020 COVID-19 lockdown, featuring audio recordings of read speech from 9 native French speakers over multiple sessions, recorded using professional microphones and soundcards, thus confirming human-generated audio data directly recorded for this new dataset."
        }
      ]
    }
  },
  {
    "id": "audibert22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6757,
      "completion_tokens": 288,
      "total_tokens": 7045
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1",
            "reasoning": "The dataset (PATAFreq) was collected from 9 speakers who recorded themselves regularly over multiple sessions during a lockdown period, with manual chunk segmentation performed by researchers as implied by 'manually segmented into chunks' (Section 2.1). The segmentation and labeling involve expert knowledge of phonetics, given the specialized acoustic measures and precise chunk segmentation described."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1",
            "reasoning": "Speakers were given instructions to record in a quiet environment, consistent microphone settings, and to report environmental noise via a questionnaire. Additionally, a dedicated application (modified MonPaGe) was used to present instructions and manage recordings for most speakers, indicating that instructions were clearly provided for data collection."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention any scoring rubrics or rating scales used for annotation or assessment of the speech material beyond self-reports of fatigue and vocal fatigue, which are not scoring rubrics for the main annotation task."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No examples of the annotation or segmentation tasks are provided or described in the paper. The paper mentions manual segmentation but does not provide detailed examples or templates for annotations."
          }
        }
      ]
    }
  },
  {
    "id": "audibert22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7947,
      "completion_tokens": 287,
      "total_tokens": 8234
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for validation or annotation of the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper indicating that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple non-expert annotators conducting quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of any AI model to perform quality assurance on the dataset or its annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although acoustic features were automatically extracted using scripts and Praat software, the paper does not describe any automated quality assurance processes such as algorithmic verification of annotations or data integrity."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not discuss any specific quality assurance procedures for the dataset annotations or recordings. There is no mention of manual or automated QA processes or checks. Therefore, no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7557,
      "completion_tokens": 379,
      "total_tokens": 7936
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset consists of speech recordings of 9 native French speakers who recorded themselves reading two texts over approximately two months, specifically collected by the authors during the COVID-19 lockdown. The data is original content created by human participants performing a reading task and is not translated, adapted, or derived from existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes data generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation or language adaptation produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation techniques to produce the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was newly collected specifically for this study; it was not aggregated or collected from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset is analyzed and acoustic features derived from the raw recordings, the raw speech recordings themselves are original and not derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8075,
      "completion_tokens": 226,
      "total_tokens": 8301
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Results) and Section 4 (Discussion)",
          "reasoning": "The paper introduces the PATAFreq recording database collected from 9 French speakers over multiple sessions and uses it to analyze intra-speaker and inter-speaker variability. The dataset is used primarily to study trends and characteristics of speech and voice features across sessions, focusing on variability and stability rather than training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8798,
      "completion_tokens": 503,
      "total_tokens": 9301
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes only native French speakers reading texts; no indication of multiple languages is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of French speech; no presence of exactly two languages is reported."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves English; the study explicitly mentions native French speakers."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1, Methods: Speech material",
          "reasoning": "The dataset consists of speech recordings from nine native French speakers reading French texts; hence the dataset is monolingual with French as the only language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although analysis uses code (e.g., Python application and Praat scripts), the dataset entries themselves are speech recordings in French, not programming code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset entries include mathematical or logical notation; features are numeric descriptors of speech but not datasets of formal symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech recordings; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages; all speech data is in natural French."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly documented as French."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (French speech), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "audibert22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6016,
      "completion_tokens": 170,
      "total_tokens": 6186
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.1 Methods - Speech material",
          "reasoning": "The paper mentions a dedicated application developed in Python used by most speakers for recordings, but does not provide any link, repository, or indication that the code is publicly available. No explicit mention or reference to publicly shared code for data collection or preprocessing is given."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 - Methods",
          "reasoning": "The paper provides a detailed description of the dataset creation process including participant demographics, recording procedure, equipment used, recording environment instructions, number of sessions per speaker, and the speech material recorded. The acoustic feature extraction process is also clearly documented with references to specific tools and scripts used. This documentation allows understanding and potential reproduction of the data collection and processing pipeline."
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7353,
      "completion_tokens": 102,
      "total_tokens": 7455
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Protocol and corpus",
          "Reasoning": "The paper introduces the Dialogs Re-enacted Across Languages (DRAL) corpus, collected by recording bilingual participants having spontaneous conversations and then reenacting utterances in another language. This data comprises matched English-Spanish utterance pairs recorded from real human speakers, thus the data are audio modality and were obtained through human generation."
        }
      ]
    }
  },
  {
    "id": "avila23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8153,
      "completion_tokens": 340,
      "total_tokens": 8493
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 2 (Protocol and corpus)",
            "reasoning": "The dataset DRAL was collected using the described protocol with pairs of nonprofessional, bilingual participants who engaged in conversations and re-enacted utterances. These participants are described as nonprofessionals, indicating non-expert human annotators as the source of the data (utterance pairs). No mention is made of expert annotation or AI-model annotation for the corpus creation itself."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 (Protocol and corpus)",
            "reasoning": "The paper describes a protocol developed (DRAL) for the data collection, where pairs are directed by a producer to select and re-enact utterances. This implies instructions are provided to participants to guide the re-enactments to achieve matched pairs. Although the full detailed instructions are not included, the mention of a protocol and the direction by a producer indicate instructions were given."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 (Protocol and corpus)",
            "reasoning": "No indication or mention is made in the paper of scoring rubrics or formal scoring criteria for annotation or re-enactment quality assessment. The protocol involves re-enactment aimed at producing matched pairs, not an explicit rubric-based evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 (Protocol and corpus)",
            "reasoning": "The paper does not present or reference specific examples provided as part of annotation guidelines or instructions. While Table 1 shows some utterances, it is an illustration of data, not annotation guideline examples."
          }
        }
      ]
    }
  },
  {
    "id": "avila23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9343,
      "completion_tokens": 395,
      "total_tokens": 9738
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the dataset was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset quality assurance is not described as being conducted by multiple human experts; no such information is present in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single human non-expert annotator in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2 (Protocol and corpus) and footnote 26 (technical report)",
          "reasoning": "The dataset (Dialogs Re-enacted Across Languages - DRAL) was collected by pairs of bilingual nonprofessional participants who engaged in conversations and then re-enacted utterances in another language under the direction of a producer. This suggests that multiple non-expert humans (the bilingual speakers) were involved in generating and validating the matched utterance pairs through re-enactment and selection. However, the paper does not explicitly detail a formal annotation or quality assurance process by experts, so the participants themselves serve as multiple non-expert annotators/validators of the dataset content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge or for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process or algorithmic quality checking of data annotations is described for the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a data collection procedure involving bilingual speakers re-enacting utterances, implying a collection and validation process, so it is not the case that no QA was conducted or documented."
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8953,
      "completion_tokens": 396,
      "total_tokens": 9349
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Protocol and corpus",
          "reasoning": "The authors developed a new data collection protocol named DRAL, where bilingual speakers have spontaneous conversations and then re-enact utterances in their other language, creating matched bilingual utterance pairs. This data is original, created entirely by human participants through natural conversations and their bilingual re-enactments, not translated, adapted, or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the dataset was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not described as being produced through human translation of content from one language to another, but rather through re-enactment by bilingual speakers."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No parts of the data are indicated to have been generated via machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a collection or aggregation of existing sources; it is newly collected data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on existing sources with modifications; it was newly recorded and created following the DRAL protocol."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of dataset creation are clearly described and documented."
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9471,
      "completion_tokens": 364,
      "total_tokens": 9835
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used exclusively for evaluation or benchmarking; rather, it supports other uses."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2, 4, 5, 6, 7",
          "reasoning": "The DRAL corpus is introduced as a cross-language dialog dataset and is primarily used for analyzing prosodic differences and patterns across English and Spanish (e.g. Sections 4 and 7 describe correlation analyses and qualitative analyses using the dataset). The paper extensively uses it to investigate prosodic features and transfer, serving as a basis for exploratory analysis rather than model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the DRAL corpus is used as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10194,
      "completion_tokens": 362,
      "total_tokens": 10556
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The authors introduce the Dialogs Re-enacted Across Languages (DRAL) corpus, which is explicitly described as comprising matched English-Spanish utterance pairs. The corpus currently has 1871 matched EN-ES utterance pairs spoken by bilingual participants. This means the dataset includes exactly two human languages, English and Spanish."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "avila23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7412,
      "completion_tokens": 168,
      "total_tokens": 7580
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 and final paragraph of the paper",
          "reasoning": "The paper states that they share all data, code, and observations at a public repository: https://github.com/joneavila/DRAL, indicating that the code related to data collection and possibly processing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Protocol and corpus) and referenced technical report [26]",
          "reasoning": "The paper provides detailed description of the data collection protocol (the DRAL protocol) in Section 2, explaining the steps and participant setup, as well as information about the collected corpus. Additionally, they reference a technical report [26] for further details and have made the dataset available, indicating good documentation of the creation process."
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11016,
      "completion_tokens": 159,
      "total_tokens": 11175
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset, Real-world Recordings and Section 4.5 Recording from a Live Performance",
          "Reasoning": "The authors introduced a new real-world dataset comprising approximately 300 video recordings from 4 different live music shows collected from YouTube, which are user (human) generated audio recordings. These recordings were used for evaluation of their crowdsourced audio enhancement method. Additionally, the paper describes synthetic audio signals generated by mixing speech or music sources with various types of additive noise, which were algorithmically generated to simulate noisy crowdsourced recordings used for method evaluation. Therefore, the data includes both human generated real recordings and model generated synthetic audio signals."
        }
      ]
    }
  },
  {
    "id": "aziz24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11816,
      "completion_tokens": 294,
      "total_tokens": 12110
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Real-world Recordings and Sections 4.3-4.5",
            "reasoning": "The dataset for real-world recordings consists of approximately 300 video recordings collected from YouTube from four different music shows. These are crowdsourced user recordings without mention of human annotation for labeling or segmentation. The data is aligned and normalized automatically. Therefore, the 'annotation' process here is essentially an automatic collection and preprocessing pipeline, not human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No sections in the paper mention annotation instructions for this dataset.",
            "reasoning": "The paper does not describe any human annotation task or provide instructions to annotators for this dataset. The dataset was collected from YouTube recordings and aligned/normalized automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No sections in the paper mention scoring rubrics for this dataset.",
            "reasoning": "No evaluation rubrics or scoring guidelines for annotation are discussed for the new dataset; evaluation is done via objective metrics and subjective listening tests, not annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation guidelines or labeled data examples for this dataset are provided in the paper.",
            "reasoning": "The paper does not include any examples for annotation guidelines or annotated exemplars for the new dataset; the dataset consists of real recordings collected without explicit annotation."
          }
        }
      ]
    }
  },
  {
    "id": "aziz24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13006,
      "completion_tokens": 317,
      "total_tokens": 13323
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert performing quality assurance on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in the quality assurance of the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.3 Model Evaluation and 4.4 Results",
          "reasoning": "The paper reports conducting subjective human listening tests following the MUSHRA protocol with participants rating the quality of recordings on a scale from 0 to 100. This indicates multiple human non-expert annotators (listeners) participated in quality assurance of the dataset outputs through subjective evaluation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models being used to perform quality assurance on the dataset or annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.3 Model Evaluation",
          "reasoning": "Objective metrics such as SI-SNR, PESQ, and STOI were used to assess the quality of the reconstructed audio as an automated and algorithmic quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are documented and performed through both automated objective metrics and multiple human listener evaluations."
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12616,
      "completion_tokens": 496,
      "total_tokens": 13112
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Real-world Recordings",
          "reasoning": "The paper states that the authors have collected real world user recordings of live music shows from YouTube, which are independent user-generated videos capturing live performances. The collection involves new data created by humans capturing events using their devices, which were then gathered by the authors for their experiments."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.1 Synthetic Data",
          "reasoning": "The paper describes synthetic audio signals used for evaluation by mixing speech or music signals with various types of noise from existing corpora. This synthetic data is generated by artificially combining and modifying existing recordings to simulate noisy audio inputs, thus these data are generated by models (processing steps) rather than raw human recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Real-world Recordings",
          "reasoning": "The real-world dataset comprises a collection of multiple different user recordings of live music shows obtained from YouTube. The authors aggregated these independent user-generated recordings without mentioning significant modification besides alignment and normalization, thus this is collated data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Synthetic Data",
          "reasoning": "The synthetic dataset is derived from existing speech and noise corpora (e.g., LibriSpeech, DEMAND, AudioSet) by combining and mixing signals with added noise to synthetically generate noisy inputs. This involves transformation and adaptation of existing data to create new evaluation material."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation process are described adequately in the paper."
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13134,
      "completion_tokens": 224,
      "total_tokens": 13358
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.4, Section 4.5",
          "reasoning": "The paper describes collecting approximately 300 real-world user recordings from 4 different music shows, and conducting evaluations using synthetic and real-world recordings to benchmark the proposed method against baselines. This is used explicitly for measuring performance, using metrics such as SI-SNR, PESQ, STOI, and subjective MUSHRA tests."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13857,
      "completion_tokens": 476,
      "total_tokens": 14333
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of multiple human languages in the dataset. The focus is on audio signals from live music shows and synthetic noisy signals, without specifying any linguistic content in more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as containing content in exactly two human languages. There is no evidence in the paper suggesting bilingual language data in the introduced datasets."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Real-world Recordings and Synthetic Data Descriptions",
          "reasoning": "The speech data used for synthetic experiments is from the LibriSpeech corpus, which contains English speech recordings. The real-world music show recordings collected from YouTube are likely to primarily contain English content or music without specific mention of other languages. There is no indication of multiple languages being present in the datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the datasets include monolingual non-English language data. Speech data is from English corpora, and music shows are not specified to be in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio signals, not code or programming language related entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical notation in the methodology descriptions, the datasets themselves do not contain mathematical or logical notations as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human-generated audio recordings; there is no mention of biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is specified (English speech and music recordings), so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio recordings with spoken content and music, which involve human language audio, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "aziz24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11075,
      "completion_tokens": 214,
      "total_tokens": 11289
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.4",
          "reasoning": "The paper explicitly states in the Abstract: 'Code, dataset, and models are available.' Furthermore, in Section 4.4 it provides a link (https://shiranaziz.github.io/crowdsourced_audio_enhancement/) where code, datasets, models and audio examples are provided. This indicates that the code related to dataset preprocessing and generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 and 4.4",
          "reasoning": "Section 4.1 describes the collection process of real-world recordings from YouTube, including alignment and normalization steps. Section 4.4 discusses evaluation using synthetic datasets created from known sources (e.g., LibriSpeech, DEMAND, AudioSet) and provides details on the synthetic data creation (using certain numbers of sources, SNR levels, and noises). This indicates that the dataset creation process is documented adequately in the paper."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7533,
      "completion_tokens": 116,
      "total_tokens": 7649
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 - DISPLACE Corpus, especially 3.1 Recording Setup and 3.2 Data Collection",
          "Reasoning": "The DISPLACE corpus is a new dataset introduced by the authors, consisting of single-channel far-field audio recordings of multi-speaker, multi-lingual conversational speech with natural overlap, noise and reverberation. The audio was recorded in real academic environments using far-field microphones capturing natural conversational speech among human participants."
        }
      ]
    }
  },
  {
    "id": "baghel23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8333,
      "completion_tokens": 228,
      "total_tokens": 8561
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 states that the data annotations were generated with professional annotators who listened to lapel microphone recordings, indicating multiple expert human annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "Section 3.3 mentions that participants were provided written consent and guidelines before recording, and the annotation process was described as careful and involving multiple quality checks, implying detailed annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "There is no explicit mention of scoring rubrics or similar evaluation frameworks provided as part of annotation guidelines; the paper only references multiple levels of quality checks, not formal rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not provide or reference any examples within the annotation guidelines or instructions. There is no mention of demonstration annotations or examples given to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "baghel23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9523,
      "completion_tokens": 408,
      "total_tokens": 9931
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions professional annotators for the data annotation but does not specify if individual annotations were performed by a single human expert. Hence, this label is not applicable."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Annotation",
          "reasoning": "The paper states that the data annotations were generated with professional annotators who listened to the lapel microphone recordings and that multiple levels of quality checks were employed before arriving at the final annotations. The involvement of professional annotators (implying experts) and multiple levels of quality assurance suggests that multiple human experts conducted and validated the annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that annotation or quality assurance was done solely by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as professional annotators, implying expertise, so this label does not apply."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention AI models being used as judges or performing quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automatic or algorithmic verification of the annotation quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly mentions a thorough annotation process involving professional annotators and multiple levels of quality checks, so quality assurance was documented."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9133,
      "completion_tokens": 417,
      "total_tokens": 9550
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Collection and Section 3.3 Annotation",
          "reasoning": "The DISPLACE dataset was created by recording actual multilingual and multi-speaker conversational speech with human participants in physical rooms. The participants spoke spontaneously on selected topics, and professional human annotators generated speech, speaker, and language annotations from close-talking microphone recordings. This process indicates that the data is original content created entirely from scratch by human contributors and is not translated, adapted, or derived from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any new data was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a collection or aggregation of existing sources without modification; rather, it was specifically recorded and annotated for the challenge."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on existing sources with modifications; it was newly recorded and annotated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and generation method of the data is clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9651,
      "completion_tokens": 525,
      "total_tokens": 10176
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the DISPLACE dataset for pre-training large models, and no unsupervised or self-supervised pre-training is discussed."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The DISPLACE challenge explicitly states that no training data is provided; participants can use proprietary or public resources for training, but the dataset itself is not used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning pre-trained models using the DISPLACE dataset; rather, it is used for evaluation purposes."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training methods such as RLHF using the DISPLACE dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.4 - Development and Evaluation set; Section 4 - Challenge Tasks; Sections 6 and 7 - Challenge Results and Summary",
          "reasoning": "The DISPLACE dataset is provided for development and evaluation sets only, specifically for benchmarking speaker and language diarization systems in challenge tracks. The paper describes the evaluation metrics and results with DER, indicating exclusive use for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 - Figure 1 and 2 (language, speaker, overlap distributions); Section 7 - Summary",
          "reasoning": "The paper analyzes the dataset characteristics such as language distribution, speaker demographics, overlap durations, and segment statistics to highlight the challenges and traits of the multi-lingual multi-speaker conversational speech in the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base to augment models or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset utility is clearly described and demonstrated through evaluation and analysis."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10374,
      "completion_tokens": 602,
      "total_tokens": 10976
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.4 and 3.2",
          "reasoning": "The DISPLACE corpus contains conversations involving multiple Indian languages including Hindi, Kannada, Bengali, Malayalam, Telugu, Tamil, and Indian English. Section 3.4 explicitly states the corpus contains these multiple languages, and Section 3.2 describes participants fluent in multiple Indian languages and Indian accented English engaging in code-mixed conversations. Therefore, the dataset contains entries with more than two human languages, qualifying it as multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves more than two languages as indicated by references to at least seven languages (Hindi, Kannada, Bengali, Malayalam, Telugu, Tamil, Indian English). Hence, it is not restricted to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple Indian languages along with Indian English as a code-mixed multilingual dataset. It is not English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly multilingual with multiple Indian languages and some English usage. It is not restricted to a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of programming, structured code-related content or datasets involving programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is conversational speech and linguistic in nature with no mention of mathematical or logical formal expressions in the dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech conversations only; there is no indication of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural Indian languages and Indian English; no constructed or fictional languages are indicated."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset are explicitly documented and annotated, including multiple Indian languages and Indian English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of human conversational speech and contains multiple human languages; thus, the dataset definitely includes language content."
        }
      }
    }
  },
  {
    "id": "baghel23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7592,
      "completion_tokens": 166,
      "total_tokens": 7758
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link referencing code availability",
          "reasoning": "The paper does not mention or provide any link to code related to data collection, preprocessing, or dataset generation. It only discusses dataset collection procedures and annotations but does not indicate public release of the associated code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: DISPLACE Corpus (subsections 3.1 to 3.4)",
          "reasoning": "The paper provides detailed descriptions of the dataset collection setup, data collection process, annotation methodology, and dataset composition across multiple subsections in Section 3. This includes recording environment, participant selection, annotation details, language and speaker distributions, and evaluation partitions. Hence, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7121,
      "completion_tokens": 92,
      "total_tokens": 7213
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, The Hume Vocal Burst Dataset",
          "Reasoning": "The Hume-VB dataset is a newly introduced large-scale dataset consisting of over 35 hours of nonverbal vocalization audio recorded from 1,702 human speakers using their own microphones at home, hence the data is human-generated audio recordings."
        }
      ]
    }
  },
  {
    "id": "baird22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7921,
      "completion_tokens": 260,
      "total_tokens": 8181
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2: The Hume Vocal Burst Dataset",
            "reasoning": "The dataset HUME-VB is collected from 1,702 speakers who provided vocal bursts from their home via their own microphones. While the speakers are likely non-expert participants providing natural vocalizations, the paper does not describe expert annotation or who precisely performed label annotation. Since the dataset consists of labeled vocal bursts from many speakers with emotional and demographic labels, it suggests multiple human non-experts performed the annotations or labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not describe detailed annotation instructions provided to annotators for labeling emotional expression or demographic traits. The description focuses on dataset creation and usage without specifying annotation guidelines or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "There is no mention of scoring rubrics or formal evaluation criteria provided to annotators in the annotation guidelines in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not reference any annotation examples being provided as guidance for annotators labeling this new HUME-VB dataset."
          }
        }
      ]
    }
  },
  {
    "id": "baird22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9111,
      "completion_tokens": 310,
      "total_tokens": 9421
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts conducted quality assurance or annotation validation on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that quality assurance was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information regarding quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automatic or algorithmic verification or validation process of the dataset annotations or content quality assurance is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the Hume Vocal Burst (HUME-VB) dataset as a pre-existing dataset first presented elsewhere (reference [18]) but provides no details or descriptions about any quality assurance procedures applied during the creation or annotation of this dataset. There is no documentation or discussion of a quality assurance process within this paper, thus no quality assurance process is documented or described here."
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8721,
      "completion_tokens": 388,
      "total_tokens": 9109
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The paper explicitly states that the Hume Vocal Bursts (HUME-VB) dataset was collected from 1,702 speakers who recorded 36:47:04 of audio data of nonverbal vocalizations in their own homes via their own microphones. This indicates original data created entirely from scratch by human contributors, not translated, adapted, or derived from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of human translation for the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation for the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources; rather, the data was freshly collected from human subjects."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the dataset is based on existing sources with modifications or adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are well specified in the paper."
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9239,
      "completion_tokens": 333,
      "total_tokens": 9572
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3, Section 4",
          "reasoning": "The paper describes training models (single-task and multi-task deep learning architectures) from scratch using the Hume Vocal Bursts dataset, without mention of starting from pre-trained models. Section 3.3 details model training from scratch with Adam optimizer, and Section 4 presents experimental results from these trainings."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The dataset is used to evaluate model performance for emotional expression, age, and country of origin prediction tasks. The paper reports metrics like Concordance Correlation Coefficient, Unweighted Average Recall, and Mean Absolute Error on validation and test splits, indicating use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and 5",
          "reasoning": "The paper analyzes model performance trends across single-task and multi-task learning, and discusses acoustic feature relevance and the relationship between emotional expression and demographic traits. Section 5 suggests further analysis of acoustic features, indicating the dataset is used for pattern analysis beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9962,
      "completion_tokens": 527,
      "total_tokens": 10489
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2 - The Hume Vocal Burst Dataset",
          "reasoning": "The HUME-VB dataset includes vocal burst samples from speakers in four countries: USA, China, South Africa, and Venezuela. This indicates that the dataset contains data in multiple languages corresponding to these countries, thus making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains data from more than two countries and presumably more than two languages, so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to English only; it includes samples from speakers in multiple countries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to a single non-English language; multiple languages from several countries are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only human nonverbal vocalizations and does not include programming or structured code-related contents."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that mathematical or formal logical expressions are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human nonverbal vocalizations and does not include biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset are identifiable based on the countries of origin of the speakers and are documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are human vocalizations and thus contain language elements, so language is present."
        }
      }
    }
  },
  {
    "id": "baird22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7180,
      "completion_tokens": 222,
      "total_tokens": 7402
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and throughout the paper",
          "reasoning": "The paper mentions using the Hume Vocal Burst (HUME-VB) dataset, which was first presented as part of a 2022 ICML Expressive Vocalizations Workshop & Competition [18], but does not provide any link or information regarding code availability for the dataset collection, preprocessing, or generation. There is no explicit mention of publicly available code associated with the dataset in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: The Hume Vocal Burst Dataset",
          "reasoning": "Section 2 provides detailed description of the HUME-VB dataset, including its size (36:47:04 hours of audio), number of speakers (1,702), demographic details (age range, gender count, countries of origin), data collection conditions (collected at subjects' homes via their own microphones), and distribution of samples across train, validation, and test splits. This constitutes a clear and transparent documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7955,
      "completion_tokens": 288,
      "total_tokens": 8243
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1, 2.3",
          "Reasoning": "The dataset includes studio-recorded speech sentences spoken by human speakers (human generated) and domestic noise recordings (human generated). Additionally, the speech-in-noise scenes were simulated by convolving these signals with Binaural Room Impulse Responses and applying hearing aid processing algorithms (model generated) to create the processed signals. The audio was presented to human hearing impaired listeners for responses, confirming human involvement."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The listener responses were recorded as voice and then transcribed using Google Cloud Speech-to-Text API (model generated). These transcripts were further validated and corrected by human transcribers (human generated). The ground-truth sentences are known and were used for alignment to compute intelligibility scores."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The dataset includes quantitative listener characteristics such as bilateral pure-tone audiograms measured at standard frequencies, hearing loss classification, and other standard hearing test results. These measurements originate from clinical human testing and are thus human generated tabular data."
        }
      ]
    }
  },
  {
    "id": "barker22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8755,
      "completion_tokens": 264,
      "total_tokens": 9019
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3",
            "reasoning": "Section 2.3 describes the listening tests involving a panel of 27 hearing-impaired listeners who responded to speech-in-noise signals processed by hearing aids. The responses were transcribed first by an automatic Speech-to-Text API and then validated and corrected by a team of transcribers. This indicates that multiple human non-expert annotators performed the transcription validation and correction annotation task for response intelligibility."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided to the transcribers or listeners. It only describes the use of the Google Cloud Speech-to-Text API and subsequent human validation without referencing specific instructions or guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No scoring rubrics or detailed scoring guidelines for annotation tasks are described in the paper regarding the transcription validation or intelligibility scoring beyond counting correctly identified words."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention of specific examples or annotated samples provided to annotators for guidance in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "barker22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9945,
      "completion_tokens": 417,
      "total_tokens": 10362
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance of dataset annotations or content was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 The Listeners and Listening Tests",
          "reasoning": "The dataset relies on listening tests wherein 27 hearing-impaired listeners participated by repeating sentences they heard. The voice recordings of their responses were transcribed using Google Cloud Speech-to-Text API, and then 'a team of transcribers validated the Speech-to-Text outputs and corrected errors.' This indicates that multiple human annotators (transcribers) performed validation, likely with hearing or speech expertise, ensuring data quality."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies use of a team of transcribers validating and correcting transcriptions, but does not specify that these are non-experts. Given the specialized nature of hearing-impaired speech intelligibility annotation, it is likely the transcribers are experts or trained annotators; no explicit mention suggests multiple non-expert annotators only."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by AI models as judges for dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 The Listeners and Listening Tests",
          "reasoning": "Initial transcriptions were generated automatically using the Google Cloud Speech-to-Text API. This automated process was then followed by human validation, combining machine and human quality assurance to ensure transcription accuracy."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described and was performed (transcription validation by multiple humans and automatic initial transcription), thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9555,
      "completion_tokens": 501,
      "total_tokens": 10056
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The panel of hearing-impaired listeners were recruited and characterized by audiograms and other hearing tests. Listening tests were conducted using software that presented processed speech-in-noise signals to these listeners in their homes. The listeners responded by repeating what they heard, and their verbal responses were recorded, transcribed, and manually corrected by human transcribers. This process resulted in original, human-generated data comprising 7233 listener intelligibility responses. Thus, this dataset is original content created by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described does not include any data generated entirely by AI or machine learning models; rather, AI/ML systems were submitted by participants for prediction tasks."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data involved translating content from one language to another via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated via automatic machine translation processes according to the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not merely aggregated or collected from existing sources; it was produced through new experimental recordings and tests."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The speech-in-noise signals used were based on prior resources: 1500 samples were selected from the 10,000 speech-in-noise scenes generated for the prior Clarity Enhancement Challenge (CEC1). These scenes were generated by simulating acoustic environments and processing with various hearing aid algorithms. Thus, the audio stimuli represent derived data based on existing generated data, with additional signal processing and hearing aid algorithm applications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10073,
      "completion_tokens": 312,
      "total_tokens": 10385
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The paper describes that participants use the provided dataset to train models predicting speech intelligibility scores. Multiple submitted systems employ supervised learning to map inputs (processed signals and listener data) to intelligibility scores, indicating usage for supervised fine-tuning of models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3, Section 4, and Section 5",
          "reasoning": "The dataset is explicitly used as a benchmark to evaluate and compare intelligibility prediction models submitted to the Clarity Prediction Challenge (CPC1). Results, correlations, and ranking of models are presented using this dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Results) and Sections 4.1 to 4.3",
          "reasoning": "The dataset is used for analyzing model performances, intelligibility prediction challenges, hearing loss modeling strategies, and binaural processing approaches, providing insights into the state of the art and model behaviors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10796,
      "completion_tokens": 545,
      "total_tokens": 11341
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses speech stimuli that are studio-recorded sentences in English, and there is no indication of inclusion of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates the use of exactly two human languages in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 The Signals, Paragraph 1; Reference [11]",
          "reasoning": "The dataset uses 1,500 speech-in-noise scenes derived from a corpus of British English speech recordings (see Section 2.1 and reference [11]). The target sentences are studio-recorded English sentences. There is no mention of other spoken languages used."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not use a non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although machine learning methods and software are used for prediction models, the dataset itself does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical or logical expressions are used in the modeling and evaluation, but the dataset itself does not contain such notation as part of its entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech stimuli and listener responses, with no biological sequences or non-human communication systems involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses natural human language (English) sentences, not constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language is clearly specified as English, so language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data, specifically English speech, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "barker22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8014,
      "completion_tokens": 171,
      "total_tokens": 8185
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section provides code links",
          "reasoning": "The paper does not include any explicit references, links, or mentions of publicly available code repositories related to the dataset construction, preprocessing, or generation. While it references prior work and datasets, there is no indication that the code used to create or process the new dataset for this challenge is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Materials",
          "reasoning": "Section 2 ('Materials') of the paper provides a detailed description of the dataset construction, including the speech-in-noise signals, hearing aid systems, listener characteristics, and intelligibility measurements. It describes the signal synthesis, HA processing, listener recruitment, and listening tests with transcription methodology, thus providing comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6663,
      "completion_tokens": 206,
      "total_tokens": 6869
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Corpus Description",
          "Reasoning": "The new dataset consists of audio recordings of patient-therapist dyadic conversations collected during eight-week Cognitive Behavioral Therapy sessions via voice and video call. These recordings were captured from real therapy sessions conducted by humans."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Corpus Description",
          "Reasoning": "The dataset includes video call recordings of the therapy sessions, as explicitly stated that the patients attended via voice and video call, thus involving recorded video data from human interactions."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Corpus Description",
          "Reasoning": "The recorded conversations were manually transcribed by humans into text, resulting in textual conversation transcripts that form part of the dataset."
        }
      ]
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7463,
      "completion_tokens": 232,
      "total_tokens": 7695
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2 Annotation task",
            "reasoning": "The paper states that two CBT experts performed the annotation of therapeutic conversations using the Working Alliance Inventory Observer-rated Shortened (WAI-O-S). This implies multiple human experts were involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation task",
            "reasoning": "Annotators were asked to listen to the session and complete the WAI-O-S questionnaire composed of 12 questions rated on a Likert scale. The use of a structured and validated questionnaire suggests clear instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation task",
            "reasoning": "The WAI-O-S questionnaire uses a standardized Likert scale from 1 (never) to 7 (ever) for responses, which constitutes a clear scoring rubric for the annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No direct mention",
            "reasoning": "The paper does not provide or mention any annotation examples or exemplar annotations provided to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8653,
      "completion_tokens": 380,
      "total_tokens": 9033
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Annotation task",
          "reasoning": "The paper states that two CBT experts performed the annotation of therapeutic conversations with the Working Alliance Inventory Observer-rated Shortened (WAI-O-S), indicating QA was conducted by multiple human experts. This is further supported by the calculation of inter-annotator agreement (IAA) between these expert annotators."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies annotators as CBT experts, thus they are not non-experts; no QA by multiple non-experts is described."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance was described as being performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is performed and documented via expert annotation and inter-annotator agreement measurement."
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8263,
      "completion_tokens": 438,
      "total_tokens": 8701
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus - Description, Section 3.2 Annotation task",
          "reasoning": "The dataset comprises original, manually collected and transcribed patient-therapist conversations from eight therapy sessions of three patients undergoing Cognitive Behavioral Therapy (CBT), recorded via voice and video calls. These are real interactions, not translated or derived from existing datasets. Additionally, the Working Alliance Inventory (WAI) ratings were manually annotated by two CBT experts based on listening to the sessions, further supporting that the data was created by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated by AI or machine learning models as original data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being translated from another language by human translators in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being applied to the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not merely collected from existing sources or aggregated but was newly recorded and transcribed in the context of the study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is original and not derived from existing datasets or sources with modifications; however, features such as lexical entrainment and turn-taking are derived analysis on the data, but the dataset itself is original."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and collection method of the data are specified clearly."
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8781,
      "completion_tokens": 226,
      "total_tokens": 9007
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and Section 5",
          "reasoning": "The corpus of patient-therapist spoken conversations collected during an eight-week CBT intervention is used primarily for analyzing correlations between speech/language features and working alliance ratings (WAI). The paper details the analysis of speech and language features extracted from the dataset and their association with working alliance metrics, providing insights rather than using the dataset for model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9504,
      "completion_tokens": 374,
      "total_tokens": 9878
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Corpus Description)",
          "reasoning": "The corpus consists of 23 Italian dyadic conversations during Cognitive Behavioral Therapy sessions, and the Italian form of the WAI Observer-rated Shortened inventory was administered (Section 2.2). This indicates that the dataset entries are in a single, non-English language\u2014Italian."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of transcribed spoken conversations in Italian, so it contains language."
        }
      }
    }
  },
  {
    "id": "bayerl22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6722,
      "completion_tokens": 164,
      "total_tokens": 6886
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part of the paper provides a link or reference to code repositories or code artifacts.",
          "reasoning": "The paper does not mention any publicly available code or provide links to code repositories for dataset construction, preprocessing, or any related analysis tools."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Corpus and subsections 3.1 Description and 3.2 Annotation task",
          "reasoning": "The paper documents the dataset creation process, including details of data collection (patient-therapist conversations over eight weeks), manual transcription, manual annotation with Working Alliance Inventory (WAI) by experts, inter-annotator agreement, and feature extraction methods. This documentation provides a clear description of the dataset collection and annotation workflow."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7097,
      "completion_tokens": 234,
      "total_tokens": 7331
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Abstract",
          "Reasoning": "The mClothoAQA dataset includes 1991 audio files. These audio files are the same as those from the ClothoAQA dataset, which is derived from the Clotho dataset known to contain real environmental audio recordings. The paper states it created the mClothoAQA dataset by machine-translating questions and answers but did not generate new audio files, so the audio is human recorded data."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3",
          "Reasoning": "The question-answer pairs for the mClothoAQA dataset were generated by machine-translating the original English questions and answers from ClothoAQA into seven other languages using Google's machine translation API, and then they were verified and adjusted by humans. Since the majority of the data creation involved machine translation (i.e., model generated), the text data is considered model generated, with minor human involvement only in verification and adjustment."
        }
      ]
    }
  },
  {
    "id": "behera23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7897,
      "completion_tokens": 276,
      "total_tokens": 8173
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3 (mClothoAQA Dataset)",
            "reasoning": "The dataset mClothoAQA was created by translating ClothoAQA questions and answers using machine translation and then verified and adjusted by humans. The original ClothoAQA dataset was crowdsourced (i.e., uses non-expert humans), thus it is reasonable and explicitly stated that multiple humans verified and adjusted, implying multiple human non-experts performed the annotation verification and adjustments."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 (mClothoAQA Dataset)",
            "reasoning": "The paper does not mention any specific instructions provided to annotators or verifiers for the translation verification and adjustment process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 (mClothoAQA Dataset)",
            "reasoning": "No mention of scoring rubrics or formal evaluation criteria for annotators verifying or adjusting translations is present in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 (mClothoAQA Dataset) / Table 3",
            "reasoning": "While some example translations are presented (Table 3), these are examples of data samples, not annotation guidelines or examples provided to annotators during the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "behera23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9087,
      "completion_tokens": 393,
      "total_tokens": 9480
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single human expert or subject matter expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the original ClothoAQA dataset is crowdsourced with multiple annotators, the mClothoAQA dataset is created by translation and human verification. The paper does not specify that multiple human experts performed QA or validation for mClothoAQA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance done by a single annotator, non-expert or otherwise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3 (mClothoAQA Dataset)",
          "reasoning": "The questions and answers for the mClothoAQA dataset were translated using machine translation and then 'verified and adjusted by humans' (Section 3). Since there is no mention that these humans are experts, and the original annotations were crowdsourced, it is most reasonable to consider these humans as non-expert annotators performing quality assurance collectively."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model being used to judge or perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 (mClothoAQA Dataset)",
          "reasoning": "The initial translation of the ClothoAQA dataset questions and answers into seven new languages was performed using Google's machine translation API, an automatic process. This process constitutes an automated step in dataset creation and initial quality Check, though final human verification was done."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a quality assurance process involving human verification of machine-translated data, so QA is documented."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8697,
      "completion_tokens": 449,
      "total_tokens": 9146
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces the mClothoAQA dataset derived from the ClothoAQA dataset, which is crowdsourced English data, but the new multilingual data is created by translation, not original human creation from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not generated by AI or machine learning models generating original content but rather is translated from an existing dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "Though the paper mentions that after machine translation, question-answer pairs were 'verified and adjusted by humans', there is no indication that the dataset was originally translated by humans. The initial translation is machine-based, with some human correction afterwards, which does not constitute full human translation."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The mClothoAQA dataset is built by translating the questions and answers from the original English ClothoAQA dataset into seven other languages using Google's machine translation API, as explicitly stated in Section 3."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a mere aggregation or collection of existing data without modification; it is generated by applying machine translation to an existing dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "mClothoAQA is derived from the ClothoAQA dataset by applying machine translation and subsequent human verification and adjustment, which constitutes modification and adaptation of the original dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9215,
      "completion_tokens": 480,
      "total_tokens": 9695
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the mClothoAQA dataset for pre-training large models on general patterns or in an unsupervised/self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that models are trained from randomly initialized parameters using the mClothoAQA dataset. Instead, they use pre-trained embeddings for audio and text as input features."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5 Experimental Results and Analysis",
          "reasoning": "The paper describes training and evaluating a multi-lingual AQA model using the mClothoAQA dataset in a supervised fashion on the question-answer pairs, fine-tuning the model's parameters for classification tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no discussion or mention of reinforcement learning or RL-based post-training methods applied with the mClothoAQA dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Experimental Results and Analysis",
          "reasoning": "The dataset is explicitly used for evaluating model performance across different languages using accuracy metrics on test splits."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 mClothoAQA Dataset and Section 5 Experimental Results and Analysis",
          "reasoning": "The authors analyze characteristics of the dataset such as question word distributions and answer word clouds, demonstrating analysis beyond purely modeling."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the dataset in supervised training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9938,
      "completion_tokens": 552,
      "total_tokens": 10490
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract and Section 3",
          "reasoning": "The paper introduces the mClothoAQA dataset, which contains audio question-answer pairs in eight human languages: English, French, Hindi, German, Spanish, Italian, Dutch, and Portuguese. This clearly indicates that the dataset includes more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of eight languages, not exactly two."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the original ClothoAQA dataset is English-only, the new dataset introduced, mClothoAQA, is explicitly multilingual, so it is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset has multiple languages including English and seven other languages, so it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio files and question-answer pairs in natural human languages; there is no mention of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of natural language questions and answers about audio; the paper does not indicate inclusion of mathematical or formal logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the audio files include environmental sounds, the dataset entries are natural language question-answer pairs, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies real human languages only; no constructed or fictional languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved in the dataset are explicitly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language entries in multiple human languages; thus, language content is present."
        }
      }
    }
  },
  {
    "id": "behera23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7156,
      "completion_tokens": 160,
      "total_tokens": 7316
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Footnote in introduction",
          "reasoning": "The paper explicitly states that the dataset and code can be accessed at https://github.com/swarupbehera/mAQA, indicating the code related to the dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (mClothoAQA Dataset)",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 3, including the use of Google's machine translation API for translating existing English QA pairs into seven other languages, human verification and adjustment, and examples of the data. This documentation is sufficient to understand how the multi-lingual dataset was constructed from the original ClothoAQA dataset."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7687,
      "completion_tokens": 142,
      "total_tokens": 7829
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Sections 1.1, 2, 5, and 6",
          "Reasoning": "The PERCEPT-R corpus introduced by the authors consists of audio recordings of child speakers of American English, specifically capturing speech containing rhotics (/\u0279/) and clinical speech sound disorders. The audio data were collected directly by research staff using participant-worn headsets (e.g., Sennheiser MKE 2) in lab-based studies. This confirms the modality as 'audio' and the origin as human-generated recordings, not model generated or of unknown provenance."
        }
      ]
    }
  },
  {
    "id": "benway22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8487,
      "completion_tokens": 336,
      "total_tokens": 8823
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 1.1 Introduction; Section 2.3 Perceptual Rating",
            "reasoning": "The paper states the PERCEPT-R corpus was labeled with perceptual judgments of rhotic/derhotic /\u0279/ sounds, and mentions multiple listeners (raters) involved in rating tokens as 'rhotic' or 'derhotic'. There is mention of 'most raters', 'unanimous ratings', and their agreement levels, indicating multiple human expert raters, likely speech language pathologists or trained experts, performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Perceptual Rating",
            "reasoning": "The paper describes the perceptual rating procedure with consensus and mentions ratings fully by raters, implying detailed annotation instructions were provided so that raters could assign consistent perceptual judgments of rhoticity, although explicit full instructions text is not included."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.3 Perceptual Rating",
            "reasoning": "The rating involved categorical judgments ('rhotic' vs. 'derhotic') and use of thresholds such as unanimous ratings and averages per token with values <0.7 considered derhotic, indicating presence of rubrics or scoring criteria guiding how ratings translate to classification."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The text does not mention provision of annotation examples or sample tokens provided to raters for training or guidance, thus no evidence of examples being included in annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "benway22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9677,
      "completion_tokens": 210,
      "total_tokens": 9887
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 1.1 Motivation and Contribution; Section 2 Data Collection",
          "reasoning": "The speech data and perceptual judgments in the PERCEPT-R corpus were collected and labeled by research assistants and speech language pathologists, who are subject matter experts in speech pathology. The paper states that data were collected directly by members of the originating study teams, most often a research assistant and a speech language pathologist, indicating expert involvement in annotation and quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9287,
      "completion_tokens": 418,
      "total_tokens": 9705
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 1.1 Motivation and Contribution, 5 Discussion",
          "reasoning": "The PERCEPT-R corpus consists of audio recordings of child speakers with typical speech and residual speech sound disorders, collected directly by the authors' research teams including research assistants and speech language pathologists. The data was generated from original recordings made specifically for this study and previous originating studies, not translated, adapted, or derived from any pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any part of the dataset was generated by AI or machine learning models; rather, the dataset consists of human-recorded child speech."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved translation from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation involved in data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the corpus compiles data from several originating studies, these consist of newly collected recordings. The data is not merely aggregated from existing datasets without modification, but instead compiled from original human-collected recordings."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not present the corpus as a transformation or adaptation of existing datasets; rather it is presented as original collected data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of dataset generation is documented in the paper."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9805,
      "completion_tokens": 460,
      "total_tokens": 10265
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4",
          "reasoning": "The PERCEPT-R corpus is used to train support vector classifiers to predict perceptual judgments of rhotic and derhotic /\u0279/. The paper demonstrates training classifiers with leave-one-participant-out cross-validation, which corresponds to supervised learning fine-tuning of models on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using RL-based post-training methods such as reinforcement learning with the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4",
          "reasoning": "The dataset is explicitly used for evaluating classifier performance with metrics such as f-metric reported for classifying /\u0279/. Leave-one-participant-out cross-validation is used as an evaluation approach."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Discussion)",
          "reasoning": "The paper discusses analyzing trends and characteristics such as distribution of participant-specific f-metrics and formant features, indicating the dataset is used for analysis beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the PERCEPT-R corpus is used as a knowledge base to augment models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the PERCEPT-R dataset for training classifiers, evaluating model performance, and analyzing speech characteristics."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10528,
      "completion_tokens": 524,
      "total_tokens": 11052
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The PERCEPT-R corpus contains child speech data in American English only, with no mention of additional human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly describes the dataset as American English child speech only, with no indication of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 (Introduction); Section 5 (Discussion)",
          "reasoning": "The corpus is described as child speakers of American English, focusing solely on American English speech, with no other languages included. This is explicitly stated throughout the paper, confirming the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not of a non-English language; it is specifically American English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio recordings and linguistic labels related to speech; there is no indication of code or programming language content within the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses features and classifiers, the dataset itself contains speech audio and labels, not mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is focused on child human speech in English, with no mention of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as American English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language entries (American English child speech), so it does contain language."
        }
      }
    }
  },
  {
    "id": "benway22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7746,
      "completion_tokens": 170,
      "total_tokens": 7916
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link",
          "reasoning": "The paper does not mention or provide any link, repository, or appendix containing the code related to data collection, preprocessing, or dataset construction for PERCEPT-R. There is no evidence of publicly available code for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 1 and 5 (Introduction and Discussion)",
          "reasoning": "The paper extensively describes the data collection process, participant demographics, recording methodology (e.g., participant-worn headset), and data composition. It details the number of participants, tokens, word shapes, and audio recording details. The documentation also discusses data labeling, integration with analysis software (Phon), and demonstrated utility for classification tasks, providing sufficient transparency and completeness of dataset creation."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7408,
      "completion_tokens": 82,
      "total_tokens": 7490
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The paper introduces a new dataset comprising 1012 recordings of isolated American English nouns spoken by 6 English native speakers. These recordings are audio data captured from human speakers, as explicitly stated in Section 3.1."
        }
      ]
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8208,
      "completion_tokens": 221,
      "total_tokens": 8429
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that the syllable boundaries were annotated manually on recordings collected by Schwab and colleagues. Manual annotation implies that human annotators performed the data annotation, and given that the dataset is described in an academic research context, it is likely that experts carried out the annotation rather than crowd-sourced non-experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no explicit mention in the paper of detailed annotation instructions being provided for the manual syllable boundary annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No details or descriptions of scoring rubrics or similar structured guidelines for annotation are stated in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide or mention any annotation examples or annotated data samples as part of the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9398,
      "completion_tokens": 412,
      "total_tokens": 9810
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset comprises 1012 recordings with syllable boundaries annotated manually. There is no direct statement of the number or expertise of annotators; however, the manual annotation implies human involvement. Given the use of a relatively small dataset and manual annotation of syllable boundaries, it is plausible that annotation was performed by experts or individuals knowledgeable in phonetics, as syllable boundary annotation is a specialized task."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention multiple annotators or multiple experts performing quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single non-expert performed the quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that multiple non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using an AI model as a judge for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though automated tools are used in the pipeline for phonetic transcription and segmentation during system operation, the dataset annotations (especially syllable boundaries) were manually annotated, so automated verification of annotations is not described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents manual annotation of syllable boundaries for the dataset, which constitutes a form of quality assurance."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9008,
      "completion_tokens": 359,
      "total_tokens": 9367
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset consists of 1012 recordings of 92 different isolated American English nouns spoken by 6 English native speakers. This data was collected by Schwab and colleagues as original speech recordings, implying that it is original content created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that data was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the data consists of translated content by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data in the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not described as collected or aggregated from existing sources without significant modification; it was newly recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset was based on existing sources with modifications or adaptations; it was newly recorded audio data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear documentation about the origin of the data."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9526,
      "completion_tokens": 381,
      "total_tokens": 9907
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 (Training a Classifier) and Section 4 (Results and Discussion)",
          "reasoning": "The paper describes the use of the newly collected data of 1012 recordings of isolated English words spoken by natives to train machine learning classifiers from scratch. Multiple classifiers including Support Vector Machine and Neural Network were trained on extracted acoustic features from the dataset to detect lexical stress."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset to fine-tune pre-trained models; it discusses training classifiers from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training techniques are mentioned in relation to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Results and Discussion)",
          "reasoning": "The dataset is used in 10-fold cross-validation for evaluating the performance of the classifiers and the final Voting Classifier, measuring F1 scores and accuracy."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results and Discussion)",
          "reasoning": "The paper analyzes the importance of different acoustic feature groups (duration, pitch, loudness, spectral) using the dataset, studying their relative impact on classification performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented models or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively and clearly used for training classifiers, evaluating them, and analyzing feature importance, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10249,
      "completion_tokens": 511,
      "total_tokens": 10760
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The new dataset introduced consists solely of English words spoken by native American English speakers, with no indication of other languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains recordings only in English; there is no indication of exactly two human languages present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Section 3.1 describes that the training data consists of 1012 recordings of isolated American English nouns spoken by native English speakers. The entire dataset is composed exclusively of English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed exclusively of English words; no non-English languages are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech recordings and their manual annotations, not code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are included in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset focuses on human speech recordings of English words, with no biological sequences or non-human communicative signals involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No fictional or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is explicitly specified as American English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (English) recordings."
        }
      }
    }
  },
  {
    "id": "bernhard22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7467,
      "completion_tokens": 157,
      "total_tokens": 7624
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.4 Training a Classifier",
          "reasoning": "The paper explicitly states that the code and the final model can be found on GitHub at 'https://github.com/vera-bernhard/stress-detector', indicating availability of code related to the dataset processing and classification pipeline."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper documents the dataset creation process in Section 3.1, describing the collection of 1012 recordings of 92 English words spoken by 6 native speakers, manual annotation of syllable boundaries, and assumptions about stress labels based on dictionaries. This provides transparency and completeness about the dataset construction."
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6499,
      "completion_tokens": 87,
      "total_tokens": 6586
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Experimental Corpus",
          "Reasoning": "The paper states that for the experiments, English speech spoken by Bengali speakers was recorded using a recording app, producing new speech data specifically collected for this study. This data consists of audio recordings of human speakers (L1 Bengali) speaking English."
        }
      ]
    }
  },
  {
    "id": "bharati23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7299,
      "completion_tokens": 198,
      "total_tokens": 7497
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that some portions of each speaker's speech were manually annotated by a linguistics expert using Praat software to create the gold standard dataset for the newly created Bengali English speech data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to the linguistics expert for the manual annotation of the Bengali English speech data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No mention of scoring rubrics or criteria used to guide or standardize the manual annotations is found in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide any annotation examples or reference materials used during manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "bharati23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8489,
      "completion_tokens": 223,
      "total_tokens": 8712
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The gold standard dataset portion of each speaker's speech was annotated manually by a linguistics expert using Praat software, indicating quality assurance was conducted by a single human expert for the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus and Section 3.2 Data pre-processing",
          "reasoning": "The majority of the speech data were annotated automatically using the Montreal Forced Aligner, which is an automatic forced alignment tool, indicating that an automatic process was also used for quality assurance of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8099,
      "completion_tokens": 487,
      "total_tokens": 8586
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The paper explicitly states that an English speech corpus spoken by Bengali speakers was recorded as there was no existing dataset for Bengali speakers' English speech. Twenty-four participants were recorded reading designed stimuli (words, sentences, passages) to capture occurrences of similar and new phonemes. The data collection was performed using a recording app with human participants, indicating the data was created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated solely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translating data from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention employing machine translation to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The authors used pre-existing datasets like the TIMIT corpus and a modified subset of the VCTK corpus for L1 English speech data to bootstrap training. These datasets were collected from existing sources without significant modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Data pre-processing and Section 3.1 Experimental Corpus",
          "reasoning": "The paper describes processes such as forced alignment with Montreal Forced Aligner, phoneme sequence generation from word sequences using g2pE, and manual annotation of portions of speech data. The modified VCTK corpus was converted from word-level to phone-level sequences, and annotations were adapted with human expert involvement, indicating derived data from existing sources with some modifications and transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the data origin and processing methods explicitly."
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8617,
      "completion_tokens": 448,
      "total_tokens": 9065
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new Bengali-English speech dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.3",
          "reasoning": "The newly created L2 English speaker (L1 Bengali) speech dataset is used in training the hybrid CNN-LSTM phoneme recognizer model from scratch as described in Section 3.1 (Experimental Corpus) and Section 3.3 (Model Architecture)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate using the new dataset to fine-tune pre-trained models with supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based methods using the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.5",
          "reasoning": "The new Bengali-English speech dataset is used for testing and evaluating the pronunciation error detection system performance as described in Section 3.5."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the new dataset for analyzing trends or characteristics beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the new Bengali-English speech dataset in training and evaluation."
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9340,
      "completion_tokens": 368,
      "total_tokens": 9708
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental Corpus",
          "reasoning": "The newly created dataset consists of English speech spoken by Bengali speakers. Thus, the dataset entries involve two human languages: English (the target L2 language) and Bengali (native L1 language of speakers). The dataset is specifically designed for L2 English speech of L1 Bengali speakers, as described in Section 3.1."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "bharati23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6558,
      "completion_tokens": 173,
      "total_tokens": 6731
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention of code availability in the paper.",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code related to data collection, preprocessing, or dataset generation. The description focuses on the dataset creation and experimental setup but does not indicate code sharing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Experimental Corpus and Section 3.2 Data pre-processing.",
          "reasoning": "The paper describes the creation of the new Bengali English speech dataset, including participant details, recording procedure, annotation methods (manual and automatic), and dataset splitting for training, validation, and testing. Additionally, data preprocessing steps using forced aligner and phoneme conversion are detailed. This provides a transparent and reasonably complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9807,
      "completion_tokens": 173,
      "total_tokens": 9980
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Section 3.3, Section 4",
          "Reasoning": "The INDICYT benchmark consists of audio files collected from YouTube videos, sourced by human language experts using YouTube Search API and filtered for CC-BY-4.0 license. These are human-captured audio segments since they are original audio recordings from videos."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4",
          "Reasoning": "The INDICYT benchmark includes manually transcribed text transcripts for a 2-hour subset of the YouTube audio data. These transcriptions were created by human annotators using the Shoonya platform ensuring high quality transcripts."
        }
      ]
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10607,
      "completion_tokens": 196,
      "total_tokens": 10803
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4",
            "reasoning": "Section 4 states that language experts proficient in Hindi were recruited to collect and transcribe the audio data for the INDICYT benchmark, indicating that multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4",
            "reasoning": "Section 4 mentions that transcription was performed using the same guideline as discussed in [19], indicating that detailed instructions for annotation were provided to the annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not mention any scoring rubrics or structured scoring criteria for annotation; it only references instructions from prior work."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "There is no mention of examples being provided in the annotation guidelines or elsewhere for the INDICYT dataset."
          }
        }
      ]
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11797,
      "completion_tokens": 508,
      "total_tokens": 12305
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4: INDICYT Benchmark - Transcription",
          "reasoning": "The INDICYT benchmark transcription involved human annotators described as language experts proficient in Hindi, implying subject matter expertise. Multiple such experts were recruited to transcribe data, ensuring high quality. This indicates quality assurance was conducted by multiple human experts, but as multiple experts are reported, single human expert QA does not apply exclusively."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 4: INDICYT Benchmark - Transcription and Collection",
          "reasoning": "The paper states that multiple language experts proficient in Hindi participated in curating and transcribing the INDICYT benchmark data, ensuring high quality annotations. This demonstrates that quality assurance was performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that quality assurance was conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly mentions language experts for annotation, and no mention is made of non-experts participating in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models (pseudo-transcribers) and evaluators (confidence scoring and multimodal similarity) are used for pseudo-labeling and filtering unlabeled data, this process relates to pseudo-label generation rather than quality assurance of the annotated INDICYT dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes automated filtering during pseudo-label generation but does not describe automated verification processes applied to human-annotated benchmark data, thus automated QA for dataset annotations is not documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents a quality assurance process involving multiple human expert annotators for the INDICYT benchmark and description of pseudo-label filtering methods; therefore, it is not the case that no QA is performed or documented."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11407,
      "completion_tokens": 506,
      "total_tokens": 11913
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4 INDICYT Benchmark",
          "reasoning": "The INDICYT benchmark was created by recruiting language experts who curated and transcribed a diverse set of audio data from YouTube. The transcription was done by human annotators following established guidelines, ensuring that the transcriptions are original content created from scratch by human contributors rather than translations or modifications of existing content."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.3 PN-pseudolab",
          "reasoning": "PN-pseudolab dataset was generated by applying the Pratinidhi pseudo-labeling framework which involves multiple base ASR models (pseudo-transcribers) transcribing large amounts of unlabeled YouTube audio. The resulting transcripts are original model-generated outputs not directly derived or translated from existing labeled data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data created by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that machine translation was used to produce any dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 PN-lab and PN-unlab datasets",
          "reasoning": "PN-lab collates publicly available labeled datasets for Hindi from multiple sources, aggregating existing datasets without significant modification. Similarly, PN-unlab includes audio collected from YouTube by automated search and filtering. Thus, these datasets are collected or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 PN-pseudolab",
          "reasoning": "PN-pseudolab is derived by applying pseudo-labeling methods on the PN-unlab unlabeled audio data using base models trained on PN-lab. The pseudo labels are thus transformed versions of unlabeled audio enhanced by model outputs and filtering mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation processes for all datasets introduced are explicitly documented."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11925,
      "completion_tokens": 468,
      "total_tokens": 12393
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced datasets are not described as being used for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 5",
          "reasoning": "The labeled dataset PN-lab, a combination of various labeled Hindi datasets, is used to train ASR models from scratch which serve as pseudo-transcribers (Section 3.2). The combined dataset (PN-lab plus the pseudo-labeled data PN-pseudolab) is used to retrain ASR models with standard recipes (Section 5)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention fine-tuning pre-trained models in a supervised fashion on the introduced datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of reinforcement learning-based post-training techniques involving the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The INDICYT benchmark is introduced as a new evaluation benchmark with human-transcribed high-quality test data to robustly evaluate Hindi ASR systems across diverse domains."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some analysis of pseudo-labeled samples is done qualitatively, the datasets themselves are not introduced primarily for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is clear and documented usage of the introduced datasets in training and evaluation."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12648,
      "completion_tokens": 560,
      "total_tokens": 13208
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced (INDICYT) and pseudo-labeling datasets are focused exclusively on Hindi language content, there is no indication of presence of more than two languages in the entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the newly introduced benchmark INDICYT and the pseudo-labeling datasets as in Hindi only. There is no mention of two languages for the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are for Hindi language ASR and contain transcriptions and audio segments exclusively in Hindi, not in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4",
          "reasoning": "The newly created INDICYT benchmark and the PN-pseudolab datasets contain entries exclusively in Hindi. The paper explicitly states that the focus is on the Hindi language, a low-resource Indian language. The audio-transcript pairs collected and annotated are all in Hindi, thus the datasets are monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described do not contain programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic notations are mentioned as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of human speech audio and transcriptions only; no biological or non-human communication data is involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are part of the dataset entries; only Hindi is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is explicitly specified as Hindi."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain spoken language audio and transcripts, so they include language content."
        }
      }
    }
  },
  {
    "id": "bhogale24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9866,
      "completion_tokens": 184,
      "total_tokens": 10050
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 4",
          "reasoning": "While the abstract mentions that the benchmark, code, and models will be made publicly available, the paper does not provide any explicit links or repositories to the code used for constructing the INDICYT dataset or for its data collection and preprocessing processes. Therefore, based strictly on the paper content, there is no evidence of publicly available code related to dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4",
          "reasoning": "Section 4, titled 'INDICYT Benchmark,' provides detailed documentation of the dataset creation process, including the identification of topics for collection, the collection procedure using YouTube CC-BY-4.0 license content, the transcription process with human annotators following specified guidelines, and statistics of the dataset. This constitutes clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10715,
      "completion_tokens": 100,
      "total_tokens": 10815
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Data",
          "Reasoning": "The paper introduces a new dataset comprising infant vocalizations extracted from long-form audio recordings made by the parents using a portable microphone near the child at home over the first 12 months of life. These are recorded human infant babbling sounds captured via human-operated audio recording equipment, indicating human-generated origin of audio modality data."
        }
      ]
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11515,
      "completion_tokens": 246,
      "total_tokens": 11761
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 and 3",
            "reasoning": "The paper describes that vocalizations were automatically extracted from long-form audio recordings using a method outlined in reference [11]. There is no mention of human annotators labeling or categorizing the vocalizations. The clustering is performed using a Dirichlet process mixture model, a computational algorithm, based on topologically augmented signal representations. Therefore, the annotation (clustering) was performed by an automatic process and not by human experts or non-experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No sections",
            "reasoning": "No explicit instructions for annotators or human annotators are described since the annotation is done automatically by an algorithm. Hence, there are no annotation instructions provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No sections",
            "reasoning": "No scoring rubrics or evaluation criteria are mentioned related to annotation guidelines, as annotation was performed via an automatic clustering model."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No sections",
            "reasoning": "The paper does not provide examples of annotation labeling or guidelines, only discusses automatic extraction and clustering methods."
          }
        }
      ]
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12705,
      "completion_tokens": 351,
      "total_tokens": 13056
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any human expert manually performing quality assurance on the dataset annotations or vocalization labels."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted any quality assurance or annotation validation of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not detail any quality assurance involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though an AI model using a Dirichlet process mixture model is used for clustering, the paper does not describe any application of AI models specifically for quality assurance or validation of the dataset annotations or segmentations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The vocalizations were automatically extracted from long-form audio recordings using an automatic detection method described in reference [11]. This automatic process serves to identify and segment baby vocalizations from continuous audio, effectively acting as an automated quality assurance step by filtering baby vocalizations from other sounds. However, no manual validation or human verification is described for these extracted segments, so the QA consists primarily of automatic processing."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described insofar as the automatic extraction of vocalizations is done through an automatic detection method, thus an automatic QA process is applied."
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12315,
      "completion_tokens": 416,
      "total_tokens": 12731
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2. Data",
          "reasoning": "The dataset consists of vocalizations of a single child from birth to one year old, recorded at home by the parents in longitudinal naturalistic settings. These vocalizations were extracted from original long-form audio recordings made by human contributors and are not derived from pre-existing datasets or adapted from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data represent real infant vocalizations collected via audio recordings. Although topological data analysis and modeling are applied to these data, the original data are not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was produced by translating data from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was produced by automatic translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources; it is newly recorded data from a longitudinal study of a single child."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors compute derived features such as Mel Frequency Cepstral Coefficients and apply topological transformations, these are feature representations derived from the original recorded vocalizations for analysis; the original dataset itself is not derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and source of the data are clearly specified."
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12833,
      "completion_tokens": 214,
      "total_tokens": 13047
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The vocalization dataset is used primarily to analyze trends, patterns, and characteristics of infant vocal productions over the first year of life. The authors cluster the vocalizations and compare their temporal distributions and acoustic profiles, providing insights into developmental changes and vocalization categorization rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13556,
      "completion_tokens": 534,
      "total_tokens": 14090
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of vocalizations from a single child acquiring French language (see Section 2 Data), with no indication of multiple languages being present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains vocalizations in only one language environment, French, as described in Section 2. There is no mention of a second language in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is from a French-speaking child; no English language data is present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The dataset contains recordings of vocalizations from a female French child recorded at home over the first year of life, thus it is monolingual in French, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of audio recordings of infant vocalizations, with no programming code or structured code content included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical models and notation to analyze the data, the dataset entries themselves do not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human infant vocalizations; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of constructed, fictional, or artificial languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the infant vocalizations is specified as French; thus the language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken infant vocalizations, hence includes language in the form of human speech sounds."
        }
      }
    }
  },
  {
    "id": "bonafos24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10774,
      "completion_tokens": 177,
      "total_tokens": 10951
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 (Data) and throughout the paper",
          "reasoning": "The paper does not provide any URLs, links, or references to publicly available code for data collection, preprocessing, or dataset generation. There is no mention of a public repository or supplementary materials containing code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data)",
          "reasoning": "The paper contains a detailed description of the dataset creation process including the source of the data (recordings by parents of a single child over 12 months), recording conditions, data preprocessing steps such as stereo to mono conversion, rescaling, exclusion criteria for vocalizations longer than 10 seconds, and specific counts and distributions of vocalizations per month. Ethical approvals are also mentioned. This constitutes sufficient documentation for understanding the dataset creation process."
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8792,
      "completion_tokens": 175,
      "total_tokens": 8967
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 'Data setup' and 'Corpus' and 'Dynamic language mixing procedure'",
          "Reasoning": "The new dataset is a simulated multilingual cocktail party database created using audio data from the GlobalPhone 2000 Speaker Package, which consists of human recorded speech audio from 22 languages. The authors apply an adapted dynamic language mixing procedure to generate multilingual mixtures by mixing the intra-language mixtures and creating overall cocktail party scenes on-the-fly during training, using sample SNRs and utterances from speakers. Thus, the base audio data is human generated (human recorded speech), and the mixtures are model generated via procedural mixing simulation within the adapted dynamic language mixing procedure as described, forming the novel dataset used for training and evaluation."
        }
      ]
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9592,
      "completion_tokens": 246,
      "total_tokens": 9838
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 (Data setup) and Section 3.3 (Training and evaluation)",
            "reasoning": "The mixtures and annotations (intra-language speaker groups in mixtures) are created using an adapted dynamic language mixing procedure which is a deterministic, simulated process; annotations correspond to the ground truth intra-language mixtures generated automatically from the original GlobalPhone 2000 Speaker Package data based on scripted mixing."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe any annotation task performed by humans or instructions provided for annotators; the data labels are produced automatically by mixing utterances per language and speaker."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "Since the annotation is automated (data mixing and label creation), no scoring rubric or quality assessment guidelines are discussed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no indication that human annotation examples or illustrative annotation guidelines were provided, as the dataset creation is based on automatic mixing."
          }
        }
      ]
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10782,
      "completion_tokens": 320,
      "total_tokens": 11102
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any use of a single human expert for quality assurance of the newly introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by multiple non-expert human annotators on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that an AI model was used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe any automated verification or algorithmic rule-based quality assurance steps for dataset annotation or validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new dataset generated by an adapted dynamic language mixing procedure based on the existing GlobalPhone 2000 Speaker Package corpus. The creation process is algorithmic and scripted, but no explicit quality assurance process such as manual annotation verification, expert review, or automated data validation beyond data preparation and normalization steps is reported. Therefore, no quality assurance process is documented or applied for the dataset beyond the described simulation and sampling procedures."
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10392,
      "completion_tokens": 405,
      "total_tokens": 10797
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 (Corpus), Section 3.1 (Dynamic language mixing procedure)",
          "reasoning": "The paper uses the GlobalPhone 2000 Speaker Package, which is a multilingual corpus of speech data recorded from 2,000 native speakers. This data was originally recorded by human speakers, representing original human-produced speech, not derived from existing synthetic or translated data. The authors created new multilingual cocktail party mixtures by mixing these original human speech recordings to simulate overlapping multilingual speech scenarios, indicating that the base data are human-created."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not simply aggregated from existing datasets; the authors generate new mixtures from the existing speech data, thus not purely collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Dynamic language mixing procedure)",
          "reasoning": "The authors create new multilingual cocktail party mixtures by combining, mixing, and adapting the clean speech recordings from the GlobalPhone corpus via an adapted dynamic language mixing procedure. This involves modifications and transformations (mixing speakers and languages at different SNRs) to generate the input and ground truth training data for their blind language separation task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10910,
      "completion_tokens": 506,
      "total_tokens": 11416
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly created dataset for unsupervised or self-supervised pre-training of large models."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Sections 3.3 and 4",
          "reasoning": "The new dataset created via the adapted dynamic language mixing procedure from GlobalPhone 2000 Speaker Package is used to train blind language separation models from scratch. Models, such as Conv-TasNet and SepFormer, are trained from randomly initialized parameters on this dataset as described in Sections 3.3 and evaluated in Section 4."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for fine-tuning pre-trained models through supervised learning. All models reported are trained from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that reinforcement learning or RL-based post-training methods such as RLHF are applied using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 (Test data), Section 4",
          "reasoning": "The paper includes a large evaluation set comprising 231 test sets created statically from the same data source to benchmark the performance of trained BLS models, demonstrating use of the new dataset for rigorous evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The authors analyze performance trends across seen, unseen, and mixed language conditions and discuss how training set composition impacts results, indicating dataset use for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for training, evaluation, and analysis in the paper."
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11633,
      "completion_tokens": 591,
      "total_tokens": 12224
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Data setup - Corpus",
          "reasoning": "The newly simulated dataset created by the authors for Blind Language Separation is based on the GlobalPhone 2000 Speaker Package, which includes speech data for 22 different human languages. The dataset's multilingual cocktail party mixtures each consist of two randomly selected languages from these 22 (e.g., Arabic, Bulgarian, Chinese Mandarin, German, Portuguese, etc.), with multiple speakers per language per mixture (Section 3.1). Hence the dataset contains entries featuring more than two human languages across the whole dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though each mixture contains exactly two languages, the dataset overall spans more than two languages (22 languages). The dataset therefore is not restricted to exactly two languages total, but is multilingual across the entire dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content; it contains multiple non-English languages from GlobalPhone."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of mixtures with multiple languages per sample; not any single monolingual language-only content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio data and does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech audio data; no entries with mathematical or logical notation are described."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human languages and speech; it does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned as part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) in the dataset are clearly specified and documented as the 22 languages from GlobalPhone."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of audio speech data with various human languages, so it includes language content."
        }
      }
    }
  },
  {
    "id": "borsdorf22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8851,
      "completion_tokens": 179,
      "total_tokens": 9030
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Conclusion sections",
          "reasoning": "The paper states in the abstract and conclusion that 'We will make all information and scripts publicly available,' indicating an intention to release code and scripts, but no explicit link or repository is provided within the paper itself. Therefore, as of the paper content, the code is not made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data setup",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 3.1 (Data setup), describing the data source (GlobalPhone 2000 Speaker Package), the language mixing strategy (adapted dynamic language mixing procedure), the training and test data creation methods, including the random sampling steps and normalization procedures. Hence, the dataset creation process is well documented within the paper."
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6846,
      "completion_tokens": 144,
      "total_tokens": 6990
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Abstract",
          "Reasoning": "UserLibri dataset contains paired audio recordings from LibriSpeech, which are recordings of human speakers reading chapters from Project Gutenberg books, indicating human-generated audio data collected from real speakers."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Abstract",
          "Reasoning": "UserLibri includes additional text-only data sourced from Project Gutenberg (raw book text) corresponding to the same books read in audio. This text data is human-written literature, not generated by models."
        }
      ]
    }
  },
  {
    "id": "breiner22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7646,
      "completion_tokens": 240,
      "total_tokens": 7886
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4",
            "reasoning": "The UserLibri dataset is created by automatically processing and reorganizing existing LibriSpeech data and Project Gutenberg texts by combining audio and text data associated with each 'user'. This dataset creation involves automated processing steps such as text boilerplate removal, sentence splitting, and data filtering, rather than manual annotation by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper describes the dataset creation process but does not provide any detailed annotation instructions or guidelines for human annotators, as the dataset was created via automated processing."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "There is no mention in the paper of any scoring rubrics or evaluation guidelines for annotations related to the UserLibri dataset, since manual annotation was not performed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not provide annotation examples for the creation or labeling of the UserLibri dataset, as it was created using an automatic process without human annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "breiner22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8836,
      "completion_tokens": 409,
      "total_tokens": 9245
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process performed by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about quality assurance performed by multiple human experts on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any quality assurance by multiple human non-experts is found in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of an AI model to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4. User Specific LibriSpeech Dataset",
          "reasoning": "The paper describes an automatic processing pipeline for the raw Project Gutenberg book texts: standardizing encodings, removing boilerplate, splitting text into sentences, removing punctuation and whitespace, uppercasing ASCII characters, and discarding sentences with overlaps in the test set (Section 4). This indicates an algorithmic, rule-based quality assurance and filtering process applied to prepare the text data associated with each user."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some form of automated text processing is documented; thus, it is not the case that no QA is applied or documented."
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8446,
      "completion_tokens": 431,
      "total_tokens": 8877
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset UserLibri is based on existing LibriSpeech audio and Project Gutenberg text data; no indication of entirely new human-created content is present."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the UserLibri dataset being generated by AI or machine learning models; the data is from existing human sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any human translation of data from other languages; the data is English text and audio."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was involved in producing the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (User Specific LibriSpeech Dataset)",
          "reasoning": "The UserLibri dataset is constructed by collecting and aggregating existing paired audio-transcript data from LibriSpeech and additional text-only data from Project Gutenberg books corresponding to the same users, without significant modification to the original data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (User Specific LibriSpeech Dataset)",
          "reasoning": "The authors processed the raw Gutenberg book text by standardizing encodings, removing boilerplate, segmenting into sentences, filtering overlapping text to avoid data leakage, and grouping audio examples by user, thus applying modifications and transformations to create the final UserLibri dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and processing of the UserLibri data are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8964,
      "completion_tokens": 323,
      "total_tokens": 9287
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5, especially 5.1 and 5.2",
          "reasoning": "The UserLibri dataset, which contains paired audio-transcripts and additional text-only data per user, is used to fine-tune personalized language models (LMs) that are then leveraged via shallow fusion to improve ASR performance per user. The personalized LMs are fine-tuned on user-specific text examples extracted from the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 5 and 6",
          "reasoning": "The UserLibri dataset is used to evaluate the effectiveness of the personalized LMs and the ASR models with shallow fusion by measuring word error rates (WER) per user on the test sets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 5.5 and 6",
          "reasoning": "The dataset is used to analyze the impact of personalization on word error rates across users, model sizes, and data amounts, including specific cases of improvements and failures, highlighting trends and characteristics of personalized ASR performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9687,
      "completion_tokens": 530,
      "total_tokens": 10217
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The UserLibri dataset introduced is based on LibriSpeech and Project Gutenberg texts, focusing solely on English content. There is no indication of multiple languages or multiple human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; it is solely English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 (User Specific LibriSpeech Dataset), Abstract",
          "reasoning": "The UserLibri dataset is created from the LibriSpeech dataset which consists of recordings of speakers reading English books from Project Gutenberg. The text-only data and paired audio-transcript data are all English. The paper explicitly states focusing on the English LibriSpeech dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is exclusively English, with no non-English single language data introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical notations; it consists of spoken English and textual English data only."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech and text data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of constructed or fictional languages in the UserLibri dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language\u2014the English text and transcripts\u2014so this label does not apply."
        }
      }
    }
  },
  {
    "id": "breiner22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6905,
      "completion_tokens": 198,
      "total_tokens": 7103
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided in the paper",
          "reasoning": "The paper does not provide any explicit link or reference to publicly available code repositories or tools used to generate or process the UserLibri dataset. Despite mentioning dataset creation details in Section 4, there is no indication that the code for dataset processing is shared or made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (User Specific LibriSpeech Dataset)",
          "reasoning": "The paper provides a fairly detailed description of how the UserLibri dataset was created\u2014explaining that the dataset is a reformatting of LibriSpeech, grouping audio data by user (speaker and book), processing Project Gutenberg texts to extract personalized text-only data corresponding to each user, and describing preprocessing steps such as removing duplicate sentences and formatting text. Section 4 thoroughly documents the dataset construction process, enough for readers to understand the pipeline."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 2811,
      "completion_tokens": 236,
      "total_tokens": 3047
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The new dataset consists of about 600 English video recordings (400 with masks, 200 without) collected from 14 actors portraying 7 emotions. These are real recordings, with human actors captured on video, as stated explicitly in Section 3.2."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The dataset includes audio recorded simultaneously with the video of actors performing emotions, as per Section 3.2 describing the 'recordings' collected. This is human-generated data captured by recording devices."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The text modality consists of ASR (automatic speech recognition) outputs generated by models from the audio recordings. There is no mention of human transcription, the text is produced by ASR models as input to the MER system."
        }
      ]
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 3611,
      "completion_tokens": 164,
      "total_tokens": 3775
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The paper states that the German and Korean datasets were annotated by 3 judges, implying multiple human experts were involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided to the annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no explicit mention of scoring rubrics being provided in the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not indicate that annotation examples were provided in the guidelines or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 4801,
      "completion_tokens": 410,
      "total_tokens": 5211
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that quality assurance was conducted by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "For the German and Korean datasets, the paper states that an internal collection of multimodal data from open license movies was annotated by 3 judges. Although the paper does not explicitly certify these judges as subject matter experts, the use of multiple human judges for annotation indicates multiple human quality assurance. Since it mentions three judges without specifying their expertise, we consider them as multiple human experts based on typical annotation standards in the field."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple non-expert annotators conducting the quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance by an AI model as a judge is not described in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention automatic or algorithmic verification for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes human annotation processes for German and Korean datasets, so it is not the case that no quality assurance was documented."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 4411,
      "completion_tokens": 423,
      "total_tokens": 4834
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.2",
          "reasoning": "The authors introduced new datasets collected by themselves: for German and Korean, an internal collection of multimodal data from open license movies was curated and annotated by 3 judges, totaling approximately 3k videos for German and 1.2k for Korean. Additionally, for the mask-wearing experiment, the authors collected about 400 English recordings with masks and 200 without from 14 actors, which are real recordings, not artificially generated or synthesized."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by translating content through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any machine-translated data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "For English, the dataset was assembled by combining two pre-existing datasets: CMU-MOSEI and IEMOCAP. The authors collated these datasets, selecting videos with consistent emotion annotations, effectively aggregating existing data without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets were adaptations or transformations of existing sources beyond collation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are explicitly described."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 4929,
      "completion_tokens": 445,
      "total_tokens": 5374
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced in the paper are not used for pre-training large models; the paper discusses training with supervised methods."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The newly introduced internal datasets for German and Korean languages are used to train separate models from scratch on multimodal emotion recognition tasks as described in Section 3.1."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new datasets for supervised fine-tuning of pre-trained models; instead, models are trained directly on these datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of reinforcement learning or RL-based post-training using these datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The newly collected English masked-face dataset is used for evaluation purposes, measuring the effect of mask wearing on multimodal emotion recognition systems."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The datasets are used to analyze the impact of different modalities and the influence of mask wearing on emotion recognition model performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base or for retrieval-augmented generation according to the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the datasets' usage in training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 5652,
      "completion_tokens": 463,
      "total_tokens": 6115
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The new datasets introduced by the authors include three human languages: English, German, and Korean. Specifically, for English they combined CMU-MOSEI and IEMOCAP; for German and Korean, they used internal collections. The presence of more than two languages in the new datasets makes the dataset multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of video, audio, and text in human languages. There is no indication that programming or structured code content is part of the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the datasets contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain human speech and visual data, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the dataset entries are explicitly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language data in English, German, and Korean."
        }
      }
    }
  },
  {
    "id": "bujnowski23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 2870,
      "completion_tokens": 220,
      "total_tokens": 3090
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories related to dataset construction, collection, or preprocessing of the newly introduced datasets (specifically the internal German and Korean datasets and mask recordings). There is no explicit mention or pointer to code sharing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper documents the dataset creation process for the new datasets introduced by the authors. Specifically, Section 3.1 describes the internal collection for German and Korean datasets: data is collected from open license movies, cut into short videos, and annotated by 3 judges, with statistics on video counts, duration and actors. Section 3.2 details the mask dataset: about 400 masked and 200 unmasked recordings from 14 actors for 7 emotions were collected using actors wearing masks. These descriptions provide transparency on the dataset creation process, although no further protocol or detailed methodology is given."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9477,
      "completion_tokens": 88,
      "total_tokens": 9565
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Table 2",
          "Reasoning": "The paper introduces YueMotion as a newly constructed speech emotion recognition dataset in Cantonese with utterances recorded by 11 speakers using personal recording devices. This indicates the data modality is audio and the data is human-generated through manual recording sessions."
        }
      ]
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10277,
      "completion_tokens": 193,
      "total_tokens": 10470
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.2",
            "reasoning": "The YueMotion dataset was recorded by 11 speakers guided during recording sessions with pre-defined sentences and emotions, indicating human involvement likely by experts to collect and label the emotion data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not mention any detailed annotation instructions given to annotators or speakers beyond the description of pre-defined sentences and emotions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no description of scoring rubrics or formal guidelines used for labeling emotions in the YueMotion dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No explicit examples or sample annotations are provided or discussed in the paper regarding the YueMotion dataset annotation."
          }
        }
      ]
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11467,
      "completion_tokens": 367,
      "total_tokens": 11834
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for the newly introduced YueMotion dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that multiple human experts performed quality assurance for the YueMotion dataset or any other newly introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that a single non-expert conducted quality assurance on the newly introduced dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple non-expert annotators performed quality assurance on the newly constructed YueMotion dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any AI model being used as a judge to perform quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic or algorithmic process used to verify or assure the annotation quality of the newly introduced dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the YueMotion dataset but provides no description or mention of any quality assurance procedures employed to validate annotations or content of this dataset."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11077,
      "completion_tokens": 391,
      "total_tokens": 11468
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper states that the YueMotion dataset is newly constructed with utterances recorded by 11 human speakers who spoke 10 predefined sentences with 6 different emotions. This content was created from scratch by human contributors during recording sessions."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets created using machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "While the paper constructs the BiMotion benchmark by collecting six publicly available datasets, BiMotion itself is a benchmark composed of existing datasets and not described as a new dataset. Since the instruction is to assess only new datasets introduced by the authors, BiMotion is not considered a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly constructed YueMotion dataset or any other new dataset is based on existing data transformed or adapted."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and creation process for YueMotion is explicitly documented."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11595,
      "completion_tokens": 605,
      "total_tokens": 12200
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not used exclusively or explicitly for pre-training large models in an unsupervised or self-supervised manner. The paper uses a pre-trained model (XLSR Wav2Vec 2.0) but does not describe the new datasets being used for pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the new datasets. Instead, they fine-tune pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 6 (Methodology - Cross-Group Data Augmentation; Results and Discussion)",
          "reasoning": "The newly constructed YueMotion dataset is used to fine-tune a pre-trained XLSR-53 model using supervised learning methods to evaluate transferability of emotion recognition ability in Cantonese adults. This supervised fine-tuning approach is explicitly described and analyzed in the paper."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the new datasets are used for reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.2 and 6.3 (Benchmark and Dataset - YueMotion Dataset; Results and Discussion - Effect of Language Distance on Transferability)",
          "reasoning": "The YueMotion dataset is used to evaluate and benchmark the transferability of emotion recognition ability, testing how models trained on other languages perform on Cantonese speech emotion recognition."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 6.2 and 6.3 (Results and Discussion - Cross-Group Data Augmentation; Effect of Language Distance on Transferability)",
          "reasoning": "The YueMotion dataset is used for analyzing linguistic distance impact and cross-group data augmentation effects, contributing to understanding speech features and transferability trends across languages."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not used as knowledge bases or for retrieval-augmented generation; their use is focused on fine-tuning, evaluation, and analysis in emotion recognition."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes practical uses of the new dataset YueMotion in supervised fine-tuning, evaluation, and analysis, thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12318,
      "completion_tokens": 655,
      "total_tokens": 12973
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 3.1; Section 3.2; Table 1; Table 2; Section 6.3",
          "reasoning": "The newly constructed dataset 'YueMotion' is introduced in Section 3.2 and contains Cantonese data. Additionally, 'BiMotion' benchmark constructed by the authors includes datasets in exactly two languages: English and Mandarin Chinese from six different public datasets (Table 1 and Section 3.1). YueMotion (Cantonese) is also newly constructed. Together, the datasets cover three human languages\u2014English, Mandarin, and Cantonese\u2014making the combined new datasets multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the BiMotion benchmark covers English and Mandarin, it is a benchmark combining existing datasets, not a newly constructed dataset. Only the YueMotion dataset is newly constructed, which is Cantonese only, hence not bilingual by itself. Therefore, no newly proposed dataset is strictly bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly constructed dataset YueMotion contains Cantonese speech data, i.e., non-English language; hence, it is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2; Table 2",
          "reasoning": "YueMotion is a newly constructed Cantonese speech emotion dataset described in Section 3.2 and Table 2, containing only Cantonese (a non-English language) utterances."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of datasets containing code or programming language content is found; datasets are speech emotion datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets described contain mathematical or formal logical expressions; datasets focus on speech emotion recordings."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets pertain to biological sequences or non-human communication; all focus on human speech emotion."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets involving fictional or artificially created languages are described; all datasets are human natural languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states the languages involved for newly constructed datasets; language identity is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "All newly introduced datasets contain human language speech data."
        }
      }
    }
  },
  {
    "id": "cahyawijaya23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9536,
      "completion_tokens": 177,
      "total_tokens": 9713
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 7 (Conclusion)",
          "reasoning": "The paper states in the abstract and conclusion sections that the code is publicly released at https://github.com/HLTCHKUST/elderly_ser, implying that the code associated with the dataset construction and experiments is available in a public repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 (YueMotion Dataset) and Section 3 (Benchmark and Dataset)",
          "reasoning": "The paper provides detailed descriptions of the new dataset YueMotion including recording details (number of speakers, gender, sentences, emotion categories) and its statistics in Table 2. The construction of BiMotion benchmark is explained by the collection from six public datasets and how the splits are handled. These descriptions provide transparency and completeness on dataset creation."
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6456,
      "completion_tokens": 97,
      "total_tokens": 6553
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The paper states that the Assamese speech data was collected from 40 voluntary native Assamese speakers reading a translated story passage, recorded at 44.1 kHz and 24-bit resolution, then downsampled. This data was recorded by humans and is audio data captured via human-operated recording equipment."
        }
      ]
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7256,
      "completion_tokens": 287,
      "total_tokens": 7543
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1 Data",
            "reasoning": "The data is described as collected from 40 voluntary native Assamese speakers from four dialect regions, implying that multiple human speakers (annotators) provided the speech recordings. These speakers whom the paper calls 'voluntary native speakers' would be considered human experts for the data collection context as native Assamese speakers belonging to the dialect groups."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Time-domain rhythm measures",
            "reasoning": "The paper mentions that the speech data files were segmented at the breath group level using Praat software, and vowel onsets and offsets were detected using MATLAB code, with annotation and analysis limited to fluent utterances of seven or more syllables without hesitation. This indicates a protocol was followed, likely containing instructions on how to annotate for breath groups and vowel boundaries."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2 Time-domain rhythm measures",
            "reasoning": "There is no mention of scoring rubrics or explicit evaluation criteria to assess annotation quality or consistency in the paper's description of annotation or data collection."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not provide any explicit annotation examples or sample annotated data, nor does it describe example annotation cases or guidelines samples."
          }
        }
      ]
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8446,
      "completion_tokens": 374,
      "total_tokens": 8820
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance or validation performed by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts validated or reviewed the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process involving a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance process involving multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for classification, the paper does not mention any AI model being used explicitly as a quality assurance judge for dataset validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process for verifying dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process applied to the dataset's annotations or content. The dataset was obtained from a previous study and the paper does not mention QA procedures."
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8056,
      "completion_tokens": 472,
      "total_tokens": 8528
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper explicitly states that the speech data used was collected from 40 voluntary native Assamese speakers across four districts in Assam, India. The data comprises read speech of an Assamese translation of an IPA standard story 'The North Wind and the Sun'. This indicates original content created by human contributors entirely from scratch, not translated or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated solely by AI or machine learning models without referencing or transforming existing data."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The speech data is derived from the Assamese translation of the 'North Wind and the Sun' passage, originally an IPA standard story. The paper mentions that the story was translated into Assamese, implying human translation of the source material before being read aloud by speakers to generate the speech data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation being used to produce the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected from multiple existing sources or aggregated without modification; rather, it was collected from human speakers for this study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The original IPA story was first translated into Assamese and then read aloud by speakers. Thus, the speech data is derived from a pre-existing text (the IPA standard story) through translation and subsequent recording, constituting a transformation of an existing source."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and method of data collection."
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8574,
      "completion_tokens": 294,
      "total_tokens": 8868
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The paper uses the speech dataset collected from Assamese speakers exclusively to evaluate the effectiveness of time- and frequency-domain rhythm measures for dialect classification using quadratic discriminant analysis (QDA). Classification accuracies, confusion matrices, and discriminability between dialects are reported, indicating the dataset's role is solely for evaluation and benchmarking of dialect classification methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The dataset is analyzed extensively to explore distinguishable patterns and trends in rhythm metrics across the Assamese dialects. Kernel density estimations and confusion matrices are used to characterize the linguistic features and discrimination capability of time- and frequency-domain rhythm features, indicating use for analysis of trends and characteristics rather than training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9297,
      "completion_tokens": 593,
      "total_tokens": 9890
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of Assamese language speech recorded from native Assamese speakers from different dialectal regions. There is no mention of any other human language included in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages, but only Assamese. The paper focuses on Assamese dialectal varieties and does not mention inclusion of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves readings of an Assamese translation of the story 'The North Wind and the Sun' and the speech data is exclusively in Assamese, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Data)",
          "reasoning": "The dataset consists of spoken Assamese language data collected from native speakers of four Assamese dialectal varieties. The data is read speech of an Assamese translation of a story, indicating entries are all in Assamese, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although MATLAB and Perl scripts are mentioned as tools used for analysis, the dataset itself does not contain programming or code entries. The dataset is composed of speech audio data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises speech signals and audio data of Assamese speech; it does not include any mathematical or formal logical notations as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech recordings of Assamese dialects; it does not contain any biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset recordings are from human natural language Assamese dialects; no fictional or constructed languages are involved."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is explicitly stated as Assamese, and dialectal varieties thereof. There is no indication that the language is unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data (Assamese speech), therefore it contains language entries."
        }
      }
    }
  },
  {
    "id": "chakraborty24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6515,
      "completion_tokens": 182,
      "total_tokens": 6697
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories for data collection, preprocessing, or dataset generation. It mentions using MATLAB and Perl scripts and Praat software, but there is no indication that these scripts or code are made available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper provides detailed documentation about the dataset creation process. Section 3.1 describes the data source, number of speakers, dialect regions, recording specifications, and content (Assamese translation of the North Wind and the Sun passage). Section 3.2 explains the preprocessing steps including breath group segmentation, vowel onset and offset detection using specific tools, and feature extraction methods. This constitutes a reasonably transparent and detailed dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6721,
      "completion_tokens": 282,
      "total_tokens": 7003
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and 3.3",
          "Reasoning": "The dataset consists of synthetic spoken instructions generated using off-the-shelf Text-to-Speech (TTS) models from the text instructions in the ALFRED dataset. The spoken audio is thus algorithmically generated by the TTS system, not directly recorded from humans. Noise is artificially introduced via systematic masking of audio segments."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The underlying sub-goal text instructions are derived from the ALFRED dataset, which contains human-annotated textual instructions for task completion in embodied environments."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 3.2",
          "Reasoning": "The visual context associated with each instruction comprises the agent's single-frame visual observations from the ALFRED environment. However, the paper does not explicitly specify the exact origin of these images, but since ALFRED is a simulation environment, these images are rendered scenes, which are synthetic but visually realistic. The paper does not clearly specify if these are human captured or synthetic; hence origin is set as unknown."
        }
      ]
    }
  },
  {
    "id": "chang23c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7521,
      "completion_tokens": 200,
      "total_tokens": 7721
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2",
            "reasoning": "The dataset of spoken instructions is created synthetically by applying off-the-shelf TTS models to text instructions extracted from the ALFRED dataset. This process involves automatic synthetic speech generation rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not describe any annotation instructions provided for human annotators since the spoken instructions are generated automatically using TTS."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No scoring rubrics or evaluation guidelines for manual annotation are presented because the dataset generation is fully automatic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No examples of annotations or guidelines are included as the data stems from an automatic speech synthesis pipeline, not human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "chang23c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8711,
      "completion_tokens": 350,
      "total_tokens": 9061
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the new synthetic spoken instruction dataset derived from ALFRED."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of QA by multiple human experts described for the newly created synthetic spoken instruction dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about QA by multiple non-expert humans for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The dataset of synthetic spoken instructions is generated by automatically applying text-to-speech (TTS) models to the written instructions from ALFRED, and noise is injected via an automatic masking process by substituting speech segments with Gaussian noise using word boundary segmentation from wav2vec 2.0. There is no mention of manual annotation or validation; the entire generation and noising process is conducted via automated procedures, which serves as a form of quality control through deterministic and repeatable transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated process for dataset creation and noising, which constitutes a quality assurance approach; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8321,
      "completion_tokens": 301,
      "total_tokens": 8622
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The dataset of spoken instructions is synthesized by applying Text-To-Speech (TTS) models from the open-source Silero library to existing written instructions from the ALFRED dataset, resulting in speech data generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The spoken instruction dataset is based on the existing ALFRED textual instructions and is transformed by synthesizing speech using TTS and further modified by systematically masking spoken words with Gaussian noise to simulate acoustic degradation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8839,
      "completion_tokens": 359,
      "total_tokens": 9198
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 5",
          "reasoning": "The synthetic spoken instruction dataset derived from ALFRED sub-goal instructions is used to train both unimodal and multimodal ASR models from scratch, including a 4-layer Transformer decoder trained from scratch (Section 3.4 and Section 4.1). This dataset serves as the supervised training data for these ASR models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.4",
          "reasoning": "The dataset is used for evaluation by transcribing the spoken instructions and passing the transcriptions to an off-the-shelf ALFRED embodied agent, measuring task success rates and the impact of noisy speech on downstream embodied task completion (Section 5.4)."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (5.1, 5.2, 5.3)",
          "reasoning": "The dataset is used to analyze the effectiveness of multimodal ASR models in improving word recovery rates and word error rate under varying noise conditions, different speakers, and environments, as well as analyzing the benefit for nouns vs. non-nouns (Sections 5.1 to 5.3)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9562,
      "completion_tokens": 508,
      "total_tokens": 10070
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is derived from the ALFRED benchmark and consists of English language instructions only. There is no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced dataset contains only English spoken instructions synthesized from English text instructions. No second human language is involved."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 \"Building a Dataset of Spoken Instructions\" and throughout the paper",
          "reasoning": "The dataset is constructed by extracting English written instructions from the ALFRED dataset and synthesizing English speech using TTS models. Evaluations are done using English instructions and speech only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English language content; no non-English language data is mentioned."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken natural language instructions, not programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include mathematical or logical symbolic expressions; it contains natural language instructions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence of biological sequences or non-human communication data in the proposed dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses English natural language; no fictional or artificial constructed language is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English throughout the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language entries in English and thus does contain language."
        }
      }
    }
  },
  {
    "id": "chang23c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6780,
      "completion_tokens": 265,
      "total_tokens": 7045
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Footnote 1, footnote 1 references the open-source Silero library; also the abstract provides a GitHub URL: github.com/Cylumn/embodied-multimodal-asr",
          "reasoning": "The paper explicitly provides a GitHub URL in the abstract for the project code (github.com/Cylumn/embodied-multimodal-asr) which presumably includes the code for data construction, and it mentions the use of open-source TTS models from the Silero library, which is publicly accessible. This indicates the code related to dataset construction and processing is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 'Building a Dataset of Spoken Instructions' and Section 3.3 'Injecting Noise into Spoken Instructions'",
          "reasoning": "The paper details the dataset creation process explicitly: it explains how they extract sub-goal text instructions from the ALFRED dataset, synthesize speech using TTS (from Silero), pair instructions with visual context, and then synthetically mask audio segments at the word level to simulate noise using wav2vec 2.0 to identify word boundaries. These procedures are described clearly, demonstrating transparency and documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8832,
      "completion_tokens": 196,
      "total_tokens": 9028
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset and Abstract",
          "Reasoning": "The paper explicitly states that the updated MISP2021-AVSR corpus is a large-scale audio-visual Chinese conversational corpus consisting of 141h of audio data collected from conversations of native Chinese speakers in 34 real-home TV rooms. This data was recorded using multiple microphone arrays, indicating it was human recorded in real scenarios, not synthesized or generated."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset and Abstract",
          "Reasoning": "The updated MISP2021-AVSR corpus also includes video data collected in the same real-home TV rooms using multiple cameras capturing far/middle views of the speakers' lip regions. The video data is explicitly human captured during the conversational recordings and not generated or simulated."
        }
      ]
    }
  },
  {
    "id": "chen22o_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9632,
      "completion_tokens": 284,
      "total_tokens": 9916
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 Dataset",
            "reasoning": "The paper describes the MISP2021-AVSR corpus as a large-scale audio-visual Chinese conversational dataset recorded in home TV rooms with multi-microphone and multi-camera setups. There is no explicit mention of human annotation processes such as transcription or labeling by human experts or non-experts. The ground truth transcripts and diarization information appear to be generated or prepared as part of the dataset creation procedure without specifying manual annotation. Thus, it implies that annotation such as labeling (transcription and diarization) is done by automatic or system processes rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention found in the paper.",
            "reasoning": "The paper does not provide any information about annotation instructions for annotators, nor does it mention any guidelines or protocols given to humans or models performing annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention found in the paper.",
            "reasoning": "There is no indication in the paper of scoring rubrics or grading criteria used during annotation or labeling processes for the dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention found in the paper.",
            "reasoning": "The paper does not provide annotation examples or sample annotated data segments as part of annotation guidelines or documentation."
          }
        }
      ]
    }
  },
  {
    "id": "chen22o_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10822,
      "completion_tokens": 394,
      "total_tokens": 11216
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that quality assurance was performed by multiple human experts or multiple annotators with subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any human annotation or quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No section or statement in the paper describes AI models used for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper notes that asynchronous samples in the training and development sets were corrected and that the dataset was double-checked, but it does not specify that this was done by an automated verification or rule-based system. Hence no explicit automated QA process is indicated."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any explicit quality assurance process for dataset annotations or content beyond correcting asynchronous samples. No information is provided on human or automated QA, so no verifiable quality assurance process can be identified."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10432,
      "completion_tokens": 434,
      "total_tokens": 10866
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Dataset",
          "reasoning": "The paper states that the updated MISP2021-AVSR corpus contains 141.24 hours of audio and video data collected from 253 native Chinese speakers in 34 real-home TV rooms. The data was recorded in real environments involving multiple variables such as TV and lighting settings. This indicates that the corpus data was created from scratch by human contributors, through actual recordings in controlled settings. There is no indication that the data was translated or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any part of the dataset was generated by AI or machine learning models. The data originates from human recordings rather than model-generated content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information in the paper suggests the data was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details in the paper indicate machine translation was applied in producing the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is newly collected data recorded in real homes; it is not aggregated or collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being based on existing sources with modifications or adaptations. The paper emphasizes the data was recorded anew."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin is explicitly described as recordings from human participants in real environments, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10950,
      "completion_tokens": 544,
      "total_tokens": 11494
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the MISP2021-AVSR corpus for pre-training large models in an unsupervised or self-supervised manner. The focus is on supervised recognition tasks."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Section 3.2 (DNN-HMM Hybrid Acoustic Model)",
          "reasoning": "The dataset is used for training DNN-HMM hybrid acoustic models from scratch, including audio-only, video-only, and audio-visual speech recognition systems. The paper describes training procedures, model architectures, and evaluation over the corpus."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that pre-trained models are fine-tuned on this dataset; the paper rather focuses on models trained specifically on this data."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning-based post-training methods such as RLHF applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Table 3, Table 5",
          "reasoning": "The dataset is used for comprehensive evaluation and benchmarking of various audio-only, video-only, and audio-visual speech recognition models under different noise and clutter conditions, as demonstrated by CER metrics reported."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 (Analysis of errors in noise condition), Section 4.2 (Analysis of errors in overlap condition)",
          "reasoning": "The paper conducts deep analysis of error types (substitution, deletion, insertion) in different scenarios, exploring the complementarity of modalities and the challenges posed by noise and overlap."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base to augment models such as through retrieval-augmented methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple practical uses of the dataset including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11673,
      "completion_tokens": 539,
      "total_tokens": 12212
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2, Dataset",
          "reasoning": "The updated MISP2021-AVSR corpus consists solely of Mandarin Chinese speech data from native Mandarin speakers without strong accents. There is no indication of additional languages present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2, Dataset",
          "reasoning": "The dataset contains only Mandarin Chinese conversational speech; there are no two languages included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2, Dataset",
          "reasoning": "The dataset is explicitly Mandarin Chinese speech; no English content is present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2, Dataset",
          "reasoning": "The dataset consists exclusively of Mandarin Chinese conversational audio-visual speech data collected from native Mandarin-speaking participants (Section 2, Table 1). It is explicitly described as a large-scale Chinese audio-visual corpus."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain programming or structured code content. Code is discussed only in model architecture and experiment descriptions, not as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation appears in the model description and equations but is not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human audio-visual speech only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as Mandarin Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language (Mandarin Chinese), so it is not applicable to mark as containing no language."
        }
      }
    }
  },
  {
    "id": "chen22o_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8891,
      "completion_tokens": 137,
      "total_tokens": 9028
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 4.3",
          "reasoning": "The abstract mentions that the code is released to promote research and Section 4.3 specifies that more details of their experiments and source code can be found in their released codebase, implying the code associated with dataset processing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "Section 2 describes in detail the dataset including size, number and gender of participants, recording conditions, devices used, corrections made to the dataset, and differences from the challenge version, indicating comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9505,
      "completion_tokens": 115,
      "total_tokens": 9620
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Synthetic DoPaCo generation",
          "Reasoning": "The authors introduce a new dataset of about 160k synthetic doctor-patient conversations (DoPaCos) generated by fine-tuning a DialoGPT model on authentic conversations and then using it to generate synthetic dialog transcripts. The data are thus text modality generated by a language model (DialoGPT), making it model generated, not human generated or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "chen23i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10305,
      "completion_tokens": 288,
      "total_tokens": 10593
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1, 2.3",
            "reasoning": "The paper states that the 'annotated DoPaCos' used for fine-tuning contain 'human-written HPI summaries' and that the dataset is used for summarization fine-tuning and evaluation. This implies that multiple human annotators who are experts or domain knowledgeable created the reference summaries for the downstream task. Although the paper does not elaborate on the number or exact process, the presence of human-written reference summaries for 1342 annotated conversations strongly suggests multiple experts performed these annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention or provide any annotation instructions for the human-generated HPI summaries or the annotated DoPaCos dataset. There is no text indicating detailed guidelines or instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of scoring rubrics, evaluation criteria, or guidelines used by annotators to score or guide annotation quality in the paper. The paper only states the presence of human-written reference summaries as ground truth."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples, exemplar annotated data, or sample annotation guidelines are provided for the human annotations in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "chen23i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11495,
      "completion_tokens": 381,
      "total_tokens": 11876
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any multiple human expert quality assurance process for the synthetic DoPaCo dataset or the annotated conversations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided on quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided on quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for generating synthetic conversations and evaluations, there is no indication that AI models were used for quality assurance of the datasets themselves."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention any automated or algorithmic verification processes applied as quality assurance to the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any explicit quality assurance process applied to the synthetic DoPaCo dataset or the annotated conversation summaries. The synthetic data are generated by the DialoGPT model fine-tuned on authentic conversations, but no post-generation validation or QA procedure is described."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11105,
      "completion_tokens": 468,
      "total_tokens": 11573
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset that is original content created entirely by humans from scratch. The annotated set of 1342 DoPaCos used for fine-tuning is a pre-existing dataset, not introduced by the authors."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper clearly describes generating about 160k synthetic doctor-patient conversations (DoPaCos) using a fine-tuned DialoGPT language model on authentic conversations (Section 2.2). This synthetic dataset is newly created by the AI model without direct copying, thus fitting the category of new data generated entirely by AI or machine learning."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating data from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors use existing datasets (authentic conversations and clinical notes), they do not describe creating or releasing these as new collated datasets, and the synthetic dataset is generated rather than collected or aggregated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The synthetic dataset is generated by adapting and fine-tuning the DialoGPT model on an existing authentic DoPaCo dataset and subsequently generating new conversations. Hence, the synthetic data is based on existing data, modified through model generation \u2013 making it derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the synthetic dataset is clearly explained in the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11623,
      "completion_tokens": 475,
      "total_tokens": 12098
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.2, 2.3, 3.1, 3.2",
          "reasoning": "The synthetic Doctor-Patient Conversation (DoPaCo) dataset is generated and used for in-domain pre-training of the Longformer-Encoder-Decoder (LED) model before fine-tuning on annotated data for the summarization task. This pre-training is unsupervised or self-supervised as described in Sections 2.2 and 2.3. The results in Sections 3.1 and 3.2 confirm the use of synthetic data in pre-training improving downstream summarization."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters using the datasets; all models start from publicly available pre-trained weights."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The annotated DoPaCo dataset with human-written summaries is used to fine-tune pre-trained LED models in a supervised manner, as described in Section 2.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any reinforcement learning post-training techniques or use RLHF with the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.4, Section 3",
          "reasoning": "The annotated DoPaCo test set and associated summaries are used for evaluation of model performance using multiple metrics (ROUGE, BERTScore, concept-based metrics) as detailed in Section 2.4 and shown in Section 3."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not primarily used for trend or pattern analysis but rather for pre-training, fine-tuning, and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base or for retrieval-augmented generation in the presented work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple practical uses of the synthetic DoPaCo dataset including pre-training and downstream task evaluation and fine-tuning."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12346,
      "completion_tokens": 550,
      "total_tokens": 12896
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses datasets consisting of English clinical conversations and notes. There is no mention of more than two languages being present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the datasets contain exactly two human languages. The study focuses solely on English medical conversations and notes."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (DoPaCo and clinical note datasets); Section 2.2 (Synthetic DoPaCo generation); throughout the paper",
          "reasoning": "The dataset consists of authentic and synthetic doctor-patient conversations and clinical notes in English only. Conversation examples, summaries, and generated text are all in English. There is no indication of any other language presence."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language datasets are introduced or discussed in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are natural language clinical conversations and notes. There is no mention or example of programming or structured code content in the data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain mathematical or formal logical expressions or symbolic representations. The content focuses on natural language text."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include biological sequences or non-human communication data. The conversations are human doctor-patient dialogues."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of the datasets described in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset entries are well specified and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain natural human language (English) in the form of conversations and clinical notes."
        }
      }
    }
  },
  {
    "id": "chen23i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9564,
      "completion_tokens": 209,
      "total_tokens": 9773
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or appendix in the paper provides links to or mentions publicly available code repositories for dataset generation or related processes.",
          "reasoning": "The paper explicitly describes the method used to generate synthetic doctor-patient conversations including fine-tuning a DialoGPT model and the generation strategy. However, there is no mention or provision of any code repository or code release for the synthetic data generation process or other dataset construction scripts."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 \"Synthetic DoPaCo generation\" and Section 2.1 \"DoPaCo and clinical note datasets\"",
          "reasoning": "The paper provides detailed documentation of the dataset creation process for the synthetic dataset, including the source authentic DoPaCo dataset, the fine-tuning of a DialoGPT model, the generation parameters, and heuristics used to create the synthetic conversations. Sampling sizes and generation strategies are described. Thus, the dataset construction pipeline is well documented within the paper."
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10318,
      "completion_tokens": 97,
      "total_tokens": 10415
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The multiPA dataset consists of 50 audio clips collected from about 20 anonymous dialog chatbot users who are native Mandarin speakers practicing English. These audio recordings were created through human speech during real-world open-response scenarios using a dialog chatbot system with users' own headsets, indicating human-generated audio data."
        }
      ]
    }
  },
  {
    "id": "chen24c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11118,
      "completion_tokens": 258,
      "total_tokens": 11376
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1 Data",
            "reasoning": "The paper states, 'We recruited five annotators with high proficiency in English to annotate the audio clips at both sentence and word levels.' This indicates multiple human experts performed the annotations on the newly collected multiPA dataset."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Data",
            "reasoning": "The paper details specific scoring scales and criteria used by annotators, e.g., sentence labels being graded from 1 (very poor) to 5 (excellent) and word level having four defined intelligibility levels. This indicates detailed annotation instructions were provided to the annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 Data",
            "reasoning": "The explicit scoring scales (1 to 5 for sentence-level attributes; four levels for word-level intelligibility with defined verbal criteria) served as rubrics guiding annotation, indicating rubrics were provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1 Data",
            "reasoning": "There is no mention or evidence of specific annotation examples being provided in the paper. Examples to guide annotators are not described."
          }
        }
      ]
    }
  },
  {
    "id": "chen24c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12308,
      "completion_tokens": 272,
      "total_tokens": 12580
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions multiple annotators were recruited for the multiPA dataset; no indication of QA by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The multiPA dataset was annotated by five annotators with high proficiency in English, which implies multiple human experts conducted the annotation, providing quality assurance through multiple expert judgments."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of QA by a single non-expert annotator for the new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as having high proficiency in English, implying expertise rather than non-expertise."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance performed by AI models to validate dataset annotations is described."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic quality checks for dataset annotation are described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the annotation process involving multiple human expert annotators for the multiPA dataset, so QA is documented."
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11918,
      "completion_tokens": 462,
      "total_tokens": 12380
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper states that the authors collected a new dataset, referred to as multiPA data, comprising 50 audio clips from about 20 anonymous dialog chatbot users (native Mandarin speakers practicing English). The data was annotated by five expert annotators proficient in English at both sentence and word levels. This information indicates the data was created entirely anew from human subjects and human annotators, representing original content not derived from preexisting datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was generated purely from AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention producing data by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate use of machine translation to produce dataset content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses a pre-existing dataset (speechocean762), this dataset is not newly introduced by the authors and thus excluded from this assessment. The newly introduced data (multiPA dataset) is specifically described as newly collected human data rather than aggregated existing data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly collected dataset is described as collected from real users and annotated from scratch. There is no mention of modifying, transforming, or adapting existing datasets to form the new dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins of the multiPA dataset are explicitly described, so the category of unknown or undocumented data origin does not apply."
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12436,
      "completion_tokens": 352,
      "total_tokens": 12788
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental setup, Data; Section 3 Experimental setup",
          "reasoning": "The authors explicitly state that they use the newly collected multiPA dataset for out-of-domain testing and train their MultiPA model using supervised fine-tuning methods on labeled data with various sentence-level and word-level scores. The paper describes fine-tuning a pretrained model (HuBERT) using multiPA data with annotated labels for both sentence and word levels, indicating supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental setup, Data; Section 3 Experimental setup",
          "reasoning": "The multiPA dataset is used as an out-of-domain evaluation set to test the generalizability and real-world utility of the MultiPA model. The paper states that they collected multiPA data from real-world open-response scenarios and used it for evaluation and benchmarking of their model's performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Experimental setup",
          "reasoning": "The authors conducted correlation analysis using the multiPA dataset to examine relationships between different pronunciation task scores, indicating the dataset is used for analyzing trends and characteristics as part of their study."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13159,
      "completion_tokens": 617,
      "total_tokens": 13776
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The newly collected multiPA dataset consists of audio clips from Mandarin native speakers practicing English, but the speech content is only in English. There is no indication that more than two languages are involved in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The multiPA dataset involves Mandarin native speakers using English for practice; however, the dataset entries themselves contain only English speech samples. The presence of Mandarin as native language background does not imply bilingual speech content in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The multiPA dataset entries are English speech utterances from Mandarin native speakers practicing English. The paper explicitly states that the data are collected from users practicing English, and annotations focus on English utterances. Therefore, the dataset contains entries with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset does not consist of non-English speech only; it is English speech produced by non-native speakers."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists solely of spoken utterances and annotations; there is no mention of any programming or structured code-related content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No parts of the dataset entries contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of human speech utterances only; no biological sequences or non-human communications are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains natural human language (English) spoken by learners; no fictional or artificial languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language content is clearly specified and described as English speech; thus, language(s) are known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken language, so they contain language and are not N/A."
        }
      }
    }
  },
  {
    "id": "chen24c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10377,
      "completion_tokens": 167,
      "total_tokens": 10544
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper mentions that they collected a new dataset (multiPA data) and that they will release the pilot data for other studies, but it does not provide any links or references to code repositories for data collection, preprocessing, or generation. There is no explicit mention or link to code related to dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper provides a detailed description of the new dataset, including the collection process (from dialog chatbot users practicing English), annotators and their proficiency, annotation scales for sentence and word levels, ethical approval (IRB), and features of the data, which constitutes thorough documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7628,
      "completion_tokens": 248,
      "total_tokens": 7876
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Abstract",
          "Reasoning": "The new dataset consists of 346 English videos collected manually from YouTube, including various types of videos like empathy training videos, therapy sessions, TV shows, movies, interviews, and TED Talks. These videos were created and recorded by humans, hence are human-generated videos."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.2",
          "Reasoning": "The audio modality is present as the videos contain spoken content. The audio was obtained from YouTube API and manually corrected and annotated. Since the audio comes from human speech in these videos, the audio data is human-generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The text modality exists as transcripts of the spoken content were manually transcribed or corrected and annotated by human annotators. The transcripts are derived from the human speech in the videos, therefore they are human-generated text."
        }
      ]
    }
  },
  {
    "id": "chen24f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8428,
      "completion_tokens": 201,
      "total_tokens": 8629
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 specifies that videos were annotated between 2023 and 2024 by 10 annotators and verified by at least one different annotator, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No explicit mention of detailed annotation instructions or guidelines provided to annotators is found in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not provide any information regarding scoring rubrics used in annotation for empathy or other labels."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 2 in Section 3.2",
            "reasoning": "Table 2 provides examples of four stages of empathy at the segment-level annotations from an interview, which serves as annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "chen24f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9618,
      "completion_tokens": 317,
      "total_tokens": 9935
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Annotations",
          "reasoning": "The paper states that 65 videos were annotated between 2023 and 2024 by 10 annotators and verified by at least one different annotator. These annotators are described as expert annotators, implying subject matter expertise. The involvement of multiple expert annotators with verification by others indicates a rigorous QA process by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as expert annotators; thus, multiple human non-expert QA does not apply."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used for quality assurance of the annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated diarization and transcription tools were used initially, the paper describes manual correction and annotation verification by human experts, indicating that quality assurance was not solely automatic process verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance process is documented and detailed; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9228,
      "completion_tokens": 373,
      "total_tokens": 9601
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is collected from publicly available videos, not created originally by human contributors from scratch. The dataset consists of existing YouTube videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation for dataset preparation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection",
          "reasoning": "The dataset is collected by aggregating existing English videos from YouTube using keyword searches. The data are thus aggregated from existing publicly available sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Annotations",
          "reasoning": "The dataset is partially derived in that selected videos are manually transcribed, diarized, and annotated with empathy labels and stages, adding manual modifications and metadata to the original collected videos."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of data generation are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9746,
      "completion_tokens": 266,
      "total_tokens": 10012
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5 (Empathy Classification and Results)",
          "reasoning": "The authors fine-tune a pretrained RoBERTa-base model using supervised learning methods on their new empathy speech dataset for the task of empathy classification."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Empathy Classification and Results)",
          "reasoning": "The new dataset is used for evaluating and benchmarking classification models to detect empathy in speech, demonstrated by the performance metrics reported on a held-out validation set."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Empathy Analysis)",
          "reasoning": "The dataset serves primarily for analyzing acoustic-prosodic and lexical features that characterize empathetic speech, investigating trends and patterns to better understand empathy expression."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10469,
      "completion_tokens": 485,
      "total_tokens": 10954
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset, Table 1; Section 6 Conclusions",
          "reasoning": "The new empathy speech dataset collected consists of 346 English videos totaling about 53 hours. The dataset is explicitly described as English in Section 3.1 and Table 1, and the conclusion reiterates that the corpus is of English empathetic videos."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only introduces an English empathy speech dataset. While there is mention of ongoing Mandarin data collection, this is not part of the newly introduced dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech and transcripts related to empathy detection, with no programming or code-related entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or inclusion of mathematical or formal logical expressions in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses exclusively on human speech data reflecting empathy, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset includes fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the entries is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken language (English) and transcripts, so cannot be considered language-free."
        }
      }
    }
  },
  {
    "id": "chen24f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7687,
      "completion_tokens": 167,
      "total_tokens": 7854
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not provide any link or mention of publicly available code repositories related to the data collection, preprocessing, or generation. While the dataset will be made publicly accessible, there is no indication that code used to collect or annotate the dataset is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Dataset), subsections 3.1 and 3.2",
          "reasoning": "The paper describes in detail the dataset collection process, including source (YouTube videos, keywords used, date range), dataset composition statistics, manual annotation processes, annotator details, and characteristics of the dataset (language, categories, speaker gender, etc.). This provides clear and transparent documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7945,
      "completion_tokens": 289,
      "total_tokens": 8234
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.2 and 2.2.3",
          "Reasoning": "The CNVSRC-Single and CNVSRC-Multi datasets are newly introduced by the authors as development and evaluation data for the challenge. CNVSRC-Single contains over 800 speech videos from one broadcaster, recorded from an online channel, clearly showing human-generated video data. CNVSRC-Multi includes videos from 23 speakers recorded in a studio and from 20 speakers from the Internet, with manual checks and controlled recording setups, confirming the data is human-generated video recordings intended for visual speech recognition."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3 and 2.2.1 to 2.2.3",
          "Reasoning": "Text transcriptions for CNVSRC datasets were generated by applying a Paraformer-based ASR model (model generated) to transcribe the speech in the videos, followed by manual checking to ensure transcription accuracy with less than 2% CER (human generated correction). Thus, the text data is partly model generated and partly human verified (human involvement). This applies to CNVSRC-Single, CNVSRC-Multi, and the CN-CVS dataset adapted for the challenge."
        }
      ]
    }
  },
  {
    "id": "chen24y_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8745,
      "completion_tokens": 202,
      "total_tokens": 8947
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.3",
            "reasoning": "The text annotations were generated using a Paraformer-based ASR system to transcribe the speech from all videos, followed by a manual check to ensure CER is below 2%. This indicates that the primary annotation was produced automatically by an ASR model, with human verification rather than manual transcription."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no indication or description in the paper that instructions were given to annotators for manual transcription, as annotation was primarily automatic ASR-based with manual checking."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics or evaluation criteria for annotation quality are described in relation to the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No examples for annotation or transcription are provided or described in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "chen24y_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9935,
      "completion_tokens": 291,
      "total_tokens": 10226
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation of quality assurance by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.3 (Text annotation)",
          "reasoning": "The paper states that a Paraformer-based ASR system was used to transcribe all videos to generate text transcriptions and that a manual check ensured a low character error rate. This indicates that an AI model was used for initial transcription as a form of QA, with human checks verifying accuracy."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification of annotations or data via algorithmic or rule-based techniques aside from the AI transcription itself."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a quality assurance process involving AI transcription and human checking, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9545,
      "completion_tokens": 509,
      "total_tokens": 10054
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2.2 and 2.2.3",
          "reasoning": "The CNVSRC-Single and CNVSRC-Multi datasets were gathered from online channels and from recording studios respectively. The CNVSRC-Multi recording studio data involved 23 speakers reading prompted sentences, which were recorded with multiple cameras and high-quality microphones, indicating original content creation by human contributors. The internet speech videos were also curated with face recognition and manual checks to ensure integrity, reflecting human involvement in data collection and preparation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any dataset was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention that data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2.3",
          "reasoning": "The CNVSRC-Multi dataset includes speeches downloaded from the internet, collected following the pipeline of [19], suggesting aggregation of existing online speech videos without significant modification, fitting the description of collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2.1 and 2.3",
          "reasoning": "The CN-CVS dataset was originally created for a video-to-speech synthesis task and did not include text transcription. For the CNVSRC challenge, the authors labeled the videos with a semi-automatic pipeline involving ASR transcription and human checking to produce text transcriptions, indicating that the data is derived from existing video-speech data with modifications (adding text labels)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of all datasets are clearly described, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10063,
      "completion_tokens": 399,
      "total_tokens": 10462
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The CN-CVS dataset is used as the primary data for pre-training the baseline systems, as described in Section 3.3. The first training phase (P1 and P2 steps) involves pre-training the models on CN-CVS video data before fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters solely on the proposed datasets; pre-training is explicitly described."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "After pre-training on CN-CVS, the models are fine-tuned on the development datasets CNVSRC-Single and CNVSRC-Multi using supervised learning methods in a fine-tuning step as described in Section 3.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning based post-training methods using the proposed datasets is made in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The CNVSRC-Single and CNVSRC-Multi datasets include evaluation sets with secret transcriptions used exclusively for benchmarking the systems during the challenge, as described in Section 2.2."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the new datasets are used primarily for analyzing trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets serve as external knowledge bases for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets have practical usage described extensively for training, fine-tuning, and evaluation in the challenge."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10786,
      "completion_tokens": 626,
      "total_tokens": 11412
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper explicitly focuses on Chinese language datasets for visual speech recognition and does not mention any datasets containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced (CN-CVS, CNVSRC-Single, CNVSRC-Multi) are all related to Chinese language visual speech recognition. No indication of bilingual content is provided."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets presented are not English language datasets but rather Chinese. No English-only dataset is introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Sections 2.2.1, 2.2.2, 2.2.3, and Section 4.2.4",
          "reasoning": "The paper introduces three new datasets: CN-CVS, CNVSRC-Single, and CNVSRC-Multi, all designed for Chinese visual speech recognition tasks. It specifically mentions the use of Chinese characters as modeling units and that CN-CVS is the largest open-source Chinese visual-speech dataset (Section 2.2.1). This confirms that the datasets contain entries in exactly one language, which is Chinese, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets focus on visual speech data (video and transcriptions) and do not include any programming or code language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the datasets contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets are human visual speech datasets and do not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of fictional or artificially created languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language used in the datasets is explicitly documented as Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain transcribed human language data (Chinese), so they cannot be classified as not containing any language."
        }
      }
    }
  },
  {
    "id": "chen24y_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8004,
      "completion_tokens": 214,
      "total_tokens": 8218
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and Section 3",
          "reasoning": "The paper describes the dataset collection and processing pipelines used for the newly introduced CNVSRC-Single and CNVSRC-Multi datasets, referencing the original CN-CVS dataset collection method from [19]. However, there is no mention or link to publicly available code repositories for the dataset construction, labeling, or preprocessing. While the organizers published baseline system model and code for system development, code related to data collection and dataset creation is not indicated as available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 and 2.3",
          "reasoning": "The paper provides detailed descriptions of the new datasets CNVSRC-Single and CNVSRC-Multi in Section 2.2, including their source, collection procedures, scenarios, and processing pipelines. Section 2.3 documents the text annotation method via a Paraformer-based ASR system with manual checking. These sections comprehensively document the dataset creation processes."
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7613,
      "completion_tokens": 178,
      "total_tokens": 7791
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 2.1 - Pre-processing",
          "Reasoning": "The MusicLyric dataset consists of over 320k audio sequences corresponding to music songs; the audio data is collected from human-created music available on YouTube and other sources, representing human recorded content rather than synthesized or AI-generated audio."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 2.1 - Pre-processing",
          "Reasoning": "The lyrics text data in the MusicLyric dataset are collected from multiple human-generated sources (e.g., Genius, Musixmatch, LyricFind), which are established lyrics providers containing human created lyrics; thus, the text modality originates from human generated content."
        }
      ]
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8413,
      "completion_tokens": 212,
      "total_tokens": 8625
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 2.4 Alignment Framework",
            "reasoning": "The annotations (time-aligned lyrics) for the new MusicLyric dataset are generated using a teacher-student paradigm involving pre-trained models (teacher) and iteratively trained student models to produce and improve pseudo-alignments. The annotation is thus performed automatically by AI alignment models rather than humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.4",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to human annotators since annotation is performed by AI models."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.4",
            "reasoning": "No scoring rubrics or grading criteria are described in relation to annotations because AI models generate alignments automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not provide any annotation examples or guidelines illustrative of annotation. The annotation is model-based generation."
          }
        }
      ]
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9603,
      "completion_tokens": 395,
      "total_tokens": 9998
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any single non-expert human annotator was involved in quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided regarding multiple non-expert human annotators participating in quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.4 (Alignment Framework)",
          "reasoning": "The quality assurance process involves the use of AI models\u2014specifically the cooperation between teacher and student models (trained AI models) to generate and validate aligned lyrics and audio pairs. The agreement between models is used as a form of quality check, with thresholds on error rates used to filter and select data. The teacher-student paradigm described effectively uses AI models to judge and ensure the quality and alignment of the dataset's annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.4 (Alignment Framework)",
          "reasoning": "The dataset generation and validation process relies heavily on automated, algorithmic methods for filtering, alignment, and consensus checking (e.g., forced alignment with dynamic programming, beam alignment, measuring cooperation rates and WER thresholds). These rule-based and programmatic procedures serve as automated verification steps that validate and improve dataset quality without human intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is clearly described using AI models and automated procedures, so it is not the case that no QA process is applied or documented."
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9213,
      "completion_tokens": 485,
      "total_tokens": 9698
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not original content created entirely from scratch by human contributors; rather, it is obtained from existing sources of audio and lyrics."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content (audio-lyrics pairs) is not generated entirely by AI or machine learning models; instead, models are used to generate alignments and filter data, not to generate original audio or lyrics."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of machine translation to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Methodology), Sections 2.1 and 2.2",
          "reasoning": "The dataset is collected from existing popular music charts (Billboard, Million Song Dataset) and various online sources (YouTube, Genius, Musixmatch, LyricFind). The authors assemble (collate) the audio and lyrics pairs from these sources, which constitutes data collection from existing datasets and web."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.4 (Alignment Framework), Section 2.1 (Pre-processing)",
          "reasoning": "The authors utilize a teacher-student paradigm where models generate alignments and perform filtering/modification on the original lyrics and audio data to create accurate time-aligned labels. This involves adaptations and transformations such as chunking, filtering, pre-processing of lyrics text, alignment corrections, and iterative model-based refinement, making the dataset derived from existing sources through these modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the source and derivation process of the data; hence, the origin is documented."
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9731,
      "completion_tokens": 359,
      "total_tokens": 10090
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5 (Experiments)",
          "reasoning": "The dataset MusicLyric is used to train models for automatic lyrics transcription from scratch. The authors train state-of-the-art models using this dataset exclusively and demonstrate improved performance on evaluation benchmarks, indicating training from scratch on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.4 (Alignment Framework) and Section 5 (Experiments)",
          "reasoning": "The authors describe iterative training of student models fine-tuning on pseudo-labeled data generated from the dataset to improve alignment and transcription quality. This indicates supervised fine-tuning use of the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.5 (Evaluation Set) and Section 5 (Experiments)",
          "reasoning": "A curated subset of the MusicLyric dataset is released as an evaluation benchmark, and models are evaluated on this set to measure performance, supporting usage for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.2 (Properties) and throughout Sections 2 and 5",
          "reasoning": "The paper analyzes properties of the dataset such as genre distribution compared to prior datasets, vocal segmentation, and transcription intelligibility, indicating the dataset is also used for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10454,
      "completion_tokens": 590,
      "total_tokens": 11044
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Pre-processing",
          "reasoning": "The dataset focuses on English language content with a filtering step to exclude non-English pairs, allowing only a limited number of foreign words to increase data size. There is no indication that more than two languages are included in the entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Pre-processing",
          "reasoning": "The dataset is filtered to contain primarily English content; there is no mention of pairs containing exactly two distinct human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Pre-processing",
          "reasoning": "The paper states that pairs that are not in English are filtered out, while allowing a limited number of foreign words. This implies the dataset largely consists of entries with English lyrics."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2.1 Pre-processing",
          "reasoning": "The dataset specifically filters out non-English language pairs, except for a limited number of foreign words, but does not contain monolingual non-English entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains music audio and lyrics aligned data and does not include programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of mathematical or logical symbolic expressions as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of human music audio and lyrics without biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention any inclusion of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset's language content is clearly specified as predominantly English with a filtering procedure applied to ensure the same."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain lyrics (text data), which are in human language; thus, the dataset involves language content."
        }
      }
    }
  },
  {
    "id": "chivriga23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7672,
      "completion_tokens": 202,
      "total_tokens": 7874
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 6 Conclusion, and Methodology Section 2",
          "reasoning": "The paper explicitly states that the dataset, toolkit, and pre-trained models are made open-source. Specifically, in the abstract, it says 'We make our dataset, toolkit, and pre-trained models open-source.' In Section 6 (Conclusion), it mentions releasing the dataset and tools fully open to the community. The methodology section describes the process in detail, supporting availability of the toolkit/code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 Methodology (subsections 2.1 to 2.4) and Section 6 Conclusion",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including data collection, filtering, preprocessing, vocal segmentation, and alignment framework. Section 2 covers these steps extensively, and Section 6 summarizes the approach. The thorough description reflects transparency and completeness for reproducibility."
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7048,
      "completion_tokens": 192,
      "total_tokens": 7240
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 and Section 2.1",
          "Reasoning": "The authors introduce a novel dataset of near homophone and synonym word pairs for their analysis. These word pairs are extracted and curated based on existing lexical databases (WordNet and Open Multilingual Wordnet) for synonyms, and phonemicized using phonetic dictionaries (CMU Pronouncing Dictionary and Epitran) to identify near homophones. The dataset uses utterances from existing speech corpora (LibriSpeech and Multilingual Spoken Words based on Common Voice) where audio segments corresponding to words are extracted using forced alignment tools. Thus, the dataset consists of human-recorded audio segments of spoken words paired by their phonetic and semantic similarities, making the modality audio and source human-generated speech recordings curated and segmented by the authors for their new dataset."
        }
      ]
    }
  },
  {
    "id": "choi24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7848,
      "completion_tokens": 266,
      "total_tokens": 8114
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The novel dataset of near homophone and synonym word pairs is curated by programmatic means using WordNet and Open Multilingual Wordnet (OMW) for synonyms, phonemicization via CMU pronouncing dictionary and Epitran with phonetic distance measure for near homophones. The selection thresholds and word pairs are derived using algorithmic procedures without mention of human manual annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any manual annotation task or instructions given to annotators for the new dataset; the dataset is derived automatically from linguistic databases and phonetic distance computations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No rubric or scoring system for manual annotations is mentioned since the dataset construction is automatic and does not involve human judgments requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 1, Section 2.1",
            "reasoning": "Table 1 in Section 2.1 provides example word pairs with their normalized Levenshtein phonetic distances illustrating the dataset content; these serve as examples of the data curated."
          }
        }
      ]
    }
  },
  {
    "id": "choi24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9038,
      "completion_tokens": 386,
      "total_tokens": 9424
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the newly curated dataset of near homophone and synonym word pairs."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of multiple human experts being involved in quality assurance for the dataset; the paper does not discuss any human expert QA process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert human annotator for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance or annotation by multiple non-expert humans; the process does not describe human-based QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA process does not involve any AI model acting as a judge or performing quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 2.2",
          "reasoning": "The paper describes the creation of the dataset and its validation through automated methods: synonyms were extracted automatically using WordNet and Open Multilingual Wordnet via NLTK; near homophones were identified via automatic phonemization using the CMU pronouncing dictionary and Epitran, followed by computing normalized Levenshtein distance to define phonetic similarity thresholds. These steps constitute an automatic, rule-based, and code-supported process serving as quality assurance for the dataset, ensuring that word pairs meet the defined phonetic and semantic similarity criteria."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described for the dataset and it involves automatic processing; therefore, the label N/A does not apply."
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8648,
      "completion_tokens": 540,
      "total_tokens": 9188
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The paper states in Section 2.1 that the authors curate a novel dataset of near homophone and synonym word pairs. These pairs are created by leveraging human-curated lexical databases such as WordNet and Open Multilingual Wordnet, and by applying defined phonetic distance thresholds to select word pairs. This curation process represents original content created from scratch by humans based on existing linguistic resources, specifically tailored to the study's requirements."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new data entirely by AI or machine learning models. The datasets used are derived or collated from existing resources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Sections 2.1 and 2.2",
          "reasoning": "The datasets such as LibriSpeech, Multilingual Spoken Words (MSW), Common Voice, WordNet and Open Multilingual Wordnet are existing datasets or databases collected from prior sources. The authors collated subsets and combined these existing data sources to facilitate their analysis (e.g., selecting dev-clean and test-clean subsets, intersecting languages supported by OMW and Epitran), but did not significantly modify their contents."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The novel dataset of near homophone word pairs is derived by applying phonetic distance calculations (using CMU pronouncing dictionary, Epitran, and Levenshtein distance) to existing words from existing datasets, setting thresholds (distance \u2264 0.4) to select pairs. This transformation from existing lexical resources constitutes a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and methods of generation for their novel dataset and experimental data."
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9166,
      "completion_tokens": 239,
      "total_tokens": 9405
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.1, 2.3 and Sections 3 and 4",
          "reasoning": "The paper introduces a novel dataset of near homophone and synonym word pairs specifically curated to analyze linguistic properties encoded in self-supervised speech models (S3Ms). The dataset is repeatedly used in similarity analyses and comparisons to understand phonetic versus semantic encoding, rather than for training or evaluation of models. The entire study revolves around analyzing trends and characteristics of representations using this dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9889,
      "completion_tokens": 611,
      "total_tokens": 10500
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper describes extracting synonyms using Open Multilingual Wordnet (OMW) covering multiple languages including English, Chinese, Italian, Spanish, Indonesian, Polish, and Swedish. For crosslingual analysis, word pairs are sampled such that one word is English and the other is from a different language among those listed, involving more than two human languages. Hence, the curated dataset includes entries across multiple languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "In the crosslingual analysis, the word pairs are constructed to contain exactly two languages: English and one other non-English language. Each pair has one English word and one word from another single language, making part of the dataset bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 and 3.3",
          "reasoning": "For the English analysis, the dataset uses English-only subsets of the LibriSpeech corpus and WordNet for synonyms, producing monolingual English word pairs."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not provide word pairs consisting of only one non-English language; non-English words only appear paired with English words in the crosslingual analysis."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of natural language word pairs and does not include any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are included in the curated dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human spoken word pairs and does not include any biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions no use of fictional or artificially constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly documented and specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of spoken words in human languages, so language content is present."
        }
      }
    }
  },
  {
    "id": "choi24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7107,
      "completion_tokens": 144,
      "total_tokens": 7251
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.1, Footnote 1",
          "reasoning": "The paper provides a footnote linking to a GitHub repository containing all the code for reproducibility: 'https://github.com/juice500ml/phonetic_semantic_probing'."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 and 2.2",
          "reasoning": "The paper details the dataset creation process in Section 2.1 (extracting synonyms and near homophones) and Section 2.2 (datasets used and how data is prepared), providing clear descriptions of data sources, processing methods, and sampling procedures."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9282,
      "completion_tokens": 182,
      "total_tokens": 9464
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset",
          "Reasoning": "The dataset is newly introduced in this paper consisting of audio samples (speech) from earnings calls by business analysts and CEOs. The speech was recorded from real conversation audio, which involves human speakers, and the audio is manually segmented into sentence-level samples using forced alignment tools. This indicates human generation via recording humans speaking."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset",
          "Reasoning": "The dataset contains human-generated manual transcripts of the audio speech samples, which are the gold standard transcriptions created by humans. Additionally, flattery annotations were done by human annotators with three expert annotators agreeing on labels, confirming human involvement in generating the textual annotations."
        }
      ]
    }
  },
  {
    "id": "christ24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10082,
      "completion_tokens": 264,
      "total_tokens": 10346
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2 Dataset",
            "reasoning": "The dataset was labeled by a team of three expert human annotators, as described in Section 2. The annotation involved span-level identification of flattery with agreement required among all three annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 Dataset",
            "reasoning": "The paper mentions that a detailed account of the annotation guidelines is provided in reference [25], indicating that detailed annotation instructions were developed and used by the annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2 Dataset",
            "reasoning": "The paper states that the annotation was based on a reliable, context-sensitive, content-analytical measure for detecting flattery (citing prior work [23,24]) and a detailed account of annotation guidelines in [25], which implies that the annotation followed a rubric or measurement scale to determine flattery."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2 Dataset",
            "reasoning": "The paper specifies that a detailed annotation guideline is provided in [25], which typically includes examples to guide annotators; although not explicitly stated in the main text, the detailed account referenced suggests examples were provided to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "christ24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11272,
      "completion_tokens": 297,
      "total_tokens": 11569
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that a team of three expert human annotators labeled the dataset, so quality assurance was not conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2, Dataset",
          "reasoning": "The dataset was labeled for instances of flattery by a team of three expert human annotators. All three annotators had to agree on an instance for it to be considered flattery, indicating a rigorous annotation and quality assurance process conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as expert human annotators, so non-expert single human QA does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as experts, so multiple non-expert annotators is not applicable."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of AI models used to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated rule-based or algorithmic quality assurance process is described for validating the annotations or dataset content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a detailed expert annotation process, so quality assurance is clearly documented."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10882,
      "completion_tokens": 428,
      "total_tokens": 11310
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of earnings call recordings and their transcripts from existing public sources, specifically Seeking Alpha. The labeling of flattery was done by expert annotators, but the underlying content was not created from scratch by human contributors for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset is generated entirely by AI or machine learning models; audio and transcripts are sourced from human speech and existing transcription."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is in English and there is no mention of translation from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset is collected from existing sources\u2014the earnings call transcripts and audio recordings from Seeking Alpha covering 2013-2018 earnings calls. The authors aggregated these existing data sources without creating new utterances."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The authors processed the existing data by splitting transcripts into sentences, labeling spans with flattery, projecting subsentential annotations to sentence-level labels, and segmenting audio into sentence-level samples using forced alignment. Thus, the dataset is derived from existing raw data via modifications and annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the data sources and the annotation process, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11400,
      "completion_tokens": 483,
      "total_tokens": 11883
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset being used exclusively for pre-training large models in an unsupervised or self-supervised way."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses fine-tuning pretrained models or training classifiers on features extracted from pretrained models; it does not describe training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2.2",
          "reasoning": "The paper describes fine-tuning pretrained models such as RoBERTa for text and Wav2Vec 2.0 or Whisper for audio on the introduced flattery detection dataset using supervised learning methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RL-based post-training methods using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Results)",
          "reasoning": "The dataset is used for evaluation and benchmarking of the flattery detection models on training, development, and test splits, as evidenced by the detailed reporting of results."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes some discussion and analysis of the dataset's speaker demographics and model performance by gender, these are secondary and the primary use of the dataset is for training and evaluation, not solely for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the dataset as a knowledge base to augment models such as retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used practically for supervised fine-tuning and evaluation as part of the machine learning pipeline."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12123,
      "completion_tokens": 484,
      "total_tokens": 12607
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced consists of speech and transcripts from US business analyst calls, with no mention of multiple languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two languages; all data is in English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset comprises transcripts and audio from earnings calls of US companies and is processed with English-language tools; no other languages are mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset explicitly uses English transcripts and speech; no non-English language data is indicated."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio speech samples and their transcripts, with no programming or code as content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of mathematical or logical notation as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech from humans only, with no biological or non-human communication systems included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English in the dataset description."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language in the form of spoken English and transcripts, so it is not applicable."
        }
      }
    }
  },
  {
    "id": "christ24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9341,
      "completion_tokens": 149,
      "total_tokens": 9490
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 6, Conclusion",
          "reasoning": "The paper explicitly states that the raw data cannot be published due to copyright restrictions and does not mention any link or availability of code related to the dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, Dataset",
          "reasoning": "The paper provides detailed information on the dataset collection: sourcing from earnings calls transcriptions by Seeking Alpha from 2013 to 2018, annotation method involving three expert annotators with unanimous agreement for flattery labeling, sentence-level framing for machine learning, and audio segmentation via Montreal Forced Aligner. Dataset statistics and partitioning details are also given in Section 2."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8501,
      "completion_tokens": 120,
      "total_tokens": 8621
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Dataset Description and Section 2.1 Data Collection",
          "Reasoning": "The MERLIon CCS Challenge dataset is a new dataset introduced by the authors, consisting of over 30 hours of Zoom video call audio recordings of parent-child shared book reading. The audio data captures spontaneous English-Mandarin code-switching child-directed speech recorded in home environments. This data was collected from human participants during video calls and manually annotated by multilingual transcribers."
        }
      ]
    }
  },
  {
    "id": "chua23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9301,
      "completion_tokens": 234,
      "total_tokens": 9535
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.3 Human Annotation",
            "reasoning": "The dataset was annotated by a team of multilingual transcribers who manually annotated each audio recording, with crosschecking by at least one senior member of the transcription team, indicating multiple human experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Human Annotation",
            "reasoning": "Detailed transcriber instructions are described, such as segmenting utterances based on intonation and pauses, handling code-switching subdivisions ('grains'), and marking boundaries carefully, indicating the presence of annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3 Human Annotation",
            "reasoning": "The paper describes detailed transcription protocol and instructions but does not mention any scoring rubrics or criteria used for annotation assessment or grading."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper references an in-house transcription protocol and conventions (citation [21]) but does not explicitly mention providing annotation examples within the paper itself."
          }
        }
      ]
    }
  },
  {
    "id": "chua23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10491,
      "completion_tokens": 393,
      "total_tokens": 10884
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single human annotator who is a subject matter expert performed the quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Human Annotation",
          "reasoning": "The dataset annotations were manually done by a team of multilingual transcribers using a high-fidelity linguistic transcription protocol. Additionally, each file has been crosschecked by at least one senior member of the transcription team. This indicates that multiple human experts were involved in the QA process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as multilingual transcribers using an in-house protocol and crosschecked by senior members, indicating expertise rather than a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting the annotators were non-experts; the protocol and crosschecking imply expert involvement."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance through an AI model is not described for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification of the annotations; QA is described as manual and human crosschecking."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented, involving multiple human experts and senior crosschecking."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10101,
      "completion_tokens": 404,
      "total_tokens": 10505
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The paper specifies that the MERLIon CCS Challenge dataset was collected as part of the Talk Together Study, where parents narrated wordless picture books to their children over Zoom. These are spontaneous spoken interactions recorded in home environments, and the audio recordings were annotated by multilingual human transcribers using a high-fidelity transcription protocol. The dataset thus consists of original audio content created entirely from scratch by human participants, without translation, adaptation, or derivation from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any audio data was generated by AI or machine learning models. All audio data is recorded human speech."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves translation from other languages by human translators. The content is spontaneous speech."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence of machine translation in dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not aggregated from existing sources but collected newly from human participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on existing sources with modifications; it is original from new recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and collection method of the dataset."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10619,
      "completion_tokens": 524,
      "total_tokens": 11143
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MERLIon CCS dataset for pre-training large models in an unsupervised or self-supervised way."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used to train models from scratch with randomly initialized parameters. In fact, baseline models are trained using other existing monolingual datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Results and Discussion)",
          "reasoning": "The paper discusses that large pre-trained models are fine-tuned on the MERLIon CCS dataset (around 30 hours) to adapt to code-switched child-directed speech and improve language identification performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of reinforcement learning post-training techniques (e.g., RLHF) using the dataset is present in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 (MERLIon CCS Challenge), Section 4 (Results and Discussion)",
          "reasoning": "The MERLIon CCS dataset is used as the evaluation benchmark for language identification and language diarization tasks in the challenge, with the Evaluation dataset being a representative subset used for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results and Discussion), Section 5 (Conclusion)",
          "reasoning": "The dataset is used to analyze the impact of short speech segments, local vernacular, rate of code-switching on model performance, and to understand challenges in language diarization, as well as for comparing model effectiveness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset being used as a knowledge base to augment models, like retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple practical uses of the dataset including fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11342,
      "completion_tokens": 562,
      "total_tokens": 11904
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The dataset involves exactly two human languages: English and Mandarin. There are no mentions of more than two languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The MERLIon CCS Challenge dataset contains English-Mandarin code-switched speech. It features spontaneous English and Mandarin utterances and code-switching between these two languages, indicating exactly two human languages are present."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "Only a small subset of recordings (20%) feature one language throughout, but these are not exclusively English; the dataset as a whole is bilingual."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "While some recordings feature only Mandarin, the dataset as a whole is bilingual English-Mandarin; the dataset is not monolingual non-English overall."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset contains speech audio data of human languages only; no programming or structured code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no indication the dataset includes mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset consists of human speech recordings; no biological sequences or non-human communication is involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No constructed or fictional languages are described as part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The languages in the dataset are clearly documented as English and Mandarin; there is no ambiguity or unspecified languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language audio data; it is not without language."
        }
      }
    }
  },
  {
    "id": "chua23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8560,
      "completion_tokens": 140,
      "total_tokens": 8700
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The paper mentions data collection as part of the Talk Together Study and details consent and IRB approval, but does not provide any link or reference to publicly available code for data collection, preprocessing, or dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Dataset Description, including 2.1 Data Collection and 2.3 Human Annotation",
          "reasoning": "The paper provides detailed descriptions of the dataset creation including data collection procedures, languages and accents, and manual human annotation protocol, indicating comprehensive documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7672,
      "completion_tokens": 109,
      "total_tokens": 7781
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2.1 and Table 1",
          "Reasoning": "The Fleurs dataset is described as a new general-purpose massively multilingual evaluation dataset in 102 languages, used for speech recognition, classification, and retrieval tasks. It consists of real recorded speech data (read speech) as indicated by the description of domains and the nature of the tasks, implying that the audio is human-generated speech recordings."
        }
      ]
    }
  },
  {
    "id": "conneau22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8472,
      "completion_tokens": 407,
      "total_tokens": 8879
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2, Section 3.2.3, Table 1",
            "reasoning": "The new dataset introduced in this paper is Fleurs, described as a new general-purpose massively multilingual evaluation dataset in 102 languages (Section 3.1 and 3.2). The Fleurs dataset covers multiple tasks: speech recognition, speech classification (LangID), and speech retrieval. Given the nature of the dataset (speech data with language identification and transcription labels across 102 languages) and its introduction as a new multilingual speech understanding evaluation dataset, it is highly plausible that the annotations (transcriptions, language identification, retrieval labels) were generated by multiple human experts or linguists familiar with those languages to ensure quality and accuracy."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2, Section 3.2.3",
            "reasoning": "The paper describes the use and creation of Fleurs for multiple tasks including speech recognition, LangID, and retrieval, with mention of task design and careful selection of languages and splits. While explicit detailed instructions are not exhaustively documented in the paper, the mention of task design, splits, and evaluation methodology implies that annotation instructions existed for annotators to follow to ensure data quality and consistency."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not explicitly describe scoring rubrics or detailed annotation guidelines with rubrics for the human annotators of Fleurs or other datasets. The evaluation metrics are described (e.g., CER, accuracy) but these are task evaluation metrics, not annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not contain or reference any annotation examples or examples of annotated data for the Fleurs dataset or other new datasets. No examples are described or included in the main paper or appendix sections."
          }
        }
      ]
    }
  },
  {
    "id": "conneau22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9662,
      "completion_tokens": 395,
      "total_tokens": 10057
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert for any of the newly introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit information or description in the paper about quality assurance performed by multiple expert human annotators on the newly introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any single non-expert human annotator conducting quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are provided regarding multiple non-expert human annotators performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models for quality assurance or as a judge on dataset quality or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence of automated verification processes for quality assurance of dataset annotations or content is provided in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper presents several datasets in the XTREME-S benchmark including the new Fleurs dataset but does not document any explicit quality assurance process, human or automated, applied to these new datasets. There is no mention of QA procedures or annotation validation. Therefore, no discernible QA process is described or applied."
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9272,
      "completion_tokens": 501,
      "total_tokens": 9773
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2.1 and Section 3.2.3",
          "reasoning": "The paper introduces Fleurs, described in Section 3.2.1 and referenced in Section 3.2.3 as having original read-speech data in 102 languages. The paper states that Fleurs is a \"new general-purpose massively multilingual evaluation dataset dubbed Fleurs\" and that it includes data with train/dev/test splits. This implies that the data was generated freshly with human speakers producing speech in these languages, constituting new data created entirely by human contributors, not translations or adaptations of pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset was generated entirely by AI or machine learning models without relying on existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No explicit mention is made of data created by human translations from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper supports that any dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 and Table 1",
          "reasoning": "The XTREME-S benchmark incorporates several pre-existing datasets, such as MLS, VoxPopuli, CoVoST-2, Minds-14, and Fleurs (new but possibly involving some aggregation). The benchmark collects these datasets without significant modification to create a comprehensive evaluation suite. Therefore, the benchmark's overall data can be considered collated from existing publicly available sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While datasets are combined into a benchmark, there is no evidence that the authors significantly modify or transform existing datasets to produce new derived datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources are specified and documented; the paper provides clear references for all datasets included."
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9790,
      "completion_tokens": 315,
      "total_tokens": 10105
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Baselines; Section 4.2 Speech recognition; Section 4.3 Speech translation; Section 4.4 Speech classification",
          "reasoning": "The new dataset Fleurs introduced in the paper is used for supervised fine-tuning of pre-trained models for various downstream tasks such as speech recognition, classification (LangID), and retrieval. Specifically, models like XLS-R and mSLAM are fine-tuned on the Fleurs dataset to evaluate performance, indicating supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 Summary and Table 1; Section 4 Results; Section 5 Discussion on test sets",
          "reasoning": "Fleurs is used as a benchmark evaluation dataset across multiple speech tasks including ASR, LangID, and retrieval. The benchmark is designed for measuring and comparing model performance in a standardized way, indicating exclusive usage for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10513,
      "completion_tokens": 596,
      "total_tokens": 11109
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Section 3, Section 3.2.3, Table 1",
          "reasoning": "The paper introduces XTREME-S, a new benchmark and dataset collection, including the newly created Fleurs dataset covering 102 languages from more than 10 language families (Abstract, Section 3). The dataset entries include speech recognition, speech classification (LangID), and speech retrieval tasks that cover these 102 languages, supporting a multilingual dataset. Minds-14 is also a multilingual dataset with 14 languages in the e-banking domain. These indicate the dataset is multilingual with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced involve multiple languages (102 in Fleurs, 14 in Minds-14, 21 language pairs in CoVoST-2), not exactly two languages. Thus, bilingual label does not apply."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced contain multiple languages, not only English. No evidence suggests a dataset with only English entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced datasets such as Fleurs and Minds-14 cover multiple languages; there is no mention of any new dataset with exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with programming or structured code-related content are introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets containing mathematical or logical symbolic notation are introduced."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are all human speech and text related; no biological or non-human communications."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence that datasets include constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All datasets introduced have clearly specified languages, e.g., Fleurs with 102 languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Datasets introduced contain human language speech data, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "conneau22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7731,
      "completion_tokens": 197,
      "total_tokens": 7928
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 and elsewhere",
          "reasoning": "The paper discusses the introduction of the XTREME-S benchmark and the new dataset Fleurs, mentioning that datasets and fine-tuning scripts are made accessible via the HuggingFace platform. However, there is no explicit mention or link to publicly available code for constructing or preprocessing the datasets, nor detailed code repositories described for dataset creation or preprocessing. Hence, code for dataset construction does not appear to be provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 and Table 1",
          "reasoning": "The paper provides detailed descriptions of the datasets included in XTREME-S, particularly Fleurs, including size, languages covered, split details, task families, and design principles for dataset and task selection. It includes information on language coverage, domains, and data regimes. Thus, the dataset creation process and its characteristics are well documented within the paper."
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7297,
      "completion_tokens": 105,
      "total_tokens": 7402
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The paper states that they collected a crowdsourced audio dataset for COVID-19 detection via a mobile app which collects three types of audio recordings (cough, breathing, speech). This data is newly collected and thus newly introduced by the authors. This data is human generated since it originates from audio recorded by human participants using a mobile application."
        }
      ]
    }
  },
  {
    "id": "dang22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8097,
      "completion_tokens": 268,
      "total_tokens": 8365
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1 Data",
            "reasoning": "The paper states that the crowdsourced audio dataset was collected via a mobile app, involving 1000 participants providing labeled samples and 2381 participants providing unlabeled samples. There is no mention of experts labeling the data; it is implied that labels come from participant self-reporting or crowdsourcing, hence annotators are multiple human non-experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention any specific annotation instructions provided for labeling the new crowdsourced audio dataset; labels are presumably user self-reported test results and symptom reports rather than annotated by instructed annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No mention of any scoring rubrics for data annotation is present in the paper for the new dataset; the labels correspond to COVID-19 test results and symptom reports, not requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide any annotation examples or guidelines that help annotators label the data; since labeling is based on self-reported test results and symptoms, examples are not applicable."
          }
        }
      ]
    }
  },
  {
    "id": "dang22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9287,
      "completion_tokens": 296,
      "total_tokens": 9583
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any indication that multiple human experts conducted quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information describing quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2 Settings",
          "reasoning": "The paper reports that all audio recordings were automatically checked using YAMNet [21] to remove unqualified ones such as those with noisy background. This indicates that an AI model (YAMNet) was used as an automated quality assurance judge to verify the audio data quality before use."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification through code or formulas on the dataset annotations is described in the paper, beyond the AI model check for audio quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset underwent an automated quality assurance step with YAMNet, so QA is documented."
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8897,
      "completion_tokens": 360,
      "total_tokens": 9257
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The dataset used is a crowdsourced audio database collected via the authors' mobile app, comprising cough, breathing, and speech recordings along with participants' demographics and clinical data. This indicates original content created from scratch by human contributors providing recordings specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated purely by AI or machine learning models. All data described are real human audio recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data involving translation from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data involving machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not described as collected or aggregated from existing sources; rather, it is newly collected from volunteers using their app."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention that the data is modified or adapted from existing datasets or sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as collected via the authors' app from volunteers."
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9415,
      "completion_tokens": 382,
      "total_tokens": 9797
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1.1, Section 4.2",
          "reasoning": "The labelled subset of 1486 audio samples is used to train the supervised model from scratch, as stated in section 3.1.1 where the supervised model f is developed and optimized using cross-entropy loss, and section 4.2 mentions model training details with tuned parameters starting from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is used to train models but not specifically stated as fine-tuning of pre-trained models with supervised methods on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of reinforcement learning or RL-based post-training techniques applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 5",
          "reasoning": "The test partition of the labelled dataset is explicitly used for evaluation of model performance (e.g., ROC-AUC, sensitivity, specificity) as described in Section 4.1 and results are discussed in Section 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.4",
          "reasoning": "The dataset is also used for analysis such as latent space visualization using t-SNE to analyse model generalisation and clustering of positive and negative samples as shown in Section 5.4."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset serves as a knowledge base or is used to augment models through retrieval."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10138,
      "completion_tokens": 544,
      "total_tokens": 10682
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data and Section 1 Abstract",
          "reasoning": "The dataset consists of audio recordings collected via a mobile app as described in Section 4.1 and the Abstract. The paper does not mention multiple languages or bilingual content in the audio recordings. Given the project context from the University of Cambridge and the mobile app, and absence of any language diversity mention, the dataset is best characterized as containing English content only. Hence, it is considered monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper mentions code implementation (Tensorflow) and model structure, these pertain to methodology and not the dataset entries themselves. The dataset entries are audio recordings, so it does not contain code/programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 3.1 FixMatch for COVID-19 detection",
          "reasoning": "Mathematical notation is present in description of methods, but the dataset entries themselves do not contain mathematical or logical symbolic data. The notation is part of the methodology explanation, not dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are human audio recordings (cough, breathing, speech). There is no indication of biological sequences (like DNA) or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the audio recordings contain any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset entries are clearly human audio recordings, implying language content exists."
        }
      }
    }
  },
  {
    "id": "dang22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7356,
      "completion_tokens": 202,
      "total_tokens": 7558
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.2 Settings",
          "reasoning": "The paper states in Section 4.2 that 'The code was implemented using Tensorflow [23], and can be found here.' Although the exact URL is not included in the excerpt, this explicit mention indicates that the code associated with the dataset and model implementation is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The paper provides a detailed description of the dataset creation in Section 4.1. It explains that the dataset is collected via a crowdsourced mobile app, outlines the types of audio recordings collected (cough, breathing, speech), and demographic and symptom information gathered along with COVID-19 test results. They specify the subset used, and how data was partitioned for training, validation, and testing, including data cleaning steps using YAMNet. This constitutes documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8023,
      "completion_tokens": 118,
      "total_tokens": 8141
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 (Dataset creation), Table 2, and Section 5 (Conclusion)",
          "Reasoning": "The paper describes creating a new dataset by manually adding contextual disfluencies into the existing fluent Vietnamese intent detection and slot filling dataset PhoATIS. This was done by 7 annotators who were undergraduate students, indicating human generation. The dataset consists of written Vietnamese text utterances annotated with disfluency labels along with intent and slot labels."
        }
      ]
    }
  },
  {
    "id": "dao22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8823,
      "completion_tokens": 269,
      "total_tokens": 9092
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 (Data Annotation Process)",
            "reasoning": "The paper states that 7 annotators who are undergraduate students strong in linguistics manually added contextual disfluencies by annotating subsets of the training data. Being undergraduate students, they are considered multiple human non-experts rather than experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 (Data Annotation Process)",
            "reasoning": "Annotators were provided explicit instructions to create disfluent versions that are semantically equivalent, natural, contain corrected disfluent words, and include both Reparandum and Interragnum disfluency types. They were also shown example disfluencies as illustrated in Table 1, indicating detailed instructions were given."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention any scoring rubrics or criteria for annotation beyond the qualitative instructions and examples, nor any formal rubric to evaluate annotation quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2 and Table 1",
            "reasoning": "The paper explicitly states that annotators were shown example disfluencies as illustrated in Table 1, demonstrating that annotation examples were indeed provided."
          }
        }
      ]
    }
  },
  {
    "id": "dao22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10013,
      "completion_tokens": 336,
      "total_tokens": 10349
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although multiple annotators were used, they are described as undergraduate students strong in linguistics and not necessarily as subject matter experts or members of the target demographic. Hence, they are not considered experts for quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed solely by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset construction)",
          "reasoning": "The dataset construction involved 7 annotators who were undergraduate students strong in linguistics (i.e., non-experts) each annotating a subset of the data by manually adding disfluencies and annotating them. This manual annotation process by multiple non-expert humans qualifies as human non-expert quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that quality assurance was conducted through automated verification or algorithmic/rule-based checks of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the quality assurance process with multiple human annotators and manual review, so it is not the case that no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9623,
      "completion_tokens": 427,
      "total_tokens": 10050
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The paper describes manually adding contextual disfluencies into the existing fluent Vietnamese dataset PhoATIS by 7 human annotators who generated disfluent versions of original utterances. This manual annotation process was performed to create a new dataset with disfluency annotations, thus constituting original content created by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset content was generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not produced by translating content from another language through human translators. Although English translations are provided for illustration, the core dataset and its disfluent variant were created in Vietnamese."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that machine translation was used to generate the dataset content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not simply collected or aggregated from existing sources without significant modification; rather, it involves manual modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The new dataset is based on the existing fluent PhoATIS dataset by manually adding disfluent words and annotating them. This constitutes a derived dataset, as it modifies and extends an existing resource."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of data generation are well documented and clearly stated."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10141,
      "completion_tokens": 568,
      "total_tokens": 10709
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for any kind of unsupervised pre-training or self-supervised learning as per the paper. Instead, it is manually annotated for disfluency detection and downstream intent detection and slot filling tasks."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is used to train models from randomly initialized parameters from scratch. The models leveraged are pre-trained language models that are fine-tuned downstream."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Implementation details; Section 4 Experimental results",
          "reasoning": "The paper describes the use of the newly created disfluent Vietnamese intent detection and slot filling dataset to fine-tune pre-trained language models (XLM-R and PhoBERT) using supervised learning methods. They fine-tune a disfluency detection model on the disfluent utterances and fine-tune a joint intent detection and slot filling model on the fluent PhoATIS dataset. This use of supervised fine-tuning for downstream models is explicitly applied."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or usage of reinforcement learning post-training techniques such as RLHF is found in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experimental results",
          "reasoning": "The dataset is used for performance measurement of the disfluency detection model and downstream intent detection and slot filling models. Results in Table 3 report test set metrics, demonstrating the dataset's role in evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Error Analysis",
          "reasoning": "The authors employ the dataset to analyze error types and the impact of disfluencies on downstream tasks, providing detailed error statistics and qualitative analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or to augment models via retrieval-augmented generation or similar mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple practical usages of the dataset in supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10864,
      "completion_tokens": 531,
      "total_tokens": 11395
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset introduced is for Vietnamese intent detection and slot filling with added disfluencies, with no mention of more than two languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries only involve Vietnamese language, and no indication of exactly two human languages used."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is based on Vietnamese language utterances; though English translations are shown for illustration, the dataset entries themselves are in Vietnamese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction and Section 2 Dataset Creation",
          "reasoning": "The proposed dataset is an extension of the Vietnamese PhoATIS dataset with added disfluency annotations, fully composed of Vietnamese language utterances as explicitly described throughout the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of spoken language utterances in Vietnamese; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no presence of mathematical or formal logical expressions in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is entirely based on Vietnamese human language utterances with no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No fictional or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is explicitly stated and documented as Vietnamese; hence the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of natural language utterances and therefore contains language."
        }
      }
    }
  },
  {
    "id": "dao22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8082,
      "completion_tokens": 225,
      "total_tokens": 8307
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Introduction; Dataset creation section; Conclusion",
          "reasoning": "The paper explicitly states: 'We publicly release our dataset with disfluency annotations to facilitate future Vietnamese SLU research and applications. Our dataset is available at https://github.com/VinAIResearch/PhoATIS_Disfluency.' While the link indicates data availability, the paper title the repository as dataset and implies the data is released, it also includes dataset creation process and mentioned code implementations with PyTorch and transformers, suggesting code (likely including preprocessing or creation scripts) are accessible at the provided repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Dataset Creation and Annotation; Paragraphs describing manual annotation and quality control",
          "reasoning": "The paper provides a detailed description of the dataset creation process including how disfluencies were manually added by annotators, the criteria for disfluency inclusion, the annotation process at syllable level, validation and revision steps by authors, and statistics of the created dataset (Table 2). This is extensive documentation about dataset creation."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5803,
      "completion_tokens": 94,
      "total_tokens": 5897
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2",
          "Reasoning": "The authors collected new speech data from literate female speakers of the Lambani and Soliga tribal languages using a software tool in a soundproof studio-quality environment. This data consists of recorded audio speech utterances in these languages, explicitly stated as newly collected dataset for their low-resource TTS work."
        }
      ]
    }
  },
  {
    "id": "dasare23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6603,
      "completion_tokens": 235,
      "total_tokens": 6838
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2",
            "reasoning": "The dataset for Lambani and Soliga tribal languages is collected by literate female speakers from the respective tribes who appear to be subject matter experts or part of the target demographic. The collection process involved skilled individuals from the community reading and recording the sentences."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "The data collection tool displays one sentence at a time, plays a voice-over for illiterate speakers, and provides an option to replay and rerecord sentences for correctness and intelligibility. This implies detailed instructions were provided to annotators (speakers) to ensure quality recordings."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No mention or description of scoring rubrics or formal scoring criteria for the annotation (recording) process is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not provide or mention any examples as part of the annotation guidelines or instructions for data collection."
          }
        }
      ]
    }
  },
  {
    "id": "dasare23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7793,
      "completion_tokens": 353,
      "total_tokens": 8146
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2: Dataset Details",
          "reasoning": "The data collection was performed with literate female speakers from the Lambani and Soliga communities, who are members of the target demographic. They have good diction and voice, indicating they can be considered experts in the language for recording purposes. This shows the quality assurance was conducted by a single human expert annotator (speaker) for each language."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes only single human speakers for each language dataset recording; there is no mention of multiple human experts involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any non-expert single human annotator performed the QA; rather, the speaker chosen was literate with good diction and a member of the target demographic."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of multiple non-expert humans performing QA is present."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models were used for TTS generation and evaluation, no AI model was used to perform quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of automated verification or QA of the dataset besides code or formula validation; dataset collection does not appear to have automated QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper does describe a QA process through the selection of speakers with good diction and recording environment, so it is not true that no QA was applied."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7403,
      "completion_tokens": 457,
      "total_tokens": 7860
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Details)",
          "reasoning": "The dataset consists of newly recorded speech data in the Lambani and Soliga tribal languages. Literate female speakers from the respective tribes recorded original spoken sentences in a soundproof studio environment producing 8 hours of Lambani and 6 hours of Soliga voice samples. This data is original and created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset was generated entirely by AI or machine learning models. The data generated by models is synthetic speech, which is not considered a dataset but model output."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Details)",
          "reasoning": "The original transcript sentences were English sentences translated into Kannada and then into Soliga and Lambani languages by literate people from the respective tribes, indicating human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper about the use of machine translation systems for data creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being collected or aggregated from existing sources without significant modification. Instead, it was created through original recordings and translations."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Details)",
          "reasoning": "The dataset is derived in the sense that the English source sentences (from Swadesh list) were translated into Kannada, then into Soliga and Lambani, thus existing content was adapted and transformed into new languages before recording speech samples."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data creation method is explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7921,
      "completion_tokens": 441,
      "total_tokens": 8362
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not used to train a model from randomly initialized parameters. The TTS models were fine-tuned from pre-trained models, not trained from scratch (Section 3)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The Lambani and Soliga datasets are used to fine-tune Nvidia's pre-trained Tacotron2 model with transfer learning, as described in Section 3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning based post-training methods such as RLHF using the proposed datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The datasets are also used for subjective (MOS) and objective (PESQ) evaluation of the TTS models by comparing original and synthetic speech (Section 3.1)."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The datasets are used primarily for analyzing speech characteristics like LP residual, F0 contour, formants to study speech patterns and quality (Section 4)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not serve as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets have documented usages in fine-tuning, evaluation, and analysis as described above."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8644,
      "completion_tokens": 489,
      "total_tokens": 9133
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2: Dataset Details",
          "reasoning": "The dataset is based on translations originating from English sentences which are translated into Kannada and then further into two distinct tribal languages, Lambani and Soliga. Thus, the dataset effectively involves more than two human languages: English, Kannada, Lambani, and Soliga."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not restricted to exactly two human languages; rather, it involves multiple languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes languages beyond English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than one non-English language (Lambani and Soliga) along with English and Kannada used in preparation."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken language recordings and translations, without any programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are included as dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes natural tribal languages (Lambani and Soliga), not constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included in the dataset are explicitly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language data."
        }
      }
    }
  },
  {
    "id": "dasare23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5862,
      "completion_tokens": 154,
      "total_tokens": 6016
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section",
          "reasoning": "The paper does not mention any link, repository, or location where code related to data collection or dataset construction for the tribal languages is made available. It only describes the data collection process and TTS system training but provides no indication that the code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Dataset Details)",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 2, including the source of sentences, translation procedure, recording environment, speaker details, and consent process. This documentation covers essential aspects of dataset construction, making it relatively transparent and reproducible in terms of data collection methodology."
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7673,
      "completion_tokens": 140,
      "total_tokens": 7813
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 and 5.2",
          "Reasoning": "The paper introduces a new dataset generated by their proposed finite-state Markov models of turn-taking behavior. The dataset consists of an activity matrix Y, representing speaker activity over time, generated by sampling from the fitted turn-taking models. Utterances from LibriSpeech are then aligned to these generated timings to create mixtures. This simulation process is algorithmic, producing time series data representing speech activity without recorded human audio, thus it is model-generated synthetic data representing temporal turn-taking behavior in conversations."
        }
      ]
    }
  },
  {
    "id": "deadman22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8473,
      "completion_tokens": 259,
      "total_tokens": 8732
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 and 5.2",
            "reasoning": "The paper describes training models (Markov models) by counting transitions from real party transcripts (CHiME-5 and AMI datasets) and generating new activity matrices Y through sampling from these models. This process is a deterministic generative simulation method, not involving human annotators, thus an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide or mention detailed annotation instructions or guidelines for human annotators related to the creation of the new dataset; the dataset is generated automatically by models trained on existing annotated corpora."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria provided to annotators for manual annotation in the creation of the new datasets since no human annotation is involved in the data generation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples of annotation tasks or guidelines are given; the new data is generated synthetically by sampling from models trained on existing datasets, therefore no annotation examples exist for this process."
          }
        }
      ]
    }
  },
  {
    "id": "deadman22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9663,
      "completion_tokens": 328,
      "total_tokens": 9991
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the new dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the newly generated datasets or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information suggesting that a single human non-expert performed quality assurance on the new datasets or annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance done by multiple human non-experts on the introduced simulated datasets or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model as a judge for quality assurance of the new simulated datasets or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset is generated automatically using Markov or semi-Markov models from existing transcripts, the paper does not describe any explicit automated quality assurance or verification processes conducted on the generated datasets or their annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new datasets generated via finite-state based generative models trained on timing information, but does not describe any quality assurance process performed on these generated datasets. There is no documentation or mention of human or automated QA procedures validating the data or annotations, hence no quality assurance process is considered present."
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9273,
      "completion_tokens": 513,
      "total_tokens": 9786
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the creation of any new dataset collected entirely from scratch by human contributors. Instead, it relies on existing datasets such as CHiME-5 and AMI for obtaining conversation transcripts and timing information; no original human-created conversation data is introduced."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.3, Section 5.2",
          "reasoning": "The authors generate new multispeaker conversation timing data by sampling from probabilistic finite-state models trained on existing conversation timing data. These generated activity matrices Y simulate turn-taking behaviour with realistic overlap statistics, allowing for arbitrary large datasets to be created. This generated data is entirely produced by the proposed generative Markov models, thus qualifying as new data from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data being translated by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or mention any use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The paper uses existing datasets (CHiME-5, AMI) to obtain transcripts and timing information, which are then used without modification to train models and create representations. This component of the data is aggregated from these existing sources without fundamental modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The generated activity matrices Y are derived by applying the finite-state generative models trained on existing datasets. Moreover, to create audio mixtures, the authors adapt LibriSpeech utterances to match modeled turn durations, which constitutes modifications and transformations of existing data, thus making the generated dataset derived from existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and generation methods of the introduced data, so the data origin is known."
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9791,
      "completion_tokens": 299,
      "total_tokens": 10090
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 and 5.3",
          "reasoning": "The generated datasets from the proposed finite-state Markov models are used to evaluate target-speaker extraction models by creating mixtures that reflect realistic turn-taking patterns. This is elaborated in Sections 5.2 Data Generation and 5.3 Results, where the datasets help measure the performance (SI-SDR) of extraction systems under realistic conversational overlap conditions."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and 5.3",
          "reasoning": "The datasets generated from the proposed models are used primarily to analyze turn-taking behavior and its effect on speaker extraction difficulty. Section 4 describes extracting interpretable vector representations from the models for characterizing parties, and Section 5.3 shows how these representations correlate with speaker extraction performance, highlighting analytical use of the datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10514,
      "completion_tokens": 562,
      "total_tokens": 11076
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the introduction of any dataset containing multiple human languages. The datasets discussed (e.g., CHiME-5, AMI, LibriSpeech) are all primarily English or not specified to include multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the new dataset introduced includes exactly two human languages. The focus is on English conversational speech."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.2 Data Generation",
          "reasoning": "The new simulated dataset is generated by aligning turn-taking behavior learned from datasets CHiME-5 and AMI with utterances from LibriSpeech (dev-clean), which is English speech. Therefore, the dataset is monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is generated using English speech from LibriSpeech; no non-English language is introduced or described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper presents mathematical formulations and discusses models, the dataset entries themselves do not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical notation is used to describe models and methods but the dataset generated does not contain such notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pertains to human speech turn-taking and includes no biological or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no fictional or artificially created languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly states the source language is English from LibriSpeech; therefore, the language is specified, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain English language speech content, so the dataset does contain language."
        }
      }
    }
  },
  {
    "id": "deadman22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7732,
      "completion_tokens": 149,
      "total_tokens": 7881
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code related to the dataset construction or the generative model for turn-taking simulation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 5.2",
          "reasoning": "Sections 3 (Framework for Modelling Turns) and 5.2 (Data Generation) describe the process of creating the simulated datasets, including the training of finite-state models on real transcript data and generation of synthetic activity matrices with LibriSpeech utterances. The process is explained with formulas and methodology, providing sufficient documentation to understand the dataset creation process."
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6163,
      "completion_tokens": 142,
      "total_tokens": 6305
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Experimental Setup)",
          "Reasoning": "The new dataset is a prepared Bengali speech corpus consisting of recorded utterances by 2 male and 2 female native speakers recorded in a soundproof room, explicitly described as audio speech data collected from human participants."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Experimental Setup)",
          "Reasoning": "The dataset includes transcriptions of the Bengali speech utterances which are text data, created by human transcription aligned with the recorded audio samples."
        }
      ]
    }
  },
  {
    "id": "deb23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6963,
      "completion_tokens": 210,
      "total_tokens": 7173
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3",
            "reasoning": "Section 3 describes data preparation involving four native Bengali speakers (2 male and 2 female) who recorded the utterances after comprehending the meaning, indicating multiple human experts performed the annotation by producing the labeled speech corpus."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "Section 3 states that participants were instructed to carefully read each sentence and comprehend their meanings before recording, indicating the presence of instructions for the annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "There is no mention in Section 3 or elsewhere about any scoring rubrics or detailed annotation scoring criteria used during data preparation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not provide any examples of annotated data or annotation examples in the guidelines or instructions discussed in Section 3 or other sections."
          }
        }
      ]
    }
  },
  {
    "id": "deb23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8153,
      "completion_tokens": 360,
      "total_tokens": 8513
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human expert performing quality assurance of the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or description in the paper of multiple human experts conducting QA on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA performed by a single human annotator without subject matter expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any QA performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for classification tasks, there is no indication that AI models have been used to perform quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based process used for QA of annotations or data."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper provides no description or mention of any quality assurance process conducted on the newly prepared Bengali speech acts dataset. There are no details on annotation validation, verification, or cross-checking by humans or machines."
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7763,
      "completion_tokens": 417,
      "total_tokens": 8180
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Experimental Setup",
          "reasoning": "The authors explicitly mention preparing a dataset of 85 utterances in Bengali, recorded by four native Bengali speakers in a soundproof room. These utterances were created for the purpose of classifying speech acts (request, question, order) and were recorded specifically for this study. This constitutes original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any part of the dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data was produced by human translators translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 4.2 Opus-MT",
          "reasoning": "The text transcripts in Bengali were translated to English using the Opus-MT model, a machine translation system based on the Marian-NMT transformer architecture. This English translation text input was used as an additional modality for classification, implying machine-translated data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was newly recorded and not merely collected or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is original and not based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is clearly documented."
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8281,
      "completion_tokens": 252,
      "total_tokens": 8533
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 Experimental Setup and Section 4 Methodology",
          "reasoning": "The paper describes using the newly prepared Bengali speech acts dataset to fine-tune pre-trained models such as wav2vec2.0 and Marian-NMT for supervised classification of speech acts. Specifically, in Section 3, the dataset preparation is described and in Section 4, the authors discuss fine-tuning these pre-trained models (wav2vec2.0 and Opus-MT) using the dataset with labeled samples to perform supervised learning for speech act classification."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9004,
      "completion_tokens": 508,
      "total_tokens": 9512
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Setup), Section 4.2 (Opus-MT), and throughout",
          "reasoning": "The prepared dataset consists of Bengali speech utterances, where each utterance is associated with a Bengali audio and its transcription, which is further translated into English text. The dataset thus effectively includes two languages: Bengali (the original speech language) and English (the translated text). The Bengali speech corpus entries are paired with English translational text, allowing bilingual content per entry."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken Bengali utterances and their textual translations; no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical symbolic notation is present in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed exclusively of human spoken languages; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural languages Bengali and English only; no constructed or fictional languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are clearly documented and identified as Bengali (spoken) and English (translated text)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human linguistic content in Bengali and English; therefore, it is not applicable to consider as containing no language."
        }
      }
    }
  },
  {
    "id": "deb23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6222,
      "completion_tokens": 174,
      "total_tokens": 6396
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link in the paper provides code or repository information for dataset construction or processing.",
          "reasoning": "The paper mentions a project page URL (https://soumitri2001.github.io/BeAts) but does not explicitly state that the code for dataset construction or data preprocessing is available there or elsewhere. No direct reference to dataset code availability is made in the paper text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Experimental Setup)",
          "reasoning": "The paper provides a description of the dataset preparation in Section 3, including details on the number of utterances per class, speaker demographics, recording conditions, data augmentation strategy, and preprocessing (standard amplitude normalization). This constitutes documentation of the dataset creation process necessary to understand and potentially reproduce the dataset."
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5394,
      "completion_tokens": 214,
      "total_tokens": 5608
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.4",
          "Reasoning": "The dataset consists of speech recordings of 20 human participants recalling self-defining autobiographic memories. These audio recordings were captured directly from human speakers as they orally recounted memories, as described in the methods section."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The speech recordings were manually transcribed into orthographic text by the authors. These transcriptions form a new text dataset derived from the participants' spoken memories."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "Memories were coded a posteriori based on video recordings of participants during the experiment. The video data were captured from human participants and used for coding the self-defining memories."
        }
      ]
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6194,
      "completion_tokens": 334,
      "total_tokens": 6528
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Coding of SDM",
            "reasoning": "The SDM were coded a posteriori based on video recordings independently by three investigators, indicating multiple human experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Coding of SDM",
            "reasoning": "The coding was performed using Singer and Blagov's classification system and scoring manual for self-defining autobiographical memories, indicating the presence of detailed annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Coding of SDM",
            "reasoning": "Use of a classification system and scoring manual implies scoring rubrics were provided for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention providing annotation examples in the guidelines for SDM coding."
          }
        },
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.3 Content analysis",
            "reasoning": "The EMOTAIX content analysis software results were manually verified by two experts, indicating manual annotation by multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper mentions using EMOTAIX software and manual verification but does not describe specific annotation instructions provided for manual verification."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of scoring rubrics related to manual verification of EMOTAIX content analysis."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of annotation examples provided for this content analysis process."
          }
        }
      ]
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7384,
      "completion_tokens": 498,
      "total_tokens": 7882
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions that three independent investigators coded the SDM based on a classification system. However, there is no explicit statement indicating that these investigators are subject matter experts or representatives of the target demographic involved in quality assurance. Therefore, we cannot confirm quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Coding of SDM",
          "reasoning": "The paper states that the SDM were coded a posteriori by three independent investigators using an established classification system, yielding substantial interrater agreement (Cohen's kappa scores reported). Although the paper does not explicitly state the investigators are subject matter experts, their role in coding and use of an established manual implies expertise. This qualifies as quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes multiple annotators but they appear to be experts or trained in the coding manual; there is no indication that multiple non-experts were used for quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of an AI model performing quality assurance on annotations or dataset content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Content analysis",
          "reasoning": "The emotional lexicon analysis was performed using the EMOTAIX-Tropes text analysis software which automatically detects and counts emotional items. Additionally, the software classification was manually verified by two experts, indicating an automated process used as part of quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple forms of quality assurance, including human expert annotation and automated software verification; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 6994,
      "completion_tokens": 437,
      "total_tokens": 7431
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.3",
          "reasoning": "The dataset consists of recordings of 20 human participants aged 25 to 35 recounting five successive self-defining autobiographical memories each. These recordings, including speech production, were collected directly from human subjects specifically for this study, constituting original data created entirely by human contributors. The memories and speech are original, not derived, translated, or generated by any model."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by any AI or machine learning model; all speech data originates from human participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from pre-existing datasets; it was recorded anew for this study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 and 2.4",
          "reasoning": "Derived data involves features extracted from the original speech recordings, such as acoustic parameters (MeanF0, MedianF0, SDF0, etc.) and emotional lexicon analysis with EMOTAIX performed on manually transcribed texts. These transformations and analyses constitute derived data based on the original human-generated recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7512,
      "completion_tokens": 266,
      "total_tokens": 7778
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Entire paper, especially Sections 2 and 3",
          "reasoning": "The newly introduced dataset consisting of 99 self-defining memories recorded from 20 participants is used primarily for analyzing vocal and emotional patterns in naturalistic speech. The dataset is manually transcribed, annotated for emotional content using EMOTAIX, and acoustic features are extracted to study relationships between speech parameters and emotional properties. There is no mention of using this dataset for training, fine-tuning, evaluation, or serving as a knowledge base; rather, it serves as the empirical foundation for analyzing trends and characteristics of emotional speech in autobiographical memory recall."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8235,
      "completion_tokens": 371,
      "total_tokens": 8606
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.3 and throughout the paper",
          "reasoning": "The dataset consists of self-defining autobiographical memories recounted orally by participants and manually transcribed. The emotional lexicon is analyzed using the French Emotaix software, indicative that the dataset is in French. However, the paper's text is in English, but the dataset itself is not described as containing English content. The paper does not explicitly state the language of the speech data, but Emotaix is a French emotional lexicon tool. Thus, the dataset contains only one human language, which is French (a non-English language). Therefore, the appropriate label is Monolingual (Non-English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.3 (Content Analysis) and methods sections",
          "reasoning": "The dataset contains transcriptions of speech in one language analyzed using Emotaix, a French emotional lexicon tool, implying the data are in French only. No mention is made of any other languages present in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "delvaux22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5453,
      "completion_tokens": 178,
      "total_tokens": 5631
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any link or reference to publicly available code repositories or scripts for dataset construction, data collection, transcription, or preprocessing. There is no mention of code sharing or accessibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 to 2.4",
          "reasoning": "The paper provides thorough documentation on the dataset creation process, including details on participant recruitment and characteristics (Section 2.1), task and data collection methods (recall of self-defining memories, speech and video recordings), transcription and scoring methods (Section 2.2), content analysis using EMOTAIX software (Section 2.3), and speech analysis parameters and tools (Section 2.4). This detailed methodological description documents the dataset creation process clearly in the paper."
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6615,
      "completion_tokens": 161,
      "total_tokens": 6776
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.2; Section 4",
          "Reasoning": "The new dataset includes speech data recorded during port-catheter placement operations, captured via multiple microphone channels. This is human-generated data as it originates from humans performing the surgeries and speaking, recorded by human-operated equipment."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.2; Section 4",
          "Reasoning": "The dataset includes X-Ray images captured during the surgical procedures (port-catheter placement). These images are human generated as they are medical imaging acquired through clinically operated X-Ray equipment during the intervention."
        }
      ]
    }
  },
  {
    "id": "demir23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7415,
      "completion_tokens": 222,
      "total_tokens": 7637
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 and Section 3",
            "reasoning": "The annotations correspond to surgical phase recognition for port-catheter placement operations in a specialized medical context, implying domain expertise was required. The paper mentions the dataset comprises 31 port-catheter placement operations but does not specify annotation personnel; however, given the medical nature, it is reasonable and typical that annotations were performed by expert personnel. No mention of AI or automatic annotation processes is provided."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention any provided annotation instructions or guidelines explicitly for the new dataset annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no discussion or mention of scoring rubrics used in annotation guidelines for the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not report any examples provided for annotation guidance, nor inclusion of sample annotations or annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "demir23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8605,
      "completion_tokens": 309,
      "total_tokens": 8914
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the PoCaP Corpus or the dataset used."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human experts for the PoCaP Corpus or the dataset used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single non-expert for the PoCaP Corpus or the dataset used."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention in the paper of quality assurance performed by multiple non-expert annotators on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any quality assurance conducted automatically by AI models as judges for the annotations or dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic or algorithmic rule-based quality assurance process performed on the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new dataset (31 port-catheter placement operations) but does not provide any description or documentation of a quality assurance process for validating the dataset annotations or content. Therefore, no QA process is documented or applied."
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8215,
      "completion_tokens": 417,
      "total_tokens": 8632
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3: Evaluation & Discussion, and Section 1: Introduction",
          "reasoning": "The dataset comprises 31 port-catheter placement operations collected by authors at Friedrich-Alexander-Universit\u00e4t Erlangen-N\u00fcrnberg, involving audio (speech) and X-ray imaging data recorded during actual surgical interventions. This indicates original data recorded from human subjects in real clinical settings, created entirely from scratch by human contributors (medical personnel and researchers) without translation or transformation from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset was generated entirely by AI or machine learning models; instead, models were trained and evaluated on real recorded data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by human translators converting content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as merely collected or aggregated from existing public sources without significant modification; it is newly recorded in this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived or adapted from existing datasets; raw audio and X-Ray images were recorded during interventions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of the dataset as newly recorded human surgical intervention data."
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8733,
      "completion_tokens": 347,
      "total_tokens": 9080
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3.2",
          "reasoning": "The paper describes using the newly collected dataset of port-catheter placement operations to train a Multi-Stage Temporal Convolutional Network (MS-TCN) with self-supervised feature extractors (wav2vec 2.0 XLSR-53 for speech and Densenet121 for X-Ray images held frozen). The training is supervised to recognize surgical phases, indicating supervised fine-tuning of pre-trained feature extractors on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The dataset is used for evaluation of the proposed surgical phase recognition framework, as results such as frame-wise accuracy and F1 scores are reported using this dataset in various experiments."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The authors analyzed temporal modeling choices and the class imbalance problem using the dataset. They evaluated different loss functions, temporal connections, and visualized phase duration distributions, indicating the dataset's use for analysis of trends and challenges in surgical phase recognition."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9456,
      "completion_tokens": 556,
      "total_tokens": 10012
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that the dataset uses German language speech audio during the port-catheter placement operations (Section 2.2: 'including German which is the language of the experiment corpus'). English is used only for the paper and references, not as the language of the dataset recordings."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 Feature Extraction Backbone",
          "reasoning": "The dataset contains speech audio in German only, as stated in Section 2.2: 'A large variant XLSR-53 is trained with 50k hours of public data from 53 languages, including German which is the language of the experiment corpus.' There is no mention of other human languages included in the speech data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes speech and X-Ray images; it does not contain programming or code-related content. Although model code is mentioned, the dataset itself does not contain programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of mathematical or formal logical expressions or symbolic representations as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio and X-Ray images; there is no biological sequence data or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states the language of the speech data as German; thus, language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech audio in German (human language) and X-Ray images; therefore, it does contain languages."
        }
      }
    }
  },
  {
    "id": "demir23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6674,
      "completion_tokens": 182,
      "total_tokens": 6856
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.3 Temporal Model, near Figure 1",
          "reasoning": "The authors explicitly provide a link to their source code at https://github.com/kubicndmr/PoCaPNet, indicating that the code related to the model and dataset processing is publicly available, which supports reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Intervention and early parts of Section 2 Proposed Method",
          "reasoning": "The paper documents the dataset creation process, including detailed description of the intervention (port-catheter placement operation), data modalities collected (speech from three microphone channels and X-Ray images), sampling rates, windowing approaches, and feature extraction backbones used. The surgical phases and their durations as well as class imbalance distributions are also documented, providing transparency about the dataset creation and its characteristics."
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7530,
      "completion_tokens": 142,
      "total_tokens": 7672
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Affect Production Task",
          "Reasoning": "The Affect Production Task uses audiovisual capture to record participants' facial affect production, involving human participants directly providing video data during the task under the guidance of a virtual dialogue agent."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Affect Production Task",
          "Reasoning": "The Affect Production Task records participants' vocal affect production via their spoken responses during the task, collected as audio data directly from human participants during recordings."
        }
      ]
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8330,
      "completion_tokens": 255,
      "total_tokens": 8585
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2.1 and 2.2.2",
            "reasoning": "Two human raters classified each participant's responses for affect production separately for facial and vocal modalities, and both raters had verified intact affect recognition abilities via a standardized test (DANVA-2). This indicates multiple human experts performed annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2.1",
            "reasoning": "The paper states raters were not instructed on how to classify affect via use of a facial affect coding system or any other operationalized set of stimuli, instead classification was based on the subjective impression of the rater. No detailed instructions were reported."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2.1",
            "reasoning": "No rubric or scoring guide was described; raters had five response options (four emotions plus neutral) but no operationalized rubric or scoring criteria is mentioned in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any information about examples being given to raters as part of annotation guidelines or training."
          }
        }
      ]
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9520,
      "completion_tokens": 459,
      "total_tokens": 9979
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2.2 Human ratings of affect production ability",
          "reasoning": "Two human raters classified participant responses separately for facial and vocal affect. These raters had intact affect recognition abilities, confirmed by scoring in the average range or higher on the Diagnostic Analysis of Nonverbal Accuracy-2 (DANVA-2), a standardized, norm-referenced facial and vocal affect recognition task. Therefore, they can be considered subject matter experts or members of the target demographic for the purposes of their affect recognition task. The use of multiple such raters constitutes multiple human experts performing the quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies that the human raters have intact affect recognition abilities, indicating expertise; thus, they are not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses potential future automation using machine learning, quality assurance in this study was not performed by an AI model as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automatic verification of code or formulas as a quality assurance process for the annotations or human rating labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a quality assurance process performed by two human expert raters; thus, stating that no QA process was applied is incorrect."
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9130,
      "completion_tokens": 419,
      "total_tokens": 9549
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Affect Production Task",
          "reasoning": "The paper describes collection of original audiovisual data where human participants (children and adolescents with and without autism) produced specific facial and vocal affect expressions in response to prompts. This data is created entirely from scratch by human contributors and does not represent translated, adapted, or derived data from existing sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data collected was generated by human participants, not by models. While automated metrics and machine learning methods are mentioned for analysis, the dataset itself consists of human-generated audiovisual affect production data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the data involves machine translation of content from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources but collected directly from new experimental procedures."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated facial and vocal metrics were derived from raw audiovisual data, these are features extracted for analysis, not the original dataset itself. The original data consists of newly collected human participant data, not transformed or adapted existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data source and collection method. Thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9648,
      "completion_tokens": 276,
      "total_tokens": 9924
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The novel Affect Production Task (APT) dataset is primarily used to evaluate affect production ability in children and adolescents with autism spectrum disorder and neurotypical controls. The study involves psychometric validation including criterion, ecological, and discriminant validity assessments of the dataset, demonstrating its use for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.4 and 4",
          "reasoning": "The dataset is used to analyze psychometric properties across various demographic factors (age, sex, race/ethnicity) and task demands, as well as to explore patterns in affect production ability. These analyses go beyond mere evaluation and focus on understanding characteristics and trends within the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10371,
      "completion_tokens": 387,
      "total_tokens": 10758
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Affect Production Task",
          "reasoning": "The Affect Production Task dataset contains participants producing emotions using vocal and facial expressions where the spoken utterances are in English only. For example, the monosyllabic utterance 'oh' and the neutral sentence 'I'll be right back' are used consistently as prompts. No other human languages are mentioned or used in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes spoken language (English). Thus, language is present in the dataset."
        }
      }
    }
  },
  {
    "id": "demopoulos24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7589,
      "completion_tokens": 162,
      "total_tokens": 7751
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section",
          "reasoning": "The paper does not provide any link, mention, or indication that the code related to data collection, preprocessing, or generation for the Affect Production Task (APT) dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.2 and 2.2.1",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the design of the Affect Production Task (APT), participant procedures, the three task conditions, computation of facial and vocal metrics, and how data was collected and processed, particularly in Sections 2.2 and 2.2.1. This documentation presents sufficient information on dataset construction for understanding and reproducibility purposes."
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8059,
      "completion_tokens": 108,
      "total_tokens": 8167
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.1., Section 2.1.2.",
          "Reasoning": "The evaluation dataset for the ProsAudit benchmark is created from the Boston University Radio News Corpus, which consists of professionally-read English news speech recordings. The authors selected and processed these human-recorded audio stimuli, inserting artificial pauses for the evaluation pairs, resulting in audio stimuli for the protosyntax and lexical tasks."
        }
      ]
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8859,
      "completion_tokens": 233,
      "total_tokens": 9092
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2.3 Human evaluation",
            "reasoning": "The human evaluation was conducted using Mechanical Turk with 389 participants from a non-expert crowd; these participants listened to stimuli pairs and made naturalness judgments."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2.3 Human evaluation",
            "reasoning": "Participants were presented with two examples at the beginning of the session and were instructed to decide which of two audios was most natural and how sure they were of their decision."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2.3 Human evaluation",
            "reasoning": "The scoring involved multiple controls specifically designed to filter out inattentive participants, requiring participants to pass example and control trials (5 out of 6) to ensure quality judgment, acting as an implicit rubric."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2.3 Human evaluation",
            "reasoning": "Two examples were presented at the beginning of each session to familiarize participants with the task."
          }
        }
      ]
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10049,
      "completion_tokens": 505,
      "total_tokens": 10554
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that multiple human experts conducted quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by a single human non-expert is found in the text."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2.3 Human evaluation",
          "reasoning": "The paper describes a human evaluation involving 389 participants recruited from Mechanical Turk who listened to stimuli pairs and judged their naturalness. Although participants were filtered for attention and language nativeness, there is no indication they were experts in prosody or linguistics. They are likely non-expert annotators. This human evaluation is done with multiple human non-expert annotators, serving as a kind of quality assessment or topline reference for the benchmark stimuli."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge or for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and 2.1.2 Material and data preprocessing and task creation",
          "reasoning": "The creation of the evaluation pairs involved automatic deletion of annotated pauses, artificial insertion of 400ms pauses at specified locations, and implementation of similarity losses to sample stimuli matching certain constraints. This procedure is algorithmic and automated, ensuring consistency and systematic data preparation. Hence, there is an automated process serving as quality assurance to ensure controlled and balanced stimuli creation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents both an automated process for dataset preparation and a human evaluation with multiple non-expert annotators, so it is not the case that no quality assurance process is described or performed."
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9659,
      "completion_tokens": 500,
      "total_tokens": 10159
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The evaluation dataset is created based on the existing Boston University Radio News Corpus, which is a pre-existing human-annotated dataset. While stimuli are modified by the authors (such as artificially inserting pauses), there is no indication that new human-created speech recordings or entirely new annotations were made from scratch by humans specifically for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models directly. AI models are evaluated using this benchmark, but the dataset itself is not AI-generated."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is in English and there is no mention of any data translated from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is mentioned or performed in the creation of the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Material and data preprocessing)",
          "reasoning": "The base dataset used for the evaluation set is the Boston University Radio News Corpus, an existing professionally-read news speech corpus with prosodic and linguistic annotations. The authors select segments following specific criteria and remove existing pauses, but the underlying data is collected from this existing source without significant modification to the original speech content."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and 2.1.2",
          "reasoning": "The authors create new evaluation stimuli by modifying the original corpus, such as deleting existing annotated pauses and artificially inserting 400ms pauses with crossfade at different locations to create pairs of natural and unnatural prosodic conditions. These transformations adapt the existing data to create a new benchmark dataset, thus it is derived data based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are well documented in the paper."
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10177,
      "completion_tokens": 376,
      "total_tokens": 10553
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset, ProsAudit, is not used for pre-training models; it is introduced as an evaluation benchmark."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of the dataset being used to train models from scratch; it serves as an evaluation benchmark."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of the ProsAudit dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No RL-based post-training or reinforcement learning methods are described using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 1 (Abstract, Introduction), Section 2 (Methods 2.1), Section 3 (Results 3.1)",
          "reasoning": "ProsAudit is explicitly introduced as an evaluation benchmark to assess structural prosodic knowledge in self-supervised speech models, providing two subtasks with corresponding metrics. The dataset is used exclusively for evaluation and benchmarking, as demonstrated by the evaluation of various SSL models and human baselines on the tasks."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 (Further analyses)",
          "reasoning": "The paper uses the ProsAudit dataset to analyze trends and characteristics in model performances, including effects of training data size and model nativeness on performance, thereby employing the dataset for analysis purposes beyond sole evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10900,
      "completion_tokens": 535,
      "total_tokens": 11435
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and throughout the paper (e.g., Abstract, Introduction, Methods 2.1.1)",
          "reasoning": "The proposed ProsAudit benchmark dataset is created using the Boston University Radio News Corpus, which is an English dataset with annotated prosodic boundaries. The benchmark and tasks are developed solely in English, as explicitly stated (e.g., Abstract: 'Our benchmark, in English, consists of two subtasks', Section 2.1.1: 'We used the Boston University Radio News Corpus ... based on the American English ToBI system'). There is no mention of inclusion of other languages in the new dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the work involves models and evaluation metrics, the dataset itself contains speech stimuli and annotations, not code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The benchmark dataset entries consist of speech audio with prosodic boundary annotations; no mathematical or formal logical expressions are contained in the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data comprises human speech from English news corpus, not biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or constructed languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are explicitly described and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken English language utterances, so language is clearly present."
        }
      }
    }
  },
  {
    "id": "deseyssel23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8118,
      "completion_tokens": 186,
      "total_tokens": 8304
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "not explicitly specified",
          "reasoning": "The paper does not provide any explicit link or mention of publicly available code repository for constructing the ProsAudit dataset or the scripts used for data preprocessing and pause insertion. While it states the dataset is part of the Zero-Resource Speech challenge evaluation metrics and leaderboard, no direct code repository is referenced in the paper text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Methods - ProsAudit benchmark (especially 2.1.1 and 2.1.2)",
          "reasoning": "The paper provides a detailed description of the dataset creation process including data source (Boston University Radio News Corpus), criteria for selecting segments, methodology for removing and inserting pauses, and constraints used for sampling stimuli pairs. This serves as thorough documentation of how the benchmark dataset is constructed, enabling reproducibility in principle."
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8009,
      "completion_tokens": 202,
      "total_tokens": 8211
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 2.3",
          "Reasoning": "The WavText5K dataset consists of audio recordings sourced from two websites providing royalty-free sound effect libraries (BigSoundBank and SoundBible), which are human-recorded natural audio files. The paper states these are unique sources not used in other datasets. The audio data is resampled and processed but originally collected from human-recorded sounds."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 2.2",
          "Reasoning": "The dataset contains free-form natural language descriptions provided by the uploaders of the audio recordings. These captions focus on isolated audio events and are manually written by humans, unlike automatically generated or model-generated text. They differ from curated or auto-extracted captions in other datasets."
        }
      ]
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8809,
      "completion_tokens": 255,
      "total_tokens": 9064
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The WavText5K dataset was sourced from public royalty-free websites BigSoundBank and SoundBible; the descriptions were provided by the uploaders of the audio recordings as free-form text. No mention is made of expert or multiple annotators manually curating or annotating the dataset; rather, the text descriptions are user-provided, implying non-expert human annotation by single individuals per audio clip."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "The paper states that descriptions are free-form provided by uploaders, without referencing any annotation instructions given to annotators or uploaders. Therefore, no instructions as part of annotation guidelines are indicated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "There is no mention in the paper of any rubric or scoring guidelines given or used during annotation for WavText5K."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "The paper does not provide or mention any annotation examples or exemplar annotations used as guidelines during annotation."
          }
        }
      ]
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9999,
      "completion_tokens": 311,
      "total_tokens": 10310
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was done by a single human expert for the WavText5K dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description in the paper of multiple human experts performing quality assurance on the WavText5K dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert auditor performed quality assurance on the WavText5K dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided in the paper about multiple non-expert annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based quality assurance processes applied to the WavText5K dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process performed on the WavText5K dataset. While it mentions that audio files were sourced from websites and some metadata was not collected, there is no information regarding validation or annotation verification by humans or machines."
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9609,
      "completion_tokens": 496,
      "total_tokens": 10105
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset collection",
          "reasoning": "The WavText5K dataset introduced by the authors consists of approximately 5000 audio-text pairs sourced from two sound effects libraries: BigSoundBank and SoundBible. These pairs include free-form descriptions provided by the uploaders of the audio recordings, indicating original human-generated content created from scratch rather than translations or adaptations. This is explicitly mentioned in Section 2.1 and Section 2.2, where the authors emphasize the natural language descriptions focusing on isolated audio events. Thus, the dataset represents new data from human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset collection",
          "reasoning": "WavText5K is compiled by collecting audio-text pairs from existing online sources, specifically BigSoundBank and SoundBible websites. The data are collected without significant modifications mentioned besides resampling audio files to 44.1 kHz. The descriptions are those originally provided by uploaders on these platforms. Therefore, this dataset could also be considered as collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors resampled audio and combined title and description for caption construction, these transformations are minimal and do not constitute substantial derivation or modification of existing sources. Hence, the data is not considered derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of dataset generation is clearly documented and described."
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10127,
      "completion_tokens": 335,
      "total_tokens": 10462
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, 4.2, 5.2",
          "reasoning": "The WavText5K dataset is explicitly used as part of the training data for the CLAP contrastive learning model from scratch with randomly initialized parameters, as described in the training methodology in Section 3.1 and experimental setup in Section 4.2. The authors demonstrate in Section 5.2 that including WavText5K in the training data improves audio-text retrieval performance, indicating that it is directly used to train models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "WavText5K is not used exclusively for evaluation; instead, evaluation is performed on other datasets such as AudioCaps and Clotho."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The paper provides quantitative analysis and discussion of the characteristics of WavText5K in Section 2.3, including statistics about the audio and text data, such as duration, word counts, and common terms, demonstrating its use for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10850,
      "completion_tokens": 546,
      "total_tokens": 11396
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces the WavText5K dataset with audio-text pairs described in natural language. There is no mention or evidence that multiple human languages are included; the descriptions are consistently in English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain exactly two human languages. The text descriptions focus on English only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 2.2 Description of isolated events and 2.3 Data analysis",
          "reasoning": "The dataset WavText5K contains natural language descriptions provided by uploaders, which are in English as indicated by example captions and comparisons with other datasets. All provided example captions and analysis are in English, supporting its monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's textual content is not in any non-English language; it is consistently in English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or evidence that the dataset entries include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of audio-text pairs with natural language descriptions without any mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural language descriptions of audio events but does not contain biological sequences, signals, or non-human communication systems as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No references or descriptions suggest the use of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language descriptions, so it is not without any language."
        }
      }
    }
  },
  {
    "id": "deshmukh23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8068,
      "completion_tokens": 156,
      "total_tokens": 8224
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.1 and Footnote 1",
          "reasoning": "The paper explicitly mentions that WavText5K is available online with a URL https://github.com/microsoft/WavText5K indicating that the dataset and related code or at least data is publicly accessible, facilitating reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (entire)",
          "reasoning": "The paper thoroughly documents the dataset creation process for WavText5K including data sources (BigSoundBank and SoundBible), data formats, resampling procedures, the nature of descriptions (free-form from uploaders), and distinctions from other datasets. This comprehensive description supports reproducibility and transparency."
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8891,
      "completion_tokens": 126,
      "total_tokens": 9017
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 (e.g., 5.1 Text-to-Audio generation, 5.2 Text-to-Music generation, 5.3 Speech Synthesis, 5.4 Noise suppression)",
          "Reasoning": "The authors collected human listening scores on audio samples generated from various models across tasks including Text-to-Audio, Text-to-Music, Text-to-Speech, and Deep Noise Suppression; these audio samples and corresponding scores represent new dataset contributions created for their evaluation."
        }
      ]
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9691,
      "completion_tokens": 331,
      "total_tokens": 10022
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 5.1, Section 5.2, Section 5.3, Section 5.4",
            "reasoning": "The paper describes multiple human listening tests conducted by recruiting multiple participants to rate audio quality on Likert scales for the new datasets collected for various tasks (Text-to-Audio, Text-to-Music, Speech Synthesis, and Noise Suppression). The listeners were recruited via Amazon Mechanical Turk with specific quality controls to ensure annotation quality, indicating the use of multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 5.1 (Human listening experiment description)",
            "reasoning": "The paper states that participants were asked to rate audio in terms of Overall Quality on a five-point Likert scale and the procedure included order randomization and quality control, implying that clear annotation instructions were provided to annotators for the human listening tests."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5.1 (Human listening experiment description)",
            "reasoning": "The use of a five-point Likert scale for rating Overall Quality serves as an explicit scoring rubric guiding the annotators to provide ratings, as described in Section 5.1 and similar human evaluations in later sections."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper.",
            "reasoning": "The paper does not explicitly mention providing annotation examples or example audio with labels to annotators for the human listening tests; hence, there is no evidence of examples being included in annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10881,
      "completion_tokens": 350,
      "total_tokens": 11231
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for validating or annotating the newly collected human listening scores or datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although human listening tests were conducted, the paper does not specify that multiple human experts or subject matter experts performed quality assurance or validated the annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted solely by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5.1 (Text-to-Audio Generation), Section 5.2 (Text-to-Music Generation), Section 5.3 (Speech Synthesis)",
          "reasoning": "The paper describes human listening experiments where multiple participants (e.g., 10 participants per audio) recruited via Amazon Mechanical Turk provided subjective ratings for audio quality, indicating multiple non-expert human annotators performed the quality assurance for the collected human listening scores."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any AI model to perform quality assurance or to judge dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of automated verification or algorithmic rule-based quality assurance of dataset annotations or content is provided in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described for the newly collected human listening scores via multiple non-expert human annotators; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10491,
      "completion_tokens": 525,
      "total_tokens": 11016
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5, subsections 5.1 Text-to-Audio generation and 5.2 Text-to-Music generation",
          "reasoning": "The paper describes the authors conducting new human listening experiments for multiple audio generation tasks. Specifically, in Section 5.1 and 5.2, they detail collecting human subjective scores (Mean Opinion Scores) from participants recruited via Amazon Mechanical Turk to assess audio quality of generated audio and music samples. These listening tests involved original human evaluations created from scratch for this study, not adaptations or translations of existing annotations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any entirely model-generated datasets as new datasets; rather, it uses existing datasets or human evaluations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being produced by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generated by machine translation from another language is described in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 Experiments and Section 4.1 Effect of Distortions",
          "reasoning": "The paper uses multiple existing well-known datasets such as AudioCaps, MusiCaps, LibriTTS, NISQA, and others as source audio data for evaluation and distortion experiments. These data are collected (collated) from existing sources without modification. The paper relies on these existing datasets for analysis."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Effect of Distortions",
          "reasoning": "The paper derives new data by adding controlled distortions (e.g., Gaussian noise, Tanh distortion, Mu-Law compression, Reverb) to existing audio samples from professionally recorded sound effects and datasets. These modifications transform the existing data, producing derived datasets used in experiments."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the origin and method of generation for all datasets it introduces or uses."
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11009,
      "completion_tokens": 333,
      "total_tokens": 11342
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 and Section 5",
          "reasoning": "The newly introduced dataset in the paper consists of human listening scores collected for various audio generation tasks (Text-to-Audio, Text-to-Music, Text-to-Speech, and Deep Noise Suppression). These datasets are used to evaluate the proposed PAM metric by measuring its correlation with human perception and comparing with existing metrics. This usage is explicitly described in sections 3 (Experiments) and 5 (PAM for audio tasks), where listening tests are conducted and PAM's performance is benchmarked."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The collected human listening scores and the related datasets are used for analysis of PAM's sensitivity to distortions as well as to evaluate prompting strategies. The dataset enables detailed analysis to understand the metric's behavior under different conditions, as described in sections 4.1 (Effect of distortions) and 4.2 (Prompting strategy)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11732,
      "completion_tokens": 626,
      "total_tokens": 12358
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new dataset containing entries in more than two human languages. All datasets used are English-centric or do not specify multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets containing exactly two human languages in the newly introduced datasets. The datasets referenced are either monolingual or not explicitly stated to be bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 5.3 (Speech Synthesis), Section 3 (Experiments), and related dataset descriptions",
          "reasoning": "The newly collected human listening scores and datasets used for evaluation, such as LibriTTS, NISQA, VoiceMOS, Blizzard Challenge Chinese TTS samples, and human evaluations for various audio tasks, are predominantly English or derived from English datasets. The paper does not report introducing new datasets in non-English languages for audio-text pairs. The text prompts used for PAM are in English, and the datasets for human evaluation appear English-centric."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset with exactly one non-English language. Although the Blizzard Challenge dataset contains Chinese TTS samples, it is used only for out-of-domain evaluation and is not introduced by the authors."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code-related content is included in the new datasets introduced by the authors."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions are present in the description of PAM computation and experimental metrics, but these are not part of any new dataset introduced by the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is included in the new datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created language data is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of entries in the datasets are specified or clearly English, no unknown language origin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The proposed datasets contain human languages (English) in the speech and text prompt data, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "deshmukh24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8950,
      "completion_tokens": 176,
      "total_tokens": 9126
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Sections 5 and 7",
          "reasoning": "The paper mentions releasing code and human listening scores, but does not provide a link, nor details about code for dataset creation or processing. The primary new datasets are the human listening scores collected for different tasks, but no code repository or explicit description of dataset construction code availability is provided in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5 (Experiments) and Abstract",
          "reasoning": "The paper documents the collection of new human listening scores for four audio tasks (TTA, TTM, TTS, DNS). It describes the listening test protocols, rating scales, participant recruitment, and quality control measures. However, the exact procedures or code for dataset creation are not described in detail beyond these experimental protocol summaries."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8173,
      "completion_tokens": 158,
      "total_tokens": 8331
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.1.1 Dataset construction",
          "Reasoning": "The new dataset introduced is the INTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge dataset. It comprises audio clips derived from public domain podcast recordings (LibriVox Community Podcasts) which are human-generated audio recordings. Packet loss traces extracted from real Microsoft Teams calls (human communication data) are applied to these audio clips by zeroing out lost packet regions to simulate packet loss. Thus, the primary modality is audio, originating from human-generated recordings (podcast audio), combined with real-world packet loss traces also originating from human communication data."
        }
      ]
    }
  },
  {
    "id": "diener22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8973,
      "completion_tokens": 426,
      "total_tokens": 9399
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1.3 Evaluation",
            "reasoning": "Crowd-sourced Comparative Mean Opinion Score (CMOS) is obtained using Amazon Mechanical Turk crowd raters, with 421 raters participating and multiple ratings collected per clip, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1.3 Evaluation",
            "reasoning": "Raters were given references and asked to rate files compared to references on a 7-point categorical scale, indicating specific instructions were provided to crowd-sourced raters for consistent comparative ratings."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1.3 Evaluation",
            "reasoning": "The CMOS rating task uses a 7-point categorical scale from -3 (much worse) to 3 (much better) relative to a reference, representing a clear scoring rubric provided to raters."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not explicitly mention providing examples of rated audio clips or example ratings to annotators in the annotation guidelines."
          }
        },
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4 Results",
            "reasoning": "Ground truth transcriptions for the evaluation word accuracy metric were created using Microsoft Azure Cognitive Services followed by human inspection and correction, indicating that a single human expert performed annotation corrections."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not describe specific annotation instructions given to the human annotators correcting transcriptions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention of scoring rubrics used by human annotators when correcting or creating transcriptions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No examples or annotation guidelines with examples for transcription correction are mentioned."
          }
        }
      ]
    }
  },
  {
    "id": "diener22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10163,
      "completion_tokens": 467,
      "total_tokens": 10630
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being conducted by a single human annotator with subject matter expertise for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human was responsible for quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4 (Results)",
          "reasoning": "The paper states that CMOS scores were obtained using Amazon Mechanical Turk with 421 raters participating, and an average of five ratings per clip were collected. This implies multiple non-expert human raters were involved in quality assurance of subjective evaluation data (crowd-sourced ratings). Specifically, ratings were used for evaluation of the blind test data, which is part of quality assurance of the annotation (comparative MOS scores)."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "Section 3.2 (PLCMOS)",
          "reasoning": "While the paper describes PLCMOS, a neural network model trained to estimate human mean opinion scores, as a metric to help evaluate PLC systems, this is used as an evaluation metric rather than as a quality assurance process for the dataset annotations themselves."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any automated verification or algorithmic rule-based quality assurance processes applied to validate dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a clear quality assurance process through the collection of multiple crowd-sourced ratings for the blind test dataset, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9773,
      "completion_tokens": 401,
      "total_tokens": 10174
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset audio content is sourced from an existing public domain podcast dataset, not created from scratch by human contributors specifically for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by AI or machine learning models; it consists of real audio clips with packet loss patterns applied."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No translation from another language by humans is mentioned regarding the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data is referenced in the dataset construction."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset construction",
          "reasoning": "The dataset is built by collecting and aggregating audio segments from an existing public domain podcast dataset and packet loss traces from actual calls, without significant modification to the original audio besides applying packet loss. This constitutes collating data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset construction",
          "reasoning": "The dataset involves applying packet loss traces sampled from real calls to segments of podcast audio by zeroing out lost packets, representing a transformation of the collated data, making it derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is documented clearly in Section 3.1.1; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10291,
      "completion_tokens": 366,
      "total_tokens": 10657
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is provided with training and validation splits specifically for participants to train Deep PLC models from scratch or using other datasets, as indicated by the description of the training split intended as a quick-start set for training PLC models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset includes supervised data (clean audio and corresponding lossy audio with packet loss regions), allowing participants to fine-tune models in a supervised manner to predict or reconstruct missing audio segments."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any usage of reinforcement learning or RL-based post-training techniques using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The blind test split of the dataset is used exclusively for evaluation and benchmarking of submitted models in a blind manner, as detailed in the description of the challenge evaluation procedure."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used primarily for analysis of trends or characteristics but rather for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models in retrieval-augmented generation or similar settings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical uses of the newly introduced dataset in training, supervised fine-tuning, and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11014,
      "completion_tokens": 395,
      "total_tokens": 11409
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset construction",
          "reasoning": "The dataset audio clips are sampled from a public domain podcast dataset (LibriVox Community Podcasts), which is an English-language podcast dataset. Furthermore, in Section 4 (Results), it is mentioned that Ground truth transcriptions for speech recognition were created and human-corrected, implying English text. No mention is made of other languages being present in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech audio, thus it contains language and cannot be marked as not applicable."
        }
      }
    }
  },
  {
    "id": "diener22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8232,
      "completion_tokens": 186,
      "total_tokens": 8418
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.1 Dataset and evaluation; specifically 3.1.1 Dataset construction",
          "reasoning": "The paper states that the dataset is now available as a fully open source dataset for use in further evaluations by any interested researcher, and provides a link to a GitHub repository https://github.com/microsoft/PLC-Challenge/tree/main/PLCMOS for the dataset and related code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1.1 Dataset construction",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including the sampling of actual packet loss traces from Microsoft Teams calls, stratified sampling based on burst loss length and packet percentage loss quantiles, and the application of these traces to audio clips from a public podcast dataset. This thorough description serves as documentation of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10621,
      "completion_tokens": 99,
      "total_tokens": 10720
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection",
          "Reasoning": "The authors created a new dataset by recording audio speech data from 42 healthy volunteers reading aloud in two controlled rooms with low and high reverberation. Recordings were made simultaneously with a condenser microphone and three smartphones, capturing human speech audio signals generated by human participants under the supervision of a trained researcher."
        }
      ]
    }
  },
  {
    "id": "dineley23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11421,
      "completion_tokens": 188,
      "total_tokens": 11609
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 Data Collection",
            "reasoning": "The data collection was overseen by a trained researcher as stated multiple times ('collection overseen by a trained researcher'), indicating expert human involvement in the annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide or mention any detailed annotation instructions given to annotators or researchers for the data collection or labeling process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention of scoring rubrics or formal criteria used to rate or evaluate annotations or labels in the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide or refer to annotation examples or guidelines illustrating expected annotation outputs or labels."
          }
        }
      ]
    }
  },
  {
    "id": "dineley23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12611,
      "completion_tokens": 420,
      "total_tokens": 13031
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance or validation performed by multiple human experts on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed QA on the dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA involving multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the paper used automated transcription via the OpenAI Whisper model for transcript alignment, this is part of feature extraction rather than quality assurance of dataset content; no AI model is described as performing QA."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Feature Extraction and Analysis",
          "reasoning": "The paper used automated transcription with the OpenAI Whisper model and automated alignment with Montreal Forced Aligner for transcript alignment. This automated processing supports feature extraction and indirectly data consistency; however, explicit QA of dataset quality by automated verification is not detailed. Since no manual QA process is described, this automated pipeline is the only implicit QA measure."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While no explicit manual QA is described, the use of a standardized automated transcription and alignment pipeline implies some QA via automatic processes rather than absence of QA."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12221,
      "completion_tokens": 368,
      "total_tokens": 12589
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The authors created a new dataset by recording speech from 42 healthy volunteers reading aloud a standard passage in two different reverberation environments. This data was newly generated from human participants during controlled recording sessions, overseen by trained researchers. There is no indication that this data was derived, translated, or aggregated from previous datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on human speech recordings; no data was generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being translated from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being translated from other languages using machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected by aggregating or combining existing datasets; it was newly collected."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on or adapted from existing sources; it is original human speech recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method are explicitly documented."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12739,
      "completion_tokens": 422,
      "total_tokens": 13161
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was used to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe usage of the dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training methods using the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset is recorded and analyzed, it is not exclusively used for evaluation or benchmarking of models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.2 Feature extraction and analysis; Abstract and Conclusion",
          "reasoning": "The dataset was primarily used to analyze the effects of room reverberation on various speech features across different recording devices and environments. The study reports detailed analyses of acoustic feature variability rather than using the data for model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as serving as a knowledge base or for augmenting models via retrieval."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly documented and used within the study for analysis purposes."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13462,
      "completion_tokens": 491,
      "total_tokens": 13953
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The dataset consists of recordings from 42 volunteers whose recordings involved reading aloud English texts, specifically The Rainbow Passage. Participants whose first language was not English were required to have sufficient English proficiency, and all transcriptions and analyses were conducted on English speech. Thus, the dataset contains only English language entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper mentions usage of software tools (e.g., Praat, Montreal Forced Aligner, OpenAI Whisper model) for automated transcription and feature extraction, the dataset itself does not contain entries of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper discusses speech features and statistical analyses but does not indicate that the dataset contains mathematical or logical expressions as part of the data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech recordings only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are mentioned or included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is specified explicitly as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset comprises human speech recordings in English and thus contains language."
        }
      }
    }
  },
  {
    "id": "dineley23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10680,
      "completion_tokens": 124,
      "total_tokens": 10804
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link regarding code availability in the paper",
          "reasoning": "The paper does not mention or provide any links to code repositories or indicate that code related to data collection, preprocessing, or dataset generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 - Data Collection and Feature Extraction and Analysis",
          "reasoning": "The paper describes in detail the data collection procedures including participant recruitment, ethics approval, recording environments, devices used, and the feature extraction methodology, demonstrating transparency and completeness in the dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7197,
      "completion_tokens": 119,
      "total_tokens": 7316
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction and Section 3.1 Datasets and Evaluation Protocols",
          "Reasoning": "The ST-EMO dataset is newly introduced by the authors. It consists of 153 hours of Chinese Mandarin speech audio data recorded by 50 male actors and 50 female actresses in various scenarios. The data were collected using Android smartphones in controlled environments and manually labeled by experts, indicating human involvement in recording and annotation, making it human generated audio data."
        }
      ]
    }
  },
  {
    "id": "ding23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7997,
      "completion_tokens": 246,
      "total_tokens": 8243
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1 (Experiments, Datasets and Evaluation Protocols)",
            "reasoning": "The paper states that after collecting the raw data for the ST-EMO dataset, they consulted four experts to judge the emotion category, requiring consistency of at least two experts with the original label for data to be considered valid. This clearly indicates annotation was done by multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide any explicit mention or description of annotation instructions given to the experts who labeled the ST-EMO dataset, so no evidence supports the presence of annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no indication or description of scoring rubrics or formal criteria/rubrics provided for annotation in the ST-EMO dataset section."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No annotation examples or example labels are mentioned or included in the description of data annotation process for ST-EMO."
          }
        }
      ]
    }
  },
  {
    "id": "ding23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9187,
      "completion_tokens": 327,
      "total_tokens": 9514
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single human expert performed the quality assurance of dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets and Evaluation Protocols, paragraph describing ST-EMO dataset collection.",
          "reasoning": "The paper reports that after collecting the raw ST-EMO data, four experts were consulted to judge the emotion category of each piece. The data was considered valid only if at least two experts agreed with the original label, indicating a quality assurance process performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any single non-expert conducting quality assurance for the ST-EMO dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies that four experts validated the labels; there is no indication that multiple non-expert annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model is reported to be used for quality assurance of the ST-EMO dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification or algorithmic processes used for quality assurance of dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents a quality assurance process involving multiple experts for validating the ST-EMO dataset; thus, QA is not absent."
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8797,
      "completion_tokens": 390,
      "total_tokens": 9187
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 (Datasets and Evaluation Protocols)",
          "reasoning": "The ST-EMO dataset was collected by the authors with 50 male actors and 50 female actresses in different scenarios, recording original Chinese Mandarin speech data. The paper states the data were recorded using Android smartphones in controlled settings, with scripts designed for text-independent, text-dependent, and improvised scenarios. The data are original human speech recordings created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data were produced by translating content from another language using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation being used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The newly introduced ST-EMO dataset was collected originally, not aggregated or compiled from existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ST-EMO dataset was not derived from existing sources with modifications; it is described as a newly collected corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection process are clearly described."
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9315,
      "completion_tokens": 279,
      "total_tokens": 9594
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.2",
          "reasoning": "The authors collected a new Chinese Mandarin speech emotion dataset ST-EMO and used it to train their proposed GLRF model from scratch, as explicitly described in Section 3.1 and Section 3.2 where the dataset is split into training and test sets, and models are trained and fine-tuned for optimal performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The ST-EMO dataset is used to evaluate and benchmark the performance of the proposed GLRF model and various baselines in Section 3.3, including performance comparisons on metrics like weighted accuracy, unweighted accuracy, and micro-F1 score."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10038,
      "completion_tokens": 544,
      "total_tokens": 10582
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The only new dataset introduced, ST-EMO, contains Chinese Mandarin speech data exclusively, with no mention of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "ST-EMO dataset is composed solely of Chinese Mandarin speech without entries in a second language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "ST-EMO dataset does not contain English entries; the English IEMOCAP dataset is pre-existing and not newly introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "ST-EMO is described as a Chinese Mandarin speech emotion dataset exclusively, indicating monolingual non-English content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset ST-EMO contains human speech audio data only, with no programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the ST-EMO dataset contains mathematical or formal logical expressions within its data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "ST-EMO dataset contains human speech recordings and does not include biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any fictional or artificially created languages; it contains natural Chinese Mandarin speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The language of the dataset ST-EMO is explicitly stated as Chinese Mandarin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language data, so it is not without language content."
        }
      }
    }
  },
  {
    "id": "ding23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7256,
      "completion_tokens": 158,
      "total_tokens": 7414
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 and Conclusion",
          "reasoning": "The paper does not include any mention or link to publicly available code repositories related to the ST-EMO dataset creation, collection, or preprocessing. There is no indication that the code for data collection or processing is released or accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Datasets and Evaluation Protocols",
          "reasoning": "The paper provides detailed documentation of the ST-EMO dataset collection process, including the number of speakers, recording conditions, devices used, duration of utterances, emotional categories, data cleaning procedures involving multiple experts determining labels, and statistics about the dataset distributions, demonstrating sufficient transparency on the dataset creation."
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8661,
      "completion_tokens": 383,
      "total_tokens": 9044
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 ARCOM's Corpus and 6. Acknowledgements",
          "Reasoning": "The dataset consists of a large corpus of 32,000 hours of French TV and radio broadcasts from 2023 provided and digitized by human-operated institutions (ARCOM and INA). The audiovisual materials were recorded from actual TV and radio broadcasts, thus human-generated data captured by broadcast and digitization methods."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 ARCOM's Corpus and 6. Acknowledgements",
          "Reasoning": "The dataset includes audio from French TV and radio channels during 2023 collected as broadcast recordings, which is human-generated audio data captured from real media sources rather than synthesized or model-generated audio."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Augmented speech transcriptions",
          "Reasoning": "The audio data was transcribed automatically using Whisper\u2019s large-v3 multilingual model and WhisperX framework. These transcriptions and timestamps are generated by AI models, constituting model-generated textual data derived from the audio modality."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.5 Face Detection and Categorization",
          "Reasoning": "From the video frames sampled at one image per second, faces were detected and classified using a model (Resnet50 DNN trained on FairFace). The inputs are human-generated images, but the processed face gender annotations are model-generated results applied to images extracted from the human-recorded broadcasts. The dataset contains these classified face data as new annotations generated by models on the original images."
        }
      ]
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9461,
      "completion_tokens": 245,
      "total_tokens": 9706
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "The paper states that the manual women presence reports were provided by French channels to ARCOM as part of legal reporting (Section 3.1). These channel reports include categorizations such as speaker gender and roles. Since these annotations are reported by multiple different TV and radio channel staff members (non-experts in the research team), this corresponds to Multiple Human Non-Experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide explicit details or references to detailed annotation instructions given to the channel annotators who prepared the manual women presence reports."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no description or mention of scoring rubrics or formal evaluation criteria provided to the channel staff for these manual annotations in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any illustrative examples or annotated samples provided to the channel annotators for guiding their labeling."
          }
        }
      ]
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10651,
      "completion_tokens": 404,
      "total_tokens": 11055
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4 Discussion",
          "reasoning": "Manual women presence reports provided by French TV and radio channels to ARCOM are used. These reports are based on multiple human annotators from the channels with subject matter expertise or knowledge of the target demographic, as they report on speaking characters categorized by gender and role. Furthermore, manual inspection is mentioned in the Discussion, indicating involvement of human experts verifying the data."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper of QA by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that quality assurance was conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for annotation (face detection, speaker gender), they are not described as judges performing quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2, 3.3, 3.4, 3.5",
          "reasoning": "Automated methods are extensively used for dataset annotation and estimation, including inaSpeechSegmenter for speaker gender and VAD, Whisper and WhisperX for speech transcription and alignment, first name gender percentage computation via lexicon, and inaFaceAnalyzer (an AI framework) for face detection and categorization. These automated processes provide systematic, reproducible annotation and verification at large scale, serving as an automated quality assurance process for these aspects of the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described and performed, both manual reports from channels and automated methods, thus QA cannot be considered absent."
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10261,
      "completion_tokens": 540,
      "total_tokens": 10801
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the authors created original data entirely from scratch by human contributors. The data consists mainly of broadcast content from TV and radio channels, which are existing materials, rather than newly created human-generated content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors applied AI and machine learning models (inaSpeechSegmenter, Whisper, inaFaceAnalyzer) to generate speech transcripts, gender segmentations, face detections, and first name extractions, these outputs are derived from existing broadcast data. The data itself is not newly generated content but processed from existing sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the data was produced by human translation from another language. The corpus is French broadcast content recorded in France, with no indication of cross-language translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate the dataset content from another language. The data originates from French TV and radio broadcasts and transcripts."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 ARCOM\u2019s Corpus",
          "reasoning": "The dataset analyzed is a large corpus of existing French TV and radio broadcasts provided by ARCOM, comprising 32,000 hours of audiovisual material from 2023. This data consists of collected and aggregated broadcast content from multiple channels without substantial modification, representing an aggregation of pre-existing media sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.2 - 3.5 Methodology; Sections 4 and 5 Results and Discussion",
          "reasoning": "The authors derived new datasets by applying various automatic processing methods\u2014speaker gender segmentation, automatic speech transcription, first name extraction for gender estimation, and face detection and categorization\u2014on the pre-existing broadcast data. These processed outputs constitute new representations derived from existing sources with transformations and adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of data generation are clearly specified and documented; hence N/A does not apply."
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10779,
      "completion_tokens": 216,
      "total_tokens": 10995
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The ARCOM's Corpus introduced by the authors is used primarily for analyzing gender representation trends, patterns, and characteristics in French TV and radio broadcasts. The paper employs this dataset to perform detailed statistical analyses and comparisons between manual channel reports and automatic descriptors to understand disparities, rather than for any model training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11502,
      "completion_tokens": 504,
      "total_tokens": 12006
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on French TV and radio broadcasts from 2023, exclusively in French as indicated throughout the paper."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries in exactly two languages; it is focused on French broadcasts only."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content is in French, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (ARCOM\u2019s Corpus), Abstract, Introduction",
          "reasoning": "The dataset consists of 32,000 hours of French TV and radio broadcasts from 2023, explicitly stated as French audiovisual media; thus it is monolingual in French, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While machine learning and analysis methods are described, the dataset itself does not include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is statistical modeling and effect size calculations mentioned, but the dataset itself does not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audiovisual human speech and facial data from broadcasts, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly stated as French and well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human spoken French language and related audiovisual contents."
        }
      }
    }
  },
  {
    "id": "doukhan24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8720,
      "completion_tokens": 172,
      "total_tokens": 8892
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories for the dataset construction or associated processing code. Although methods and tools used are described, no code is shared or indicated as accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Sections 3 (Method) and 4 (Results), including the data sources, sampling periods, classifications, processing tools, and algorithms used for speaker gender segmentation, speech transcription, face detection and classification, and first name gender attribution. This documentation enables reproducibility of the described dataset creation within the limits of access to source audiovisual materials, although the materials themselves are not publicly available due to copyright."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6828,
      "completion_tokens": 173,
      "total_tokens": 7001
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.3 Recording",
          "Reasoning": "The MD3 dataset includes more than 20 hours of audio recordings of conversational speech from human speakers in India, Nigeria, and the United States. The recordings were made using human participants conversing via virtual meetings, recorded through human-operated microphones and speakers without any AI generation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.4 Transcription",
          "Reasoning": "The dataset contains orthographically transcribed tokens of the recorded conversations. Transcriptions were performed by human speech transcription professionals and reviewed by a second worker, thus the text data originates from manual human annotation of the recorded audio."
        }
      ]
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7628,
      "completion_tokens": 207,
      "total_tokens": 7835
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.4",
            "reasoning": "Section 3.4 states that orthographic transcription was performed by speech transcription professionals (human annotators) from the same geographic locale, and each transcription was reviewed by a second worker for accuracy, indicating multiple human non-expert annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.4",
            "reasoning": "The paper does not mention any detailed annotation instructions being provided to the transcription professionals, only that professionals performed transcription and review."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.4",
            "reasoning": "There is no indication in Section 3.4 or elsewhere that scoring rubrics or explicit criteria were provided for the transcription annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention providing any annotation examples or sample transcriptions as guidance."
          }
        }
      ]
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8818,
      "completion_tokens": 418,
      "total_tokens": 9236
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that quality assurance was performed by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.4 Transcription",
          "reasoning": "Orthographic transcription was performed by speech transcription professionals, indicating human annotators with expertise. Each transcription was reviewed by a second worker for accuracy, thus QA involved multiple human experts (professionals and reviewers) ensuring transcription accuracy. Additionally, transcription was performed by individuals from the same geographic locale as the speakers, increasing the likelihood that they are experts or members of the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that QA was performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an AI model (Whisper) was used to evaluate speech recognition performance on the dataset, this does not constitute quality assurance for dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of automated verification or rule-based techniques used as quality assurance for dataset content or annotations is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a human transcription and review process for quality assurance, so QA is documented."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8428,
      "completion_tokens": 520,
      "total_tokens": 8948
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Elicitation) and Section 4 (Dataset)",
          "reasoning": "The dataset was collected by recruiting human speakers from three locales (India, Nigeria, and the United States) to participate in conversational information-sharing tasks (guessing games). These dialogues were recorded as they naturally occurred during the task, resulting in original content created entirely from scratch by human contributors. The paper details the recruitment, recording, and transcription of these conversations, indicating the data is newly created human speech, not a translation or adaptation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or models. All data was produced by human participants during conversations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced from translations of other language data into English by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine-translated data in the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While image prompts were drawn from existing public datasets, the speech and conversational data was newly recorded and not aggregated without modification. The core dataset is original human speech data, not a collation of existing speech corpora."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2.1 (Image prompts) and Section 3 (Elicitation)",
          "reasoning": "The elicitation used prompts partly drawn from existing image datasets (FoodX-251, CalTech-UCSD Birds, Stanford Dogs) to stimulate natural conversation. Although the speech content is new, the stimuli for the conversations derive from existing sources, adapting those images into the context of a guessing game. Thus, while the conversational data is new, part of its elicitation is derived from prior existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data collection method and origins; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8946,
      "completion_tokens": 469,
      "total_tokens": 9415
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the MD3 dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MD3 dataset to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that MD3 is used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No discussion or evidence in the paper supports the use of MD3 for reinforcement learning-based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset could be used for evaluation, the paper does not demonstrate or describe exclusive use of MD3 for benchmarking or performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Dialect features, Section 4 Dataset, Section 5 Conclusion",
          "reasoning": "The paper primarily uses MD3 for analyzing dialectal variation, trends, and linguistic characteristics across English dialects from India, Nigeria, and the United States. The authors present statistical analyses of dialect features, discourse markers, and syntactic patterns, illustrating its utility for linguistic study and dialect robustness in spoken language processing."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that MD3 is used as a knowledge base for augmenting models or enabling retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents usage of the MD3 dataset for linguistic and dialect analysis."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9669,
      "completion_tokens": 573,
      "total_tokens": 10242
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of English conversational speech samples from three dialects (India, Nigeria, United States) but does not include other human languages in the dataset entries themselves."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although speakers' first languages include Telugu (India) and Yoruba (Nigeria), the dataset content itself is exclusively English speech from these dialects; no entries contain two distinct human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 3 Elicitation; Section 4 Dataset",
          "reasoning": "The dataset MD3 contains conversational speech in English from speakers in India, Nigeria, and the United States. The data is described as English dialogues only; while speakers' first languages may vary, the dataset entries themselves are solely English conversational speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries consist exclusively of English content, without any non-English language-only entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset comprises conversational speech transcripts and audio; no programming or code-related content is indicated."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication of inclusion of mathematical or logical expressions in the dataset samples."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human conversational speech and contains no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of use of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language content is well documented as English; there is no uncertainty about the language(s) present in the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains conversational speech and transcripts, therefore language is present."
        }
      }
    }
  },
  {
    "id": "eisenstein23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6887,
      "completion_tokens": 176,
      "total_tokens": 7063
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper states that the dataset will be made publicly available with the publication, but there is no mention or link to any code repository or specific code used for data collection, preprocessing, or generation. Therefore, the code is not made publicly available according to the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including speaker recruitment (Section 3.1), the elicitation setup and tasks (Sections 3, 3.2), recording conditions (Section 3.3), transcription methods (Section 3.4), and detailed dataset statistics and dialect feature analysis (Section 4). This comprehensive description ensures transparency and completeness of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9383,
      "completion_tokens": 134,
      "total_tokens": 9517
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Tasks and Datasets, Table 1",
          "Reasoning": "The paper explicitly introduces five novel speech datasets (Cog1, Cog2, Cog3, Cog4, and Phys) collected through recording voice samples from volunteers undergoing cognitive or physical load induction tasks. These datasets are audio recordings manually captured from human speakers under controlled experimental protocols, confirming the origin is human generated audio data. The descriptions include details on speaker counts, languages, tasks, and duration, indicating these datasets are original and human generated speech audio."
        }
      ]
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10183,
      "completion_tokens": 348,
      "total_tokens": 10531
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1 Tasks and Datasets",
            "reasoning": "The paper describes that volunteers (participants) performed the speech tasks and recordings were labeled according to experimental conditions (with or without load). There is no indication that expert annotators labeled the data; instead, labeling was done based on experimental conditions (objective labels). Thus, annotation is effectively an automatic process based on experimental conditions, or raters were the participants themselves or staff supervising the experiment. Since the labels come from experimental conditions rather than subjective human annotation, labelling is likely done automatically or by multiple human non-expert staff or via experimental design."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Tasks and Datasets",
            "reasoning": "The paper details the protocols used to collect and label speech under cognitive or physical load\u2014each dataset's tasks and conditions are described as part of experimental procedures, indicating that instructions were provided to participants on performing the tasks (e.g., reading scripted text while counting syllables, memorizing digits, etc.). These serve as instructions for generating labeled data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 Tasks and Datasets",
            "reasoning": "There is no mention of scoring rubrics or criteria for qualitative or subjective annotation; labels correspond to task conditions (with/without load), not graded or subjective scoring that would require rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not provide any annotation examples to clarify how labeling decisions were made; labeling is based on experimental condition rather than subjective judgment requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11373,
      "completion_tokens": 317,
      "total_tokens": 11690
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about multiple human experts performing quality assurance or annotation checks for the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe a single human non-expert conducting quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators performing QA or validation of the data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA of dataset annotation or content is not reported to be performed by any AI model; no such process is described in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated or algorithmic process for quality assurance or content validation of the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces five new speech datasets but does not provide any description or mention of a quality assurance process for dataset annotations or labels. There is no explicit documentation or discussion of how the experimental conditions (load/no load) were validated or any annotation validation performed beyond the experimental protocol. Therefore, one must conclude that no quality assurance process is described or applied to these new datasets."
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10983,
      "completion_tokens": 380,
      "total_tokens": 11363
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Tasks and Datasets",
          "reasoning": "The paper introduces five novel speech datasets collected from human participants performing tasks under induced cognitive or physical load. The speech data was recorded directly from volunteers specifically for this study, using various experimental protocols with original tasks such as counting audio stimuli, memorizing digits, describing images while multitasking, and before/after physical exercise. These are original data created entirely from scratch by humans as per the experiment descriptions."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not generated by AI or machine learning models but collected from human recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is described as part of the data creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not stated as being collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not derived from existing sources but are newly collected data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection methodology are fully documented and described in Section 2.1."
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11501,
      "completion_tokens": 222,
      "total_tokens": 11723
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.3 and 3",
          "reasoning": "The four novel cognitive load datasets and the physical load dataset introduced in the paper are used exclusively as downstream tasks for evaluation and benchmarking of different audio representation models. The evaluation pipeline involves splitting these datasets into training and test sets, training linear classifiers on the representations without fine-tuning, and reporting performance metrics on unseen speaker-independent test sets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12224,
      "completion_tokens": 546,
      "total_tokens": 12770
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 Tasks and Datasets, Table 1",
          "reasoning": "The Cog4 dataset contains speech data in 9 different languages, making the overall dataset multilingual. The Cog1 and Phys datasets include English and French (2 languages), Cog2 and Cog3 include Mandarin Chinese only. Cog4's inclusion of 9 languages makes the combined proposed dataset multilingual."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 Tasks and Datasets, Table 1",
          "reasoning": "The Cog1 and Phys datasets include data recorded in exactly two languages: English and French."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset exclusively contains English language entries; English is only present along with other languages in Cog1, Phys, and others."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Tasks and Datasets, Table 1",
          "reasoning": "Cog2 and Cog3 datasets contain only Mandarin Chinese, a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain speech audio recordings; there is no indication that programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the datasets contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Datasets consist of human speech recordings under cognitive and physical load; no biological sequences or non-human communication are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "None of the datasets contains fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the datasets are clearly documented and specified in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "All datasets contain human language speech recordings, so language is present."
        }
      }
    }
  },
  {
    "id": "elbanna22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9442,
      "completion_tokens": 181,
      "total_tokens": 9623
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or explicit mention found",
          "reasoning": "The paper does not provide any explicit mention or link to code repositories related to the construction, preprocessing, or generation of the newly introduced datasets. The methodology describes the dataset collection protocols and methods but does not state that code or scripts used during data collection or processing are shared publicly."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Tasks and Datasets) and Table 1",
          "reasoning": "The paper provides detailed descriptions of the five new datasets in Section 2.1 and summarizes them in Table 1, including information on languages, number of utterances, speakers, duration, and specific protocols used for cognitive and physical load induction. The description includes experimental setups, task designs, and speech recording conditions, providing sufficient documentation about dataset creation."
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5443,
      "completion_tokens": 116,
      "total_tokens": 5559
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Dataset",
          "Reasoning": "The new dataset introduced is a corpus of TV and radio speech samples from the French National Institute of Audiovisual (INA), containing speech recordings of 1025 speakers over four time periods (1955-56, 1975-76, 1995-96, 2015-16). These are human speech recordings collected from media broadcasts, thus audio data of human origin."
        }
      ]
    }
  },
  {
    "id": "elie24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6243,
      "completion_tokens": 244,
      "total_tokens": 6487
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 Dataset; Section 2.3 Acoustic-to-articulatory inversion",
            "reasoning": "The dataset is constructed from automatic transcription (Whisper), automatic forced alignment (MFA), and automatic acoustic-to-articulatory inversion with Maeda's model. No manual annotation or labeling beyond evaluation subsets is described, and the primary processing is automatic."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1 Dataset; Section 2.3 Acoustic-to-articulatory inversion",
            "reasoning": "The paper does not mention the provision of annotation instructions or guidelines for annotators because the process is automatic, not manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 Dataset; Section 2.3 Acoustic-to-articulatory inversion",
            "reasoning": "No scoring rubrics are described as the annotation is via automatic algorithms; thus rubric-based evaluation is not applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No annotation examples or sample annotations are provided because the dataset creation involved automatic processing without manual annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "elie24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7433,
      "completion_tokens": 398,
      "total_tokens": 7831
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes manual transcription of a subset of data by four people, but there is no information indicating that these annotators were subject matter experts or part of the target demographic for specialized quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention a single human non-expert performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset",
          "reasoning": "Four people manually transcribed a subset of 81 segments to evaluate transcription quality, with no indication they were experts. This suggests quality assurance involved multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using an AI model as a judge for quality assurance in the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset",
          "reasoning": "The quality assurance for transcription includes an automatic selection method leveraging source separation algorithms to evaluate speech-to-noise ratio and discard noisy excerpts, which constitutes an automated quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes including manual transcription checks and automated filtering are described; thus, QA is present and documented."
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7043,
      "completion_tokens": 477,
      "total_tokens": 7520
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset",
          "reasoning": "The new dataset consists of speech samples collected from French media archives spanning 60 years, including 1025 speakers with metadata on gender and age. The data represents human-produced speech recordings, curated and selected by the authors for their analysis. Thus, the original speech data was created entirely by human speakers, not translated or adapted from other datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors apply an acoustic-to-articulatory inversion model to estimate articulatory parameters, the generated articulatory data are derived features based on the original speech recordings rather than original data generated entirely by models without input data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced by human translation from another language. The dataset consists solely of French speech from media archives."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine translation applied to the data. The corpus is French media speech."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset",
          "reasoning": "The speech data was collected from existing French media archives (INA). Thus, the dataset is an aggregation of existing recordings without significant modification of the audio data itself."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 2.2 Acoustic feature extraction and 2.3 Acoustic-to-articulatory inversion",
          "reasoning": "The articulatory parameters used for analysis are derived from the original acoustic signals through feature extraction (formant estimation) and inversion with Maeda\u2019s model, representing transformed and adapted data generated from the original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation methods are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7561,
      "completion_tokens": 310,
      "total_tokens": 7871
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2.1-4",
          "reasoning": "The introduced dataset, derived from French media archives with acoustic and articulatory feature extraction, is used primarily to analyze trends and characteristics in articulatory configurations across genders and time periods. The study employs statistical models to interpret changes in parameters like larynx height and lip protrusion, focusing on patterns rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8284,
      "completion_tokens": 542,
      "total_tokens": 8826
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes a dataset composed solely of French media speech recordings. No mention of any other language is made."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes two languages. All data is French speech."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of French speech, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset",
          "reasoning": "The corpus collected is explicitly described as based on French TV and radio archives with automatic transcription and alignment done using French phonetic dictionaries and models. The entire dataset and analysis focus on French speech data only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper mentions algorithms and software (e.g., Whisper, Montreal Forced Aligner), the dataset itself consists of speech data and is not composed of programming code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 2.2 Acoustic feature extraction, Section 2.3 Acoustic-to-articulatory inversion",
          "reasoning": "The paper presents mathematical formulas and modeling to describe the methods used, but the dataset being evaluated consists of speech samples and their associated extracted features, not mathematical expressions as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned; the dataset is real French speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries (French) is clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (French speech), so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "elie24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5502,
      "completion_tokens": 136,
      "total_tokens": 5638
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link present",
          "reasoning": "The paper does not mention any link, repository, or location where the code for data collection, preprocessing, transcription, forced alignment, acoustic feature extraction, or articulatory inversion is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 to 2.4",
          "reasoning": "The paper provides detailed descriptions of the dataset construction and processing steps including corpus origin, speaker demographics, transcription method and evaluation, forced alignment, acoustic feature extraction, inversion procedure, and statistical analysis, documenting the dataset creation process transparently."
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10950,
      "completion_tokens": 145,
      "total_tokens": 11095
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2, 2.1 and 2.2",
          "Reasoning": "The Lombard-GRID-2mix dataset is derived from the Audio-Visual Lombard GRID speech corpus which contains audio recordings of 54 human speakers speaking sentences in both normal and Lombard speech styles (human generated). Two-speaker mixtures are then simulated using scripts from Isik et al. [27] to create mixture audio data (model generated). The mixture creation process programmatically combines the human-recorded audio signals, thus the final dataset includes both human-generated base signals and model-generated mixtures."
        }
      ]
    }
  },
  {
    "id": "ewert24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11750,
      "completion_tokens": 237,
      "total_tokens": 11987
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 and 2.3",
            "reasoning": "The dataset Lombard-GRID-2mix is constructed by simulating two-speaker mixtures using scripts from Isik et al. [27], originally for wsj0 mixtures, applied to the Audio-Visual Lombard GRID corpus recordings. This mixing procedure and SNR assignment follow an automated, scripted approach rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not provide any mention of annotation instructions since the dataset creation is performed via an automatic simulation process rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No scoring rubrics or evaluation guidelines for manual annotation are described as the mixture creation is an automated scripted process without human labeling or subjective scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No examples of annotations or manual label instructions are mentioned; the process is a computational mixture simulation pipeline without examples needed for human annotators."
          }
        }
      ]
    }
  },
  {
    "id": "ewert24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12940,
      "completion_tokens": 407,
      "total_tokens": 13347
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the Lombard-GRID-2mix dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss quality assurance by multiple human experts related to the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance by a single non-expert human for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance by multiple non-expert humans for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 and 2.3",
          "reasoning": "The dataset creation involves the use of automated scripts from Isik et al. [27] for mixture simulation, applying specific SNR sampling and mixing procedures automatically. The addition of speech-shaped noise is also performed via an automated process using script-based noise generation based on WSJ0 corpus with defined signal processing methods. This constitutes automatic verification and consistency in dataset generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is evidence of an automatic quality assurance process through the use of simulation scripts and defined protocols, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12550,
      "completion_tokens": 501,
      "total_tokens": 13051
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset Lombard-GRID-2mix is derived from the Audio-Visual Lombard GRID Speech Corpus, which contains speech utterances recorded by 54 human native British English speakers in both normal and Lombard speaking conditions, triggered by noise. The paper states that the Lombard speech was recorded via a carefully designed experimental setup involving human speakers responding to noise stimuli and a human listener. Thus, the speech data are original recordings of human speech, created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated or synthesized by AI or machine learning models but rather from human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset content consists of translated data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset content consists of machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely aggregated from existing sources but involved novel recordings and generation of speech mixtures; thus, it is not simply collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 and 2.3",
          "reasoning": "The Lombard-GRID-2mix dataset is derived from the Audio-Visual Lombard GRID Speech Corpus by forming two-speaker mixtures using scripts originally designed for WSJ0-2mix dataset creation. The mixtures combine speech utterances, sometimes with added speech-shaped noise generated from existing WSJ0 corpus speech data, processed through phase randomization and mixing methods. This represents transformation and adaptation of existing speech data into new mixture datasets for the tasks at hand."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method are well specified in the paper."
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13068,
      "completion_tokens": 439,
      "total_tokens": 13507
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The Lombard-GRID-2mix dataset is explicitly used to train speech separation systems from scratch using supervised learning, as described in Section 3.2 where the authors detail the training procedure from randomly initialized parameters with a supervised loss (negative SI-SDR). Several systems are trained from scratch using different subsets of Lombard-GRID-2mix."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4",
          "reasoning": "The dataset is also used for supervised fine-tuning of pre-trained separation models, as detailed in Section 3.2. The authors perform finetuning experiments on Lombard-GRID-2mix subsets and discuss their efficacy in Section 4."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or demonstrate reinforcement learning post-training techniques such as RLHF applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The Lombard-GRID-2mix dataset is used for performance evaluation and benchmarking of speech separation systems, as described in Section 4 through detailed experiments evaluating models on normal and Lombard speech test sets and noisy versions."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Abstract, Section 4",
          "reasoning": "The dataset is used to analyze the impact of Lombard speech on separation performance and the benefits of including such data during training, which constitutes an analysis of trends and characteristics of speech styles in speech separation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset serves as a knowledge base to augment models through retrieval or related mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has practical usage demonstrated in the paper through training, finetuning, evaluation, and analysis as described above."
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13791,
      "completion_tokens": 256,
      "total_tokens": 14047
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 The Audio-Visual Lombard GRID Speech Corpus",
          "reasoning": "The proposed dataset Lombard-GRID-2mix is derived from the Audio-Visual Lombard GRID Speech Corpus, which contains recorded utterances by native speakers of British English. The corpus uses the Grid corpus syntax, which is in English. Thus, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ewert24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11009,
      "completion_tokens": 186,
      "total_tokens": 11195
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 (Construction of Lombard-GRID-2mix) and abstract",
          "reasoning": "The paper explicitly states in the abstract that all scripts to simulate the Lombard-GRID-2mix dataset are made publicly available. This indicates that code related to dataset construction and simulation is accessible for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Construction of Lombard-GRID-2mix)",
          "reasoning": "Section 2 provides comprehensive details about the dataset construction process, including the data source (Audio-Visual Lombard GRID Speech Corpus), the grouping and mixing strategy, SNR sampling, use of simulation scripts from Isik et al. [27], consideration of noise addition and splitting of data for training/validation/testing. This level of detail constitutes good documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7678,
      "completion_tokens": 108,
      "total_tokens": 7786
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The authors introduce a new in-house meeting code-switching dataset collected from real meeting recordings, manually transcribed, comprising 3000 hours of monolingual English training data, 3000 hours of monolingual Mandarin training data, and 3000 hours of Mandarin-English code-switching training data, indicating human-recorded speech audio data."
        }
      ]
    }
  },
  {
    "id": "fan23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8478,
      "completion_tokens": 193,
      "total_tokens": 8671
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Datasets",
            "reasoning": "The in-house meeting code-switching dataset is described as manually transcribed, indicating human experts performed the annotation to ensure accurate and reliable transcriptions."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide any explicit description or mention of detailed annotation instructions provided to annotators for the in-house meeting code-switching dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no information in the paper about scoring rubrics or any criteria provided to annotators during the annotation process of the in-house meeting code-switching dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention providing annotation examples or guidelines with examples to annotators for this dataset."
          }
        }
      ]
    }
  },
  {
    "id": "fan23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9668,
      "completion_tokens": 295,
      "total_tokens": 9963
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information regarding quality assurance being conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention of quality assurance performed by multiple human experts in the dataset description or elsewhere in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert annotators performed QA on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models as judges for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automatic QA processes such as automated verification of annotations or code beyond using standard ASR training procedures."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces an in-house meeting code-switching dataset but provides no information on the quality assurance procedures applied to its manual transcriptions or annotations. The only mention is that the dataset is manually transcribed, without further QA details. Therefore, no QA process is documented for this new dataset."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9278,
      "completion_tokens": 405,
      "total_tokens": 9683
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper states that the authors evaluated their method on two datasets: SEAME, which is an existing public corpus, and an in-house meeting code-switching dataset. The in-house meeting dataset is described as collected from recordings of real meetings and manually transcribed, indicating that the data was newly created by human contributors from scratch rather than derived or translated from pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The in-house meeting dataset is described as newly recorded and manually transcribed; there is no indication it was collated or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data does not appear to be derived from existing sources with modifications or adaptations; rather it was newly collected and transcribed."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of data generation for the in-house meeting dataset is documented in Section 4.1."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9796,
      "completion_tokens": 489,
      "total_tokens": 10285
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The newly introduced in-house meeting code-switching dataset consisting of 3000 hours monolingual and 3000 hours code-switching speech is used as training data to train the model architecture from scratch, as evidenced by the training details in Section 4.2 and the improvement of results in Section 4.4 when trained on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using the new dataset for fine-tuning a pre-trained model; instead, training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that reinforcement learning or RL-based post-training methods such as RLHF are used with the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.4",
          "reasoning": "The in-house meeting code-switching dataset has dedicated test sets (test ma, test n, test cs) used to evaluate the model\u2019s performance, showing MER reductions and assessing effectiveness in real-world scenarios."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the dataset solely for analysis of trends or characteristics but for model training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using the dataset as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The in-house meeting code-switching dataset is clearly used for training from scratch as well as for evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10519,
      "completion_tokens": 537,
      "total_tokens": 11056
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced contain exactly two human languages, Mandarin and English, and do not include more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper introduces an in-house meeting code-switching dataset comprising Mandarin and English speech data, as well as the SEAME dataset which consists of Mandarin-English code-switching speech. Both datasets contain exactly two human languages, Mandarin and English."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No new datasets with only English content exclusively are introduced; the dataset contains bilingual speech with Mandarin and English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No new datasets with only a non-English language exclusively are introduced; the dataset contains bilingual speech with Mandarin and English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or evidence of datasets containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not include mathematical or formal logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets focus on human speech in Mandarin and English; no biological or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are present in the described datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages included in the datasets are explicitly specified as Mandarin and English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain human language (Mandarin and English) transcriptions; thus, this label does not apply."
        }
      }
    }
  },
  {
    "id": "fan23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7737,
      "completion_tokens": 211,
      "total_tokens": 7948
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The paper mentions the use of two datasets: SEAME and an in-house meeting code-switching dataset. The SEAME dataset is a publicly available corpus referenced by [28], but the in-house meeting dataset is proprietary and collected by the authors. There is no mention or link to any code repository for data collection, preprocessing, or dataset construction for the in-house dataset or for SEAME within this paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "The paper provides some descriptive details about the datasets used, including data amounts and evaluation sets, but does not provide any documentation or detailed methodology on how the in-house meeting code-switching dataset was constructed, collected, or processed. The SEAME dataset is referenced from another paper but not described in detail here. Therefore, the documentation of the dataset creation process is not presented in this paper."
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7594,
      "completion_tokens": 190,
      "total_tokens": 7784
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 and Section 3",
          "Reasoning": "The novel dataset includes audio recordings from the Image Description Task, where participants describe an image while their microphone records their speech. This data is human generated as it is manually spoken by participants and recorded during the study conducted on the Thymia Research platform."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.2 and Section 3",
          "Reasoning": "The dataset includes behavioral performance metrics from the n-Back Task, such as precision, recall, false-positive rates, and reaction times. These are tabular data derived from participant interactions (clicks and responses) during the cognitive task, making it human generated through participant input during the study."
        }
      ]
    }
  },
  {
    "id": "fara22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8394,
      "completion_tokens": 347,
      "total_tokens": 8741
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.2 and 3 Dataset",
            "reasoning": "The participants performed tasks for the new Thymia dataset which is collected online without researcher supervision; answers such as psychiatric questionnaires (like PHQ-8) represent self-reports by participants, not manual annotations by multiple annotators. There is no mention of multiple annotators labeling data, so the 'annotation' in this case refers to data generated by the participants themselves as experts on their own state (patients or controls). Therefore, annotation here is via participant responses, best categorized as Single Human Expert (patient self-report as domain experts)."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Online Study Setup",
            "reasoning": "The paper states that participants performed several tasks online, including standardized questionnaires and specific speech and cognitive tasks, which implies that instructions were given to participants to complete these tasks correctly remotely."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 and 2.2.1-2.2.2",
            "reasoning": "The psychiatric questionnaires, such as PHQ-8, are standardized instruments with established scoring rubrics; similarly, the n-Back task has well-defined performance metrics (precision, recall, false-positive rate). This indicates that rubrics for scoring or evaluation were inherently used in data collection protocols."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide nor mention specific examples of annotation or detailed instructions exemplified to participants or annotators. There is no explicit mention of example annotations or sample data provided in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "fara22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9584,
      "completion_tokens": 318,
      "total_tokens": 9902
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple human expert annotators performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human non-expert annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by an AI model as judge is described in the paper."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Quality Controls",
          "reasoning": "The paper describes an automated audio quality pipeline that assesses audio files based on feature quality and speech activity to flag and reject poor quality recordings. This constitutes an automatic, algorithmic verification process ensuring data quality for audio recordings. Additionally, for the n-Back task, performance metrics are checked against expected ranges and vary logically with known covariates such as age, indicating automated verification of behavioural data quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described through automated pipelines and checks, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9194,
      "completion_tokens": 438,
      "total_tokens": 9632
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3",
          "reasoning": "The dataset comprises original content collected directly from human participants through tasks such as the Image Description Task and n-Back working memory task, conducted remotely using the Thymia Research Platform. Participants are native English-speaking adults who performed these tasks on their personal devices, and the data consists of speech recordings and behavioral responses newly generated in this study, not translated, adapted, or derived from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any part of the dataset being generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any use of machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as aggregated or collected from existing databases or sources; instead it was newly collected for this study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The raw human-collected data (speech recordings and n-Back task behavioral data) was processed to extract acoustic, linguistic, and performance features using established feature extraction tools and algorithms, creating derived features for analysis and modeling purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and generation method of the data is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9712,
      "completion_tokens": 275,
      "total_tokens": 9987
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3.1 and 5 (Models and Results)",
          "reasoning": "The dataset is used to train binary Random Forest classifiers from scratch to predict depression presence or absence. The models are trained and cross-validated from randomly initialized parameters with this dataset, indicating supervised training rather than fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Results and Discussion)",
          "reasoning": "The dataset is used for evaluating model performance on depression classification, including on test hold-out sets, to benchmark predictive effectiveness."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Results and Discussion)",
          "reasoning": "The authors perform detailed feature importance analyses and regression to analyze associations between dataset features and specific depression symptoms, using the dataset primarily for analytic insights."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10435,
      "completion_tokens": 575,
      "total_tokens": 11010
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes only a single language, English, as stated in Section 2.2: participants were adult, native English speakers."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains no indication of two languages present; only English is specified as the language of participants (Section 2.2)."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 'Online Study Setup', and Section 3 'Dataset'",
          "reasoning": "The dataset consists solely of native English-speaking participants (Section 2.2), and speech data recorded from them in English (Image Description Task, Section 2.2.1). This is explicitly stated and implies all the speech data is in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset exclusively contains English speech, as participants were native English speakers (Section 2.2)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain code or programming languages; although code was used for feature extraction and modeling, the dataset itself does not contain programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech and behavioral data, but no entries with mathematical or formal logical notation are present as data points."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is human speech and behavioral task performance; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed or fictional languages in the dataset; all speech recorded is natural English from native speakers."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the speech data is well documented as English spoken by native English speakers (Section 2.2)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains linguistic data in the form of English speech, thus language is present."
        }
      }
    }
  },
  {
    "id": "fara22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7653,
      "completion_tokens": 162,
      "total_tokens": 7815
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention regarding code availability for dataset construction.",
          "reasoning": "The paper does not include any links, references, or statements indicating that the code used for data collection, preprocessing, or dataset construction is publicly available. No repository or codebase is mentioned."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3 (Data Collection and Dataset).",
          "reasoning": "The paper documents the dataset creation process extensively, including detailed descriptions of the data collection platform (Thymia Research Platform), participant recruitment criteria, task designs (Image Description Task and n-Back Task), data modalities collected, quality control procedures, and dataset splits. This information provides transparent and relatively complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8368,
      "completion_tokens": 155,
      "total_tokens": 8523
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1: Introduction; Section 5.1: Datasets; Table 2; Table 3",
          "Reasoning": "The paper explicitly states the creation of a new dataset of 6,681 instances of voice-friendly hints, consisting of triples: initial question, related questions, and follow-on hint, across 9 different domains. The dataset includes annotations and is used for pretraining and training hint generation models. The hints are human-written, as indicated by 'human written hints' in the abstract and annotation guidelines in the methodology. There is no indication that the data were generated by models or derived from unknown sources."
        }
      ]
    }
  },
  {
    "id": "faustini23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9168,
      "completion_tokens": 302,
      "total_tokens": 9470
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 5.3.2 Human Evaluation and Section 7 Human Evaluation Studies",
            "reasoning": "The paper describes that human evaluations involve multiple annotators listening to hints and judging their correctness, naturalness, coverage, and retention. These tasks are typical for crowd or non-expert annotators rather than only subject-matter experts. There is no explicit claim that experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4 (Data Collection) and Section 2 (Linguistic Task and Background)",
            "reasoning": "The paper states that annotators were provided with guidelines based on linguistic desiderata (Table 1) outlining characteristics of good hints (e.g., cohesion, length) and syntactic transformations to construct utterances. This constitutes detailed instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "There is no mention of scoring rubrics or criteria for quantitative grading in the annotation guidelines. The annotations focus on producing voice-friendly hints and human evaluation on metrics such as coverage and syntactic correctness, but no fixed rubric evaluation is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not mention providing annotation examples or illustrative samples in the instructions given to annotators. Only guidelines and linguistic desiderata are described."
          }
        }
      ]
    }
  },
  {
    "id": "faustini23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10358,
      "completion_tokens": 388,
      "total_tokens": 10746
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human annotator who was a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human annotators with subject matter expertise or from the target demographic performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit information suggesting that a single human non-expert conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5.3.2 Human Evaluation and Section 7.1 Syntactic Correctness and Coverage",
          "reasoning": "The dataset of voice-friendly hints was created by human annotators following guidelines. Human evaluators, likely multiple non-expert annotators, performed syntactic correctness and coverage evaluations across hundreds of examples (e.g., assessing 500 hints). The paper does not clarify whether these annotators were experts or non-experts, but since no mention of expertise is present, it is appropriate to assume multiple human non-expert annotators conducted the quality evaluations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance being performed by an AI model as a judge of dataset annotations or quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of automated verification or rule-based algorithmic checking of dataset annotations for quality assurance in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents human evaluation of the dataset annotations and measurement of syntactic correctness, coverage, and naturalness with human annotators, indicating that quality assurance processes were in place."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9968,
      "completion_tokens": 480,
      "total_tokens": 10448
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Abstract; Section 5.1 (Datasets)",
          "reasoning": "The paper introduces a new dataset of 6,681 instances comprising triples of initial questions, related questions, and human written follow-on hints created following detailed annotation guidelines and quality checks, as described in the Abstract and Section 5.1. The data is original content created from scratch by human annotators specifically for this task, not translated or adapted from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset is generated entirely by AI or machine learning models without reference to existing data. Generated hints by models are outputs during experiments, not datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset involved human translation from another language; the dataset appears to be originally created in English."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is described or used in the dataset creation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not merely collected or aggregated from existing sources; it was constructed via annotation from related questions and human authored hints."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets); Section 2 (Linguistic Task and Background)",
          "reasoning": "The dataset includes related questions retrieved/generated by a voice assistant system (i.e., existing questions), from which human annotators derived the follow-on hints by applying linguistic transformation principles such as converting direct questions into indirect content clauses. Thus, the data is based on existing question sources but involves transformations and adaptations by human annotators."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of dataset creation is explicitly described, so the origin is specified and documented."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10486,
      "completion_tokens": 533,
      "total_tokens": 11019
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 5.1 (Datasets), and Section 5.2 (Baselines and Approaches)",
          "reasoning": "The paper explicitly states that the SINGLE-HINTS dataset (also called RS data) of voice-friendly hints is used for pretraining the hint generation approaches to learn the syntactic transformations required to convert questions into inquisitive content clauses. This pretraining helps the model to perform rewriting operations foundational to the task before fine-tuning for voice-friendly hint generation."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the proposed dataset. All Transformer models used (BART, T5) are pre-trained models further fine-tuned or pretrained on the dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2 (Baselines and Approaches) and Section 6 (Evaluation)",
          "reasoning": "The dataset is clearly used to fine-tune pre-trained Transformer models (BART, T5) in a supervised setting for the main task of generating voice-friendly hints from related questions. The fine-tuning is done after the pretraining stage or independently (direct hint generation)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of reinforcement learning based post-training methods (such as RLHF) being applied on the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3 (Evaluation Metrics), Section 6 (Evaluation), and Section 7 (Human Evaluation Studies)",
          "reasoning": "The dataset or derived splits (test set of the hint generation dataset) is used to evaluate the quality of generated hints using automatic metrics (BLEU, ROUGE, BERTScore) as well as human evaluation on correctness, naturalness, coverage, and retention."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as used primarily for analysis of trends, patterns, or other characteristics without training or evaluation. The focus is on supervised modeling and assessment."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used or described as a knowledge base to augment models such as in retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes and demonstrates multiple uses of the new dataset in pretraining, supervised fine-tuning, and evaluation stages."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11209,
      "completion_tokens": 537,
      "total_tokens": 11746
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or indicate that the dataset includes more than two human languages. All dataset examples and linguistic discussion focus solely on English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention that the dataset contains exactly two human languages. The dataset is described as dealing with English questions and hints only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Throughout the paper, particularly Abstract and Section 2 (Linguistic Task and Background)",
          "reasoning": "The dataset consists of input questions, related questions, and human-written hints all in English. The linguistic desiderata and syntactic analysis focus exclusively on English, and all examples provided are in English. There is no mention of other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries that are in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural language questions and hints; it does not include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of mathematical or formal logical expressions in the dataset entries. The content is natural language questions and hints."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include non-human communication data such as biological sequences or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages appear in the dataset or paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly English; it is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains natural language (English) data, so it is not without language."
        }
      }
    }
  },
  {
    "id": "faustini23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8427,
      "completion_tokens": 227,
      "total_tokens": 8654
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention indicating code availability is present in the paper.",
          "reasoning": "The paper states that the dataset will become publicly available but does not mention releasing any code related to dataset construction, data collection, or preprocessing. There is no link, footnote, or appendix referencing code repositories associated with the dataset or experiments."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 5.1 (Datasets) and 6 (Evaluation with Automated Metrics) provide dataset descriptions and statistics; Sections 2 and 3 outline linguistic desiderata and dataset construction rationale.",
          "reasoning": "The paper provides detailed descriptions of the new dataset, including its composition (6,681 instances), domains covered (9 domains), and the process for collecting voice-friendly hints with annotation guidelines and quality checks. Tables 2 and 3 summarize dataset statistics for training, development, and testing splits. The linguistic desiderata and objectives guiding dataset construction are articulated in Section 2. This information documents the dataset creation process sufficiently for understanding and reproducing the dataset at a high level."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7827,
      "completion_tokens": 178,
      "total_tokens": 8005
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.2",
          "Reasoning": "The paper describes recordings of speech audio signals of 5 subjects who produced 600 logatomes at different speaking rates, recorded in a soundproofed audio studio with microphones and audio interfaces, indicating human-generated audio data."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.2",
          "Reasoning": "Tongue movements were measured using Electro-Optical Stomatography (EOS) which includes optical distance sensors mounted on individualized artificial palates worn by 5 subjects. These articulatory measurements are sensor data recorded from human subjects, hence human generated signal/sensor data."
        }
      ]
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8627,
      "completion_tokens": 224,
      "total_tokens": 8851
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.4 Data analysis",
            "reasoning": "The diphthong segmentations and annotations such as marking acoustic start and end times of diphthongs and articulatory transition times were performed manually by the researchers as described in Section 2.4, indicating single human experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide specific details or direct references to annotation instructions or guidelines given to annotators for manual segmentation or annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention or indication of scoring rubrics or formal criteria used in annotation processes; the annotation appears to be a manual marking of timepoints rather than rating or scoring that would require rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not present or mention any examples of annotated data or examples provided to annotators as guidance."
          }
        }
      ]
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9817,
      "completion_tokens": 353,
      "total_tokens": 10170
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any single human non-expert performing quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple human non-experts involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although analysis used automated regression and signal processing techniques, these serve for data analysis rather than explicit quality assurance or validation of annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any formal quality assurance or validation steps for the annotation or dataset besides initial manual marking of acoustic diphthong boundaries and exclusion of unusable samples. No explicit QA process or annotator verification is documented."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9427,
      "completion_tokens": 375,
      "total_tokens": 9802
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Subjects, corpus and experimental procedure",
          "reasoning": "The data is collected from five healthy male subjects who produced logatomes containing German diphthongs at various speaking rates, while articulatory and acoustic signals were recorded. This new data collection involved human participants and original recordings, not derived from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation of data or content between languages performed by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate or adapt any data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected by aggregating or collecting existing data sources; rather, it was newly recorded from experimental sessions."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some calculations and processing were performed on the recorded data, the dataset itself is original and not based on pre-existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and method of data generation."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9945,
      "completion_tokens": 325,
      "total_tokens": 10270
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2 (Method), 2.4 (Data analysis), and 3 (Results)",
          "reasoning": "The dataset of tongue movement and acoustic recordings of German diphthongs is primarily used for analyzing articulatory-acoustic relationships, characterizing trends and patterns in tongue movements relative to acoustic intervals, rather than for training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates usage of the dataset for analysis of articulatory and acoustic characteristics, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10668,
      "completion_tokens": 542,
      "total_tokens": 11210
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper's proposed dataset consists exclusively of German diphthongs spoken by native German speakers. There is no indication that more than two languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only German language data; no second language is present."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although English translations appear in the text, the dataset entries are German language utterances only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (Subjects, corpus and experimental procedure)",
          "reasoning": "The dataset consists of recordings of 5 native German speakers producing logatomes containing German diphthongs, spoken within German carrier phrases. Thus, the dataset is monolingual and non-English (German)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or code-related content is included in the dataset; the data consists of speech and articulatory signals only."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the paper uses mathematical notation for analysis and modeling, the dataset entries themselves do not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech production data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset only contains naturally occurring German language utterances; no fictional or constructed languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is clearly specified as German."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains linguistic content in German and thus is not without language."
        }
      }
    }
  },
  {
    "id": "fietkau22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7886,
      "completion_tokens": 163,
      "total_tokens": 8049
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No code repository or link is provided in the paper.",
          "reasoning": "The paper does not mention the availability of any code related to data collection, preprocessing, or analysis, nor does it provide any repository link or URL for code access."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method) and subsections 2.1, 2.2, 2.3, 2.4 describe the data collection, corpus, measurement setup, and data processing.",
          "reasoning": "The paper provides a detailed account of the dataset creation process, including the recording setup, subjects, corpus composition, recording procedure, sensor calibration, and data postprocessing steps, giving transparency about how the new dataset was constructed."
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7252,
      "completion_tokens": 165,
      "total_tokens": 7417
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The new dataset consists of about 1000 hours of an in-house dataset containing medical conversations in English. The audio recordings were collected from conversations involving doctors and patients with varying acoustic conditions, recorded using a device carried by the doctor, thus involving human participants and human-operated recording devices."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The transcribed text of the in-house medical conversations was manually transcribed and annotated for PII sequences by human annotators, confirming human generation of the textual modality associated with the new dataset."
        }
      ]
    }
  },
  {
    "id": "flechl22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8052,
      "completion_tokens": 232,
      "total_tokens": 8284
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2 De-identification",
            "reasoning": "The paper states that de-identification starts with manual annotation of the transcribed text where PII is tagged and given labels. Manual annotation implies human annotators, and the complexity and critical nature of PII tagging in medical conversations suggest use of multiple human experts to ensure consistency and correctness."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 De-identification",
            "reasoning": "Section 2 mentions the manual annotation of PII entities with specific categories (~30 labels), which implies existence of instructions for annotators to follow a consistent labeling schema and guidelines for identifying PII tokens."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not mention any scoring rubrics or explicit criteria for quality assessment of the annotations, beyond the labeling categories."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not mentioned in the paper",
            "reasoning": "No annotation examples or guidelines containing example annotations are provided or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "flechl22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9242,
      "completion_tokens": 344,
      "total_tokens": 9586
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 (De-identification) and Section 4 (Data)",
          "reasoning": "The paper states that the training data consists of manually transcribed medical conversations where PII sequences are annotated afterwards. Given the sensitive nature of medical data and the requirement to tag PII with roughly 30 labels, it is highly likely that this annotation was performed by human experts or persons with domain knowledge. Although the paper does not explicitly state the number or the expertise level of annotators, the manual annotation on medical conversations suggests expert involvement."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple human experts participated in annotation or quality assurance. Therefore, this cannot be confirmed."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed QA or annotation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert annotators were involved."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by using an AI model as judge for dataset annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an automatic or algorithmic verification process for quality assurance of the annotations or dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A manual annotation process of PII sequences is described, indicating that some form of quality assurance through human inspection is performed. Thus, the QA process is documented."
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8852,
      "completion_tokens": 478,
      "total_tokens": 9330
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of any completely new original content from scratch by human contributors. The training data used is an in-house dataset of medical conversations that are existing data, not newly created from scratch."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2 ('Text-to-speech')",
          "reasoning": "The paper describes generating artificial audio for surrogate PII tokens using text-to-speech (TTS) synthesis with state-of-the-art neural TTS models. This synthetic audio is newly generated by AI models without direct reference or transformation of existing audio data for those tokens."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that data was produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 ('Data')",
          "reasoning": "The original training data is an in-house medical conversation corpus collected and transcribed previously, thus collated from existing real-world recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 ('Splicing using audio from the corpus') and Sections 3.1 to 3.3 in general",
          "reasoning": "The paper describes generating training data variants by replacing PII tokens with surrogate tokens and creating corresponding audio either by splicing together fragments of existing audio corpus recordings (possibly combining audio pieces from different speakers) or by TTS synthesis. This process creates adapted or transformed data derived from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and methods of generating augmented data, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9370,
      "completion_tokens": 286,
      "total_tokens": 9656
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The in-house medical conversation dataset is used to train end-to-end ASR models from scratch, as described in Section 4.2. Various models are trained on the de-identified and augmented data subsets for 80 epochs until convergence."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The dataset is used for evaluation in testing the ASR models' performance by measuring word error rates and F1 scores on the held-out test data with and without de-identification and augmentation methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.3 and 5",
          "reasoning": "The paper analyzes trends in recognition performance degradation due to de-identification and recovery by augmentation methods, with detailed analysis of recall, precision, and diarization error metrics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10093,
      "completion_tokens": 515,
      "total_tokens": 10608
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as containing medical conversations conducted in English only (Section 4.1), with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages; it is exclusively English as described in Section 4.1."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The proposed dataset consists of English medical conversations exclusively, as explicitly stated in Section 4.1: 'The training data consist of a subset of about 1000 hours of an in-house dataset containing medical conversations conducted in English covering about twenty different medical specialties.'"
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English datasets or languages are mentioned in the paper."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio and transcriptions; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech conversations only, no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned or included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the dataset are clearly stated as English; no uncertainty is indicated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain natural language (spoken English), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "flechl22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7311,
      "completion_tokens": 174,
      "total_tokens": 7485
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section provides code or code repository links.",
          "reasoning": "The paper does not mention or provide any link to publicly available code for data collection, de-identification, surrogate generation, TTS processing, or splicing used in constructing the dataset or generating augmented training data."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Data, Section 3 De-identification, Section 3 Strategies for performance recovery",
          "reasoning": "The paper includes detailed descriptions of the training and test datasets, the manual annotation and labeling of PII, the forced alignment for audio intervals, surrogate generation methods, and two audio generation methods (TTS and splicing). These sections document the dataset creation and augmentation processes thoroughly, providing transparency on how the datasets were constructed and de-identified."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7575,
      "completion_tokens": 152,
      "total_tokens": 7727
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The Earnings21 dataset consists of audio recordings of earnings calls from public companies, which are recordings of real human speech, hence human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data",
          "Reasoning": "The authors created and released contextual biasing lists (word and phrase lists compiled from manually curated PERSON and ORG entities tagged by spaCy) accompanying the Earnings21 dataset. These biasing lists are text data manually curated for this work, hence human-generated text."
        }
      ]
    }
  },
  {
    "id": "fox22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8375,
      "completion_tokens": 230,
      "total_tokens": 8605
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Data",
            "reasoning": "The biasing lists for the Earnings21 dataset were manually curated from PERSON and ORG entities automatically tagged by spaCy, indicating human involvement in annotation, likely multiple experts for entity curation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Data",
            "reasoning": "The paper states that the biasing lists were manually curated but does not mention detailed annotation instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Data",
            "reasoning": "No mention is made of scoring rubrics or criteria for annotators in creating or evaluating the biasing lists."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.1 Data",
            "reasoning": "The paper provides examples of biasing list entries such as \u201cNokia\u201d, \u201cSuzanne Sitherwood\u201d, \u201cPepsico\u201d, and \u201cElon Musk\u201d from the lists that are publicly available in the Earnings21 repository, effectively serving as examples."
          }
        }
      ]
    }
  },
  {
    "id": "fox22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9565,
      "completion_tokens": 447,
      "total_tokens": 10012
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance for the dataset annotations or content was performed by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper that multiple human experts performed quality assurance on the datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single non-expert human annotator on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human non-experts performed quality assurance on the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of an AI model acting as a judge or for quality assurance purposes for the dataset content or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses automated methods such as spaCy for named entity recognition to create biasing lists, and alignment tools to extract error segments, these are part of dataset creation and model training processes rather than explicit quality assurance or verification of dataset annotation quality. There is no clear description of automated quality assurance processes for validating dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any explicit quality assurance process applied to the new datasets introduced (contextual biasing lists accompanying the Earnings21 dataset). The biasing lists were manually curated from named entity outputs but details of verification or validation of this curation are not provided."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9175,
      "completion_tokens": 478,
      "total_tokens": 9653
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The authors created new contextual biasing lists for the Earnings21 dataset, manually curated from named entities detected in the Earnings21 references using spaCy. The lists include a 'best case scenario' list and a 'distractor' list. This is original content created by human contributors from scratch relevant to this research, as described in Section 4.1."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any dataset generated solely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data produced by human translation of text from another language is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of existing data is described or referenced in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The Earnings21 contextual biasing lists are compiled from existing named entities automatically tagged by spaCy on the Earnings21 references, with manual curation. Hence, the biasing lists are aggregated from existing sources (the Earnings21 dataset's reference annotations) without fundamental modification, fitting the definition of collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The biasing lists derive from automatic entity recognition outputs which are then manually curated to produce final biasing lists. This involves filtering and selecting entities, thus representing data based on existing sources with transformations or adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the source and method of generation of the new biasing lists; thus, the data origin is documented."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9693,
      "completion_tokens": 338,
      "total_tokens": 10031
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Data, Section 5.1 Earnings21 Benchmark Baselines",
          "reasoning": "The authors introduce contextual biasing lists created to accompany the Earnings21 dataset to form a public benchmark for contextual ASR evaluation. They report results on the Earnings21 test set using these biasing lists to measure model performance, particularly recall of bias terms, rare, and OOV words to evaluate recognition improvements."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The Earnings21 contextual biasing lists are clearly used for evaluation purposes as a benchmark; thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10416,
      "completion_tokens": 543,
      "total_tokens": 10959
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper introduces the Earnings21 dataset used for evaluation, which contains earnings calls from a variety of public companies in English only. There is no mention of multiple human languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only English content, with no indication of entries in exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The Earnings21 dataset, introduced in this paper with contextual biasing lists, consists of earnings calls from public companies and is described as containing proper nouns and phrases in English only. The paper does not mention the presence of any other languages, indicating the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain non-English language content exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper discusses models and code implementations, the dataset entries themselves are natural language transcripts, not programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are human language transcripts from earnings calls without presence of mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human language speech transcripts; there is no reference to biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of dataset entries is clearly identified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language transcripts, so language is definitely present."
        }
      }
    }
  },
  {
    "id": "fox22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7634,
      "completion_tokens": 360,
      "total_tokens": 7994
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.1 Data and Section 6 Conclusions",
          "reasoning": "The paper states that for the Earnings21 dataset, they created contextual biasing lists and that both the lists and Earnings21 dataset are publicly available in the Earnings21 repository [3]. The paper mentions that these resources enable replication by other researchers. Also, the ASR model used is open-source from the WeNet toolkit and datasets like GigaSpeech are open-source, implying that the required code and data are accessible. Although an explicit link to code generating the biasing lists is not directly given in the provided excerpt, the mention that both lists can be found in the Earnings21 repository supports that at least the dataset and the biasing lists are publicly provided. There is no explicit mention of code for dataset construction being publicly available, but since the biasing lists are constructed by manual curation from spaCy entity tagging and are released, it suggests at least partial reproducibility support. Hence, considering the code used to generate the contextual biasing lists is likely shared or easily replicable, the code availability is assessed as true."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The paper provides descriptive detail about how the contextual biasing lists were created: they were manually curated from PERSON and ORG entities automatically tagged by spaCy from the Earnings21 references. The authors describe the creation of a best case list and a distractor list by adding Fortune 500 companies and famous CEOs. The sizes of these lists and examples are provided. Also, procedural details regarding audio segmentation and scoring against references are given. This constitutes clear and sufficient documentation of how the dataset and biasing lists were created and used."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7996,
      "completion_tokens": 190,
      "total_tokens": 8186
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "The paper states that approximately 436 hours (around 200,000 utterances) of reading speech by Chinese L2 adult learners were collected and used for the masked language modeling (MLM) pre-training, indicating this is a new, human-recorded non-native speech dataset introduced by the authors for pre-training."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "Along with the non-native speech, the authors collected the associated prompt text used in the read aloud tasks for the MLM pre-training. This prompt text is human-generated as it was prepared for eliciting learner speech during data collection, representing a new dataset introduced by the authors for pre-training purposes."
        }
      ]
    }
  },
  {
    "id": "fu23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8796,
      "completion_tokens": 249,
      "total_tokens": 9045
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Data description and Table 1",
            "reasoning": "The ByteRead dataset is described as an internal dataset collected from Bytedance's education product and contains human-annotated fluency scores. The Speechocean762 dataset is a public non-native English speech corpus collected from multiple speakers annotated with human fluency scores. The paper references human annotation for these fluency scores, implying multiple human experts conducted the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not provide any direct information or mention detailed annotation instructions given to annotators for the ByteRead dataset or Speechocean762 dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "No explicit details on scoring rubrics are described or referenced within the paper for the human annotations in the ByteRead or Speechocean762 datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not mention providing examples or annotation examples in the annotation guidelines for the datasets."
          }
        }
      ]
    }
  },
  {
    "id": "fu23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9986,
      "completion_tokens": 409,
      "total_tokens": 10395
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance of the datasets' fluency scores was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about multiple human expert annotators performing quality assurance or annotation on the datasets. Although it mentions 'human-annotated scoring data', there is no detail on how many annotators or their expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no specific mention in the paper about quality assurance done by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance done by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses AI models for fluency scoring and pre-training but does not indicate that an AI model was used as a judge for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any automatic verification or algorithmic/rule-based quality assurance process applied to the fluency scoring labels or dataset content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper provides no explicit information or details about any quality assurance process for the annotation/scoring of the datasets used (including the introduced internal dataset). Hence, no quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9596,
      "completion_tokens": 483,
      "total_tokens": 10079
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper states that the authors collected approximately 436 hours (about 200,000 utterances) of reading speech by Chinese L2 adult learners and prompt text for MLM pre-training. Furthermore, the ByteRead internal dataset of 14,000 English utterances is described as collected from Bytedance's education product, indicating original content created by humans. This data was created from scratch by human contributors for the purpose of this research."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used to generate datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The authors also use pre-existing datasets such as LibriSpeech and Speechocean762. The LibriSpeech dataset (960 hours) and the Speechocean762 dataset are publicly available datasets collected previously; incorporating these without significant modification constitutes collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1 and Section 4.2",
          "reasoning": "The authors derive phone-level raw features (deep features, phone sequence, and duration) from the raw audio and forced alignment using an acoustic model and forced aligner. These features are created by transforming existing raw audio data into new representations used for model training."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly details dataset sources and methods of generation, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10114,
      "completion_tokens": 604,
      "total_tokens": 10718
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1 and Table 1 (Data splitting), Section 4.1 (Speech corpora)",
          "reasoning": "The authors introduce a new unlabeled dataset consisting of approximately 436 hours (about 200,000 utterances) of reading speech by Chinese L2 adult learners and prompt text used exclusively for self-supervised phonetic and prosody-aware pre-training. This dataset is used to pre-train their model by masking phones and durations and reconstructing them, as described in Section 3.1."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new pre-training dataset to train models from scratch; instead, it uses it for pre-training only."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Fine-tuning for fluency scoring; Section 4.1 and Table 1 (Data splitting); Section 5 Results",
          "reasoning": "The authors use human-annotated labeled datasets, including ByteRead and Speechocean762, which are new or internal datasets, to fine-tune the pre-trained model using supervised learning with fluency scores."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence of reinforcement learning based post-training (e.g., RLHF) using the dataset is provided."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Speech corpora); Section 5 Results and analyses",
          "reasoning": "The Speechocean762 and ByteRead datasets, including their splits, are used to evaluate and benchmark the fluency scoring performance of models pretrained and fine-tuned with the new datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2 Ablation studies",
          "reasoning": "The authors perform an analysis of the contributions of phonetic and prosodic components in the pre-training dataset to fluency scoring performance, using ablation studies."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new datasets introduced in the paper are used in multiple stages of the machine learning pipeline, including pre-training, supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10837,
      "completion_tokens": 461,
      "total_tokens": 11298
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1, 'Speech corpora', and throughout the paper",
          "reasoning": "The new dataset introduced by the authors for pre-training and fine-tuning is described as consisting of English speech data from Chinese L2 adult learners reading English prompts (Section 4.1). The pre-training corpus contains approximately 436 hours of reading speech by Chinese L2 adult learners, and the fine-tuning uses ByteRead, an internal English dataset, and Speechocean762, a public English non-native speech corpus. All data is English speech produced by second language learners, and the prompt texts are in English. There is no mention of any other human language being present in the proposed datasets. Therefore, the datasets introduced are monolingual with English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language (English), so the N/A category does not apply."
        }
      }
    }
  },
  {
    "id": "fu23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8055,
      "completion_tokens": 223,
      "total_tokens": 8278
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper does not provide any links or mention availability of code related to dataset construction, preprocessing, or generation. There is no indication that the code used for constructing the new datasets or for the data processing is publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Speech corpora) and Section 4.2 (Feature extraction)",
          "reasoning": "The paper documents the dataset collection and preparation processes to some extent. It describes the collection of a large unlabeled non-native speech dataset (~436 hours, about 200,000 utterances) for pre-training and an internal dataset named ByteRead of 14,000 English utterances used for fine-tuning and evaluation. It also details the data splitting in Table 1. Additionally, feature extraction procedures and forced alignment details are described in Section 4.2. However, it lacks comprehensive step-by-step details about how the datasets were collected or constructed and does not provide exhaustive code or scripts for reproducibility."
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7205,
      "completion_tokens": 113,
      "total_tokens": 7318
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract and Section 3.1",
          "Reasoning": "The paper introduces an in-house 2.6k-hour Chinese dataset with labeled Mandarin-Cantonese speech recorded for ASR model training, explicitly mentioned as 'our in-house 2.6k-hour Chinese dataset' used in experiments. It consists of human-recorded speech signals used for training ASR models, thus the data is audio modality with human-generated origin."
        }
      ]
    }
  },
  {
    "id": "fu23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8005,
      "completion_tokens": 227,
      "total_tokens": 8232
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1, Section 4.2",
            "reasoning": "The new dataset is an in-house 2.6k-hour Chinese dataset (Mandarin-Cantonese) introduced in the paper. Such datasets for ASR typically require expert human annotation for transcription. The paper details experiments conducted on this dataset but does not mention any model-generated or automatic annotations, implying that human experts performed the annotations for supervised training."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not explicitly provide or describe any detailed annotation instructions or guidelines for the transcription of the new in-house Chinese dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no mention in the paper of any scoring rubrics or quality control criteria applied during annotation of the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not include or refer to any annotation examples or annotated samples for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "fu23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9195,
      "completion_tokens": 386,
      "total_tokens": 9581
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human experts for dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human annotator was involved in quality assurance of the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify involvement of multiple non-expert human annotators in dataset quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mentions are made of AI models being used as judges for dataset quality assurance or annotation validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe any automated quality assurance processes, such as automated verification of code, annotations, or dataset content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper references existing datasets (such as LibriSpeech and an in-house 2.6k-hour Chinese speech dataset) but does not introduce any new datasets or describe any quality assurance process for dataset annotations or content. Therefore, no QA process is applied or documented for datasets in this paper."
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8805,
      "completion_tokens": 423,
      "total_tokens": 9228
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any original speech data was created entirely from scratch by human contributors. The datasets used are existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was generated by AI or machine learning models; rather, it uses existing speech datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by human translation between languages in the provided paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine-translated data for training the models or evaluation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 (implicit in descriptions of datasets), Section 1 Introduction",
          "reasoning": "The paper uses existing datasets such as the 1k-hour English LibriSpeech dataset and an in-house 2.6k-hour Chinese dataset. The Chinese dataset is described as in-house but no indication it is newly created; rather it appears collected from existing sources. Therefore, the data appears to be collected and aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any extensive transformations or modifications to existing datasets to create new datasets; it focuses on model fusion methods rather than data adaptation or derivation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are clearly specified as existing datasets, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9323,
      "completion_tokens": 339,
      "total_tokens": 9662
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The proposed datasets, including the in-house 2.6k-hour Chinese labeled dataset and the 1k-hour English labeled dataset, are used to fine-tune the fused model after model fusion via moderate supervised training. This is described in Sections 4.1 and 4.2 where the ASR systems are fine-tuned on these labeled datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The datasets, including the original LibriSpeech test-clean set and the in-house Chinese test sets, are used for evaluation of recognition performance (WER and CER) of the individual and fused models as shown in Section 4.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4.3",
          "reasoning": "Datasets are used for analysis of the complementarity and optimization properties of SL and SSL models via error analysis and loss landscape visualization, as discussed in Section 3.1 and the ablation studies in Section 4.3."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10046,
      "completion_tokens": 530,
      "total_tokens": 10576
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4 (Main experiment), Abstract",
          "reasoning": "The newly introduced datasets are the in-house 2.6k-hour Chinese dataset which includes Mandarin and Cantonese languages, and the 1k-hour English LibriSpeech dataset. Since the Chinese dataset includes two different languages and the study involves both English and Chinese datasets, the combined new dataset spans more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the Chinese dataset consists of Mandarin and Cantonese (two languages), the overall proposed dataset considered in the study also includes English data. Therefore, the dataset is not limited to exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is not only English but also includes Chinese languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced includes English and Chinese, so it is not monolingual in non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are speech recognition datasets and do not include mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of human speech data and do not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in the proposed datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the datasets are explicitly stated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human languages (English, Mandarin, Cantonese)."
        }
      }
    }
  },
  {
    "id": "fu23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7264,
      "completion_tokens": 219,
      "total_tokens": 7483
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part in the paper mentions code availability for dataset construction or processing.",
          "reasoning": "The paper does not mention or provide any link or information regarding the availability of code repositories related to data collection, preprocessing, or dataset construction. The experimental section describes datasets used but does not refer to releasing code for dataset creation or processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 and 4.2 (Experimental Setup and Main experiment)",
          "reasoning": "Although the paper does not introduce a completely new dataset, it uses an in-house 2.6k-hour Chinese dataset and a 10k-hour unlabeled Mandarin-Cantonese dataset described with details such as data composition and labeling ratio. The paper provides some information about the dataset labels, language ratio, and evaluation setup, which indicates some documentation about the dataset creation and usage process. However, the details are limited and do not constitute a complete dataset construction description, but the available details are enough to mark documentation as present."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7251,
      "completion_tokens": 137,
      "total_tokens": 7388
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 and 3.1.2 (Data collection and Data annotation)",
          "Reasoning": "The authors collected a novel Mandarin Cued Speech corpus consisting of videos recorded by five volunteers using their personal mobile phone front-facing cameras. These videos are human-recorded data capturing lip and hand movements in Mandarin CS, annotated manually with ELAN and Praat tools. The data collection and annotation processes are explicitly described, confirming the videos are human-generated and captured through human involvement as part of this new dataset introduced by the authors."
        }
      ]
    }
  },
  {
    "id": "gao23c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8051,
      "completion_tokens": 178,
      "total_tokens": 8229
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1.2",
            "reasoning": "The paper states that annotations for hand and acoustic temporal segments were manually done and double-checked, indicating multiple human annotators performed the annotation with verification."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1.2",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1.2",
            "reasoning": "There is no information in the paper about any scoring rubrics or criteria used during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1.2",
            "reasoning": "The paper does not describe providing any annotation examples or exemplar annotations for guidance."
          }
        }
      ]
    }
  },
  {
    "id": "gao23c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9241,
      "completion_tokens": 422,
      "total_tokens": 9663
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that the quality assurance was conducted by a single human annotator with subject matter expertise or as a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.2 Data annotation",
          "reasoning": "The annotations were all manually done and double-checked, indicating that multiple human annotators likely participated. Given the nature of the dataset (Mandarin CS corpus with phoneme-level annotations for lip and hand movements), it is highly probable that annotators have relevant expertise or belong to the target demographic. Although the paper does not explicitly state annotator expertise, the careful annotation and double-checking imply a rigorous process typical of expert involvement."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence suggests that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence suggests that quality assurance was conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model being used as a judge or for quality assurance of the annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated verification or algorithmic quality assurance process applied to the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A clear quality assurance process is described involving manual annotation and double-checking, so QA is present."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8851,
      "completion_tokens": 369,
      "total_tokens": 9220
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The paper states that the authors collected a new multi-cuer Mandarin cued speech corpus consisting of videos recorded by five volunteers (three normal hearing and two hearing-impaired) who learned Mandarin CS and recorded themselves. The videos were manually annotated as well. This data is original content created entirely from scratch by human contributors and not derived from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any new data was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention producing data through human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating data by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was collected newly from participants rather than collated from existing datasets or sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is newly created and not derived by modifying or transforming existing corpora."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as collected directly from newly recorded videos by the authors."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9369,
      "completion_tokens": 459,
      "total_tokens": 9828
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Vowel recognition",
          "reasoning": "The Mandarin CS corpus was used to train a Slowfast video classification network for multi-modal vowel recognition. The network was initialized randomly and trained from scratch using the collected corpus data (Section 4.2)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the dataset for supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning based post-training methods involving the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Results and Discussion",
          "reasoning": "The dataset was used to evaluate the performance of hand preceding time (HPT) prediction models via metrics such as MSE, MHCD, and vowel recognition accuracy, thus serving an evaluation purpose."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Hand preceding analysis and Section 5.2 Comparison between normal and hearing-impaired",
          "reasoning": "The dataset was used primarily to analyze lip-hand asynchrony phenomena, differences between normal and hearing-impaired cuers, and statistical characteristics of hand preceding time."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset being used as a knowledge base for augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is demonstrably used for training, evaluation, and analysis purposes."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10092,
      "completion_tokens": 526,
      "total_tokens": 10618
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a Mandarin Cued Speech (CS) corpus containing Mandarin language data only; no other human languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset contains only Mandarin CS data and does not involve exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced does not contain any English language content; it is focused on Mandarin Chinese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Data collection",
          "reasoning": "The newly developed corpus is a Mandarin Cued Speech corpus, as explicitly stated in Section 3.1.1. The dataset contains Mandarin Chinese videos only, with no mention of any other human language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of annotated video data with lip and hand gesture temporal segments; it does not contain programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation for modeling and analysis, the dataset itself does not contain entries with mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human Cued Speech video data and annotations, not biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mandarin CS encodes Mandarin Chinese phonemes; it is not a constructed or fictional language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as Mandarin Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data (Mandarin Chinese) and thus does not fall under N/A."
        }
      }
    }
  },
  {
    "id": "gao23c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7310,
      "completion_tokens": 175,
      "total_tokens": 7485
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any URL, link, or mention of a public repository for code related to dataset construction, data collection, or annotation procedures. The methodology and experimental setup describe data collection, annotation tools, and preprocessing, but no code availability is stated."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Mandarin CS Corpus) and Section 3.1.1 and 3.1.2",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including number and profile of cuers, recording conditions, corpus size, annotation methodology, annotation tools used (ELAN and Praat), and manual checking procedures. This documentation sufficiently describes the dataset creation process for understanding and potential reproduction."
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9922,
      "completion_tokens": 202,
      "total_tokens": 10124
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Source; Abstract; Section 4.1 Experiment Setup; Table 1",
          "Reasoning": "The dataset, LibriCrowd, contains 100 hours of English speech audio recordings collected from the LibriVox project, which is a public resource of audio books read by human volunteers. These audio recordings are human speech recorded by humans reading books."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Source; Section 4.1 Experiment Setup; Table 1; Abstract",
          "Reasoning": "The associated transcriptions in LibriCrowd are crowdsourced human transcriptions collected via Amazon Mechanical Turk workers transcribing the audio recordings. The text data is manually transcribed by 4433 human transcribers, as explicitly stated in the paper."
        }
      ]
    }
  },
  {
    "id": "gao23f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10722,
      "completion_tokens": 252,
      "total_tokens": 10974
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2. Data Collection, Paragraphs 1 and 2",
            "reasoning": "The paper states that the LibriCrowd dataset is collected via crowdsourcing on Amazon MTurk with 4433 human transcribers participated, who are non-expert annotators. It specifically mentions no entrance exam or specific training was required, indicating annotators are non-experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2. Transcription Task Design, Paragraph 1",
            "reasoning": "The paper describes the user interface and instructions provided to annotators, including ignoring punctuation, transcribing digits as words, and allowing marking uncertain words with question marks, showing the presence of detailed instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention found",
            "reasoning": "The paper does not mention any specific scoring rubrics or detailed scoring criteria provided to annotators in the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention found",
            "reasoning": "The paper does not present or reference any annotation examples given to annotators in the guidelines or instructions."
          }
        }
      ]
    }
  },
  {
    "id": "gao23f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11912,
      "completion_tokens": 456,
      "total_tokens": 12368
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human experts with relevant expertise performed quality assurance. Human adjudicators are mentioned but not characterized as experts or multiple expert annotators performing QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not described as being done by any single human non-expert annotator alone."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2 and Table 1",
          "reasoning": "The dataset, LibriCrowd, was crowdsourced from Amazon Mechanical Turk with 4433 human transcribers, who are considered non-experts. Each audio was transcribed by five different workers. This indicates that quality assurance involved multiple non-expert humans performing transcription tasks."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (Confidence Estimation) and Section 3.2 (Error Correction)",
          "reasoning": "The paper describes the use of machine learning models (confidence estimation model and error correction model) to evaluate and improve transcription quality. These AI models detect low-quality transcriptions and correct transcription errors, thus performing quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification of annotations using algorithmic or rule-based verification independent of AI models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes quality assurance processes involving multiple non-expert humans and AI models, thus quality assurance is documented."
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11522,
      "completion_tokens": 500,
      "total_tokens": 12022
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2. Data Collection; Section 4.1 Experiment Setup",
          "reasoning": "The paper describes the collection of the LibriCrowd dataset consisting of 100 hours of English speech transcribed by 4433 human transcribers via crowdsourcing on Amazon MTurk. The transcriptions are original human-created content, produced entirely from scratch, not translated or adapted from pre-existing material. This is explicitly stated in Section 2.1 where the authors mention starting from raw audios and collecting human transcriptions for building the speech corpus."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated entirely by AI or machine learning models without human transcription. Models are only used for quality estimation and error correction of human transcriptions, not to generate original transcriptions."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is English speech transcriptions collected by humans; there is no mention of the data being produced via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Source",
          "reasoning": "The audio files are collected from the LibriVox project, an existing source of English speech audio books. The authors collected and aggregated audio from this existing source without significant modification, then collected new human transcriptions for them. Thus, the base audio data is collated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes processing such as confidence estimation and error correction to improve transcription quality, the dataset itself is not described as derived or adapted from existing transcription datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12040,
      "completion_tokens": 340,
      "total_tokens": 12380
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The LibriCrowd dataset is used to fine-tune pre-trained ASR models such as Wav2Vec2 and WavLM using supervised learning methods. The paper presents experiments in Section 4.3 demonstrating that using LibriCrowd with improved transcription quality leads to a significant reduction in word error rate (WER) of these ASR models, indicating its utility in supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "LibriCrowd is used for evaluating the quality of human transcriptions by measuring Transcription Word Error Rate (TWER) against ground truth, as well as for evaluating the impact of transcription error on ASR performance. The dataset serves to benchmark transcription quality and ASR model performance under different transcription noise conditions."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The dataset is employed to analyze the correlation between transcription errors and ASR model performance, exploring trends and characteristics of transcription quality impact on downstream tasks."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12763,
      "completion_tokens": 509,
      "total_tokens": 13272
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset LibriCrowd contains only English speech transcriptions; there is no mention of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are exclusively English speech transcriptions; there is no indication of exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment Setup; Table 1 Dataset Statistics",
          "reasoning": "The LibriCrowd dataset consists of 100 hours of English speech transcribed by human annotators. The data source is LibriVox audiobooks with English text as ground truth, which confirms the dataset is purely monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any non-English language transcriptions; it is exclusively English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech transcriptions and does not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are human speech transcriptions and do not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is explicitly specified as English, based on source and annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains linguistic content (English speech transcriptions)."
        }
      }
    }
  },
  {
    "id": "gao23f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9981,
      "completion_tokens": 149,
      "total_tokens": 10130
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3",
          "reasoning": "The abstract states \"We release the dataset and code to benefit the research community.\" The paper also describes ML-in-the-loop modules (CEM and ECM) and quality improvement code. Thus, code related to data collection and processing is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "Sections 2 (Data Collection) and 3 (Quality Improvement) provide detailed documentation on dataset creation, including transcription task design, worker selection, confidence estimation model, error correction model, and quality metrics. The paper thoroughly documents all aspects of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7044,
      "completion_tokens": 153,
      "total_tokens": 7197
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, especially 2.1 Statistics and 2.2 Collection",
          "Reasoning": "The Mandarin Dysarthria Speech Corpus (MDSC) is introduced as a new dataset consisting of 17 hours of audio recordings from 21 dysarthric and 25 control speakers. Participants read wake-up words and non-wake-up words recorded in quiet indoor environments at 16 kHz sampling rate. This data was collected through direct human involvement, i.e., recordings of actual human speech from dysarthric and non-dysarthric individuals, clearly stated in Sections 2.1 and 2.2."
        }
      ]
    }
  },
  {
    "id": "gao24c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7844,
      "completion_tokens": 215,
      "total_tokens": 8059
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.3",
            "reasoning": "Section 2.3 states that intelligibility evaluation uses transcription by five expert annotators to calculate annotation accuracy for dysarthric speech, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "The use of five expert annotators transcribing recordings compared against standard text implies there are instructions or guidelines to perform the transcription consistently, as described in Section 2.3."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "The annotation accuracy calculation as a percentage of accurately transcribed words serves as an objective scoring rubric for the experts' transcription accuracy mentioned in Section 2.3."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention provision of annotation examples in any section; no explicit examples are described as part of the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "gao24c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9034,
      "completion_tokens": 506,
      "total_tokens": 9540
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.3 Intelligibility Evaluation",
          "reasoning": "The paper states that five expert annotators were involved in transcribing the recordings and calculating the annotation accuracy to derive the subjective intelligibility score. This indicates that quality assurance was conducted by multiple human experts for transcription accuracy evaluation. However, since multiple experts were involved, this label applies only if considering each annotator individually. Given the instruction to use multiple labels except mutually exclusive ones, and that annotators are experts, this category can be true if considering individual annotators."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Intelligibility Evaluation",
          "reasoning": "The paper explicitly states that five expert annotators transcribed the recordings and compared them against the standard text. The average of five annotators' percentages was used as the intelligibility score. This constitutes quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.3 Intelligibility Evaluation",
          "reasoning": "In addition to expert annotation, the authors employ open-source ASR models (Paraformer and WENET) to transcribe the speech, measuring recognition accuracy as an objective intelligibility criterion. This use of AI models as an evaluator constitutes quality assurance performed by AI models."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention automated verification of annotation or data through code or algorithmic rule-based techniques as part of QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a quality assurance process involving multiple expert annotators and AI model evaluation; thus, QA is documented and applied."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8644,
      "completion_tokens": 397,
      "total_tokens": 9041
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (MDSC), particularly 2.2 Collection",
          "reasoning": "The Mandarin Dysarthria Speech Corpus (MDSC) was collected directly from 21 dysarthric and 25 non-dysarthric human speakers using original recording sessions conducted in a quiet indoor environment with specific recording protocols. The data consists of utterances spoken by these human participants and is not described as translated, adapted from existing datasets, or generated by models. Hence, the data is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models. All datasets are from human recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that machine translation was used for the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources; it was newly collected."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as modified or transformed from existing data; it is newly recorded speech."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly documented."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9162,
      "completion_tokens": 581,
      "total_tokens": 9743
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe training models from randomly initialized parameters using the Mandarin Dysarthria Speech Corpus (MDSC). Instead, it uses pre-trained models fine-tuned later."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The MDSC is used to fine-tune pre-trained Wake-up Word Spotting (WWS) models. Section 3.1 describes fine-tuning the speaker-independent control model (SIC) with the dysarthria training set (D-train) to form the SID model. Section 3.2 describes further fine-tuning the SID model with personalized enrollment utterances from individuals to form speaker-dependent dysarthria models (SDD). This demonstrates supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning post-training methods or RLHF in relation to using the MDSC."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The MDSC is used for extensive evaluation and benchmarking of different wake-up word spotting models (SIC, SID, SDD) on dysarthric speech samples. Section 4.2 discusses testing and performance results on the MDSC test sets to assess model robustness and challenges."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and Section 5 (Conclusion)",
          "reasoning": "The paper conducts comprehensive analysis of trends and characteristics of dysarthric speech such as significant intra-domain variance and limited data volume. It also analyzes intelligibility scores correlation to system performance, highlighting the dataset's utility in understanding speech disorder characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for retrieval-augmented generation or similar tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The MDSC dataset is clearly used for training (fine-tuning), evaluation, and analysis as described in multiple sections. Hence, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9885,
      "completion_tokens": 538,
      "total_tokens": 10423
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The dataset, MDSC, is specifically described as a Mandarin dysarthria speech corpus, with all participants being native Mandarin speakers and no mention of any additional languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The paper only specifies one human language, Mandarin; there is no indication of two languages present in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The dataset is explicitly described as Mandarin and not English; no English-only data is indicated."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset MDSC is described as a Mandarin Dysarthria Speech Corpus, with all speakers being native Mandarin speakers and the wake-up words and commands recorded are in Mandarin."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations; such notation appears only in analysis and equations outside the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech recordings only; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains native Mandarin speech only; no fictional or artificially created languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly specified as Mandarin; it is not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists entirely of spoken language data; it does contain language."
        }
      }
    }
  },
  {
    "id": "gao24c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7103,
      "completion_tokens": 175,
      "total_tokens": 7278
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section explicitly provides code availability for dataset construction.",
          "reasoning": "The paper does not mention any code repository or link to code related to data collection, preprocessing, or dataset generation. The corpus is stated to be released at a website for data download, but no associated code for reproducing the dataset is indicated."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (MDSC), subsections 2.1, 2.2, and 2.3.",
          "reasoning": "The paper provides detailed documentation of dataset statistics including number of speakers, demographics, recording conditions, utterance types, and intelligibility evaluation methodology. The data collection process is described (e.g., recording environment, microphone distance, text transcription using a mobile application), providing transparency on dataset creation."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8052,
      "completion_tokens": 172,
      "total_tokens": 8224
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The new lexical stress corpus was compiled using human recorded speech audio from the Bulgarian Parliament plenary sessions (2010-2018) and professional audiobook recordings. These recordings were made by human speakers and professionally recorded, thus the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Section 3.4",
          "Reasoning": "The corpus includes human-generated manual transcriptions aligned to the speech audio from the Parliament sessions and audiobooks, as well as standard Bulgarian dictionary entries added as separate utterances. These texts originate from human created manual transcriptions and curated dictionaries."
        }
      ]
    }
  },
  {
    "id": "geneva23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8852,
      "completion_tokens": 236,
      "total_tokens": 9088
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 and 3.3",
            "reasoning": "The lexical stress corpus is automatically compiled by employing an automatic speech recognition (ASR) system with a specialized acoustic model that differentiates stressed and unstressed vowels. The annotation of lexical stress is performed automatically through decoding that classifies vowels as stressed or unstressed based on acoustic likelihood from audio-text alignment."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No detailed manual annotation instructions are described for annotators, as the annotation is done automatically via ASR. The process relies on models and not human annotators following instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "There is no mention of scoring rubrics or criteria for human annotation, since the annotations are generated automatically by the ASR system."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not provide or reference manual annotation examples or guidelines, since annotation is an automatic process without human annotators explicitly guided by examples."
          }
        }
      ]
    }
  },
  {
    "id": "geneva23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10042,
      "completion_tokens": 303,
      "total_tokens": 10345
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance of the dataset involved a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 6, Conclusion",
          "reasoning": "The evaluation of the synthesized speech correctness was performed manually by a group of 5 native Bulgarian speakers supervised by a professional linguist, indicating quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate involvement of multiple non-expert annotators for quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not performed by an AI model as a judge; rather, the ASR system was used for automatic annotation, not quality verification."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Automatic annotation",
          "reasoning": "The lexical stress corpus was automatically annotated using an ASR acoustic model trained to differentiate stressed and unstressed vowels and decoded to select the most probable stress pattern, representing an automated verification process for dataset annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described in the paper including both automated annotation and human evaluation."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9652,
      "completion_tokens": 460,
      "total_tokens": 10112
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was created entirely from scratch by human contributors without reference to existing data. Instead, the data is compiled and annotated using a mixture of existing audio and texts."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated entirely by AI or machine learning models without using any existing data. Instead, machine learning models are used to annotate existing aligned data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The lexical stress corpus was automatically compiled by collecting large amounts of data from existing sources: recordings and manual transcriptions from the Bulgarian Parliament plenary sessions (2010-2018) and professional audiobooks paired with texts from the Chitanka library. This constitutes collating data from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The collected aligned data is automatically annotated with lexical stress marks by leveraging an ASR system trained on a phonetic lexicon with stress information. Moreover, the corpus is extended by adding dictionary word types with canonical stress patterns. This indicates modifications and transformations applied to the existing collected data, categorizing the resulting corpus as derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are well described in the paper, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10170,
      "completion_tokens": 437,
      "total_tokens": 10607
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the lexical stress corpus being used to train a dedicated Accentor model, not for unsupervised or self-supervised pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The Accentor model is trained from scratch on the large lexically diverse lexical stress corpus (Section 4) to learn to explicitly model lexical stress placement in text."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being used to fine-tune pre-trained models via supervised methods; models are trained directly on the datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning or RL-based post-training techniques applied to their datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The lexical stress corpus is explicitly used for training the Accentor model, not exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper analyzes lexical stress phenomena and dataset characteristics, the compiled lexical stress corpus is used primarily for model training rather than merely analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation; it is directly used for training models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The lexical stress corpus is actively utilized in model training and thus has practical utility."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10893,
      "completion_tokens": 416,
      "total_tokens": 11309
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1.3, Section 3, Section 6",
          "reasoning": "The lexical stress corpus introduced in this paper is compiled from Bulgarian parliamentary sessions and Bulgarian audiobooks specifically for Bulgarian text-to-speech (TTS) systems. The paper explicitly states the focus on Bulgarian language data (e.g. Section 1.3 states demonstration on Bulgarian TTS, Section 3 details data sources from Bulgarian parliament and Bulgarian audiobooks). Hence, the proposed new dataset is monolingual, containing only Bulgarian language entries."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language text and corresponding speech data; thus, it is language-relevant."
        }
      }
    }
  },
  {
    "id": "geneva23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8111,
      "completion_tokens": 208,
      "total_tokens": 8319
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any explicit link, URL, or mention of publicly available code repositories for the dataset construction, automatic alignment, or annotation code used to compile the lexical stress corpus. There is no indication that code for data processing or stress annotation is shared publicly."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Lexical stress corpus)",
          "reasoning": "Section 3 of the paper provides detailed documentation on the dataset creation process, including data sources (Bulgarian Parliament videos and audiobooks), the automatic alignment method using a hybrid DNN-HMM ASR system, alignment challenges and techniques, and the automatic annotation procedure using an ASR acoustic model with stressed and unstressed vowels phonemes. Further information on extended corpus creation by adding dictionary word types is also given, along with detailed statistics in Table 2. This constitutes clear and relatively complete documentation of dataset construction."
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7698,
      "completion_tokens": 100,
      "total_tokens": 7798
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Gathering",
          "Reasoning": "The new dataset is a corpus consisting of nearly five hours of irony-annotated naturalistic conversational speech collected from comedy podcast recordings provided in WAV format. This data originates from human-generated speech recordings, as it consists of naturalistic conversations from podcast hosts and guests, confirmed by the paper's data gathering description."
        }
      ]
    }
  },
  {
    "id": "gent22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8498,
      "completion_tokens": 198,
      "total_tokens": 8696
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1 Data Gathering",
            "reasoning": "The paper states that a team of annotators was trained and that each episode was labeled by two annotators with agreement required to include samples in the corpus, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Data Gathering",
            "reasoning": "Annotators were trained to perform irony annotation using a discourse-based irony labeling method introduced in a cited source [29], indicating detailed annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention any scoring rubrics or quantitative scoring guidelines being used during the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention providing annotation examples or sample annotations in the guidelines or training."
          }
        }
      ]
    }
  },
  {
    "id": "gent22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9688,
      "completion_tokens": 340,
      "total_tokens": 10028
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single human expert was responsible for quality assurance of the irony annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Gathering",
          "reasoning": "The irony annotation was performed by a team of annotators trained to perform irony annotation based on the discourse-based irony labeling method. Each episode was labeled by two annotators, and only samples for which both annotators agreed were included in the corpus. Although the paper does not explicitly state the annotators were experts, the use of a trained team and agreement between multiple annotators indicates multiple human experts performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit evidence that annotators were non-experts; annotation team was trained and disagreement samples were excluded, which implies care and expertise."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models as judges for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is given of automated verification of annotations; automatic processes mentioned pertain to feature extraction and preprocessing rather than annotation QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A clear quality assurance process is described involving multiple annotators and agreement, so QA is documented."
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9298,
      "completion_tokens": 408,
      "total_tokens": 9706
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Gathering",
          "reasoning": "The paper describes the collection of a new corpus of nearly five hours of irony-annotated, naturalistic conversational speech from comedy podcasts. The recordings were provided directly by the podcast creator, and human annotators labeled the data for irony based on conversational context. This corpus is original, created from scratch by human contributors, and is not derived from existing datasets. This is explicitly stated in the description of the data gathering process."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated artificially by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that machine translation was used to generate any part of the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although data was sourced from podcasts, the paper treats this as original data collection rather than collating previously published datasets without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not based on existing datasets with modifications or transformations; rather, it was newly collected and annotated."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method is clearly documented."
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9816,
      "completion_tokens": 305,
      "total_tokens": 10121
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.4 Model Architectures and Section 3 Results",
          "reasoning": "The new corpus constructed of naturalistic irony-annotated speech (Section 2.1) is used as training data to develop and train deep learning models from scratch, as detailed in the model architecture description and training procedures in Section 2.4 and reported performance results in Section 3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Results and Section 4 Discussion",
          "reasoning": "The corpus is also used for evaluation of model performance through cross-validation under speaker-dependent and speaker-independent conditions, with metrics including F1, AUC, and accuracy reported."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Discussion",
          "reasoning": "The dataset is analyzed to examine the impact of prior speaker knowledge on irony classification performance, as well as to compare acoustic and textual modalities, which constitutes an analytical usage beyond pure training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10539,
      "completion_tokens": 532,
      "total_tokens": 11071
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a new corpus from the Sad Boyz Podcast featuring naturalistic conversational speech. There is no indication that the dataset includes more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only one language, English, with no mention of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Gathering",
          "reasoning": "The dataset consists of naturalistic conversational speech from English-language comedy podcasts (Sad Boyz Podcast). All the speech and transcriptions used in the corpus are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly English speech; no non-English languages are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Section 2.2 Acoustic Feature Selection and Extraction",
          "reasoning": "While the study uses programming tools and algorithms such as Keras and Python for preprocessing and modeling, the dataset itself contains speech data, not programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations; it only includes natural spoken language data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists entirely of human speech from podcasts; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of all entries in the dataset is clearly documented and explicitly English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of naturalistic speech data in English, so it clearly contains human language."
        }
      }
    }
  },
  {
    "id": "gent22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7757,
      "completion_tokens": 128,
      "total_tokens": 7885
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.4 Model Architectures",
          "reasoning": "The paper states that model implementation, preprocessing, training, and evaluation code can be found at https://github.com/helengent/Irony-Recognition, indicating code accessibility including preprocessing steps related to the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Data Gathering",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including data source selection, annotation methodology, annotator agreement, utterance balancing, and statistical properties of the resulting corpus."
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8065,
      "completion_tokens": 110,
      "total_tokens": 8175
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Dataset",
          "Reasoning": "The paper introduces a novel dataset comprising speech recordings from 20 patients (10 women and 10 men) recorded in a clinical setting. The speech data are human speech recordings collected using a Zoom Q8 recorder during the patients reading neutral texts and other speaking tasks. The data are clearly stated as recorded human speech samples, hence 'audio' modality with human-generated origin."
        }
      ]
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8865,
      "completion_tokens": 206,
      "total_tokens": 9071
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 Dataset",
            "reasoning": "The paper states that the study doctor assessed the near-term suicide risk of each participant on a Likert scale. This implies a single human expert performed the annotation regarding suicide risk labels."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "There is no mention or description of detailed annotation instructions provided to the annotator in the paper."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2 Dataset",
            "reasoning": "The annotation used a Likert scale from 1 to 6 to assess suicide risk with specified intervals defined for no risk, suicidal without intention, and high risk with intention to act, indicating a rubric for scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "The paper does not provide or mention any annotation examples or detailed sample annotations for the suicide risk assessment."
          }
        }
      ]
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10055,
      "completion_tokens": 304,
      "total_tokens": 10359
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2: Dataset",
          "reasoning": "The paper states that the study doctor assessed the near-term suicide risk of each participant using a Likert scale. The use of 'study doctor' implies a single human expert with subject matter expertise performed the annotation of suicide risk, thus quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance or labeling; only a single study doctor is mentioned."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotator is described as a study doctor, implying subject matter expertise, so not a non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of multiple non-expert annotators involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset annotation does not involve AI models for quality assurance; AI models are used for classification after dataset establishment."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated process is described for quality assurance of dataset annotations; segmentation and feature extraction involve algorithms, but annotation quality assurance is manual."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes annotation by a study doctor; thus, quality assurance is documented and applied."
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9665,
      "completion_tokens": 381,
      "total_tokens": 10046
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset consists of speech recordings collected from 20 patients diagnosed with behavioral or mental disorders, recorded during emergency admission to a psychiatric department. The speech samples were recorded by human subjects reading neutral texts and spontaneous speech, thus the data represents original recordings created from scratch by human contributors for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not consist of data generated entirely by AI or machine learning models. Although features are extracted using models, the raw dataset consists of human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was produced by translation from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation applied to the data is found in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated or collected from existing sources; it was collected by the authors specifically for this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is original human speech recordings and not derived or adapted from existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly specified as recordings from human patients during emergency psychiatric admissions."
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10183,
      "completion_tokens": 315,
      "total_tokens": 10498
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4 \"Suicide Risk Classification\" and Section 4.1 \"Classification Results\"",
          "reasoning": "The paper describes training classifiers (SVMs) on the novel speech dataset to classify high vs. low suicide risk using supervised learning methods, thus fine-tuning or training models leveraging this data for the specific task."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 \"Classification Results\"",
          "reasoning": "The dataset is used to evaluate classification performance via leave-one-speaker-out cross-validation for suicide risk assessment, measuring balanced accuracy, indicating its use for evaluation and benchmarking the models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 \"Experiments and Results\" and Section 5 \"Conclusions and Future Work\"",
          "reasoning": "The authors analyze gender-specific speech patterns and acoustic feature correlations with suicide risk using the dataset, providing insights into paralinguistic markers, thus the dataset is employed for analytic purposes beyond pure classification."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10906,
      "completion_tokens": 362,
      "total_tokens": 11268
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset consists of speech recordings of 20 patients reading three neutral German short stories, explicitly indicated as German texts: \u201cDer Nordwind und die Sonne\u201d, \u201cGleich am Walde\u201d, and \u201cDer Hund und das St\u00fcck Fleisch\u201d. No other languages are mentioned, and the speech content is monolingual German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gerczuk24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8124,
      "completion_tokens": 153,
      "total_tokens": 8277
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No direct references to code availability are found in the paper.",
          "reasoning": "The paper does not provide any links, URLs, or mentions of publicly available code repositories for dataset construction, data collection, preprocessing, or generation. Therefore, the code associated with the dataset is not publicly available according to the paper content."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2. Dataset",
          "reasoning": "The paper documents the dataset creation process, including participant demographics, clinical assessments, recording devices, speech tasks, and ethical approval details in Section 2. This provides transparency about how the dataset was collected and the characteristics of the subjects, indicating sufficient documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7824,
      "completion_tokens": 115,
      "total_tokens": 7939
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data Collection",
          "Reasoning": "The paper reports the collection of a new clinical pediatric audio dataset for asthma detection. The audio data were recorded using far-field non-contact microphones in a physician's office under real conditions, with each participant performing specific tasks. The recordings were manually annotated and verified by human annotators and a pediatric specialist for cough and wheezing presence, confirming human generation and involvement in the data creation process."
        }
      ]
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8624,
      "completion_tokens": 224,
      "total_tokens": 8848
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that the audio recordings were segmented and labeled for each task by two human annotators, and the quality of segments was verified by an independent annotator. Afterward, a pediatric specialist labeled the cough samples for wheezing. This indicates multiple human experts (annotators and a pediatric specialist) performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to the annotators or specialists for labeling the audio data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of any scoring rubrics or formal evaluation criteria used for annotation in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide or indicate the presence of annotation examples or annotated samples provided as guidance to the annotators."
          }
        }
      ]
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9814,
      "completion_tokens": 256,
      "total_tokens": 10070
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 (Data Collection)",
          "reasoning": "The paper states that after initial data labeling and validation, a pediatric specialist (an expert) labeled the cough samples for the presence of wheezing, indicating that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 (Data Collection)",
          "reasoning": "Audio recordings were segmented and labeled for each task by two human annotators, and the quality of the segments was verified by an independent annotator. Since no mention is made that these annotators are subject matter experts, it is appropriate to assume they are non-expert annotators, indicating multiple human non-experts performed quality assurance on task segmentation and labeling."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9424,
      "completion_tokens": 484,
      "total_tokens": 9908
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection",
          "reasoning": "The paper states that the authors collected clinical pediatric audio data for identifying pediatric asthma in collaboration with healthcare institutions. The data was recorded using a non-contact microphone in physician offices from 27 participants (healthy and asthma diagnosed children). The recordings were manually segmented and labeled by human annotators and a pediatric specialist labeled the cough samples for the presence of wheezing. This indicates the data was created entirely from scratch by human contributions in a clinical setting."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new audio data was generated entirely by AI or machine learning models. Although they use pre-trained models for segmentation and active learning for annotation, the core audio data itself is recorded from human subjects, not synthesized by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset involves machine translation from another language, as the data is audio recordings of respiratory sounds collected directly."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as aggregated from existing datasets without modification. Although the paper uses pre-existing datasets like AudioSet or COUGHVID for evaluation or background synthesis, the main dataset was newly collected."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived or transformed from existing sources. The authors collected new original recordings and labeled them, rather than adapting or modifying existing datasets for this asthma detection task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is specified and documented clearly as newly collected clinical audio data recorded from pediatric participants."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9942,
      "completion_tokens": 499,
      "total_tokens": 10441
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly collected dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used to train models from scratch; models used include pre-trained audio neural networks (PANNs) and few-shot anomaly detection methods."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The collected dataset is used to fine-tune models such as DevNet and a Linear Regression model on MFCC features in the active learning pipeline to detect wheezing events using supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of reinforcement learning post-training techniques such as RLHF with the introduced dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 3.3",
          "reasoning": "The dataset is used for evaluation purposes by assessing the performance of automatic segmentation (Section 3.2) and the active learning pipeline's effectiveness in retrieving wheezing cases (Section 3.3)."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 3.3",
          "reasoning": "The paper provides analysis of segmentation precision and recall, and annotation cost in the active learning iterations, to characterize the dataset's strengths and the system's performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset as a knowledge base to augment models, such as through retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has multiple practical uses in training (fine-tuning), evaluation, and analysis as described above."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10665,
      "completion_tokens": 510,
      "total_tokens": 11175
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains data from pediatric patients speaking English only, no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention of exactly two human languages in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data Collection",
          "reasoning": "The paper repeatedly mentions tasks such as reciting ABCs, numbers, vowels and naming colors and foods all in English. The participants are in a clinical setting in the USA and produce spoken English. There is no mention of any other spoken human language being collected."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset entries are clearly in English, no non-English languages described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of clinical audio recordings of human speech and lung sounds, no programming language or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions are present in the dataset; the paper only discusses model architectures and algorithms separately."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio recordings of human lung sounds and speech, but no biological sequences or non-human communication systems like animal signals or DNA."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language is explicitly described and observable as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset does contain human language (English) in the form of speech tasks, thus not applicable."
        }
      }
    }
  },
  {
    "id": "ghaffarzadegan23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7883,
      "completion_tokens": 171,
      "total_tokens": 8054
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention found in the paper",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code or code repositories related to the data collection, preprocessing, or dataset construction. There is no indication that the dataset creation or active learning framework code has been made accessible for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data Collection and Section 2 Proposed System",
          "reasoning": "The paper documents the dataset creation process in detail, including the data collection setup, participant demographics, recording equipment and environment, tasks performed, annotation process including human annotators and validation, and the overall pipeline for audio data segmentation and annotation. This constitutes sufficient documentation of the dataset creation process for reproducibility and ethical assessment purposes."
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8983,
      "completion_tokens": 208,
      "total_tokens": 9191
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Table 1",
          "Reasoning": "The DeToxy dataset introduced by the authors consists of spoken utterances collected from various openly available speech databases (e.g., CMU-MOSEI, Common Voice, IEMOCAP). These are human speech recordings manually annotated for toxicity, thus the audio modality is human generated and explicitly stated as sourced from human-recorded speech."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Table 1",
          "Reasoning": "DeToxy includes textual transcripts associated with the spoken utterances, either originally available or obtained via ASR (wav2vec-2.0) and subsequently manually annotated for toxicity by three professional annotators. The annotations and transcripts are human generated or verified, so the text modality is human generated and explicitly described in the paper."
        }
      ]
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9783,
      "completion_tokens": 237,
      "total_tokens": 10020
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that all utterances filtered by model-based toxicity classifier were manually annotated by 3 professional annotators with majority voting used for final label. The annotation was done using audino tool taking both text and speech into account, and the Cohen's Kappa inter-annotator agreement is reported, indicating multiple human expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper defines toxicity for annotation and discusses annotation process but does not provide or mention detailed annotation instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any scoring rubrics or specific annotation guidelines or criteria beyond toxicity definition, nor any rubric-based scoring system."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No examples of annotated data or annotation guidelines with examples are presented or referenced in the paper for the DeToxy dataset annotation."
          }
        }
      ]
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10973,
      "completion_tokens": 520,
      "total_tokens": 11493
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human annotator with subject matter expertise or membership in the target demographic performing quality assurance alone on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "Although multiple human annotators (three professional annotators) performed annotations and majority voting was used for final labels, the paper does not specify that these annotators were subject matter experts or members of the target demographic. Without evidence of expertise, this label is not applicable."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset annotation involved manual annotation by three professional annotators using both textual transcripts and speech files, with majority voting and a reported inter-annotator agreement (Cohen's Kappa=0.76). The paper lacks explicit information that the annotators were subject matter experts or from the target demographic; hence, the default assumption is that they are non-expert annotators. Thus, quality assurance was conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Before manual annotation, a textual toxicity classifier (based on BERT) was trained and used to filter utterances by selecting datasets with at least 10% utterances flagged as toxic. This AI model was employed in a filtering/selection step, thereby serving as a form of quality assurance in dataset construction before human annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification techniques such as code or formula validation for quality assurance of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a quality assurance process involving multiple human annotators and an AI model, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10583,
      "completion_tokens": 505,
      "total_tokens": 11088
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset DeToxy is constructed by aggregating utterances from various openly available speech databases and manually annotating them for toxicity. However, the speech utterances themselves are not created from scratch by human contributors; they are sourced from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any utterances or data samples are generated entirely by AI or machine learning models as original content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is restricted to English spoken utterances and the paper does not mention producing data by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data creation via machine translation from another language in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3: DeToxy and DeToxy-B, especially Section 3.1 Dataset Collection and Annotation",
          "reasoning": "DeToxy is explicitly described as a subset collated from various existing openly available speech databases. The core audio data was collected from multiple public datasets such as Common Voice, Switchboard, CMU-MOSEI, and others, aggregating them into a large unified dataset without generating new audio recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset Collection and Annotation",
          "reasoning": "The authors applied manual annotations to the collected utterances to label toxicity, using a 2-step approach involving a BERT-based toxicity classifier for pre-selection followed by professional human annotations. Additionally, for datasets lacking transcripts, they used ASR models to generate transcripts. These modifications and additions are transformations applied to existing source data, thus the data can be considered derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origins and methods of generating the dataset, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11101,
      "completion_tokens": 301,
      "total_tokens": 11402
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 and 6",
          "reasoning": "The DeToxy-B dataset is used to fine-tune pre-trained models such as BERT BASE and wav2vec-2.0 for toxicity classification, as described in Section 4 with baselines provided, and experimental results in Section 6 demonstrate model fine-tuning and evaluation on different splits of DeToxy-B."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6",
          "reasoning": "The dataset is used for performance measurement and benchmarking of toxicity classification models and baselines, with explicit test splits (Test and Test-T) used for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4, 6",
          "reasoning": "The dataset is used to analyze the advantages and disadvantages of different approaches (2-step vs End-to-End) for speech toxicity classification, including trends in model performance on trigger terms and dependency on gold transcripts."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11824,
      "completion_tokens": 606,
      "total_tokens": 12430
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 Introduction and Section 3.1 Dataset Collection and Annotation",
          "reasoning": "The dataset DeToxy is described as the first publicly available toxicity annotated dataset for the English language only. There is explicit mention that it contains English spoken utterances, with no indication of inclusion of other human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 Introduction and Section 3.1 Dataset Collection and Annotation",
          "reasoning": "The dataset is described exclusively as English language data. There is no mention or indication of any second human language being included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3 Introduction",
          "reasoning": "The dataset DeToxy is explicitly described as a toxicity annotated dataset for the English language. The source datasets, annotation process, and baselines all focus on English spoken utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset is not stated to include any single non-English language; it focuses purely on English data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset comprises spoken utterances with text transcripts focused on toxicity classification; there is no mention of code or programming language contents."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "There is no indication that the dataset contains entries involving mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset involves human spoken utterances only and does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No mention of fictional or constructed languages in the dataset; it is composed entirely of natural English speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.1 Dataset Collection and Annotation",
          "reasoning": "The language of the dataset entries is explicitly specified as English; therefore, language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken and transcribed utterances in the English language, thus it contains language and is not applicable to this category."
        }
      }
    }
  },
  {
    "id": "ghosh22b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9042,
      "completion_tokens": 137,
      "total_tokens": 9179
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 5 Experimental Setup",
          "reasoning": "The paper explicitly states: 'We make all our code and data publicly available on GitHub.' This indicates that the code related to dataset construction, preprocessing, and generation is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 DeToxy and DeToxy-B",
          "reasoning": "Section 3 provides detailed information on dataset collection, annotation process, filtering strategy using a textual toxicity classifier, manual annotation procedure with inter-annotator agreement, data sources, and dataset statistics. This comprehensive description documents the dataset creation process clearly."
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7252,
      "completion_tokens": 118,
      "total_tokens": 7370
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2.1 Data Collection",
          "Reasoning": "The BhavVani dataset is described as a novel Hindi Speech Emotion Recognition dataset consisting of approximately 13 hours of audio from 8,734 utterances curated from the popular Indian sitcom. The dataset was created and annotated by human annotators noted in the acknowledgments, indicating human involvement in recording and annotation. Therefore, the modality is audio, and the data origin is human generated."
        }
      ]
    }
  },
  {
    "id": "goel24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8052,
      "completion_tokens": 257,
      "total_tokens": 8309
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 6, Acknowledgments",
            "reasoning": "The paper states that multiple individuals from IIIT Delhi, named Adamya, Aditya, Akshat, Arnav, Arush, Divyanshi, Kartikay, Kuleen, Manaswi, Manya, Rishitej, Rushil, Sahaj, Tanish, Tanishq, and Yash, contributed to annotating the BhavVani dataset. This indicates multiple human annotators who are likely non-experts, as there is no indication they are domain experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not explicitly mention providing detailed annotation instructions for the BhavVani dataset annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "There is no mention of scoring rubrics or detailed guidelines for annotating the BhavVani dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not provide or mention annotation examples for the BhavVani dataset."
          }
        }
      ]
    }
  },
  {
    "id": "goel24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9242,
      "completion_tokens": 420,
      "total_tokens": 9662
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance for the novel BhavVani dataset was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided in the paper indicating that multiple human experts or domain experts performed quality assurance on the BhavVani dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that a single human non-expert performed quality assurance for the BhavVani dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.2.1 Data Collection and Section 7 Acknowledgments",
          "reasoning": "The paper states that multiple individuals from IIIT Delhi (named in acknowledgments) were involved in annotating the BhavVani dataset. There is no indication these annotators were subject matter experts or specifically trained experts, thus they are considered non-expert annotators. This implies quality assurance was conducted by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model being used for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based checking for dataset annotation quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance process is described indirectly through the involvement of multiple annotators, so it is not absent or undocumented."
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8852,
      "completion_tokens": 429,
      "total_tokens": 9281
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.2.1",
          "reasoning": "The paper introduces BhavVani, a novel Hindi Speech Emotion Recognition dataset, which comprises 8,734 utterances from audio clips curated from a popular Indian sitcom. It was annotated by human contributors (acknowledged in Section 7), indicating the data was created/collected and annotated by humans from scratch. This original dataset is not described as being translated or derived from existing datasets, but collected and annotated specifically for this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was generated entirely by AI or models without reference to pre-existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset obtained through human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data used or introduced."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper benchmarks five existing datasets, it does not indicate any new dataset created by collecting or aggregating existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new Hindi dataset BhavVani is described as a new collected dataset, not derived or adapted from existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the novel dataset BhavVani is clearly documented as collected and annotated by humans, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9370,
      "completion_tokens": 414,
      "total_tokens": 9784
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.2, 4, and 5",
          "reasoning": "The BhavVani dataset is used to fine-tune pre-trained models using supervised learning methods for speech emotion recognition, as demonstrated in the benchmarking experiments (Table 1) where models including CAMuLeNet are trained and evaluated on BhavVani for unseen speaker emotion recognition."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using reinforcement learning or RL-based post-training techniques with the BhavVani dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 and Table 1",
          "reasoning": "The BhavVani dataset is also used for evaluation and benchmarking purposes to measure performance of SER models on unseen speakers, as part of the comprehensive benchmarking alongside other datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the BhavVani dataset is primarily used for analysis of trends or characteristics independent of training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10093,
      "completion_tokens": 585,
      "total_tokens": 10678
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The novel dataset introduced by the authors, BhavVani, contains Hindi utterances only and does not include more than two languages. Other datasets mentioned are existing and not novel."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "BhavVani dataset is described as Hindi-only with no indication of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The introduced BhavVani dataset is specifically a Hindi language dataset and does not contain English-only utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Our Novel Hindi SER Dataset: BhavVani)",
          "reasoning": "The authors introduce the BhavVani dataset solely in the Hindi language with approximately 8,734 utterances. It is explicitly stated that it is a Hindi SER dataset designed to enrich research in Indian languages, thus strictly monolingual (Hindi)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No relevant section",
          "reasoning": "The dataset contains speech utterances in Hindi language and no indication of code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No relevant section",
          "reasoning": "No part of the dataset contains mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No relevant section",
          "reasoning": "The dataset relates to human speech emotions and contains utterances only, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No relevant section",
          "reasoning": "BhavVani consists of natural human Hindi language only, no constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The language of the dataset is explicitly stated as Hindi, so the language is known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language entries (Hindi utterances), so it does contain language."
        }
      }
    }
  },
  {
    "id": "goel24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7311,
      "completion_tokens": 183,
      "total_tokens": 7494
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 'Proposed Methodology' and footnote 1",
          "reasoning": "The paper provides a URL to a GitHub repository (https://github.com/arnav10goel/CAMuLeNet) in Section 2, indicating that code relevant to the methodology, which likely includes data preprocessing and possibly dataset handling, is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 'Our Novel Hindi SER Dataset: BhavVani' and subsections",
          "reasoning": "The paper documents the dataset creation process for the new BhavVani dataset in Section 3.2, describing the data collection (curated from an Indian sitcom), duration, number of utterances, average clip length, and annotation contributors. This provides transparency and some completeness regarding the dataset creation."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7718,
      "completion_tokens": 108,
      "total_tokens": 7826
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data",
          "Reasoning": "The paper introduces a new dataset containing speech recordings from 55 speakers (39 pwMS and 16 HC) collected with a microphone system (AKG 555L headset microphone with audio interface) at clinical sites in Switzerland. Audio recordings are human speech collected through supervised recording sessions involving patients and controls, explicitly described as newly recorded data for this study."
        }
      ]
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8518,
      "completion_tokens": 240,
      "total_tokens": 8758
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 Data",
            "reasoning": "The data was collected in a clinical setting with participants recruited and recordings supervised by study nurses, which are experts in the clinical domain. The recording involved guided speech tasks indicating expert involvement in annotation and recording."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Data",
            "reasoning": "The recording sessions consisted of several speech tasks with defined protocols (e.g., picture description task from Western Aphasia Battery and standard text passages), indicating detailed instructions were provided to the annotators or study nurses during data collection."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Overall Paper",
            "reasoning": "The paper does not mention any use of scoring rubrics or rating scales for annotators to score or categorize the speech data; rather, features were extracted automatically from audio recordings."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Overall Paper",
            "reasoning": "No examples of annotations or example guidelines are presented or referenced in the paper. The paper focuses on acoustic feature extraction and ML analysis rather than manual annotation with provided examples."
          }
        }
      ]
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9708,
      "completion_tokens": 431,
      "total_tokens": 10139
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA carried out by a single human expert. No information about expert-based annotation or validation of the dataset content is given."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple human experts or annotators. There is no mention of expert review or validation for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indications or descriptions are provided that a single non-expert human conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert humans performing QA or annotation validation on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although machine learning models are used for classification experiments, the paper does not state that any AI model was used as a judge or for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Data processing",
          "reasoning": "The dataset underwent automated preprocessing steps including voice activity detection (VAD) segmentation and loudness normalization using algorithmic techniques. Manual audio quality checks were also carried out before analyses, but the main segmentation and processing relied on automated methods, suggesting that quality assurance of audio segment boundaries and normalization was performed through an automatic process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some form of automated processing and manual audio quality checks are mentioned, indicating some level of quality assurance is present and documented."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9318,
      "completion_tokens": 448,
      "total_tokens": 9766
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data",
          "reasoning": "The paper describes data collected at the Department of Neurology, Inselspital, Bern, Switzerland and the University of Bern, Switzerland, obtained through a clinical observational study with participants providing consent. The dataset consists of original speech recordings of 39 patients with relapsing MS and 16 healthy controls. The recordings include multiple speech tasks such as free speech and read speech, recorded using a microphone setup. This indicates the data was created entirely from scratch by human contributors (the patients and controls) and is original content, not translated or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated or synthesized by AI or machine learning models. All data originates from human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the data was produced by translation from another language using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate or create the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as aggregated from existing sources; rather, it was newly recorded from human participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While acoustic features were extracted from the raw audio, the dataset itself (raw speech recordings) is not derived or adapted from existing data sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clearly the origin of the data as new human speech recordings collected for the study."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9836,
      "completion_tokens": 491,
      "total_tokens": 10327
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of the new dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Predictive Modelling)",
          "reasoning": "The dataset is used to train machine learning models from scratch, specifically support vector machines (SVM) and k-nearest neighbors (KNN), to classify people with MS and healthy controls based on acoustic features extracted from the speech data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of the dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RLHF techniques applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 (Predictive Modelling)",
          "reasoning": "The dataset is split into training and a fixed hold-out test set to evaluate and report the performance of machine learning classification models, serving as evaluation data for benchmarking model accuracy."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 (Statistical Feature Analysis) and the Discussion section",
          "reasoning": "The dataset is used to analyze and identify statistically significant acoustic features that distinguish people with MS from healthy controls, providing insights into speech characteristics related to MS."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset as a knowledge base for augmenting models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used for training, evaluation, and analytical purposes, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10559,
      "completion_tokens": 487,
      "total_tokens": 11046
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper presents a dataset of speech recordings from German-speaking participants only, with no mention of entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset only contains entries in German; no indication of exactly two human languages is given."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is in German, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data",
          "reasoning": "The dataset consists of speech recordings from a German-speaking cohort only (explicitly stated in the abstract and Section 2.1). Thus, it contains exactly one non-English human language, German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio recordings with no programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are contained in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns human speech recordings and does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are natural human language speech (German), with no mention of fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly stated as German."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language, so language is present."
        }
      }
    }
  },
  {
    "id": "gonzalezmachorro23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7777,
      "completion_tokens": 137,
      "total_tokens": 7914
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.1 Data",
          "reasoning": "The paper states that the recorded data cannot be published due to ethics committee restrictions, and there is no mention of any publicly available code or repository related to data collection or processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Data and Section 2.2 Data processing",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including participant recruitment criteria, ethical approval, recording setup, speech tasks used, data processing steps such as downsampling and voice activity detection, and partitioning strategy for train/test sets."
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7611,
      "completion_tokens": 92,
      "total_tokens": 7703
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Datasets",
          "Reasoning": "The paper introduces a new manually transcribed dataset called the MPS dataset, consisting of audio recordings of elementary school children reading L2 English text aloud. These recordings were collected from nearly 2500 students in government schools, making it a human-generated audio dataset captured via recordings."
        }
      ]
    }
  },
  {
    "id": "gothi24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8411,
      "completion_tokens": 333,
      "total_tokens": 8744
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 Datasets, especially second paragraph",
            "reasoning": "The paper states that manual transcription is underway for the recordings of elementary school children reading L2 English. The dataset is newly introduced (the MPS dataset) and speech from 1110 unique speakers is manually transcribed at the word level to produce ground-truth miscues labels. There is no indication that expert linguists or experts performed the annotation; the typical scenario for such tasks with large-scale elementary school data is multiple human non-expert annotators or transcribers performing manual annotation. Hence, we select 'Multiple Human Non-Experts' as the annotator category."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2 Datasets",
            "reasoning": "The paper does not describe any annotation guidelines, instructions, or protocols provided to the transcribers for the manual transcription or labeling. The absence of description suggests no formal instructions are documented in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 Datasets",
            "reasoning": "There is no mention of scoring rubrics or criteria to rate or score the transcriptions or miscues. The paper only states that manual transcription is done and labels are derived by comparing the manual transcription with canonical text prompts. No rubrics are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not provide or mention annotation examples or sample transcriptions in the annotation guidelines. No examples or exemplar annotations are given in the main text or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "gothi24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9601,
      "completion_tokens": 336,
      "total_tokens": 9937
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human expert annotators involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that a single non-expert human conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2 Datasets",
          "reasoning": "The dataset consists of manual transcriptions of oral readings by 1,110 elementary school children. The transcription is ongoing and detailed enough to label types of reading disfluencies. However, the paper does not specify that the transcribers are experts; given the context of large-scale manual transcription of children reading, it is most plausible that multiple non-expert human annotators performed the transcription and error labeling."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used in experiments for miscue detection, the paper does not describe AI models used in the quality assurance of dataset transcriptions or annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any automated verification or algorithmic rule-based quality assurance applied to validate manual transcriptions or labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes manual transcription and labeling of the dataset, therefore quality assurance is present and documented."
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9211,
      "completion_tokens": 426,
      "total_tokens": 9637
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The paper states that audio recordings of oral reading by approximately 2,500 elementary school children in India were collected in summer 2023, with manual transcription underway. The dataset, termed the MPS dataset, consists of manually transcribed and labeled connected text readings with ground-truth miscue annotations. This data was recorded and transcribed by human contributors entirely from scratch, representing original content and not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the new dataset is generated entirely by AI or machine learning models without reference to existing data. The paper uses models for analysis but not for dataset generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involves human translation of content from another language. Although subjects are L2 English learners with diverse home languages, the data consists of oral reading in English, not translated text."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by any machine translation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not simply collected or aggregated from existing sources; it is newly recorded from the field."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the dataset is derived or transformed from an existing dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and collection procedures for the new dataset."
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9729,
      "completion_tokens": 215,
      "total_tokens": 9944
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2 and 4",
          "reasoning": "The newly introduced Maharashtra Primary Schools (MPS) dataset is explicitly used as a test dataset for evaluation purposes. It is manually transcribed and labeled for reading miscues and is used to benchmark the performance of various ASR-based miscue detection methods in the experiments described in Sections 2 and 4."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10452,
      "completion_tokens": 564,
      "total_tokens": 11016
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2 Datasets",
          "reasoning": "The MPS dataset comprises oral reading recordings of elementary school children with diverse home languages in India. Although the reading texts are in L2 English, the children use a variety of home languages including Hindi and potentially others, as stated: 'The students, who come with diverse home languages, are introduced to both Hindi and English reading and writing in Grade 1.' Additionally, reading errors include several non-English words transcribed phonemically, indicating presence of multiple languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While English and Hindi are mentioned, the dataset includes diverse home languages beyond Hindi, thus exceeding exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset texts are in L2 English, but contain reading errors including non-English words and phonemic transcriptions for those, indicating other languages are present."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "English is present in the reading texts and transcripts, so dataset is not strictly monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio and transcriptions, no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset contains human oral reading recordings without biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages are described explicitly: English reading prompts with diverse home languages and non-English words transcribed phonemically."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes human speech and transcripts which qualify as language content."
        }
      }
    }
  },
  {
    "id": "gothi24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7670,
      "completion_tokens": 153,
      "total_tokens": 7823
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any links or references to publicly available code pertaining to the dataset construction, data collection, preprocessing, or annotation processes. No repository or supplementary material regarding code is indicated in the text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Datasets",
          "reasoning": "The paper provides detailed documentation about the dataset creation process in Section 2, including how audio recordings were collected from elementary school children across multiple schools, the ethical clearance obtained, the nature of the reading prompts, the transcription process, error labeling schema, and statistics of the dataset. This documentation provides clarity on dataset composition and annotation, supporting reproducibility and assessment."
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7301,
      "completion_tokens": 122,
      "total_tokens": 7423
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Open track; Section 5 Conclusions",
          "Reasoning": "The paper states that the authors curated a new speech dataset of Singaporean dialects from YouTube videos featuring Singaporean English and Mandarin spoken speech. This data consists of spoken audio recorded by humans (Singaporean speakers), hence audio modality and human-generated data. No evidence suggests the data was generated or simulated by models. The data source is explicitly described and curated, so its origin is known."
        }
      ]
    }
  },
  {
    "id": "gupta23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8101,
      "completion_tokens": 258,
      "total_tokens": 8359
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2",
            "reasoning": "The new dataset is the Singaporean dialect English-Mandarin speech data curated from YouTube videos. The speech segments were generated using an open-source Voice Activity Detection (VAD) model (marblenet) to segment audio files. This is a deterministic algorithmic process, not manual annotation by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "There is no mention in the paper of any detailed annotation instructions being provided for annotators of the YouTube-curated dataset. The data processing was done via VAD and automated filtering."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "There is no description or indication of any scoring rubrics provided for annotators related to the new YouTube dataset, since annotation was not manual but automated segmentation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not mention any annotation examples or sample instructions related to annotators for the new YouTube dataset. The dataset was curated programmatically with no human annotation or example guidelines noted."
          }
        }
      ]
    }
  },
  {
    "id": "gupta23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9291,
      "completion_tokens": 400,
      "total_tokens": 9691
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process being performed by a single human expert on the newly curated dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance or annotation validation on the new Singaporean dialect English-Mandarin speech dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about any QA conducted by a single non-expert annotator for the new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert human annotators performed quality assurance on the newly introduced dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Resources, paragraphs 2 and 3",
          "reasoning": "The paper reports that the model trained on the closed task was used to filter the newly collected Singaporean dialect speech segments from YouTube by selecting only those segments on which the model made prediction errors, thereby introducing diversity into the training set. This use of a trained AI model for filtering serves as a quality assurance step in curating the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2 Data Resources",
          "reasoning": "Automated Voice Activity Detection (VAD) based on the Marblenet model was used to generate speech segments from audio files in both the National Speech Corpus and YouTube data. This automated segmentation process can be considered a form of automatic quality assurance to identify and segment speech regions from raw audio reliably."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents quality assurance processes involving automated tools and a trained AI model for dataset curation and filtering, so it is incorrect to say no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8901,
      "completion_tokens": 434,
      "total_tokens": 9335
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the new dataset was recorded or created directly from human contributors in a new recording session or original content creation. Instead, the data is curated from existing YouTube videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was generated by any AI or machine learning model in a generative manner. The data originates from real-world audio recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translating the data by human translators from another language into the target languages or any translation process."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper states that the new Singaporean dialect English-Mandarin speech dataset was curated by collecting speech data from existing YouTube videos. Speech segments were extracted using a voice activity detection model but no significant modification is described, indicating data collection or aggregation from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The data was processed by applying voice activity detection to generate speech segments and then filtered by using a model trained on the closed task to select segments where the model made errors, thus applying transformations and filtering on the original data, making it derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and process of generation of the new dataset is documented explicitly in the paper."
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9419,
      "completion_tokens": 235,
      "total_tokens": 9654
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 3.5.3",
          "reasoning": "The new dataset curated from YouTube videos with Singaporean dialect English and Mandarin speech is used in the open track to fine-tune a pre-trained speaker recognition model for the language identification task. This usage is described in Section 2.2 where the data is introduced and Section 3.5.3 where fine-tuning on this dataset along with others is detailed."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10142,
      "completion_tokens": 532,
      "total_tokens": 10674
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The introduced new dataset contains only English and Mandarin languages, therefore it is not multilingual in the sense of containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.2 and Abstract",
          "reasoning": "The paper explicitly states that the newly curated dataset from YouTube contains English and Mandarin speech data spoken by Singaporean speakers (Section 2.2 and Abstract). This dataset contains exactly these two human languages, making it bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is not exclusively English; it includes Mandarin as well."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes both Mandarin and English, not just a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of spoken language audio segments and does not include programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are speech audio segments; no mathematical or logical notation is present."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is human speech audio, no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No fictional or artificial languages are mentioned as part of the curated dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages in the curated dataset are explicitly stated as English and Mandarin; there is no ambiguity or unspecified language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language (English and Mandarin); it is not language-free."
        }
      }
    }
  },
  {
    "id": "gupta23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7360,
      "completion_tokens": 244,
      "total_tokens": 7604
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.2, Section 3 and Footnotes 1 and 2",
          "reasoning": "The paper states that the curated Singaporean dialect English-Mandarin speech dataset prepared from YouTube videos will be released for public use on their project repository (footnotes 1 and 2 with URLs). Also, Section 3 mentions that changed code files and training/fine-tuning scripts used in experiments are included in the supplementary material and are available on their project repo. This implies that code related to dataset creation, including VAD segmentation and filtering, is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2",
          "reasoning": "Section 2.2 clearly documents the dataset creation process for the new Singaporean dialect dataset curated from YouTube. It details the source channels, use of VAD (marblenet) to segment speech, filtering samples on which the base model made errors to increase diversity, and the total hours and sample counts per language. Additionally, they provide the complete list of YouTube video links in supplementary material for reference. This constitutes sufficient documentation on dataset construction."
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10970,
      "completion_tokens": 213,
      "total_tokens": 11183
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
          "Reasoning": "DreamVoiceDB consists of recordings of 900 speakers sampled from existing datasets (LibriTTS-R and VCTK), which are human speech recordings. The dataset was annotated by speech and language experts. Therefore, the audio data is human-generated, i.e., recorded from human speakers."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
          "Reasoning": "The dataset contains natural language descriptors generated by OpenAI's GPT4 API based on combinations of expert-annotated keywords for each speaker, used to create text prompts describing voice timbre. Thus, the text descriptions are model-generated. Additionally, the annotation keywords were selected by human speech experts and collected through expert annotators' surveys, representing human-generated text annotations."
        }
      ]
    }
  },
  {
    "id": "hai24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11770,
      "completion_tokens": 264,
      "total_tokens": 12034
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
            "reasoning": "The paper states that a total of 8 expert annotators with expertise in speech and language pathology, speech and accent coaching, singing coaching, and transcription work annotated 900 speakers, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
            "reasoning": "The paper describes a comprehensive three-stage process including expert consultation, survey design with reference audio examples, Likert and binary scales for annotation, and a test run to recruit annotators, indicating detailed annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
            "reasoning": "The annotation used defined scales such as Likert scales for objective attributes and binary scales for subjective keywords, reflecting the presence of scoring rubrics in the survey design."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
            "reasoning": "The paper explicitly states that questions were designed with reference audio examples to facilitate objective category annotations, confirming the use of annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "hai24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12960,
      "completion_tokens": 302,
      "total_tokens": 13262
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes annotation by multiple annotators, not a single expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
          "reasoning": "The dataset annotations were performed by 8 expert annotators with expertise in speech and language pathology, speech and accent coaching, singing coaching, and transcription. Each of the 900 speakers was annotated once by each expert annotator, indicating a quality assurance process involving multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any annotation or QA was performed by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states expert annotators were used, so annotators are not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any AI model being used to validate or perform quality assurance for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic verification process for annotation quality assurance is described for the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper details a comprehensive expert annotation and quality process, so it is not the case that no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12570,
      "completion_tokens": 495,
      "total_tokens": 13065
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The paper introduces DreamVoiceDB, a new dataset of voice timbre annotations for 900 speakers. These annotations were performed by 8 expert human annotators with speech-related expertise, who evaluated speakers from existing datasets using expert-designed surveys with reference audio and scales to provide high-quality human-created labels and natural language descriptors generated from these annotations using GPT-4. This process involves original human judgment and annotation, not merely translation or direct aggregation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speaker samples from existing datasets with human annotations. There is no indication that data samples themselves are generated entirely by AI models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating content from another language via human translators in the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that content in the dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The DreamVoiceDB dataset is constructed by sampling 900 speakers from existing multi-speaker datasets LibriTTS-R and VCTK. The audio data is thus collected from pre-existing sources, though with extensive new annotation. Therefore, the audio samples themselves are collated from existing datasets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The original audio data from LibriTTS-R and VCTK is enhanced with newly created detailed human annotations for voice timbre characteristics, plus prompt generation using GPT-4 based on these annotations. This constitutes a derived dataset as it modifies and enriches existing data with additional metadata and labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the dataset origin and annotation process."
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13088,
      "completion_tokens": 322,
      "total_tokens": 13410
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Settings",
          "reasoning": "The DreamVoiceDB dataset of 900 speakers with annotated text prompts is used as training data to train the proposed text-guided voice conversion and generation models (DreamVC and DreamVG) from scratch, as described in Section 4.1."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using DreamVoiceDB to fine-tune pre-trained models in a supervised manner."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training techniques applied on DreamVoiceDB."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used exclusively for evaluation; the paper uses other speakers from LibriTTS-R dev set for validation and testing purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 DreamVoiceDB: Voice Timbre Dataset",
          "reasoning": "The authors analyze agreement scores among annotators and perform keyword distribution and label alignment analyses to ensure dataset quality and richness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The DreamVoiceDB is not described or used as a knowledge base for augmenting models via retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13811,
      "completion_tokens": 618,
      "total_tokens": 14429
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The DreamVoiceDB dataset is constructed by sampling speakers from LibriTTS-R and VCTK, which are primarily English speech datasets. There is no indication of inclusion of multiple languages beyond English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The dataset entries are derived only from English speech corpora (LibriTTS-R and VCTK). No second language presence is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (DreamVoiceDB: Voice Timbre Dataset), Introduction, and Experimental Settings (Section 4.1)",
          "reasoning": "DreamVoiceDB is composed of 900 speakers sampled exclusively from LibriTTS-R and VCTK datasets, both of which are English multi-speaker corpora. The paper explicitly references these sources and does not mention inclusion of other languages, confirming the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The dataset is derived solely from English-language datasets, with no indication of any non-English languages included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries consist of speech audio files and their corresponding text-based annotative labels, not programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain mathematical or logical symbolic expressions; it contains natural language annotations describing voice timbre."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is based on human speech data only, with no inclusion of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language source of the dataset is clearly documented as English speech from LibriTTS-R and VCTK."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain English language audio and text annotations; hence it is not non-linguistic data."
        }
      }
    }
  },
  {
    "id": "hai24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11029,
      "completion_tokens": 150,
      "total_tokens": 11179
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and Introduction",
          "reasoning": "The paper references an official website for the dataset but does not provide any link or mention of the code used for data collection, preprocessing, or generation. There is no indication that code has been publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: DreamVoiceDB: Voice Timbre Dataset",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including expert consultation, keyword selection, survey design, data collection by expert annotators, and analysis for alignment and label integration. This detailed process is described in Section 2 with a figure and description of methodology, indicating comprehensive documentation."
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5587,
      "completion_tokens": 99,
      "total_tokens": 5686
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Dataset",
          "Reasoning": "The CNVVE dataset consists of 950 audio samples of non-verbal voice expressions collected from 42 human participants who voluntarily donated their voice recordings, as explicitly described in Section 3.3. The data are human-generated recordings captured via smartphone microphones, demonstrating clear human involvement in recording the audio modality."
        }
      ]
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6387,
      "completion_tokens": 244,
      "total_tokens": 6631
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The dataset was collected via a dedicated website where 42 participants anonymously and voluntarily donated their voice recordings. There is no indication that these participants were experts; rather, they are presumably typical speakers contributing samples. Thus, the annotators (recorders) can be considered multiple human non-experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The data collection website provided the purpose and type of voice data sought, including example recordings and the written representation of each expression (e.g., \u201cUh-huh\u201d), guiding participants to provide the correct types of non-verbal voice expressions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "There is no indication in the paper of formal scoring rubrics or criteria for annotation or labeling provided to participants."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The website demonstrated example recordings of the expressions to participants, providing concrete examples for correct recordings."
          }
        }
      ]
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7577,
      "completion_tokens": 357,
      "total_tokens": 7934
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating quality assurance was conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset was collected from multiple participants, no quality assurance process involving multiple non-expert annotators validating or checking the data is described."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any AI model used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated algorithmic or rule-based verification process for the quality of dataset annotations or content is documented."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance procedure applied to validate the dataset or its annotations. The dataset comprises voice recordings collected from participants, and while corrupt recordings were removed, no explicit QA process is reported."
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7187,
      "completion_tokens": 405,
      "total_tokens": 7592
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Collecting non-verbal voice expressions), specifically Sections 3.1 and 3.3",
          "reasoning": "The dataset was created from recordings donated by 42 human participants who recorded non-verbal voice expressions themselves through a dedicated website. The paper explicitly states that these recordings are original audio samples collected from human contributors, not derived or translated from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was generated by AI or machine learning models; all data are human voice recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any process involving translation of content by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation usage in the data creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources but newly collected recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.3 (Data augmentation)",
          "reasoning": "Data augmentation techniques were applied to the original recordings, resulting in transformed audio files (e.g., background noise addition, pitch shifting). These augmented data are based on existing human-recorded samples with systematic modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data collection process and augmentation methods, so the data origin is well specified."
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7705,
      "completion_tokens": 368,
      "total_tokens": 8073
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Section 5.1",
          "reasoning": "The CNVVE dataset is used to train a convolutional neural network model from scratch to classify non-verbal voice expressions, as described in Section 4 (Method for classifying NVVEs) and evaluated in Section 5.1. The model is trained using the dataset without mention of pre-training, indicating training from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1, Section 5.4",
          "reasoning": "The dataset is used for evaluation through 5-fold cross-validation testing and the presentation of confusion matrices in Section 5.1 and 5.4, benchmarking the classification performance of the models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2, Section 5.3, Section 5.4",
          "reasoning": "The dataset is used for analysis to study feature sets (MFCC vs. Mel Spectrogram), effects of number of mel bands (Section 5.2), the impact of data augmentation techniques on model accuracy (Section 5.3), and in analyzing model performance confusion matrices (Section 5.4). These uses indicate analytical purposes to understand the dataset and model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8428,
      "completion_tokens": 417,
      "total_tokens": 8845
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Table 1",
          "reasoning": "The dataset consists of recorded non-verbal voice expressions labeled using English representations (e.g., \u201cUh-huh\u201d, \u201cmm-hmm\u201d, \u201cHush\u201d, \u201cPsst\u201d, \u201cAhem\u201d, etc.). There is no mention of recordings in other human languages or bilingual/multilingual content. The expressions and instructions provided to participants were in English, indicating the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of non-verbal voice expressions which are vocal sounds produced by humans, therefore they contain language-related audio signals and cannot be classified as having no language."
        }
      }
    }
  },
  {
    "id": "hedeshy23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5646,
      "completion_tokens": 168,
      "total_tokens": 5814
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Introduction and footnotes",
          "reasoning": "The paper states in the Introduction that the CNVVE dataset, code for creating the voice recognition system, trained models, and other voice processing tools are publicly available at https://github.com/hedeshy/CNVVE. This indicates that the code associated with the dataset and processing is made publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Collecting non-verbal voice expressions) and 3.3 (Dataset)",
          "reasoning": "The paper provides detailed documentation about the dataset creation process, including the data collection procedure via a dedicated website, participant demographics and consent, recording specifications, dataset composition, and associated metadata. This thorough description confirms that dataset creation process is well documented."
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6559,
      "completion_tokens": 245,
      "total_tokens": 6804
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3: Data",
          "Reasoning": "The paper describes selecting 27 files from the CORAAL corpus containing about 10 hours of African American English speech audio. This audio data was originally recorded human speech from sociolinguistic interviews, thus is human generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3: Data",
          "Reasoning": "The authors produced multiple human transcript versions (CORAAL original transcripts, Rev human transcripts, Rev (+AA tag) human transcripts, Amberscript transcripts) of the same 10 hours of audio. These transcripts were manually created by human transcribers from the audio, so are human generated text data."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3: Data",
          "Reasoning": "Two ASR-produced transcript versions (Rev ASR model and OpenAI Whisper model outputs) of the same audio were generated by automated speech recognition systems, indicating model generated text data."
        }
      ]
    }
  },
  {
    "id": "heuser24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7359,
      "completion_tokens": 371,
      "total_tokens": 7730
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3, Data; Section 6, Discussion",
            "reasoning": "The paper describes that the Rev and Rev (+AA tag) transcripts were generated by professional transcribers via Rev.com, who worked on separate chunks of the audio without access to the full file. The Amberscript transcript was produced by transcribers selected by a salesperson. These transcribers are professional but not described as subject-matter experts or linguists familiar with African American English specifically. The CORAAL transcripts were produced by linguistic researchers familiar with AAE and are pre-existing dataset transcripts, so only the new datasets from Rev, Rev (+AA tag), and Amberscript are considered newly introduced data for this paper."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 6, Discussion; Footnote 8",
            "reasoning": "The paper mentions that Rev transcribers were provided with a style guide that instructs them on transcription style, such as using English grammar conventions and maintaining integrity of spoken content. The paper includes a footnote linking to the Rev transcription style guide, indicating the presence of detailed instructions for these transcribers."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe the presence of scoring rubrics or detailed scoring criteria associated with the transcription annotation process. Only a style guide with instructions is mentioned, but no rubric or scoring system is reported."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper suggests that clarifying the style guide with examples could be helpful in the future, implying that current annotation guidelines do not include examples. Therefore, no annotation examples are currently provided."
          }
        }
      ]
    }
  },
  {
    "id": "heuser24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8549,
      "completion_tokens": 515,
      "total_tokens": 9064
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single human expert annotator on the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3 Data, and Abstract and Discussion",
          "reasoning": "The original CORAAL transcripts were produced and corrected by multiple researchers familiar with African American English (AAE), indicating multiple human experts performed quality assurance on that dataset. Additionally, the paper states that the CORAAL transcript versions were developed by a team of linguistic researchers, each file passing through multiple stages of transcription and editing by researchers with full access to the full audio file."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3 Data",
          "reasoning": "The Rev, Rev (+AA tag), and Amberscript human transcripts were produced by professional transcribers who were each assigned sections of the audio; these transcribers are considered non-experts in AAE compared to the linguistic researchers. Additionally, these transcripts were quality verified and improved upon by senior transcribers, implying multiple non-expert humans were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that AI models were used for quality assurance of the human-produced transcripts or other datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4 Methods",
          "reasoning": "The authors used an automated alignment tool (fstalign) to align transcript pairs and algorithmic/rule-based tests to quantify transcription differences; this automated verification and reproducible code and tests provide an automated quality control process for analyzing and validating dataset content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple quality assurance processes for the transcripts, including human expert review, professional human transcribers with senior review, and automated algorithmic analyses."
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8159,
      "completion_tokens": 501,
      "total_tokens": 8660
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3. Data",
          "reasoning": "The paper describes that the authors produced multiple human transcript versions of the same 10 hours of African American English speech from the CORAAL corpus. These transcripts include newly created human transcripts by professional transcribers solicited specifically for this study (e.g., Rev, Rev (+AA tag), Amberscript). The CORAAL transcripts themselves were produced previously, but the other human transcripts were newly created for this study, representing original content created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3. Data",
          "reasoning": "Two ASR-produced transcripts were generated using machine models: Rev ASR and OpenAI's Whisper. These machine-generated transcripts are newly created data outputs from speech recognition models applied to the CORAAL audio files, representing original content generated by AI models distinct from existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the data was produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the data was generated by machine translating content from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not simply collected or aggregated from existing sources without modification; rather, new transcriptions were generated either by humans or ASR models."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the transcripts are all aligned to and derived from the same original audio, the new transcripts represent new transcription efforts rather than transformed existing transcripts; thus, they are not considered derived in the sense of transformations of existing written data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origins and generation methods of all new datasets, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8677,
      "completion_tokens": 335,
      "total_tokens": 9012
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1, Figures 1 and 2",
          "reasoning": "The newly introduced dataset consists of six transcript versions (four human-produced and two ASR-produced) of the same 10 hours of African American English speech from CORAAL. These transcripts are used to compute word error rates (WER) between pairs of transcripts, both human-human and ASR-human comparisons, to evaluate and benchmark ASR system performance under different transcription styles. The dataset serves as reference transcripts to measure and compare ASR performance, as shown in their WER analysis results."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 5.2 and 6",
          "reasoning": "The dataset is used to analyze the nature and sources of transcription differences between transcript versions, including hypotheses about verbatim versus non-verbatim styles, morpho-syntactic differences characteristic of African American English, and reduction variations. The analysis examines these differences to understand stylistic variability, biases in transcription and ASR systems, and the implications for fair evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9400,
      "completion_tokens": 532,
      "total_tokens": 9932
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces a dataset consisting of transcripts of African American English speech only; no indication of multiple languages being included is given."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves only African American English speech and transcripts, with no mention of two distinct languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 Data and throughout the paper",
          "reasoning": "The dataset consists of transcripts of African American English (AAE) speech, which is a variety of English. The paper explicitly discusses transcription of AAE, a variety of English, and no other languages are mentioned in the dataset. All transcripts and analyses pertain to English content within AAE."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is clearly English (specifically AAE), so it is not a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human and ASR-produced transcripts of speech; no programming or code-related content is present."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or logical symbolic representations, only natural language transcripts."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns human speech transcripts only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any constructed or fictional languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are clearly described as African American English; the language is specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language transcripts, so it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "heuser24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6618,
      "completion_tokens": 179,
      "total_tokens": 6797
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4 Methods, footnote 6 and 5",
          "reasoning": "The paper explicitly links to open source tools and code repositories used for alignment and testing (fstalign, and https://github.com/revdotcom/speech-datasets/tree/main/coraal-multi), indicating that code used for processing and analysis of the dataset is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Data and Section 4 Methods",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including selection of audio files from the CORAAL corpus, descriptions of the six transcripts versions produced (four human, two ASR), the transcription style guides used, and the methods for alignment and difference testing. This constitutes clear documentation of the dataset construction and annotation process."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5625,
      "completion_tokens": 193,
      "total_tokens": 5818
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data set",
          "Reasoning": "The new dataset consists of unscripted speech recordings from 8 native speakers of Southern British English collected through storytelling, map tasks, and informal discussions. The speech data were manually recorded from human speakers, thus the modality is audio and the source is human generated."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Preprocessing",
          "Reasoning": "The raw audio F0 contours were extracted and processed and subsequently transformed into grayscale PNG images of F0 contours (28 by 28 pixels) through algorithmic methods such as smoothing, interpolation, normalization, and image conversion. This transformation was fully automated and generated by computational processing, without human authorship, thus the image data are model generated."
        }
      ]
    }
  },
  {
    "id": "hu24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6425,
      "completion_tokens": 208,
      "total_tokens": 6633
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 Data set",
            "reasoning": "The paper states that the speech data were annotated for pitch accents by one expert annotator based solely on the F0 shape, and a second expert independently annotated 12% of the data to assess inter-annotator agreement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1 Data set",
            "reasoning": "No explicit description or mention of detailed annotation instructions or guidelines for the expert annotators is provided in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 Data set",
            "reasoning": "The paper does not mention any scoring rubrics or formal evaluation criteria given to annotators for the pitch accent annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 Data set",
            "reasoning": "No annotation examples or example annotations are provided or referenced for the annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "hu24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7615,
      "completion_tokens": 298,
      "total_tokens": 7913
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset annotations for pitch accents were performed by one expert annotator, described explicitly as an expert who labeled accents based on F0 shape."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "A second expert annotator independently annotated 12% of the accents to perform a quality check, yielding high inter-annotator agreement, thus multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that the annotations or quality assurance were done by a non-expert single human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper of multiple non-expert annotators involved in the quality assurance process."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models to perform quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of automated verification or algorithmic quality assurance processes applied to validate annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the quality assurance process involving expert human annotators, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7225,
      "completion_tokens": 425,
      "total_tokens": 7650
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset consists of unscripted speech collected from 8 native speakers of Southern British English, recorded through specifically designed tasks (storytelling, map task, informal discussions). The speech data were manually annotated for pitch accents by human experts based on well-defined phonetic criteria, resulting in 2,025 accents. This indicates that the dataset was newly created from scratch by human contributors and is not adapted or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any data solely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any data produced by translating content from another language through human translators is made."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any machine-translated data is present."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources; rather it was freshly recorded and annotated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The raw speech data were processed through multiple steps including F0 extraction, normalization, smoothing, interpolation, and conversion into grayscale images to be used for classification. These transformation and preprocessing steps represent derived data based on the original recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly specified and documented."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7743,
      "completion_tokens": 433,
      "total_tokens": 8176
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.3, Model structure",
          "reasoning": "The dataset of pitch accent images is used to train a basic neural network model from randomly initialized parameters to classify pitch accents. The paper describes the model training on the dataset without mention of pre-training or transfer learning, indicating training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that the dataset is used for fine-tuning a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using reinforcement learning or RL-based post-training techniques on the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used solely for evaluation; it is central to training and testing the model, thus not exclusive to evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4 (Results and Discussion)",
          "reasoning": "The dataset is analyzed to assess model prediction accuracy differences between classes and to interpret discrepancies between model and human annotations, which involves analyzing patterns and characteristics of the pitch accent data."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as serving as a knowledge base to augment models through retrieval or other methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used practically for training and analysis within the paper."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8466,
      "completion_tokens": 560,
      "total_tokens": 9026
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset contains only Southern British English speech from native speakers. No indication of multiple languages involved."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset contains only one language, English (Southern British English). No second language is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset consists of unscripted speech from native speakers of Southern British English, explicitly indicating English language content only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset is exclusively English, specifically Southern British English, so it is not non-English monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "General paper content",
          "reasoning": "The dataset entries consist of images representing F0 contours of speech; no programming or code content is part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "General paper content",
          "reasoning": "While data processing involves statistics and smoothing techniques, the dataset itself contains images of pitch contours, not explicit mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "General paper content",
          "reasoning": "The dataset pertains to human speech pitch accents and contains no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The dataset involves natural language (English) only; no constructed or fictional languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 2.1 Data set",
          "reasoning": "The language of the dataset entries is clearly documented as Southern British English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries represent pitch accent images derived from speech data and thus involve language; therefore, dataset does contain language."
        }
      }
    }
  },
  {
    "id": "hu24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5684,
      "completion_tokens": 188,
      "total_tokens": 5872
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.3 Model structure",
          "reasoning": "The paper explicitly states that the code used for training the neural network model is available at https://osf.io/esb4g/?view_only=4409bab750b24dcd9be308bc2d66, indicating that at least part of the code related to data processing and model training is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Data set and Section 2.2 Preprocessing",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including information about the participants, the speech tasks, pitch accent annotation procedures, F0 extraction, normalization, smoothing, time registration, and image generation steps. This thorough description spans sections 2.1 and 2.2, offering transparency and completeness necessary for reproducibility."
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9120,
      "completion_tokens": 93,
      "total_tokens": 9213
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Evaluation; Section 1 Introduction",
          "Reasoning": "The internal dataset is introduced as a new dataset with 8 keywords uttered 10 times by 50 speakers, recorded in a clean environment. This dataset contains recorded human speech audio for keyword spotting evaluation, confirming it is audio modality and human generated."
        }
      ]
    }
  },
  {
    "id": "huang22l_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9920,
      "completion_tokens": 233,
      "total_tokens": 10153
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and Section 3.2",
            "reasoning": "The paper describes the use of an automatic segmentation tool, Montreal Forced Aligner, to generate word-level annotations from Librispeech audio data. The internal dataset with eight keywords and 400 speakers is used for evaluation, but the annotation process is not described as involving human annotators. Therefore, the data annotation was performed by an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not provide any explicit instructions or annotation guidelines for human annotators, as the annotation is performed automatically using Montreal Forced Aligner on Librispeech data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No mention of scoring rubrics or criteria for annotation quality assessment is made in the paper, as the annotation is automatic without human involvement."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No annotation examples are provided in the paper for the automatic annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "huang22l_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11110,
      "completion_tokens": 296,
      "total_tokens": 11406
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for any new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance performed by multiple human experts on any newly introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of quality assurance by a single non-expert human annotator is provided in the text."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving multiple human non-expert annotators for the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance being performed by an AI model acting as a judge for dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic rule-based quality assurance applied to new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces an internal dataset but does not document any quality assurance process applied to it. There is no description of annotation validation, human or automated QA, or any procedures to ensure the quality or consistency of the dataset. Therefore, no explicit quality assurance process is considered present."
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10720,
      "completion_tokens": 489,
      "total_tokens": 11209
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2 Evaluation",
          "reasoning": "The paper introduces a new internal dataset with 8 keywords named after actual home appliances, uttered 10 times by 50 speakers, collected by the authors themselves (Section 3.2). This dataset is original content created entirely from scratch by human contributors, as it records human speakers uttering these keywords uniquely for the study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any datasets generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was produced via machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Training and Section 3.2 Evaluation",
          "reasoning": "The authors use publicly available datasets such as Librispeech, Hey-Snips, and the MS-SNSD noise datasets, which were collected from existing sources without significant modification; they are aggregated and utilized for training and testing. Specifically, Librispeech and Hey-Snips are existing datasets used 'as is'."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Training and Section 3.2 Evaluation",
          "reasoning": "The authors apply modifications and transformations such as segmenting Librispeech audio into 1s clips using forced alignment, data augmentation by adding noise and simulating far-field effects on both training and evaluation datasets (Hey-Snips and the internal dataset). Thus, datasets derived from existing sources with applied transformations are used."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the datasets used is specified and documented."
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11238,
      "completion_tokens": 286,
      "total_tokens": 11524
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The authors describe using the Librispeech dataset to train their QbyE-MLPMixer model from randomly initialized parameters using supervised learning with cross-entropy loss. This indicates training from scratch on this labeled dataset for keyword spotting."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4",
          "reasoning": "The authors evaluate their proposed MLPMixer model using the publicly available Hey-Snips dataset and an internal dataset. These datasets are used exclusively for benchmarking and performance measurement, as described in Sections 3.2 and 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The paper includes ablation studies analyzing input representations and activation functions using datasets to understand their impact on performance, indicating usage for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11961,
      "completion_tokens": 590,
      "total_tokens": 12551
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed datasets described (Hey-Snips and the internal dataset) contain only English language speech as evident from the descriptions and training on the Librispeech dataset which is English-only (Section 3.1 and Section 3.2). There is no indication of multiple languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of datasets containing exactly two languages. The datasets are either English or not specified as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Training), Section 3.2 (Evaluation)",
          "reasoning": "The new internal dataset created by the authors contains utterances of eight English keywords (e.g., 'LG Styler', 'Hey LG') spoken by 50 speakers, as stated in Section 3.2. The publicly available Hey-Snips dataset used is also English-only. The training data (Librispeech) is English audiobooks, confirming the monolingual English nature of the datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with non-English language content are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain audio speech data; there is no programming or code content included in the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain mathematical or symbolic expressions. Mathematical notations appear only in the methods section for model description, not as part of dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain human speech audio, not biological sequences or other non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The proposed datasets are composed of spoken human language audio; hence, language is present."
        }
      }
    }
  },
  {
    "id": "huang22l_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9179,
      "completion_tokens": 160,
      "total_tokens": 9339
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section in the paper",
          "reasoning": "The paper does not provide any URL, footnote, or mention indicating that the code for data collection, preprocessing, or dataset construction is publicly available in any accessible repository."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "Section 3.1 and 3.2 describe dataset usage only",
          "reasoning": "The paper uses publicly available datasets (Hey-Snips, Librispeech) and an internal dataset for evaluation but does not provide documentation on dataset creation or data collection procedures. There is no description of how the internal dataset was collected, generated, or constructed; only the number of speakers and keywords is mentioned. Therefore, no documentation on dataset creation is provided."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8636,
      "completion_tokens": 187,
      "total_tokens": 8823
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The ConEC dataset is described as consisting of earnings calls audio, which are real-world audio recordings of human speech, collected along with supplementary materials. It is introduced by the authors as a real-world contextual ASR test set and used to evaluate the proposed methods, thus it is a new dataset introduced by the authors."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The ConEC dataset also includes real-world supplementary materials such as presentation slides, earnings news releases, and lists of meeting participants' names and affiliations. These textual contexts are human-generated textual data and are introduced as part of the ConEC dataset by the authors."
        }
      ]
    }
  },
  {
    "id": "huang24f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9436,
      "completion_tokens": 213,
      "total_tokens": 9649
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Dataset description and data preparation",
            "reasoning": "The paper explains the generation of artificial biasing lists during training using rare words beyond frequency thresholds, and the application of hand-crafted linguistic rules to perturb text. These procedures involve deterministic rules and predefined algorithms rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not provide or mention any detailed annotation instructions given to annotators for the construction or labeling of the new datasets or contexts."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not applicable as annotations are generated automatically",
            "reasoning": "The dataset preparation uses rule-based processes and no mention of scoring rubrics or criteria for annotation quality is provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not applicable as annotations are generated automatically",
            "reasoning": "No examples of annotation or manual labeling procedures are provided in the paper for the new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "huang24f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10626,
      "completion_tokens": 331,
      "total_tokens": 10957
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the datasets introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about quality assurance conducted by multiple human non-experts for the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used to perform quality assurance on the datasets introduced by the authors."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "Section 4.1 (Datasets)",
          "reasoning": "While the paper describes the generation of artificial biasing lists during training and the use of hand-crafted linguistic rules for creating alternative spellings (text perturbation), these pertain to data augmentation techniques and not explicitly to quality assurance or validation of dataset annotations. The paper does not explicitly state any automated quality assurance or validation process applied to dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied to the new datasets introduced (e.g., ConEC). No human or automated QA or verification procedure is documented for validating dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10236,
      "completion_tokens": 414,
      "total_tokens": 10650
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any entirely new datasets created from scratch by human contributors were introduced by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generation of original data purely by AI or models; it focuses on methods to improve ASR using existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of data produced by human translation from another language described as new dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss creation of new data via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors used existing public datasets (LibriSpeech, SPGISpeech, ConEC), which are pre-existing and not newly collated by the authors as datasets themselves."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The authors use existing datasets (LibriSpeech, SPGISpeech, ConEC) and generate artificial biasing lists for contexts by extracting rare words and adding distractors. They also perturb reference transcriptions with alternative spellings based on hand-crafted linguistic rules as a form of data augmentation. These modifications indicate derived data based on existing datasets with transformations applied, rather than entirely new datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and modifications are documented in the paper; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10754,
      "completion_tokens": 500,
      "total_tokens": 11254
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any new dataset exclusively for pre-training large models on general patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets) and Section 4.3 (Results)",
          "reasoning": "The new datasets SPGISpeech and ConEC are used to train contextual ASR models from scratch or with contextual biasing training with the proposed techniques, as indicated by the descriptions of training on these datasets and evaluation results on them."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new datasets for supervised fine-tuning of pre-trained models. The training described is end-to-end contextual ASR model training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training methods applied to the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Datasets) and Section 4.3 (Results)",
          "reasoning": "The ConEC dataset is used as a real-world evaluation set for measuring performance of the contextual ASR models; similarly, test parts of SPGISpeech and LibriSpeech contexts are used for benchmarking. Thus, the datasets serve an evaluation purpose."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the datasets for analysis of trends or characteristics; the focus is on training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as serving as a knowledge base for model augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is clear documented usage of the introduced datasets for training and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11477,
      "completion_tokens": 570,
      "total_tokens": 12047
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any datasets that contain more than two human languages. The datasets used and introduced (LibriSpeech, SPGISpeech, ConEC) are all English-based."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset contains exactly two human languages. The datasets are English-only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The new dataset introduced in the paper for experiments includes LibriSpeech, SPGISpeech, and ConEC, all described to be English datasets. The ConEC dataset, a real-world earnings call dataset with associated English supplementary materials, is also English. The paper explicitly states these datasets are English and uses English text for training, text perturbation, and contexts."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset in a non-English language. All datasets used for training and evaluation are English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are speech and text datasets for ASR. No programming or structured code-related content is included in the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses model architecture and uses mathematical notation in descriptions, the datasets themselves do not contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are speech and text from human languages only; no biological or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset includes constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the language (English) of the datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of speech and text data in English and thus do contain human language."
        }
      }
    }
  },
  {
    "id": "huang24f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8695,
      "completion_tokens": 168,
      "total_tokens": 8863
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Introduction section, sentence 'Our implementation and experiment results are available in the ConEC repository: https://github.com/huangruizhe/ConEC.'",
          "reasoning": "The paper explicitly states that their implementation and experiment results are available in the ConEC repository, providing a link to the GitHub repository where code related to the experiments and dataset use can be accessed."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets and Introduction section",
          "reasoning": "The paper provides detailed information on the datasets used (including LibriSpeech, SPGISpeech, and ConEC), their statistics, and how contexts and biasing lists are generated for training and evaluation, thus documenting the dataset creation and processing procedures relevant to their experiments."
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7352,
      "completion_tokens": 162,
      "total_tokens": 7514
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset",
          "Reasoning": "The FisheyeMeeting dataset is introduced as newly collected data by the authors using a fisheye panoramic camera in multi-party meeting scenes, resulting in fisheye panoramic video recordings. This represents the video modality captured by human-operated equipment."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset",
          "Reasoning": "The FisheyeMeeting dataset includes corresponding multi-channel audio data captured by a circular multi-channel microphone array. This multi-channel audio data is human-recorded through physical microphone arrays during meetings, thus originating from human involvement."
        }
      ]
    }
  },
  {
    "id": "huang24g_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8152,
      "completion_tokens": 198,
      "total_tokens": 8350
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 Dataset",
            "reasoning": "The dataset annotation was performed by multiple annotators with basic understanding of the ASD task, who cross-checked and corrected annotations, indicating multiple non-expert humans conducted the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset",
            "reasoning": "Annotation requirements are standardized and described, including visible speaker definition and boundary annotation rules, indicating detailed annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 Dataset",
            "reasoning": "No mention or evidence of scoring rubrics or scoring criteria in the annotation process is present; only annotation requirements and procedures are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1 Dataset",
            "reasoning": "The paper does not mention providing specific annotation examples within the guidelines or supplementary materials."
          }
        }
      ]
    }
  },
  {
    "id": "huang24g_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9342,
      "completion_tokens": 422,
      "total_tokens": 9764
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that multiple annotators with basic understanding conducted the annotations and cross-checking, but there is no indication that any single annotator was a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors mention multiple annotators performed the annotation and cross-checking, but there is no information to confirm that these annotators had subject matter expertise or belonged to the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The annotation was performed by multiple annotators rather than a single annotator, so this category does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The dataset annotation was done by multiple annotators with basic understanding of the ASD task who cross-checked and corrected segments. There is no claim that these annotators were experts; thus, it is appropriate to consider them non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of AI models acting as judges or quality assurance tools during dataset annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based verification process for annotation quality assurance is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a human annotation and cross-checking process; therefore, QA was performed and documented."
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8952,
      "completion_tokens": 385,
      "total_tokens": 9337
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The FisheyeMeeting dataset was collected by the authors themselves using a fisheye panoramic camera and a six-channel microphone array in multi-party roundtable meeting scenes. The paper explicitly states that multiple annotators manually annotated the data with cross-checking and corrected the annotations, confirming that the dataset is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation used in generating the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected directly from new recordings and is not aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is original recordings and annotations; it is not derived by modifying or adapting existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the dataset creation procedure and thus the data origin is documented."
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9470,
      "completion_tokens": 318,
      "total_tokens": 9788
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 (Loss function), Section 4.1 (Comparison with other methods)",
          "reasoning": "The paper describes training ASD models on the newly introduced FisheyeMeeting dataset from scratch by defining loss functions and reporting training results, demonstrating the dataset's use in training models with randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 (Evaluation metric), Section 4.1 (Comparison with other methods)",
          "reasoning": "The FisheyeMeeting dataset is used for evaluation purposes by employing mean average precision (mAP) to measure and compare the performance of the proposed and state-of-the-art ASD methods on its validation set."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Ablation study)",
          "reasoning": "The dataset is used to analyze contributions of different features and encoders by conducting ablation studies, which assess the impact of scene images, spatial spectrum heatmaps, and feature encoder architectures on performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10193,
      "completion_tokens": 501,
      "total_tokens": 10694
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any linguistic content or annotations in English or any other language for the proposed FisheyeMeeting dataset; the focus is on audiovisual data and audio from meetings, without mention of spoken language content or its type."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audiovisual and audio recordings; it does not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes mathematical techniques and algorithms for audio processing (e.g., SRP-PHAT), the actual dataset described (FisheyeMeeting) does not contain mathematical or logical notation as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human audiovisual and audio data, not biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificial languages in the dataset."
        },
        "Unknown": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The dataset is described as audiovisual recordings and multi-channel audio from meeting scenes with multiple participants, but the paper does not specify the language(s) spoken or annotated in the dataset. Therefore, the linguistic content is unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human communication (audio and video), so language is involved, though unspecified."
        }
      }
    }
  },
  {
    "id": "huang24g_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7411,
      "completion_tokens": 200,
      "total_tokens": 7611
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Conclusion",
          "reasoning": "The paper mentions at the end of Section 5 (Conclusion) that 'The source code will be released.' No explicit link, repository URL, or details about the availability of the code related to dataset construction or processing are provided in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "There is detailed documentation on the dataset creation process in Section 3.1. The paper describes the recording setup (fisheye panoramic camera and six-channel circular microphone array), data collection duration (42 hours recorded, 6 hours selected for the dataset), participant details, segmentation, face detection, trajectory forming, annotation process with multiple annotators and annotation instructions, as well as dataset statistics and splits."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6931,
      "completion_tokens": 108,
      "total_tokens": 7039
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The paper introduces the use of the USC 75-Speaker Speech MRI Database, which consists of real-time MRI video data capturing vocal tract articulation of speakers. This dataset was recorded from human speakers producing continuous English speech, collected through MRI imaging processes requiring human speakers, thus human-generated video data is present in this new dataset introduced by the authors for analysis."
        }
      ]
    }
  },
  {
    "id": "huang24i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7731,
      "completion_tokens": 278,
      "total_tokens": 8009
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3 (Frame Selection) and Section 3.4 (Feature Extraction)",
            "reasoning": "The paper describes an automated pipeline used to process MRI video data and extract articulatory positional information. Transcriptions were obtained using an ASR model (whisper-large-v3), alignments were derived using the Montreal Forced Aligner, and automated segmentation was applied to extract articulatory features. No mention is made of humans performing the primary annotation in the new dataset, indicating that annotations are generated by an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not describe any detailed manual annotation instructions provided to annotators, as the data processing and annotation were conducted automatically through models and algorithms."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria provided for annotation tasks, consistent with an automatic process without human annotators requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide annotation examples or manuals as annotation was performed automatically via models and algorithmic pipelines, thus no examples were needed or provided."
          }
        }
      ]
    }
  },
  {
    "id": "huang24i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8921,
      "completion_tokens": 336,
      "total_tokens": 9257
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of QA involving multiple human experts for dataset validation in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of QA performed by multiple human non-expert annotators in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the Whisper-large-v3 ASR model is used for transcription, it is employed for data processing and not described as a quality assurance judge for dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 (Frame Selection) and Section 3.4 (Feature Extraction)",
          "reasoning": "The dataset preparation and annotation involve automated pipelines including automatic speech recognition with Whisper-large-v3, forced alignment with Montreal Forced Aligner, automatic detection of inter-speech pauses, contour segmentation for articulators, and extraction of constriction task variables via algorithmic computation, indicating that the quality assurance relies on automated verification processes rather than manual validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated pipeline used for quality assurance processes in dataset annotation and extraction, so QA is not absent."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8531,
      "completion_tokens": 375,
      "total_tokens": 8906
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new data collection conducted by the authors themselves. Instead, it uses a publicly available dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data analyzed is not generated by a model, but is based on real MRI imaging of speech production."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation of any data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The study uses the publicly available USC 75-Speaker Speech MRI Database, which was collected by others, and aggregates data from speakers of various demographic backgrounds for analysis."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.2, 3.3, and 3.4 Methods",
          "reasoning": "The authors process the existing MRI data by applying an automated pipeline including forced alignment, segmentation, and feature extraction to derive articulatory constriction features representing articulatory settings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is specified and documented as a publicly available database."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9049,
      "completion_tokens": 249,
      "total_tokens": 9298
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results) and Section 5 (Discussion)",
          "reasoning": "The dataset, a large real-time MRI corpus of vocal tract articulation from speakers of various linguistic backgrounds speaking English, is used primarily to analyze articulatory setting differences statistically across different speaker groups. The paper conducts multiple linear regression analyses and examines patterns and trends in articulatory postures rather than using the data for model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the dataset for scientific analysis of articulatory settings, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9772,
      "completion_tokens": 565,
      "total_tokens": 10337
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The dataset contains English speech data spoken by speakers with different L1 backgrounds (American English, Indian English, L2 English speakers from China and Korea), but all utterances are in English. No other languages are spoken in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The dataset is not bilingual; it contains only English speech, though speakers have different native languages. There are no entries with exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "All speech data in the dataset are English. Speakers are from different language backgrounds, but the speech samples analyzed are exclusively English utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.1 Data",
          "reasoning": "The dataset does not contain non-English speech samples; all speech is English regardless of speaker L1."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech MRI video data and derived articulatory measurements, not programming code or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 3.4 Feature Extraction",
          "reasoning": "Mathematical notation is used to describe the method for feature extraction but the dataset entries themselves do not contain mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pertains to human speech articulated during English speaking; it does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset are clearly described and the content is exclusively English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken English language data; thus the dataset is not devoid of language."
        }
      }
    }
  },
  {
    "id": "huang24i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6990,
      "completion_tokens": 216,
      "total_tokens": 7206
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention in the paper",
          "reasoning": "The paper does not include any mention or link to code repositories or software for the automated pipeline, data processing, or data extraction used in the construction or analysis of the dataset. There is no indication that the code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Methods), including subsections 3.1 Data, 3.2 Preliminary Data Analysis, 3.3 Frame Selection, 3.4 Feature Extraction",
          "reasoning": "The paper provides detailed documentation of the dataset creation process: it describes the data source (USC 75-Speaker Speech MRI Database), the criteria for subject selection, the cleaning and filtering steps such as removal of non-linguistic tasks, the automated pipeline steps including speech transcription, forced alignment, pause detection parameters, and feature extraction methods with mathematical definitions. This thorough description facilitates understanding of the dataset creation and preprocessing steps, enhancing reproducibility even though code is not available."
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 4552,
      "completion_tokens": 119,
      "total_tokens": 4671
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Production test, Sections 2.1 and 2.2",
          "Reasoning": "The new dataset consists of recordings from twenty Japanese-learning children during a picture-naming task conducted by the authors. Recordings were made with a Marantz digital recorder and microphone capturing the children's spoken utterances in a controlled experimental setting. This audio data is human-generated as it originates from real children speaking, captured by human-operated recording devices during the study."
        }
      ]
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 5352,
      "completion_tokens": 256,
      "total_tokens": 5608
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1",
            "reasoning": "The paper states that a picture-naming task was conducted with 20 Japanese-learning children and recordings were manually analyzed for vowel devoicing occurrences. This implies that the annotation was performed by experts analyzing audio data, likely a single human expert or a research team expert who identified devoiced vowels in the recordings."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not explicitly describe detailed annotation instructions given to annotators for labeling vowel devoicing. The methodology section does not specify annotation protocols or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "No scoring rubrics or formal criteria are described in the paper for annotating devoiced vowels. The paper only mentions calculating rates of devoicing occurrences, implying a direct coding without complex rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide annotation guideline examples or sample annotations illustrating how devoicing was coded. Example words are listed for the tasks, but not for annotation guideline purposes."
          }
        }
      ]
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 6542,
      "completion_tokens": 394,
      "total_tokens": 6936
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human expert performed quality assurance or annotation validation on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any process involving multiple human experts for quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance being performed by a single human non-expert for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance carried out by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any AI model being used to verify or validate the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic method applied as quality assurance for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a novel dataset based on a production experiment with children and reports on data collection and analysis procedures. However, it does not describe any quality assurance process applied to validate or verify the dataset annotations or content. There is no mention of annotator involvement in QA, multiple checks, expert validation, or automated verification. Therefore, no QA was evidently performed or documented."
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 6152,
      "completion_tokens": 364,
      "total_tokens": 6516
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper describes a production experiment conducted by the authors with twenty Japanese-learning children aged 3 and 4 years, recording their speech in a controlled environment via a picture-naming task. The recorded speech data are original and created entirely from scratch by human participants, not derived or adapted from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention in the paper of any AI or machine learning model generating data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data produced through human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data produced through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated or collected from existing sources; rather, it was freshly collected through the experiment."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is original and not based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection method are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 6670,
      "completion_tokens": 302,
      "total_tokens": 6972
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2, 3, and 4 (Production test, Results, and Discussion)",
          "reasoning": "The paper introduces a new dataset consisting of production recordings of Japanese-learning children aged 3 and 4 years, collected via a picture-naming task. This dataset is used primarily to analyze patterns and trends in high vowel devoicing (HVD) development, such as devoicing rates by age and position in words. There is no indication that the dataset is used for training, pre-training, fine-tuning models, or evaluation benchmarks. Instead, the dataset serves to provide empirical evidence for linguistic analysis and understanding the developmental trajectory of HVD in children."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A clear practical usage of the dataset is described, namely its use for analyzing linguistic developmental patterns of high vowel devoicing in children."
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 7393,
      "completion_tokens": 250,
      "total_tokens": 7643
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset consists of speech data collected from twenty Japanese-learning children from Tokyo and surrounding areas, who named pictures using Japanese words. All recorded speech materials are in Japanese, and the paper explicitly discusses features of phonetics and phonology of Japanese, indicating the dataset is solely in Japanese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "hwang24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 4611,
      "completion_tokens": 200,
      "total_tokens": 4811
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention or provide any link or reference to code related to the dataset creation, data collection, preprocessing, or analysis. No code repository or software is referenced anywhere in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Production test), subsections 2.1 Participants and procedure and 2.2 Material",
          "reasoning": "The paper documents the dataset creation process in detail in Section 2. It describes the participants (20 Japanese-learning children, age and demographics), the recording procedure (picture-naming task, recording environment, recording equipment), and the materials used (35 pictures representing familiar words). It also explains which data were analyzed (only words involving devoiceable vowels) and criteria for exclusion (noise, laughter, disfluencies, whispers). Thus, the dataset creation and collection process is described clearly and transparently within the paper."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6503,
      "completion_tokens": 476,
      "total_tokens": 6979
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Stimuli and data recording; Section 2.3 Data analysis and measurements",
          "Reasoning": "The new dataset consists of recordings of four native speakers of Tripolitanian Libyan Arabic producing a list of 108 utterances, including 30 target words with minimal or near-minimal pairs involving singleton and three types of geminate consonants. The data was collected through human speech recording sessions and manually segmented and annotated for phonetic analysis. Thus, it is audio data generated by human speakers as part of the study."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Stimuli and data recording",
          "Reasoning": "The stimuli used for recording were compiled lists of written words in the local Arabic dialect with specified phonological properties. The word lists and their transcriptions are human-created text data specifically crafted for this study's experiments, used as prompts for speakers."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Table 2 in Section 3.1 Results",
          "Reasoning": "The study presents tables showing aggregated statistical results of durational measurements (means, standard deviations, and counts) of the singleton and geminate consonants from the recorded data. These tables are human-generated summaries based on the processed human-recorded data of the study's new dataset."
        },
        {
          "Modality": "graph",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Figures 1, 2 and 3 in Section 3 Results",
          "Reasoning": "The paper includes graphical representations (bar charts and frequency plots) of the acoustic measurements such as durations, RMS amplitudes, and formant frequencies from the new dataset analyzed. These graphs illustrate analysis of the recorded human speech data."
        }
      ]
    }
  },
  {
    "id": "issa23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7303,
      "completion_tokens": 232,
      "total_tokens": 7535
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.2 and 2.3",
            "reasoning": "The data collection involved four native speakers of TLA who performed the recordings, and data labeling was done semi-automatically with manual checking, as described in Section 2.3. There is no indication of multiple annotators or crowdsourcing; the author appears to be the expert who conducted annotations and data verification."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators or data labelers. The data labeling is described as semi-automatic and checked by hand but without documented annotation guidelines or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section",
            "reasoning": "No mention or evidence of scoring rubrics or explicit criteria for annotation are provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit section",
            "reasoning": "The paper does not provide any annotation examples or annotated sample data to illustrate the annotation process or guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "issa23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8493,
      "completion_tokens": 306,
      "total_tokens": 8799
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe any quality assurance process conducted by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset or annotation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert humans conducted quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models for quality assurance or validation of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Data analysis and measurements",
          "reasoning": "The durational measurements and acoustic analyses were performed semi-automatically using Praat scripts, with measurements obtained automatically and then checked by hand. This indicates an automated process was used for measurement extraction and verification, providing systematic and replicable quality assurance for the annotation data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A QA process is described implicitly via semi-automatic labeling and manual checking in data analysis, so it is not accurate to say no QA process was applied."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8103,
      "completion_tokens": 365,
      "total_tokens": 8468
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper describes a new dataset constructed by the authors specifically for this study, comprising 30 real minimal or near-minimal utterances designed for the investigation of geminate types in Tripolitanian Libyan Arabic. These utterances were produced by four native human speakers in controlled recording sessions, indicating original data collection and human-generated content created entirely from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the data was created by translating existing data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of machine-translated data in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was newly recorded by the authors rather than collected or aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived from existing datasets with modification; it is newly elicited data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and collection method are clearly documented."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8621,
      "completion_tokens": 258,
      "total_tokens": 8879
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Results and Section 4 Discussion and Conclusion",
          "reasoning": "The dataset compiled and analyzed in this study is used primarily to analyze acoustic characteristics and phonetic correlates of geminate and singleton consonants in Tripolitanian Libyan Arabic. The paper presents detailed statistical analyses of durational and non-durational correlates using the dataset, focusing on trends and patterns rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the use of a new dataset collected and analyzed specifically to examine phonetic properties of gemination, indicating practical usage beyond no-utility."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9344,
      "completion_tokens": 434,
      "total_tokens": 9778
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of utterances in Tripolitanian Libyan Arabic, a single language."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that exactly two languages are included; only one language, TLA, is documented."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English utterances; it is based on Tripolitanian Libyan Arabic."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (Speakers) and Section 2.2 (Stimuli and data recording)",
          "reasoning": "The new dataset introduced consists solely of utterances in Tripolitanian Libyan Arabic (TLA), a non-English language. This is confirmed by descriptions of native speakers of TLA and stimuli consisting of TLA words only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain programming or structured code content; only recorded spoken utterances in TLA are used."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any mathematical or formal logical expressions as part of the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human spoken language data only; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on a natural language dialect (Tripolitanian Libyan Arabic), not a constructed or fictional language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly identified and documented as Tripolitanian Libyan Arabic; thus, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of spoken utterances in TLA, which is a human language, so the dataset contains language."
        }
      }
    }
  },
  {
    "id": "issa23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6562,
      "completion_tokens": 179,
      "total_tokens": 6741
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not provide any links, URLs, or references to publicly available code repositories or software scripts used for data collection, preprocessing, or analysis. Although it mentions use of PRAAT and scripts, these appear to be custom or standard scripts not provided or linked in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method)",
          "reasoning": "The paper provides detailed documentation on dataset creation including speaker recruitment (Section 2.1), stimuli compilation and description (Section 2.2), and detailed recording procedure with sentence carrier and repetitions. It also describes data analysis and measurement procedures (Section 2.3), including annotation and acoustic measurements methods. Hence, the dataset creation and processing is transparently documented in the Method section."
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 3032,
      "completion_tokens": 182,
      "total_tokens": 3214
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Introduction; Section 2",
          "Reasoning": "The dataset RUSAVIC introduced by the authors contains audio recordings of 20 drivers in natural driving conditions (in-the-wild recordings). These audio recordings are captured via microphones during driving, implying direct human generation and recording of natural speech."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Introduction; Section 2",
          "Reasoning": "The RUSAVIC dataset also contains video recordings of the drivers, simultaneously recorded at 30 frames per second with resolution 1280\u00d7720 pixels as described in Section 2. The videos capture the driver's face, especially the mouth region during speech, and are recorded with human involvement (drivers speaking naturally in a vehicle)."
        }
      ]
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 3832,
      "completion_tokens": 262,
      "total_tokens": 4094
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 1, Section 3",
            "reasoning": "The paper introduces a new dataset, the RUSAVIC corpus, explicitly described as developed by the authors for training their models, containing audio-visual speech data of 20 drivers recorded in-the-wild. As a specialized multi-speaker audio-visual corpus in cars, it is implied the dataset development involved expert annotation or collection process by the authors, who are domain experts in audio-visual speech recognition, not just an automatic process or crowdsourcing."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided for the RUSAVIC dataset annotation; there is no description of instructions in the main text."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no explicit mention of scoring rubrics or structured criteria used for annotations in the RUSAVIC dataset within the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no reference to annotation examples provided in the annotation guidelines for RUSAVIC or any newly introduced dataset in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 5022,
      "completion_tokens": 280,
      "total_tokens": 5302
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that a single human expert performed quality assurance on the RUSAVIC dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided regarding multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert annotators performing quality assurance or annotation validation for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or algorithmic quality assurance applied to the RUSAVIC dataset within the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied to the RUSAVIC dataset or any other new dataset. Quality assurance is neither documented nor referenced, thus no QA process is considered present based on the provided content."
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 4632,
      "completion_tokens": 401,
      "total_tokens": 5033
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 3 Conclusions",
          "reasoning": "The paper states that the authors trained their models on the RUSAVIC dataset containing in-the-wild audio and video recordings of 20 drivers. The corpus was collected by the authors themselves as original content recorded in natural driving conditions, indicating data created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as being collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications, transformations, or adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origin and creation method of the new dataset (RUSAVIC), which was recorded by the authors, so the data source is documented."
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 5150,
      "completion_tokens": 208,
      "total_tokens": 5358
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 1 and 2",
          "reasoning": "The paper states that the authors trained audio and video models on their new RUSAVIC dataset, which contains in-the-wild recordings of 20 drivers. This indicates the dataset is used to train models from scratch for audio-visual speech recognition in a driving context."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 5873,
      "completion_tokens": 251,
      "total_tokens": 6124
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 2 DAVIS Architecture",
          "reasoning": "The RUSAVIC dataset used for training contains Russian language audio-visual speech data from 20 different drivers (Section 1). The dataset and system recognize the most frequent drivers' requests in Russian, indicating the dataset contains only one non-English human language \u2014 Russian."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ivanko22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 3091,
      "completion_tokens": 209,
      "total_tokens": 3300
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 Conclusions, footnote 1",
          "reasoning": "The paper states that the source code, dataset, and trained models are available 'by request' via a provided URL (https://mobiledrivesafely.com). However, there is no indication that all code related to data collection, preprocessing, and dataset generation is publicly available in a repository such as GitHub. Hence, the code is not openly accessible for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Introduction (Section 1) and Section 2 DAVIS Architecture",
          "reasoning": "The paper describes the creation of their RUSAVIC dataset and gives references to prior publications [6,7] that discuss the corpus. Additionally, Section 2 details audio and video recording specifications, signal acquisition, preprocessing steps such as mouth region extraction and spectrogram computation. This provides documentation on the dataset creation process and preprocessing, sufficient to understand the data characteristics and experimental setup."
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7023,
      "completion_tokens": 125,
      "total_tokens": 7148
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Svarah and Conclusion",
          "Reasoning": "Svarah is a new benchmark dataset introduced by the authors. It contains 9.6 hours of transcribed English audio recorded from 117 Indian speakers with diverse accents. The audio data was collected by having speakers record read speech, extempore speech, and utterances corresponding to everyday use cases using a mobile app. This process involves direct human participation and manual recording of voice samples, confirming the data is human-generated audio."
        }
      ]
    }
  },
  {
    "id": "javed23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7823,
      "completion_tokens": 224,
      "total_tokens": 8047
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3, Transcription of voice samples",
            "reasoning": "The paper states that undergraduate and postgraduate students fluent in English were recruited to transcribe the data. They were involved in quality checks and transcription, indicating multiple non-expert human annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3, Transcription of voice samples",
            "reasoning": "Transcribers were instructed to verify audio quality and ensure responses matched the prompt, and to follow the same transcription guidelines as used for the Switchboard corpus, indicating detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3, Transcription of voice samples",
            "reasoning": "The paper does not mention any scoring rubrics for transcription annotation or evaluation criteria beyond instructions, so no rubrics were provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3, Transcription of voice samples",
            "reasoning": "No examples or sample annotations are described in the paper\u2019s transcription guideline section or elsewhere."
          }
        }
      ]
    }
  },
  {
    "id": "javed23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9013,
      "completion_tokens": 387,
      "total_tokens": 9400
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance; transcribers were students and no expert QA team is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any QA done by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3, 'Transcription of voice samples'",
          "reasoning": "The transcription and quality checking were carried out by multiple undergraduate and postgraduate students who were fluent in English but were not described as subject matter experts. They verified audio quality, checked topical relevance, segmented, and transcribed data following guidelines, indicating QA was done by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models being used as judges in the quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic verification process for quality assurance of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented and described involving multiple human annotators checking transcription quality and topicality."
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8623,
      "completion_tokens": 427,
      "total_tokens": 9050
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Svarah",
          "reasoning": "The paper explicitly states that the Svarah dataset was collected by recruiting human speakers from diverse geographic locations across India who read, spoke extempore on given topics, and responded to prompts in various everyday use cases. The audio data was recorded directly from these speakers, and transcriptions were created by recruited human transcribers. This process indicates original data created entirely from scratch by human contributors, not derived, translated, or adapted from any existing dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not consist of translated content. The data collected is English speech from native speakers of regional Indian languages but is not translated from any other language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used in data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions collecting text prompts from Wikipedia for reading tasks, the audio data itself is original human speech recordings. The dataset is not merely collated from existing speech sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not described as modified or adapted from existing datasets; it was freshly recorded."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation method is clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9141,
      "completion_tokens": 303,
      "total_tokens": 9444
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Results & Discussions) and Section 6 (Conclusion)",
          "reasoning": "The Svarah dataset is introduced as a benchmark specifically designed for evaluating ASR systems for Indian accents. It is used in the paper to benchmark the performance of six open-source and two commercial ASR models, as detailed in Section 5. The authors emphasize that their dataset is intended for robust evaluation and highlight performance gaps across different models and accents."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Results & Discussions)",
          "reasoning": "The dataset is used not only for evaluation but also for analyzing trends and performance characteristics, such as comparing WERs across different models, accents, and speech types (read speech, extempore, use cases). The detailed analysis of model performance and accent-based variation indicates the dataset's utility for analytical purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9864,
      "completion_tokens": 500,
      "total_tokens": 10364
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains English speech from speakers who are native speakers of various Indian languages, but the speech content itself is only in English. The native languages are metadata about speakers, not languages present in the speech data entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain exactly two human languages; speech content is only in English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (Svarah), multiple paragraphs",
          "reasoning": "Svarah dataset contains only English speech data, as stated: it contains Indian-accent English data collected from speakers whose native languages are Indian languages, but all utterances are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is exclusively English speech, not monolingual in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only, with no biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly stated as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries do contain language (English)."
        }
      }
    }
  },
  {
    "id": "javed23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7082,
      "completion_tokens": 147,
      "total_tokens": 7229
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Conclusion",
          "reasoning": "The paper mentions in both the abstract and conclusion that the benchmark as well as their code have been made publicly available. This indicates that the code related to the dataset creation and evaluation scripts is accessible, implying code availability for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Svarah)",
          "reasoning": "Section 3 of the paper provides detailed steps involved in creating the Svarah dataset, including recruitment of speakers, text and prompt collection, use case creation, data collection method via a mobile app, and transcription procedures. This comprehensive description serves as documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8466,
      "completion_tokens": 500,
      "total_tokens": 8966
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Dataset Construction; Section 4.1 Dataset details; Abstract",
          "Reasoning": "The CI-MOEI dataset is constructed by annotating text from the CMU MOSEI and IEMOCAP datasets, which are real speech-based multimodal emotion datasets with human-provided transcriptions. The annotation process involves human annotators identifying sentiment expressions in the textual modality, confirming that the text data is human generated. This is explicitly described in Section 2.2 and Section 4.1 and summarized in the Abstract."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Dataset Construction; Section 4.1 Dataset details; Abstract",
          "Reasoning": "The CI-MOEI dataset includes audio recordings from the CMU MOSEI and IEMOCAP datasets, which consist of real scenes and actor performances providing raw speech. This speech data is originally human recorded and not artificially synthesized. Hence, the audio modality of the CI-MOEI dataset is human generated as per the paper's description in Section 2.2 and Section 4.1, along with the Abstract."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Dataset Construction; Section 4.1 Dataset details; Abstract",
          "Reasoning": "The CIM-MOEI dataset is formed by augmenting the MPQA dataset's text data, which is human authored as a well-known opinion mining corpus. The textual modality remains human generated since it comes from MPQA sentences labeled with sentiment polarity, as described in Section 2.2 and 4.1 and in the Abstract."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Dataset Construction; Section 4.1 Dataset details; Abstract",
          "Reasoning": "The audio modality in the CIM-MOEI dataset is synthesized speech generated by Text-to-Speech (TTS) technology (Azure TTS with multiple speakers and emotional styles) based on the text from MPQA. Therefore, this audio data is model generated, as explicitly stated in Section 2.2 Dataset Construction and Section 4.1 Dataset details, and summarized in the Abstract."
        }
      ]
    }
  },
  {
    "id": "jia24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9266,
      "completion_tokens": 219,
      "total_tokens": 9485
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Dataset Construction",
            "reasoning": "The paper states that three university students majoring in English and one native English speaker served as annotators. The three students provided assessments, and the native speaker made the final decision, indicating multiple human non-expert annotators with expert adjudication."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Dataset Construction",
            "reasoning": "The paper mentions that annotation guidelines were established aligned with MPQA, aimed at identifying sentiment expressions and polarities, implying the presence of instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 Dataset Construction",
            "reasoning": "The paper does not mention any scoring rubrics or detailed evaluation criteria provided as part of the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 Dataset Construction",
            "reasoning": "The paper does not explicitly mention providing annotation examples or sample annotations in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "jia24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10456,
      "completion_tokens": 518,
      "total_tokens": 10974
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions that a native English speaker made final decisions on annotations, and an expert in OEI intervened in uncertain cases, but there is no indication that annotation or quality assurance was conducted solely by a single expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "Section 2.2 Dataset Construction",
          "reasoning": "The paper states that annotation was performed by three university students majoring in English (annotators), a native English speaker made the final decision, and uncertain items were referred to an expert in OEI. However, the students and native English speaker are not explicitly described as experts or members of the target demographic. The expert in OEI was only consulted in uncertain cases, so full multiple expert annotation or QA is not established."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of QA or annotation performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Dataset Construction",
          "reasoning": "The annotation and quality assurance process involved three university students majoring in English and one native English speaker who made final decisions. There is no explicit indication that these annotators have subject matter expertise in OEI or belong to the target demographic. Hence, these annotators are considered non-experts, and since multiple annotators were involved, this category applies."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used for quality assurance or validation of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated process for verifying or validating annotations apart from the use of TTS technology for speech synthesis (which pertains to data generation, not quality assurance)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a quality assurance process involving multiple human annotators and an expert consultation step, so QA is indeed performed and documented."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10066,
      "completion_tokens": 478,
      "total_tokens": 10544
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Dataset Construction",
          "reasoning": "The CI-MOEI dataset was constructed by recruiting human annotators (three university students and one native English speaker) to annotate existing multimodal emotion datasets (CMU MOSEI and IEMOCAP) according to opinion expression identification guidelines. This process involved manual annotation with human judgment and expert adjudication, producing original annotated data specific for the MOEI task."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entirely generated by AI or machine learning models without reference to existing data. Although synthesized speech was generated, it was from existing text sentences, not new content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by translating content via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation systems according to the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Selection",
          "reasoning": "The CI-MOEI dataset's source text and audio are derived from existing open-source datasets CMU MOSEI and IEMOCAP. The paper indicates using these datasets and annotating them, indicating collating existing sources as a base for their dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Dataset Construction",
          "reasoning": "The CIM-MOEI dataset is derived by applying Text-to-Speech (TTS) synthesis technology on existing OEI dataset MPQA sentences to create corresponding synthetic speech, thus modifying existing data by transformation and augmentation to create a multimodal dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The sources and methods of dataset generation are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10584,
      "completion_tokens": 496,
      "total_tokens": 11080
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly introduced datasets (CI-MOEI and CIM-MOEI) for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets were used to train models from randomly initialized parameters. The models are fine-tuned from pre-trained LLMs."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 4.4",
          "reasoning": "The paper explicitly describes fine-tuning pre-trained LLMs (e.g., Vicuna and LLaMA2) on the constructed datasets CI-MOEI and CIM-MOEI using supervised learning to improve OEI task performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or evidence in the paper that the datasets are used for reinforcement learning post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.1 and 4.4",
          "reasoning": "The datasets are used not only for training but also for performance evaluation and benchmarking the proposed STOEI method and baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4 (Fine-grain analysis of joint speech-text)",
          "reasoning": "The datasets enable analysis of model performance variations (e.g., sentence length effects) and the role of speech-text multimodality in OEI."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base for retrieval-augmented generation or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets have documented uses in fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11307,
      "completion_tokens": 492,
      "total_tokens": 11799
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets CI-MOEI and CIM-MOEI introduced in the paper are constructed from English datasets (CMU MOSEI, IEMOCAP, and MPQA) and contain only English content, with no mention of other languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced do not contain exactly two human languages; only English is used as per the datasets constructed and described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2, Dataset; Abstract; Conclusion",
          "reasoning": "The newly constructed datasets CI-MOEI and CIM-MOEI are based on English datasets CMU MOSEI, IEMOCAP, and MPQA, all of which consist of English speech and text. The paper explicitly mentions in Section 2 that these datasets are derived from English data, and Section 5 notes that experiments are conducted only on English datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are based exclusively on English language resources; no non-English languages are included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets pertain to multimodal opinion expression identification involving text and speech; no programming or structured code content is included or mentioned."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical formulas describing model architectures and losses, the datasets themselves do not contain mathematical or logical notation as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain human speech and text data; no biological sequences or non-human communication systems are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve English natural language data; there is no mention or use of fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets' language is explicitly stated as English, derived from known English datasets such as MPQA, CMU MOSEI, and IEMOCAP."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain human language data (English text and speech), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "jia24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8525,
      "completion_tokens": 185,
      "total_tokens": 8710
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract; Conclusion",
          "reasoning": "The paper states in the abstract and conclusion that the authors will publicly release the code and datasets to facilitate future research, but it does not provide any actual links or explicit details confirming the public availability of the code at the time of writing. Hence, there is no evidence within the paper that the code used for dataset construction is currently publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Dataset)",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 2, including data selection strategies, annotation guidelines, annotator details, consensus procedures, and the process of using TTS to synthesize speech for MPQA. It describes the datasets CI-MOEI and CIM-MOEI construction steps clearly, enabling understanding and reproducibility of the dataset creation methodology."
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6927,
      "completion_tokens": 109,
      "total_tokens": 7036
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The dataset described is a dysarthric speech dataset constructed by the authors, consisting of Chinese speech data collected from 64 patients with dysarthria and 20 healthy participants, including about 31 hours and 18 hours respectively. The data collection tasks involved reading Chinese characters, words, and sentences, indicating that the audio data was recorded with human involvement."
        }
      ]
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7727,
      "completion_tokens": 299,
      "total_tokens": 8026
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The dataset consists of Chinese speech data collected from 64 patients with dysarthria and 20 healthy participants. The paper indicates collection of Frenchay Dysarthria Assessment (FDA) scale information, which typically requires expert clinical assessment. This implies that multiple human experts were involved in data collection and annotation due to the clinical nature of the data and the patient population."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper states that the dysarthric speech dataset was collected following the approach outlined in [23], which involves reading tasks with Chinese characters, words, and sentences. This suggests that detailed instructions and protocols were followed to guide data collection and annotation consistently."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1 Dataset",
            "reasoning": "The paper mentions that subjects were assessed using the Frenchay Dysarthria Assessment (FDA) scale and segmented into four severity levels based on specific scoring thresholds. The use of the FDA scale demonstrates the presence of rubrics for annotating speech severity."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide or reference any specific annotation examples or guidelines with example annotations for the new dysarthric speech dataset. No explicit mention of example annotations is found in the text."
          }
        }
      ]
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8917,
      "completion_tokens": 359,
      "total_tokens": 9276
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit description of QA performed by multiple human experts or members of the target demographic for the constructed dysarthric speech dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any QA by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information regarding QA by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using any AI model to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of automated verification processes for dataset annotation or content quality assurance described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new Chinese dysarthric speech dataset but does not document any quality assurance process related to the validation, annotation verification, or quality checking of the dataset content."
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8527,
      "completion_tokens": 374,
      "total_tokens": 8901
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 - Dataset",
          "reasoning": "The paper states that the authors constructed a dysarthric speech dataset comprising Chinese speech data collected from 64 patients with various degrees of dysarthria and 20 healthy participants. The collection involved reading tasks of Chinese characters, words, and sentences, indicating original content created entirely from human contributors and collected from scratch for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human translation involved in the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation involved in the dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper indicates the dataset was collected from patients and healthy participants for this study, not aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as constructed from original collection efforts rather than being derived or adapted from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin is clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9045,
      "completion_tokens": 305,
      "total_tokens": 9350
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The newly constructed Chinese dysarthric speech dataset is used to fine-tune the pre-trained Whisper model using LoRA as a supervised fine-tuning step to improve dysarthric speech recognition performance. This fine-tuning is explicitly described in Section 4.1 (Experimental Setup) and results are given in Section 4.2 (General Result Analysis)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The same dysarthric speech dataset is used to evaluate and benchmark model performance after adaptation, showing Character Error Rate reductions compared to baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "Further analysis is conducted on the dataset to study the correlation between the Perceiver-Prompt effectiveness and speaker discrimination or dysarthria severity levels, as detailed in Section 4.3."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9768,
      "completion_tokens": 558,
      "total_tokens": 10326
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset introduced consists exclusively of Chinese speech data from dysarthric patients and healthy participants. There is no mention of multiple languages or multilingual content."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset contains only Chinese speech data. No data from exactly two human languages is included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The data is explicitly Chinese speech and not English. The paper focuses on Chinese dysarthric speech recognition."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The new dataset is constructed for Chinese dysarthric speech recognition (Section 4.1). The dataset comprises only Chinese speech from patients and healthy speakers, explicitly stated and used for experiments."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No mention of code-related dataset",
          "reasoning": "The paper does not introduce any dataset containing programming or structured code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No dataset with mathematical or logical notation introduced",
          "reasoning": "While some mathematical notation is used to explain methods, no datasets contain such content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No biological or non-human communication datasets mentioned",
          "reasoning": "The dataset introduced focuses on human speech; no mention of biological sequences or animal communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No constructed language dataset mentioned",
          "reasoning": "No references to fictional or constructed languages in the new dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Language is documented in Section 4.1 Dataset",
          "reasoning": "The language used in the dataset is clearly indicated as Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken language data, so it is not language-absent."
        }
      }
    }
  },
  {
    "id": "jiang24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6986,
      "completion_tokens": 168,
      "total_tokens": 7154
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 Experimental Setup - Dataset",
          "reasoning": "The paper describes the constructed Chinese dysarthric speech dataset including the number of participants, hours of speech, collection tasks, and severity levels, but does not provide any links, references, or indications that code used to build, preprocess, or manage the dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Experimental Setup - Dataset",
          "reasoning": "The dataset creation process is described in Section 4.1 with details of number of patients and controls, amount of data collected, types of speech tasks, severity level assessments, training and test splits. This provides documentation of the dataset construction process, though no further protocol details or collection scripts are included."
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8898,
      "completion_tokens": 254,
      "total_tokens": 9152
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction; Section 2 Data Simulation",
          "Reasoning": "The LibriheavyMix dataset introduced in this paper is a 20,000-hour synthesized corpus for multi-talker overlapping speech scenarios with reverberation. The audio data is based on Libriheavy, which contains human-recorded speech audio. However, the multi-talker overlapping reverberant audio is simulated programmatically by mixing source utterances and applying reverberation to simulate real-world far-field conditions, as described in Section 2. Thus, the audio data modality is both human generated (original speech recordings) and model generated (simulated mixtures and reverberation)."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction",
          "Reasoning": "The dataset includes transcripts with punctuation, casing, and text context, inherited from the Libriheavy ASR corpus. These transcripts are produced by humans as part of the original dataset. There is no indication that the transcripts were generated or modified by models for the new dataset; therefore, the text data modality is human generated only."
        }
      ]
    }
  },
  {
    "id": "jin24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9698,
      "completion_tokens": 192,
      "total_tokens": 9890
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 (Data Simulation) and Introduction",
            "reasoning": "The LibriheavyMix dataset is synthetically generated by simulating multi-talker overlapping speech using distributions derived from target sessions and algorithmic procedures detailed in Section 2. This indicates the dataset is created via an automatic simulated process rather than manual annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No explicit mention of manual annotation instructions is present since the dataset is generated through an automatic simulation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No scoring rubrics are described or needed because no manual scoring or annotation takes place."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No annotation examples are provided as the dataset is created automatically rather than annotated manually."
          }
        }
      ]
    }
  },
  {
    "id": "jin24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10888,
      "completion_tokens": 337,
      "total_tokens": 11225
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any involvement of multiple human expert annotators for quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper that quality assurance was conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert humans performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models as judges for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 (Data Simulation)",
          "reasoning": "The dataset is synthesized via a simulation process based on statistical distributions derived from target sessions to model pauses and overlaps between speakers. This process uses predefined distributions and rules to generate the dataset, indicating an automatic procedural generation of data rather than manual annotation. No human annotation or manual QA of the synthesized data is described, implying that quality assurance relies on the correctness and robustness of the automated simulation algorithm."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While no explicit manual QA procedures are described, the dataset construction relies on an automatic data simulation process which inherently serves as quality control, thus 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10498,
      "completion_tokens": 408,
      "total_tokens": 10906
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as original recordings created entirely from scratch by human contributors; rather, it is based on existing speech data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the dataset content is generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any human translation process in the creation of this dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve machine translation of content as described in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset is large-scale and involves existing speech segments, it is not merely collected or aggregated without modifications; the paper details a simulation process."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2: Data Simulation",
          "reasoning": "The LibriheavyMix dataset is derived from the existing Libriheavy corpus by simulating overlapped speech using distributions extracted from target sessions, adding reverberation for far-field scenarios, and creating mixtures with multiple speaker turns. Thus, it is based on existing data with transformations and adaptations applied as described in the data simulation methodology."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation method is clearly documented in the paper, particularly in Section 2, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11016,
      "completion_tokens": 232,
      "total_tokens": 11248
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The paper presents baseline experiments on speech separation and multi-talker ASR conducted by training models on the proposed LibriheavyMix dataset, indicating usage for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The dataset is used as a benchmark to evaluate the performance of speech separation, multi-talker ASR, and speaker diarization models by reporting metrics such as word error rate and diarization error rate."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11739,
      "completion_tokens": 410,
      "total_tokens": 12149
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced, LibriheavyMix, is based on Libriheavy which is an ASR corpus of English audiobooks. No evidence in the paper states that the data contains multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of exactly two languages in the dataset; the dataset is derived from English audiobook data only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Introduction and Section 2 - Data Simulation",
          "reasoning": "LibriheavyMix is constructed based on Libriheavy, which is an ASR corpus derived from English audiobooks (LibriSpeech and related). The dataset contains English speech and transcripts with punctuation and casing. There is no mention of other languages included."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of any monolingual non-English data in the dataset; all data is English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries contain speech and transcripts, no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although mathematical notations appear in method descriptions and algorithms, they are not part of dataset entries but are part of methodological presentation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are mentioned in the dataset description."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly specified as English; not undocumented or unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken language (English); hence this is not applicable."
        }
      }
    }
  },
  {
    "id": "jin24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8957,
      "completion_tokens": 128,
      "total_tokens": 9085
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link provided",
          "reasoning": "The paper discusses the dataset creation methods and simulation process in Section 2, but does not provide any links or references to publicly available code repositories for the dataset generation or related code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data Simulation)",
          "reasoning": "The paper details the data simulation process in Section 2, providing algorithms and explanations on how overlapped speech is simulated, including relevant distributions and sampling strategies. This documentation of dataset creation is transparent and detailed within the paper."
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8372,
      "completion_tokens": 277,
      "total_tokens": 8649
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Training Dataset",
          "Reasoning": "The MSP-Podcast Release 1.9 dataset is used as the training dataset for ParaCLAP. It consists of natural English speeches extracted from podcast recordings, totaling over 110 hours of human-recorded speech across 55,283 utterances from over 1,200 speakers. This dataset is introduced as the source of real human-generated audio data used to create training pairs for the new ParaCLAP model."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Query generation, Section 3.1 Training Dataset",
          "Reasoning": "The text queries paired with the MSP-Podcast audio data are generated by the authors using a novel templating method. These textual queries are constructed based on dataset labels (emotion categories, gender, and binned dimensional attributes) and expert acoustic features (e.g., pitch, intensity, jitter, shimmer, duration) extracted from audio samples. These features and labels are converted into natural language sentences using predefined templates and then combined (e.g., using 'and'). Thus, the text data for training is algorithmically generated by the authors rather than sourced from human-generated text."
        }
      ]
    }
  },
  {
    "id": "jing24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9172,
      "completion_tokens": 200,
      "total_tokens": 9372
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 Training Dataset",
            "reasoning": "The MSP-podcast dataset used for training is annotated using crowd-sourcing for labels including 9 emotion types, indicating multiple non-expert annotators contributed to the labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1 Training Dataset",
            "reasoning": "The paper states the dataset is annotated via crowd-sourcing but does not provide details on specific instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 Training Dataset",
            "reasoning": "No mention of annotation rubrics or scoring guidelines is made in the paper describing the MSP-podcast labeling process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1 Training Dataset",
            "reasoning": "The paper lacks any reference to annotation examples or sample labels provided to annotators for guidance."
          }
        }
      ]
    }
  },
  {
    "id": "jing24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10362,
      "completion_tokens": 413,
      "total_tokens": 10775
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset or annotation quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the training dataset (MSP-Podcast) was annotated via crowd-sourcing (Section 3.1), the paper does not explicitly detail the quality assurance process or indicate multiple non-expert annotators performed QA under the authors' supervision or as part of this work. Importantly, MSP-Podcast is a pre-existing dataset, not newly introduced by this paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models as judges for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based quality assurance process on dataset annotations is described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not introduce any new datasets, but rather uses the existing MSP-Podcast dataset for training. The paper describes methods to generate textual queries from existing labels and expert features but does not document any quality assurance process on any newly annotated or introduced dataset."
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9972,
      "completion_tokens": 459,
      "total_tokens": 10431
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MSP-podcast dataset used to train the ParaCLAP model is an existing dataset extracted from podcasts and annotated via crowdsourcing. The paper does not indicate that the authors created any new dataset entirely from scratch by human contributors themselves."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper suggests that any new data was generated entirely by AI or model generative processes without reference to existing data. The authors generate text queries based on templates and existing labels/features rather than generating original data samples from scratch via models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation or translation processes."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 (Training Dataset)",
          "reasoning": "The MSP-podcast dataset is an existing, publicly available dataset composed of natural English speech from podcasts. It was collected and annotated previously; thus, it is collated data that the authors used for training."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 (Query generation)",
          "reasoning": "The authors create new text queries for training by deriving them from existing dataset labels and expert acoustic features (eGeMAPS features) extracted from the MSP-podcast utterances. They generate templated sentences that combine these labels and features into textual prompts, thus modifying and adapting existing data to form new training pairs."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation method are specified clearly."
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10490,
      "completion_tokens": 331,
      "total_tokens": 10821
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate training models from randomly initialized parameters using the new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.3",
          "reasoning": "The MSP-Podcast dataset, constructed with novel audio-text pairs via query templates, is used for training the ParaCLAP model in a supervised manner with contrastive learning, fine-tuning both audio and text encoders along with projection layers, as detailed in Sections 3.1 and 3.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training methods such as RLHF using the new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not used exclusively for evaluation or benchmarking; rather, it is used for training as well."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analyzing trends or characteristics but for training the model."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used or described as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11213,
      "completion_tokens": 632,
      "total_tokens": 11845
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Training Dataset and Section 3.2 Test Datasets",
          "reasoning": "The new dataset introduced for training, MSP-Podcast, contains only English speech data, and the queries generated are in English. Although test datasets include some German datasets, these are used for evaluation only and are pre-existing datasets, not newly introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1 Training Dataset and Section 3.2 Test Datasets",
          "reasoning": "The proposed training dataset MSP-Podcast is monolingual English and does not contain two human languages. The test datasets include German language data, but these are pre-existing, not newly introduced datasets."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Training Dataset: 'MSP-podcast Release 1.9 [13] is utilised to train our ParaCLAP models. The MSP-podcast data comprises natural English speeches ...'",
          "reasoning": "The newly introduced dataset for training (MSP-Podcast) consists entirely of natural English speech, and the generated text queries are all in English, indicating monolingual English dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly introduced dataset includes only non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain any programming or structured code-related content. The text queries are natural language sentences related to paralinguistic features."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation in methodology sections, the dataset entries themselves do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human speech audio and natural language queries; it does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language (English text queries and spoken English audio)."
        }
      }
    }
  },
  {
    "id": "jing24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8431,
      "completion_tokens": 229,
      "total_tokens": 8660
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract and Section 1",
          "reasoning": "The paper states that their code and resources are publicly available at https://github.com/KeiKinn/ParaCLAP, but this repository contains code and resources related to the ParaCLAP model training and evaluation. However, they do not introduce any new datasets; instead, they use an existing dataset (MSP-Podcast) for training and various existing datasets for evaluation. They create text queries for this existing dataset but do not present new data collected or constructed as a dataset. Therefore, there is no code associated with new dataset construction or generation described or linked in the paper."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "N/A",
          "reasoning": "The paper does not describe or document the creation of any new dataset; it uses the MSP-Podcast dataset (which is publicly available) for training. Their method involves generating text queries from existing labels and expert features, but this is part of data preprocessing for training rather than a new dataset creation. Therefore, no documentation of new dataset creation is present."
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7586,
      "completion_tokens": 138,
      "total_tokens": 7724
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and 3.3",
          "Reasoning": "The paper explicitly states the SASV protocols use the ASVspoof 2019 LA database, which includes bona fide utterances recorded from 107 human speakers (Human Generated) and spoofed utterances generated via 19 state-of-the-art voice conversion (VC), text-to-speech (TTS), and hybrid TTS-VC algorithms (Model Generated), thus the new SASV challenge data is based on this audio data with both human speech and model-generated spoofing."
        }
      ]
    }
  },
  {
    "id": "jung22c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8386,
      "completion_tokens": 261,
      "total_tokens": 8647
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3",
            "reasoning": "The SASV protocols involve automatic generation of trial types (target, bona fide non-target, spoofed non-target) applied to ASVspoof 2019 LA data, with clearly defined partitions for training, development, and evaluation as per Section 3.3. Labels for utterances as target, bona fide non-target, and spoofed non-target derive from the known dataset partitions and known generation methods, not manual annotation by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any manual annotation process requiring instructions; the dataset uses predefined labels from ASVspoof 2019 LA and VoxCeleb2, so no annotation instructions are provided or needed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or guidelines for annotation scoring are mentioned since labeling is based on dataset partition metadata and protocol definitions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No example annotations or exemplar utterances for annotation are described, as the labeling is automated from dataset metadata and no manual annotation was performed."
          }
        }
      ]
    }
  },
  {
    "id": "jung22c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9576,
      "completion_tokens": 316,
      "total_tokens": 9892
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by any single human annotator who is an expert for the datasets involved."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention in the paper of multiple human experts performing quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that dataset quality assurance was performed by AI models acting as judges."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic QC process used as quality assurance for the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available datasets (VoxCeleb2 and ASVspoof 2019) and defines new SASV protocols based on these existing datasets. There is no mention of any additional new dataset introduced by the authors nor any accompanying quality assurance process described for dataset annotations or content. Therefore, no quality assurance process is applied or documented for any new dataset since none is introduced."
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9186,
      "completion_tokens": 470,
      "total_tokens": 9656
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper utilizes existing public databases (VoxCeleb2 and ASVspoof 2019) and does not introduce any new original data created entirely from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses spoofed utterances generated by various VC, TTS, and hybrid algorithms in the ASVspoof database, no new data generated entirely by AI or ML models without reference to existing data is introduced by the authors themselves."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by human translation of content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The SASV challenge data uses publicly available VoxCeleb2 and ASVspoof 2019 LA databases, which are existing datasets collected from online videos and speech corpora. The SASV protocols collate these existing data sources without significant modification for the challenge."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The SASV protocols derive a new trial structure combining bona fide target, bona fide non-target, and spoofed non-target trials from the ASVspoof 2019 LA data. They adapt the existing data for a new integrated evaluation scenario (spoofing-aware speaker verification)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origins and sources of the data used, including how the data is derived or collated. Thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9704,
      "completion_tokens": 277,
      "total_tokens": 9981
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Baselines",
          "reasoning": "The ASVspoof 2019 LA database is used to train the DNN back-end fusion model (B2 baseline) using supervised learning methods as described in Section 4.2, where the model is trained using the training partition of the ASVspoof 2019 LA dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 SASV protocols and Section 5 Challenge results",
          "reasoning": "The SASV protocols are designed for evaluation involving target, bona fide non-target, and spoofed non-target trials. The dataset is used exclusively for evaluation and benchmarking of SASV solutions as detailed in Sections 3.3 and 5."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10427,
      "completion_tokens": 610,
      "total_tokens": 11037
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "The dataset is generated from the VCTK source database, which consists of speech data from 107 speakers, but all content is in English. The paper does not mention multiple languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "There is no indication that the dataset includes entries in exactly two human languages. Only English is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "The ASVspoof 2019 LA dataset is generated from the VCTK database, which is an English multi-speaker corpus. The dataset consists of speech data in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "No mention of any non-English language; the dataset is English only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section describing new datasets containing code",
          "reasoning": "The paper describes dataset for speech and spoofing detection but does not include any programming or structured code content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No mention in the dataset description sections",
          "reasoning": "The dataset consists of speech utterances and labels. No mathematical or logical symbolic notation is part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No section discussing any such data",
          "reasoning": "Datasets contain human speech data only, no biological sequences or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No section regarding fictional or constructed languages",
          "reasoning": "Dataset entries are natural speech only; no fictional languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2 ASVspoof 2019",
          "reasoning": "Languages in the dataset are explicitly known and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes human language speech data, so it is not the case that there is no language data."
        }
      }
    }
  },
  {
    "id": "jung22c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7645,
      "completion_tokens": 228,
      "total_tokens": 7873
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section explicitly mentions code availability for dataset construction",
          "reasoning": "The paper discusses datasets used in the challenge, such as VoxCeleb2 and ASVspoof 2019 LA, both publicly available existing datasets. These datasets were not introduced or constructed by the authors themselves. The paper provides open source baseline models and systems but does not mention any code or scripts related to dataset construction or generation being released. Therefore, the code used for constructing any dataset is not provided or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and throughout the paper",
          "reasoning": "Although the authors did not introduce new datasets, the paper describes in detail the characteristics of the utilized datasets (VoxCeleb2 and ASVspoof 2019 LA) in Section 3, including their sources, composition, and partitions. It also details the protocols derived for SASV evaluation based on these datasets. This constitutes clear documentation of dataset usage and creation process for the protocols, contributing to reproducibility of experiments using these datasets with the described protocols."
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7511,
      "completion_tokens": 125,
      "total_tokens": 7636
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 SpeechEQ Dataset",
          "Reasoning": "The SpeechEQ Dataset (SEQD) introduced in the paper is a new Mandarin SER dataset consisting of 2.3 hours of speech audio recorded from 20 human speakers using a Huawei phone in a controlled environment. The dataset is explicitly described as consisting of audio clips recorded by humans and judged by human judges ensuring the emotion matches the utterance. Therefore, the data modality is audio, and the origin is human-generated recordings."
        }
      ]
    }
  },
  {
    "id": "kang22d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8311,
      "completion_tokens": 229,
      "total_tokens": 8540
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2",
            "reasoning": "Section 2.2 states that each utterance was judged independently by three judges, and agreement by all three was required for the emotion to be accepted, indicating multiple human experts performed annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Speakers independently wrote sentences describing the emotion for each of the 25 emotions and produced utterances accordingly, implying they had instructions guiding the content and expression. Also, the process of judges checking and requiring re-recording until agreement suggests detailed annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not describe any formal scoring rubrics or criteria used by judges to evaluate utterances beyond agreement on accuracy; no explicit rubric is mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "There is no mention or description of annotation examples or exemplar utterances provided as guidelines during annotation."
          }
        }
      ]
    }
  },
  {
    "id": "kang22d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9501,
      "completion_tokens": 395,
      "total_tokens": 9896
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for the SpeechEQ Dataset (SEQD)."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2, SpeechEQ Dataset",
          "reasoning": "Each utterance in the SEQD dataset was judged independently by three judges, and if more than one judge found the emotion to be inaccurate, the speaker re-recorded the utterance until all three judges agreed that the emotion matched. Although the identities and expertise of these judges are not explicitly stated, the presence of multiple judges who perform independent assessments and mutual agreement before acceptance indicates a quality assurance process by multiple human annotators. The judgment process strongly suggests these annotators are likely experts or at least knowledgeable about the emotional content, but since the paper does not explicitly clarify this, we categorize them as multiple human experts as a conservative interpretation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes three judges performing independent annotations and requiring consensus, but it does not explicitly state that these judges are non-experts. Therefore, this label does not apply."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any AI model performing quality assurance for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance process for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a human-judged quality assurance procedure for the SpeechEQ Dataset, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9111,
      "completion_tokens": 374,
      "total_tokens": 9485
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 SpeechEQ Dataset",
          "reasoning": "The SpeechEQ dataset was created by human speakers who independently wrote sentences describing emotions for each of the 25 emotions in the SpeechEQ Metric and performed recordings in everyday conversational tones. Each utterance was judged independently by three human judges for accuracy, and re-recorded if the emotion was not correctly reflected, indicating original data creation by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translation using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of data generated through machine translation is provided."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SpeechEQ Dataset is newly constructed; it is not described as being collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset does not appear to be derived or adapted from existing datasets; rather, it is newly created."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly specified and documented."
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9629,
      "completion_tokens": 285,
      "total_tokens": 9914
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3.3",
          "reasoning": "The SpeechEQ Dataset (SEQD), a new Mandarin SER dataset constructed by the authors, is used to train models from scratch within a multitask learning framework. This is shown in Section 2.2 where the dataset construction is described, and in Section 3.3 where ablation studies and experimental results demonstrate training on this dataset from the ground up to improve emotion recognition performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The SpeechEQ Dataset is also used for evaluation as part of the experiments presented in Section 3.3, where the model's performance on this dataset is measured and compared under different configurations, demonstrating its use for benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10352,
      "completion_tokens": 566,
      "total_tokens": 10918
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The SpeechEQ Dataset (SEQD) introduced by the authors is a Mandarin SER dataset only, with 1648 audio clips from 20 Mandarin-speaking speakers, indicating only one language."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The SEQD is explicitly described as Mandarin only. Although the paper performs experiments on English datasets such as IEMOCAP, these are not newly introduced datasets by the authors."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The only new dataset introduced by the authors is the SEQD, which is in Mandarin; English datasets mentioned are pre-existing."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The newly introduced SpeechEQ Dataset (SEQD) consists exclusively of Mandarin language speech clips, confirming it is monolingual and non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech audio clips; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are speech audio clips labeled with emotion classes and intensities, not containing math or logic notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech for emotion recognition, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No indication of fictional or artificial languages in the dataset; data is natural Mandarin speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the SpeechEQ Dataset is clearly stated as Mandarin; it is not unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human spoken language, so it cannot be classified as having no language."
        }
      }
    }
  },
  {
    "id": "kang22d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7570,
      "completion_tokens": 122,
      "total_tokens": 7692
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided",
          "reasoning": "The paper does not mention any link to code repositories or provide information about the availability of code related to the SpeechEQ dataset or its construction process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2",
          "reasoning": "The paper provides detailed documentation on the construction of the SpeechEQ Dataset in Section 2.2, describing its size, recording conditions, speaker information, labeling process involving three judges, and emotion categories and intensities covered by the dataset."
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7200,
      "completion_tokens": 358,
      "total_tokens": 7558
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 Recording Environment",
          "Reasoning": "The corpus includes recorded speech of subjects and teleoperators during persuasive dialogues using a teleoperated android, collected via speech microphones, involving human audio recordings."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 Recording Environment",
          "Reasoning": "The corpus contains frontal face images captured by a webcam of the human dialogue participants during the persuasive dialogues, thus these images were captured via human-operated devices."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 Recording Environment and Section 3.3 Action Unit",
          "Reasoning": "The corpus includes facial features in the form of Action Units (AUs) extracted automatically from images of participants' faces recorded by human-operated cameras; the AU signals represent sensor data derived from human behavior recordings."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract and Section 3. Corpus Annotation",
          "Reasoning": "Transcriptions of the recorded speech were manually transcribed and annotated with dialogue-act labels by trained annotators, indicating human-generated textual data."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.5 Questionnaires to Subjects and Section 2.6 Follow-up Survey",
          "Reasoning": "Structured questionnaire data were collected from participants regarding their personality, awareness, attitudes, and behavioral changes, which are represented as tabular data originating from direct human input."
        }
      ]
    }
  },
  {
    "id": "kawano22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8000,
      "completion_tokens": 192,
      "total_tokens": 8192
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.4, Section 3.2",
            "reasoning": "A single trained annotator is explicitly mentioned for annotating dialogue-act labels based on the extended ISO-24617-2 dialogue-act tags in Section 2.4 and 3.2."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not explicitly describe detailed annotation instructions provided to the annotator for dialogue-act annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "There is no description of scoring rubrics or specific standards used during the dialogue-act annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "No annotation examples for dialogue-act labeling or other annotation tasks are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "kawano22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9190,
      "completion_tokens": 251,
      "total_tokens": 9441
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.2 Dialogue-Act",
          "reasoning": "The dialogue-act annotation was performed by one trained annotator, indicating quality assurance by a single human expert. The annotator is described as trained and knowledgeable in the ISO-24617-2 dialogue-act tag standard, which implies subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Action Unit",
          "reasoning": "Automatic annotation of facial action units (AUs) was conducted using the open-source library PyFeat. This automatic AU prediction constitutes a form of automated quality assurance in labeling facial expressions since it applies algorithmic methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents annotation procedures and describes both human expert annotation and automated annotation, indicating quality assurance processes are present."
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8800,
      "completion_tokens": 437,
      "total_tokens": 9237
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Data Collection of Persuasive Dialogues), Section 2.2 (Instructions to Subjects)",
          "reasoning": "The dataset consists of dialogues recorded from interactions between human subjects and a teleoperated android robot (controlled by human operators). The speech and facial expressions were collected from real human participants engaging in persuasive dialogues, and accompanying questionnaires provide user personality and behavioral data. The data was generated explicitly for this study, involving human participants creating original dialogue and multimodal behavior data during experiments."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by AI or machine learning models; the dialogues were produced through human participants interacting via teleoperation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any translation by humans was performed to produce the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used in generating the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as aggregated from existing sources but rather as newly collected experimental data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some annotations like action units and dialogue-act tags were derived from the recorded data, the underlying data (dialogues and multimodal recordings) are original and not derived from prior datasets. The dataset itself is not derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are clearly documented in the paper; therefore, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9318,
      "completion_tokens": 219,
      "total_tokens": 9537
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 - Corpus Analysis",
          "reasoning": "The dataset is extensively used to analyze trends and factors affecting persuasion success through various feature analyses (e.g., personality traits, impressions, emotions, action units, dialogue-acts) and training classifiers to predict persuasion outcomes. The primary usage described is to understand and analyze patterns in persuasive dialogues rather than to train or evaluate models directly."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10041,
      "completion_tokens": 644,
      "total_tokens": 10685
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The corpus is described as collected with dialogues involving Japanese participants and is not stated to contain multiple languages. No mention is made of multiple languages being involved or transcribed."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "There is no evidence or statement in the paper that indicates the data includes exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.2 (Instructions to Subjects), Section 3 (Corpus Annotation)",
          "reasoning": "The paper does not indicate the use of English language for the corpus. Instead, the corpus appears to be Japanese-based dialogue, with references to annotation with the Japanese dialogue-act standard and facial AU extraction for Japanese participants."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (Instructions to Subjects), Section 3.2 (Dialogue-Act)",
          "reasoning": "The corpus consists of persuasive dialogues collected from 60 subjects in Japan, with annotation according to the extended ISO-24617-2 dialogue-act tag standard applied to Japanese. The presence of Japanese is strongly implied by references to Japanese dialogue corpus and settings, making the dataset monolingual and non-English (Japanese)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset does not contain entries of programming or structured code content as described. Although machine learning models were trained on features, the dataset itself is dialogue and audio-visual data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No indication that the dataset entries contain formal mathematical or logical symbols. The features used are linguistic and multimodal but do not include symbolic math expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The dataset includes human speech and facial action units but no biological sequences or non-human communication such as animal signals or DNA."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "No mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Not applicable",
          "reasoning": "The language of the dataset is well documented as Japanese (non-English)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (dialogue transcription and annotations), so it is not language-free."
        }
      }
    }
  },
  {
    "id": "kawano22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7259,
      "completion_tokens": 156,
      "total_tokens": 7415
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not mention or provide any URL or repository link for code related to data collection, preprocessing, or generation. No indication is given that code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 Data Collection of Persuasive Dialogues and 3 Corpus Annotation",
          "reasoning": "The paper documents the dataset creation process in detail, including data collection methods (persuasion domains, instructions to subjects and teleoperators, recording environment), data annotation (dialogue-act labels, facial features), and questionnaires. These parts describe how the multimodal persuasive dialogue corpus was constructed and annotated, revealing transparency and completeness in the documentation."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 16247,
      "completion_tokens": 108,
      "total_tokens": 16355
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction",
          "Reasoning": "The paper introduces a new benchmark setting named \"split GSC,\" which is described as a subset of Google Speech Commands dataset version 2 (GSC v2). This dataset consists of audio recordings of spoken keywords, which are human-generated recordings. The paper explicitly states that split GSC is introduced as a benchmark for few-shot open-set keyword spotting."
        }
      ]
    }
  },
  {
    "id": "kim22h_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 17047,
      "completion_tokens": 238,
      "total_tokens": 17285
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 1 Abstract and Section 1 Introduction",
            "reasoning": "The paper introduces a new benchmark setting named split GSC, which is a subset of the existing Google Speech Commands (GSC) dataset version 2. No human annotation process for this dataset or a new dataset is described. The dataset is constructed algorithmically by splitting the existing dataset for the few-shot open-set benchmark."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "There is no description in the paper of any annotation instructions being provided because the dataset is a constructed benchmark split of an existing dataset, not manually annotated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "The paper does not describe any rubrics for annotation related to the split GSC benchmark dataset construction."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "No annotation examples or guidelines are provided in the paper for the split GSC dataset because it is a data split rather than a newly annotated dataset."
          }
        }
      ]
    }
  },
  {
    "id": "kim22h_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 18237,
      "completion_tokens": 283,
      "total_tokens": 18520
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper proposes AI models for open-set detection, it does not describe an AI model performing quality assurance or validation on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification processes or code/formula-based quality assurance involved for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the split GSC benchmark (a subset of GSC ver2) for few-shot open-set keyword spotting but does not document any quality assurance process conducted to validate dataset annotations or content for the newly introduced benchmark."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 17847,
      "completion_tokens": 416,
      "total_tokens": 18263
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of new datasets entirely generated by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset introduced was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any dataset created by human translation of content from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data used or produced in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 1 Introduction, Section 3 Method",
          "reasoning": "The authors introduce a new benchmark setting named 'split GSC,' which is described as a subset of the Google Speech Commands (GSC) version 2 dataset. This indicates that the new dataset is collated from an existing source without significant original data creation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the new dataset is a subset (split) of GSC ver2, the paper does not detail any modifications, transformations, or adaptations applied to the original data beyond selection, so it is not considered 'derived' per the rubric."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origin of the new dataset (split GSC) clearly from the existing GSC ver2, so the data source is documented."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 18365,
      "completion_tokens": 456,
      "total_tokens": 18821
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 and various experimental sections",
          "reasoning": "The split GSC dataset, newly introduced as a benchmark setting in this paper, is used to fine-tune and train few-shot open-set keyword spotting models, including the proposed Dummy Prototypical Networks (D-ProtoNets). The dataset is utilized in supervised learning settings where labeled support and query samples are provided to train models for classification and open-set detection."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning methods or usage of the dataset for RL-based post-training."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Abstract, Section 3, Experimental results",
          "reasoning": "The split GSC benchmark is used for evaluation and benchmarking of the proposed method against baselines for few-shot open-set keyword spotting. It serves to measure the performance, including detection rates and AUROC metrics, of models on unseen keywords and open-set classes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No explicit usage of the new dataset solely for analytical or trend analysis purposes is described."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base for retrieval augmentation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The split GSC dataset is explicitly used for both supervised fine-tuning and evaluation, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 19088,
      "completion_tokens": 556,
      "total_tokens": 19644
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset, split GSC, is a subset of Google Speech Commands (GSC) version 2 and is designed for keyword spotting in English only; there is no indication of multiple human languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset containing exactly two human languages; it focuses on English keyword spotting."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction",
          "reasoning": "The introduced dataset, split GSC, is a subset of the Google speech commands dataset (GSC) which contains English language keywords. The paper focuses on keyword spotting for English keywords such as 'Hey, Google' and 'Hey, Siri', indicating the dataset entries are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the dataset entries are in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio utterances for keyword spotting; there is no mention of programming or structured code being part of the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical expressions to describe the method, these do not pertain to the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio samples (spoken keywords) and does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the dataset includes artificial or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as English; it is not unspecified or unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language audio and therefore includes entries with language."
        }
      }
    }
  },
  {
    "id": "kim22h_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 16306,
      "completion_tokens": 219,
      "total_tokens": 16525
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper about code availability for dataset creation.",
          "reasoning": "The paper does not include any link or reference to code repositories related to the construction or processing of any new dataset introduced. There is no indication that the authors have made dataset-related code publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 1 and Section 3 (Method), especially around the introduction of split GSC benchmark setting.",
          "reasoning": "The paper introduces a new benchmark setting named split GSC, which is a subset of the Google Speech Commands (GSC) version 2 dataset, for few-shot open-set keyword spotting. The process and rationale for the split and the construction of episodes for FSOSR are described in the text, particularly focusing on how the known and unseen classes are separated for training and evaluation. Although the dataset is derived from an existing one (GSC ver2), the paper documents the newly established split and benchmark formulation clearly to enable reproducibility of the experimental setting."
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7103,
      "completion_tokens": 353,
      "total_tokens": 7456
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.2",
          "Reasoning": "The paper introduces new datasets of user annotations of ASR hypotheses, where annotators listen to audio recordings and provide quality judgments or side-by-side hypothesis choices. This audio data is captured and annotated with human involvement, as explicit in Section 3.1 (HypRating) and 3.2 (HypChoice)."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.2",
          "Reasoning": "The ASR output hypotheses and reference transcriptions used in the user annotation datasets are textual data corresponding to recognized speech and manual references. These textual transcriptions are human-generated or derived from human-created references as indicated by the user rating and choice studies described in Sections 3.1 and 3.2."
        },
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Table 1 and Table 3",
          "Reasoning": "The paper includes tabular data summarizing the correlation coefficients and model performance metrics, but this is a presentation format rather than a dataset introduced by the authors. No new tabular datasets with uncertain provenance are explicitly introduced; thus, no new tabular dataset with known origin is introduced."
        }
      ]
    }
  },
  {
    "id": "kim22p_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7903,
      "completion_tokens": 262,
      "total_tokens": 8165
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "The paper describes 'HypRating' involving 73k user-ratings where annotators listen to audio and rate hypotheses, and 'HypChoice' with 38k user annotations for hypothesis pairs, indicating multiple non-expert users performed the annotations to capture user perception."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The annotators were asked to rate hypotheses with four defined levels ('exact match', 'useful hyp', 'wrong hyp', 'nonsense hyp') and choose among three options in pairwise tasks, indicating annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The four discrete rating categories correspond to assigned integer values (0 to 3) for quantification, which acts as an annotation rubric guiding scoring; similarly, choices in the pairwise task were quantified as -1, 0, and 1."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not explicitly mention provision of annotation examples or sample annotated items in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "kim22p_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9093,
      "completion_tokens": 472,
      "total_tokens": 9565
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention that a single human annotator who is a subject matter expert or member of the target demographic performed quality assurance on the user-annotated ASR quality datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human annotators with subject matter expertise or belonging to the target demographic performed quality assurance on the user-annotated ASR quality datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert human annotator performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.2 (HypRating and HypChoice)",
          "reasoning": "The user annotations for ASR quality come from multiple human annotators who listen to audio and rate or choose hypotheses. However, the paper does not specify that these annotators are experts or members of the target demographic; thus, it is reasonable to assume they are non-experts. The datasets HypRating (71k user ratings) and HypChoice (38k user annotations) both involve multiple annotators performing quality assurance by assigning ratings or choices."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance is not described as being performed by AI models; the annotations come from humans."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based quality assurance processes used to validate the user annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes user annotations as the quality assurance method. Therefore, it is not the case that no quality assurance is performed or documented."
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8703,
      "completion_tokens": 410,
      "total_tokens": 9113
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper describes user annotation data collected through human annotators who listened to audio and rated ASR hypotheses (HypRating with 73k ratings and HypChoice with 38k pair annotations). These annotations reflect original content created from scratch by human contributors, specifically created for this study, and are not translations or adaptations of pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset that was generated entirely by models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that data was produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset was merely collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset uses existing concepts like ASR outputs and user ratings, the new datasets (user annotations) are collected newly and do not appear to be based on adaptations or transformations of existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin/method is explicitly documented in Sections 3.1 and 3.2 for user ratings and choices."
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9221,
      "completion_tokens": 311,
      "total_tokens": 9532
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.2, 4, 5.1",
          "reasoning": "The paper introduces two new datasets: 'HypRating' with 73k user-ratings of ASR hypotheses and 'HypChoice' with 38k user annotations for ASR hypothesis pairs. These datasets are used exclusively to evaluate and benchmark the correlation between proposed SemDist metrics and user perception of ASR quality, as well as their relation to downstream NLU tasks. The datasets serve solely for performance measurement and correlation analysis."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3, 5.4",
          "reasoning": "The datasets are also used for analyzing trends and characteristics of ASR evaluation metrics. In particular, analysis of ranking gaps between WER and SemDist and modeling user judgment based on the datasets provide insights into how different metrics reflect semantic correctness and user satisfaction."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9944,
      "completion_tokens": 486,
      "total_tokens": 10430
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly introduced datasets contain exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, and 4",
          "reasoning": "The dataset entries involve English utterances used for automatic speech recognition evaluation and user annotation. There is no mention of any other language in the datasets; the examples and all transcriptions are in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as containing only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech utterances and their transcriptions, not programming code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses metrics and formulas, the dataset itself does not contain mathematical or logical symbolic expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset relates to human speech data and does not include biological or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language (English) as they are transcripts of speech and user ratings."
        }
      }
    }
  },
  {
    "id": "kim22p_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7162,
      "completion_tokens": 203,
      "total_tokens": 7365
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section mentions code availability for dataset construction.",
          "reasoning": "The paper does not provide any link, repository, or explicit mention of code being publicly available related to the construction, collection, or annotation process of the user-annotated datasets (71k user ratings and 36k hypothesis choices). There is no supplementary material or appendix describing code availability for the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.1 and 3.2 describe dataset collection and annotation process.",
          "reasoning": "The paper documents the creation and annotation details of the datasets in Sections 3.1 (Hypothesis Rating) and 3.2 (Side-by-Side Hypothesis Choice). It explains the annotation protocols, the size of the datasets (71k and 36k annotations), the rating/scoring criteria used for user judgement, and how these annotations were quantified, which provides transparency into the dataset creation process."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10538,
      "completion_tokens": 151,
      "total_tokens": 10689
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2 (Data and Synthesis)",
          "Reasoning": "The new dataset introduced is the synthesized speech samples created using a neural text-to-speech (TTS) system trained on the ThinkComputers Corpus (TCC). The TCC itself is human-generated audio from recordings of a podcast by two male speakers. The synthesized samples were generated by the TTS model (Tacotron 2 variant) trained on this human-recorded data, thus the samples are computer-generated audio but based on human speech recordings. Therefore, modality is audio, with dual origin: human-generated source recordings and model-generated synthesized samples."
        }
      ]
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11338,
      "completion_tokens": 330,
      "total_tokens": 11668
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 Experiment 1 and Section 3.2 Experiment 2",
            "reasoning": "The annotation (rating) tasks were performed by recruited self-reported native speakers of English via the crowdsourcing platform Prolific, with 20 participants in Experiment 1 and 32 participants in Experiment 2, none described as experts. Hence, multiple human non-experts provided the ratings."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Experiment 1 and Section 3.2 Experiment 2",
            "reasoning": "The participants were given a rating task with specific instructions to rate speakers on competence, confidence, sincerity, friendliness (Experiment 1) and competence plus anxiety (Experiment 2). The procedure included randomized presentation and defined rating scales (1-7), implying instructions were provided for these annotations."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 Experiment 1 and Section 3.2 Experiment 2",
            "reasoning": "Participants rated each audio clip on defined scales (e.g., from 1 to 7) for measures like competence, confidence, sincerity, etc. This implies presence of rubrics or criteria for rating the speaker on these dimensions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention providing examples of annotated items or example audio clips for the rating tasks to participants. Therefore, we must conclude that examples were not explicitly provided in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12528,
      "completion_tokens": 364,
      "total_tokens": 12892
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single human non-expert was involved in quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any QA process involving multiple human non-experts on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset is synthesized using a neural TTS model, there is no mention that an AI model performed quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based algorithmic QA process is described for validating the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied to the datasets introduced. The creation of the stimuli is detailed, but no mention of validation, checking, or QA procedures for the dataset annotations or content is made."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12138,
      "completion_tokens": 434,
      "total_tokens": 12572
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data used for synthesis is derived from an existing speech corpus; no new human-generated original content was created from scratch by human contributors specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2 Data and Synthesis",
          "reasoning": "The paper describes the creation of synthetic speech stimuli generated by a neural text-to-speech (TTS) model trained on spontaneous speech. The stimuli with varying disfluencies were synthesized entirely via AI with controlled modifications, constituting new data generated by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of translating data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation for data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the TTS model was trained on an existing corpus, the study does not present any datasets simply collected or aggregated without modification; the focus is on generated synthesized speech data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Data and Synthesis",
          "reasoning": "The TTS synthesis is based on the ThinkComputers Corpus, an existing spontaneous speech dataset, which was then processed and adapted via a neural model with added embeddings and prosody control to create the new stimuli. This constitutes data derived from existing sources with transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation methods are thoroughly described in the paper; thus, the source is documented."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12656,
      "completion_tokens": 282,
      "total_tokens": 12938
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The newly created synthesized speech stimuli with varying disfluencies are used exclusively to conduct listener perception experiments evaluating speaker competence, confidence, sincerity, and friendliness. The dataset serves to benchmark and measure the effects of disfluency types and frequencies on perception, not for model training."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Discussion)",
          "reasoning": "The dataset is used primarily to analyze patterns about how different disfluencies affect listener judgments and the cognitive processing associated with disfluency, as well as the discounting effect when disfluency is attributed to anxiety."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the use of the new synthesized speech stimuli dataset for evaluation and analysis purposes in perception experiments."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13379,
      "completion_tokens": 407,
      "total_tokens": 13786
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset introduced is based solely on American English spontaneous speech from a podcast."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only one language, English, with no mention of any second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2: Data and Synthesis; Section 3: Perception experiments",
          "reasoning": "The new dataset is synthesized speech based on recordings of two male American English speakers from a podcast, as detailed in Section 2. The entire study and experiments use stimuli in English exclusively."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "All speech content used for the dataset is in English; no non-English language data is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the model implementation mentions PyTorch and programming frameworks, the dataset itself contains only speech stimuli, no programming code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical symbolic data is part of the dataset; it consists of spoken utterances only."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech only; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The data uses natural English language only; no constructed or fictional languages are introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly documented as American English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset entries consist of spoken English utterances, hence containing language."
        }
      }
    }
  },
  {
    "id": "kirkland23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10597,
      "completion_tokens": 135,
      "total_tokens": 10732
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not provide any link, repository, or mention that the code used for constructing the dataset or synthesizing the speech stimuli is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Data and Synthesis",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, describing the source of the data (ThinkComputers Corpus from podcast recordings), preprocessing steps (audio enhancement), the training and modifications of the Tacotron 2 TTS model, and how disfluencies were synthesized and controlled for in the stimuli."
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5852,
      "completion_tokens": 116,
      "total_tokens": 5968
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Stimuli",
          "Reasoning": "The new dataset consists of recorded speech audio from speakers in the ATR speech database set C, which the authors specifically selected for the rating experiment. The speech stimuli are audio recordings of a Japanese sentence. These recordings were originally captured under quiet conditions, indicating human involvement in recording. This dataset was newly gathered and used by the authors for their perceptual experiments, specifically selected speakers for rating sessions."
        }
      ]
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6652,
      "completion_tokens": 290,
      "total_tokens": 6942
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3 and 2.4",
            "reasoning": "Section 2.3 describes that forty undergraduate students participated as raters in the rating experiment. These raters belong to the general undergraduate student population with no noted specialization, indicating non-expert human annotators. The rating experiment was their annotation task."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "Section 2.4 details that raters were instructed to listen to stimuli and rate voice quality on an online form. The procedure involved randomized scales and conditions, indicating that instruction was provided to guide raters on the annotation task."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 and 2.4",
            "reasoning": "The rating scales were 14 bipolar 7-point semantic differential scales, described in Section 2.2. These scales serve as rubrics for scoring voice qualities, clearly defining the rating metrics and anchors, and detailed in the description of rating procedure in Section 2.4."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No explicit mention or description of annotation examples or example annotated items is present in the paper for the rating experiment. The paper only details scales and procedure but not illustrative examples for raters."
          }
        }
      ]
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7842,
      "completion_tokens": 340,
      "total_tokens": 8182
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the rating experiments involved multiple human raters, these raters were undergraduate students with no mention of subject matter expertise or belonging to a target demographic expert group; thus, they are not considered experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study did not conduct quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 (Raters) and Section 2.4 (Rating procedure)",
          "reasoning": "Quality assurance corresponds to the validation of voice ratings through multiple human participants (40 undergraduate students) who were not identified as subject matter experts but native speakers performing voice quality ratings. The diversity of ratings from multiple human non-experts serves as the QA process for the perceptual data collected."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model performing quality assurance or judgment of dataset annotations or ratings."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic rule-based QA processes are described in the paper regarding dataset annotation or content validity checking."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance for the new rating data is clearly documented as being conducted by multiple human non-expert raters; thus, N/A is not appropriate."
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7452,
      "completion_tokens": 418,
      "total_tokens": 7870
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the authors created new speech data entirely from scratch by human speakers specifically for this study. Instead, they used recordings from existing datasets and original recordings of professional narrators and voice actors, implying partial use of pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention translating data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation processes involved in the dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Stimuli",
          "reasoning": "The speech stimuli were collected from multiple existing speech databases (ATR speech database sets B and C, ASJ continuous speech corpus, RWCP news speech corpus) and original recordings, aggregated for use in this study. This indicates data collation from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors selected subsets of speakers based on perceptual scores and normalized stimulus amplitudes, these processes do not qualify as substantial transformation or adaptation of original data to constitute 'derived' data as defined."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are documented sufficiently; hence, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7970,
      "completion_tokens": 224,
      "total_tokens": 8194
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.2, 2.4 and 3.1-3.4",
          "reasoning": "The authors collected and used a set of speech samples from selected speakers to analyze perceptual properties of voices using semantic differential ratings and factor analysis. The dataset was used primarily to examine trends and characteristics of penetrating voices rather than for training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8693,
      "completion_tokens": 525,
      "total_tokens": 9218
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described in the paper consists exclusively of Japanese language speech samples; no other languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only involves Japanese language speech samples with no indication or presence of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No English language speech data are included in the dataset; all speech data are in Japanese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Stimuli and throughout the paper (e.g., Abstract, Section 2.1 Stimuli)",
          "reasoning": "The dataset compiled by the authors includes speech of 124 Japanese speakers uttering a Japanese sentence \"Gaijin san wa kan-peki shugi de aru\". No other languages are included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of recorded speech audio samples only; it does not include any programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical symbolic expressions are part of the dataset itself; mathematical notation is used only in analysis sections."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech audio only; there are no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All speech data are in Japanese, a natural human language; no constructed or fictional languages are used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset (Japanese) is clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (Japanese) speech, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "kitamura22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5911,
      "completion_tokens": 171,
      "total_tokens": 6082
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link or reference to code repositories, scripts, or software used to process or generate the dataset. It primarily uses existing datasets and original recordings but does not discuss code release or sharing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Stimuli) and Section 2 (Procedure)",
          "reasoning": "The paper provides detailed documentation on dataset construction, including the sources of speech data (public speech corpora and original recordings), description of stimuli preparation with babble noise, number of speakers selected, and rating procedures. Although code is not provided, the description of dataset creation is clear and detailed enough to understand how the data samples were selected and processed."
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6641,
      "completion_tokens": 88,
      "total_tokens": 6729
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset",
          "Reasoning": "The paper introduces a new publicly available speech dataset consisting of audio recordings from 50 speakers labeled with vocal intensity categories. The recordings were collected by the authors using human speakers performing two speaking tasks, so the data modality is audio and originates from human generation via recording."
        }
      ]
    }
  },
  {
    "id": "kodali23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7441,
      "completion_tokens": 263,
      "total_tokens": 7704
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2",
            "reasoning": "The paper describes a new dataset collected by the authors with labeled intensity categories (soft, normal, loud, very loud) assigned based on the target intensity category adopted by the speaker in each recording. Since the dataset is newly introduced, it is implied that the annotation involved experts establishing these labels for the vocal intensity categories."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not mention any detailed annotation instructions given to annotators for labeling vocal intensity categories; the labels correspond to the target intensity category adopted by the speakers themselves, not an external annotation process guided by instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No mention of scoring rubrics or detailed criteria for annotating vocal intensity category is present in the paper. The labels correspond to speaker intensity categories during recording rather than a rubric-based annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not provide annotation examples or guidelines related to vocal intensity category labeling because the annotations are given based on speakers' instructed intensity during data recording, not manual annotation requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "kodali23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8631,
      "completion_tokens": 269,
      "total_tokens": 8900
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset annotation or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple human experts for the new dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that a single non-expert conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automatic or algorithmic process used for quality assurance of the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the dataset and its collection but does not mention any quality assurance process or validation method applied to the dataset annotations or labels. Hence, no QA process is documented."
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8241,
      "completion_tokens": 428,
      "total_tokens": 8669
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The paper states that a new publicly available dataset is used, which includes speech produced by 50 human speakers (25 male and 25 female) in four intensity categories. The speech was recorded via two speaking tasks (sentence and paragraph reading) performed by human contributors. This dataset was created entirely from scratch by human speakers and is not translated, adapted, or derived from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was recorded from human speakers, not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was created by human-translating speech data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation used to generate the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses texts for reading taken from existing sources (TIMIT sentences, weather forecast excerpt, and a novel), the speech data itself was newly recorded by human contributors specifically for this study, not merely collected or aggregated from existing databases without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the reading texts come from pre-existing sources, the speech audio is newly produced, so the dataset is not derived data with modifications but rather original human recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the dataset origin as newly recorded human speech data."
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8759,
      "completion_tokens": 259,
      "total_tokens": 9018
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 Classifier",
          "reasoning": "The dataset comprising speech signals with four intensity categories is used to train a classifier (SVM) in a supervised manner, where the models are trained to classify vocal intensity categories using the labeled intensity class information."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Results",
          "reasoning": "The dataset is used to evaluate classification performance of the models by measuring accuracy and confusion matrices for different features and speaking tasks."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Results",
          "reasoning": "The dataset is analyzed to study the effect of speaking tasks and layer-wise performance of embeddings on classification accuracy, examining misclassification patterns and class-wise accuracies."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9482,
      "completion_tokens": 508,
      "total_tokens": 9990
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The paper states the new dataset contains speech produced in English only (Section 2). There is no mention of multiple languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The dataset entries are only in English (Section 2), no two-language content is described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset consists of speech produced in English using four vocal intensity categories (soft, normal, loud, very loud). This is explicitly stated in Section 2 and the dataset is all English speech data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2",
          "reasoning": "The dataset is specified to have only English speech, so no non-English monolingual data is present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech audio data only, no programming or code related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There are no mathematical or formal logical expressions included in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only, no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural English speech with no mention of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly specified as English in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language speech audio (English), so it is not without language."
        }
      }
    }
  },
  {
    "id": "kodali23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6700,
      "completion_tokens": 160,
      "total_tokens": 6860
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2",
          "reasoning": "The paper mentions a publicly available dataset with a URL for more details (https://bit.ly/3tLPGRx), but does not provide any link or indication that the code used for data collection, preprocessing, or generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 and 3.1",
          "reasoning": "The paper documents the dataset creation process, including number of speakers, age ranges, speaking tasks, sentences and paragraphs used, number of repetitions, and labeling of intensity categories. Preprocessing steps such as silence removal and amplitude normalization are described in Section 3.1. This information provides transparent documentation about the dataset creation and preprocessing."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7771,
      "completion_tokens": 209,
      "total_tokens": 7980
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 3 Data processing pipeline; Section 4 Experiments",
          "Reasoning": "The new dataset introduced is LibriTTS-R, which is a cleaned and restored version of the LibriTTS corpus audio data. The speech samples are originally human speech recordings, as LibriTTS consists of speech audio originally recorded from humans. Although LibriTTS-R applies speech restoration via a model to improve audio quality, the resulting audio remains based on original human-recorded speech samples with improved sound quality, not artificially generated speech. Therefore, the modality is audio, and the data is human generated (recorded) but not model generated. The dataset corresponds directly to improved versions of original human speech samples from LibriTTS, as detailed throughout the abstract and Section 3 (Data processing pipeline) and the experiment sections describing subjective tests on the ground-truth audio."
        }
      ]
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8571,
      "completion_tokens": 255,
      "total_tokens": 8826
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1.1",
            "reasoning": "Section 4.1.1 describes subjective experiments where over 100 paid native English speaking subjects evaluated speech samples via MOS and side-by-side preference tests. These subjects are typical crowdworkers or non-expert evaluators (not described as experts), hence multiple human non-experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1.1",
            "reasoning": "The subjects were instructed to rate naturalness on a 5-point MOS scale with 0.5 increments, and to perform side-by-side comparison on a 7-point scale, indicating clear instructions were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1.1",
            "reasoning": "The MOS scale and SxS scale described in Section 4.1.1 serve as scoring rubrics guiding the annotators how to rate the samples."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not mention providing annotation examples or examples of rated samples to the evaluators."
          }
        }
      ]
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9761,
      "completion_tokens": 415,
      "total_tokens": 10176
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the LibriTTS-R dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert human annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of an AI model as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3: Data processing pipeline",
          "reasoning": "The quality improvement of the dataset was achieved by applying an automatic speech restoration (SR) model called Miipher to the original LibriTTS corpus. This model is an automated processing technique that restores and enhances the audio quality of the recordings. The paper describes the SR model's training and application in detail (Section 3), indicating an automated algorithmic approach serving as quality assurance through enhancement of the dataset audio quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process was applied via the automated speech restoration model, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9371,
      "completion_tokens": 428,
      "total_tokens": 9799
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not original content created entirely from scratch by human contributors. It is derived from pre-existing data and modified."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated entirely by AI or machine learning models without reference to existing data; rather, the dataset is based on existing speech samples that have been processed by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the original LibriTTS corpus was collated from other sources such as LibriVox and Project Gutenberg, the new dataset LibriTTS-R is not simply aggregated from existing sources; it applies transformations via restoration."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Abstract; Section 3; Section 5",
          "reasoning": "LibriTTS-R is based on the existing LibriTTS corpus, with speech restoration applied via the Miipher speech restoration model to enhance sound quality. The constituent samples remain the same textually and speaker-wise but are transformed in quality through model-based cleaning. Thus, the dataset is 'derived' with modifications and transformations applied to existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9889,
      "completion_tokens": 531,
      "total_tokens": 10420
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the LibriTTS-R dataset for pre-training large models in an unsupervised or self-supervised manner. The dataset is introduced and used in the context of supervised TTS model training."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2.1 and 4.2.2",
          "reasoning": "The paper describes training multi-speaker TTS models from scratch using LibriTTS-R as the training data. This is evidenced by experiments training acoustic models and neural vocoders on LibriTTS-R, demonstrating improved speech naturalness compared to models trained on the original LibriTTS dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning pre-trained models using LibriTTS-R. The dataset is used for training models from scratch rather than fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training methods are described in the paper involving the LibriTTS-R dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset is evaluated for speech quality and naturalness in subjective tests, these evaluations are performed on the dataset samples to assess improvements from speech restoration, not for using the dataset as an evaluation benchmark."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use LibriTTS-R primarily for analysis of trends or characteristic studies. The focus is on dataset improvement and subsequent model training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The LibriTTS-R dataset is not described as being used as a knowledge base to augment models through retrieval-based methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The LibriTTS-R dataset is explicitly used for training TTS models from scratch and is central to the experiments described."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10612,
      "completion_tokens": 427,
      "total_tokens": 11039
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 2; Section 3.1",
          "reasoning": "The paper states that the LibriTTS-R dataset is derived from the LibriTTS corpus which itself is based on LibriSpeech and LibriVox English audiobooks, with speech data from English speakers. Multiple references throughout the paper confirm the dataset contains English speech and corresponding English text transcripts only. For instance, Section 2 mentions the dataset consists of English speech and texts, and the speech restoration model is trained on various English accents. No mentions of other languages are made for the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of human language speech and corresponding text transcripts, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "koizumi23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7830,
      "completion_tokens": 182,
      "total_tokens": 8012
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not provide any link or mention of publicly available code for the speech restoration model or the data processing pipeline used to create the LibriTTS-R dataset. Although the dataset itself is available for download, the code used for its construction, including the speech restoration model 'Miipher', is not shared with the public in this paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including the speech restoration model (Section 3), its features, training data, and the data processing pipeline applied to LibriTTS. Subjective evaluation experiments and data splits are described in Section 4. Therefore, the dataset creation documentation is transparent and sufficiently detailed within the paper."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7935,
      "completion_tokens": 254,
      "total_tokens": 8189
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, and 3.3",
          "Reasoning": "The Dusha dataset includes audio recordings collected from two sources: crowd-sourced acted recordings from non-professional actors pronouncing assigned text with emotions, and real-life podcast recordings featuring genuine emotions. All audio data originated from human speech, captured using human-operated equipment or gathered from existing podcasts, thus constituting human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Sections 3.1, 3.2, and 3.3",
          "Reasoning": "The dataset contains transcripts associated with each audio recording. The transcriptions are the genuine textual forms of the spoken utterances, originally produced by humans. The text for acted recordings was derived from real user voice assistant commands and chit-chat. Podcast transcripts were obtained from a speech recognition solution pretrained on existing data, but this is not described as model-generated text per se for the Dusha dataset itself; the focus is on pairing human speech with text transcripts, so text modality is human-generated transcriptions."
        }
      ]
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8735,
      "completion_tokens": 234,
      "total_tokens": 8969
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3 Post-processing and annotation",
            "reasoning": "The annotations were performed by multiple participants on a crowd-sourcing platform after training and passing an exam. They are described as non-professional actors and crowd-sourced platform workers, thus not experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Post-processing and annotation",
            "reasoning": "Annotators were given specific instructions on how to label each recording with emotion classes including detailed descriptions for Positive, Neutral, Sadness, Anger/Irritation, and Other."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 Post-processing and annotation",
            "reasoning": "The annotation task includes a quality control process with control tasks ('honeypots') and passing score thresholds, which implies rubrics or scoring criteria were provided to ensure annotation quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not explicitly mention providing annotation examples or example audio clips to annotators within the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9925,
      "completion_tokens": 363,
      "total_tokens": 10288
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator with expertise or being a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple subject matter experts or members of the target demographic performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that quality assurance was conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Post-processing and annotation",
          "reasoning": "The emotional annotations were conducted on a crowd-sourcing platform by multiple annotators who are described as non-professional actors and crowd workers who passed a training and exam, indicating non-expert annotators contributed to the annotation and quality assurance process."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using AI models as judges in quality assurance of annotation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Crowd subset collection and Section 3.3 Post-processing and annotation",
          "reasoning": "The paper describes using an automatic pseudo-labeling step via a BERT-based classifier to pre-label texts for emotion classes, and applying the Dawid-Skene algorithm, an automatic aggregation mechanism, to combine multiple annotations with a confidence threshold, constituting algorithmic quality assurance processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes quality assurance processes involving multiple non-expert annotators and an automatic aggregation mechanism, thus QA is present."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9535,
      "completion_tokens": 477,
      "total_tokens": 10012
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The Crowd subset was collected by having non-professional human actors record text passages in various emotional tones on a crowd-sourcing platform (Yandex Toloka). This process involved original voice recordings created by human participants acting emotional content from scratch, not derived or translated from existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the dataset was generated or synthesized entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of Russian speech recordings; there is no mention of data produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention that the data was generated by machine translation systems from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The Podcast subset was collected by selecting and aggregating audio segments from 6,240 existing Russian podcasts. While the podcast audio was sliced into 5-second segments and normalized, the core data consists of existing natural speech recordings without significant alteration."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "Both subsets underwent preprocessing steps such as filtering, pseudo-labeling using a BERT-based text classifier for sampling emotional utterances, audio slicing, normalization, and annotation with aggregated emotion labels. These modifications represent adaptations and transformations applied to either newly-recorded or collated data, hence making the dataset derived from original raw recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents data sources and methods of generation for both subsets; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10053,
      "completion_tokens": 410,
      "total_tokens": 10463
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 5 Conclusion",
          "reasoning": "The paper states that the acted subset of the Dusha dataset, collected via crowd-sourcing, is suitable for model pre-training, aiming to capture general patterns for speech emotion recognition."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Baseline implementation details",
          "reasoning": "The baseline model is trained from scratch using the Dusha dataset parts to demonstrate the applicability of this new dataset for model training without pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Podcast subset collection, Section 5 Conclusion",
          "reasoning": "The podcast subset of Dusha contains real-life, natural emotions with unbalanced class distribution and is elaborated to be used for fine-tuning pre-trained models to adapt to real-world scenarios."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or describe any use of the Dusha dataset for reinforcement learning or RL-based post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 Benchmark results",
          "reasoning": "The dataset is used to evaluate the baseline model, with test subsets from both Crowd and Podcast domains employed for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the dataset for analysis of trends or characteristics separate from training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the Dusha dataset serves as a knowledge base for retrieval augmentation or similar augmenting methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has multiple documented uses including pre-training, fine-tuning, training from scratch, and evaluation; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10776,
      "completion_tokens": 573,
      "total_tokens": 11349
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists exclusively of Russian speech recordings; no other languages are indicated in the paper."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only Russian language speech. There is no indication of a second human language present."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are strictly in Russian language; English content is not part of the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 3 (Data acquisition); Section 5 (Conclusion)",
          "reasoning": "The Dusha dataset is a newly introduced speech emotion recognition corpus containing approximately 350 hours of Russian language speech data, including both crowd-sourced acted and real-life emotions from podcasts. The paper explicitly states it is the first speech emotion corpus in Russian (Abstract). Both the 'Crowd' and 'Podcast' subsets exclusively comprise Russian utterances as detailed throughout Sections 3 and 5."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or code-related content is present in the dataset entries; the dataset comprises audio speech and transcripts."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include mathematical or logical symbolic expressions, only speech and their transcripts in natural language."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists solely of human spoken language, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Only natural Russian language data is included; no constructed or fictional languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is clearly specified as Russian throughout the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (Russian speech and transcripts), so this option does not apply."
        }
      }
    }
  },
  {
    "id": "kondratenko23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7994,
      "completion_tokens": 195,
      "total_tokens": 8189
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 1 (Abstract), Section 5 (Conclusion), Footnote 4 (GitHub link)",
          "reasoning": "The paper explicitly mentions that all data and processing scripts are released on a GitHub repository (Footnote 4: https://github.com/salute-developers/golos/tree/master/dusha). This indicates that the code related to dataset processing and benchmarking is publicly available and accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Data acquisition), 3.1 (Crowd subset collection), 3.2 (Podcast subset collection), 3.3 (Post-processing and annotation), and 4 (Dataset overview)",
          "reasoning": "The paper provides detailed documentation about the dataset creation process including data sources, collection methods, annotation procedures, post-processing, aggregation mechanism, and dataset statistics. This extensive description supports reproducibility and transparency."
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7066,
      "completion_tokens": 288,
      "total_tokens": 7354
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction; Section 3.1 Data sources of automatic-strada; Section 3.3 Annotated-strada\u2019s curation",
          "Reasoning": "The STraDa dataset introduced by the authors contains downloadable music audio excerpts and full-length tracks (annotated-strada) that are original recordings by human singers. The audio data is collected from sources including Deezer API and YouTube links with timestamps. The audio is human generated as it consists of original singing voices recorded by humans, not synthesized or model-generated."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction; Section 3.1 Data sources of automatic-strada; Section 3.2 Data matching and processing of automatic-strada; Section 3.3 Annotated-strada\u2019s curation",
          "Reasoning": "The dataset includes rich metadata such as singer gender, birth year, active country, music genre, lyrics language, release date, and lead singer identification, which are manually curated and cross-validated from music encyclopedias (MusicBrainz, Wikidata, Discogs) and Deezer metadata. This metadata is structured in tabular form and originates from human-generated sources and manual curation, not from automated synthetic processes."
        }
      ]
    }
  },
  {
    "id": "kong24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7866,
      "completion_tokens": 214,
      "total_tokens": 8080
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.3",
            "reasoning": "The annotated-strada dataset is described as being \"manually verified\" and \"manually annotated\" to ensure accuracy and balance across subgroups, implying multiple human experts were involved in curation and annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "The dataset is carefully curated and annotated to ensure balance and accuracy; thus, the process implies the presence of detailed annotation instructions to guide annotators for consistent and reliable labels."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not explicitly mention any scoring rubrics or formal evaluation criteria being provided for annotators in the annotation guidelines for annotated-strada."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No annotation examples or examples sets are described or referenced in the paper for annotated-strada annotations."
          }
        }
      ]
    }
  },
  {
    "id": "kong24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9056,
      "completion_tokens": 377,
      "total_tokens": 9433
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single human expert annotator for the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts performed quality assurance on dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Annotated-strada\u2019s curation",
          "reasoning": "The annotated-strada subset was manually curated and annotated to ensure balanced and accurate labels. Manual verification was performed for these 200 tracks, producing 1200 annotated segments. Although the paper does not specify the expertise level of the annotators, since the process is described as manual annotation and verification, it implies human involvement without clearly stating expert status\u2014thus applying multiple human non-expert quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description in the paper suggests that an AI model was used for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Data matching and processing of automatic-strada",
          "reasoning": "The automatic-strada subset involved cross-validation of metadata across four different data sources using matching techniques based on album names, track names, singer metadata, and role information. This automated process includes algorithmic filtering such as removing tracks with multiple lead singers to ensure annotation consistency, representing an automated quality assurance approach."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents quality assurance processes both manual and automated, so 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8666,
      "completion_tokens": 464,
      "total_tokens": 9130
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The annotated-strada subset consisting of 200 tracks was manually curated and annotated by the authors to ensure balance across genders, languages, and age groups. The collection and annotation process was conducted manually rather than via automated means, indicating original content created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset being generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The automatic-strada subset was created by collecting and aggregating data from four existing data sources (Deezer, MusicBrainz, Wikidata, and Discogs) without significant modification beyond matching and filtering metadata entries. This indicates a collated dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "The automatic-strada data was processed to ensure only single lead singer tracks were included, and metadata was cross-validated across sources. Annotated-strada involved segment extraction and annotation with timestamps from full-length tracks, constituting modifications or transformations applied to existing data sources, hence derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the methodology and origins of the datasets introduced; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9184,
      "completion_tokens": 309,
      "total_tokens": 9493
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The automatic-strada subset of STraDa is used to fine-tune a pre-trained x-vector model for singer sex classification, as described in Section 4. The authors specifically fine-tune the model on this dataset using supervised methods to improve classification performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 and Section 3.3",
          "reasoning": "The annotated-strada subset is used exclusively for evaluation and bias analysis of trained SSC models, as detailed in Section 4.3 and Section 3.3. The dataset is balanced and manually annotated to provide reliable performance measurement across subgroups."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3.2 and Section 4",
          "reasoning": "Annotated-strada enables bias analysis across different subgroups such as gender, language, and age in singer sex classification tasks, as discussed in Section 4.3.2."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9907,
      "completion_tokens": 512,
      "total_tokens": 10419
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 3.1; Table 1",
          "reasoning": "The automatic-strada subset of STraDa contains 25194 tracks in 35 languages, explicitly mentioned in the Abstract and Section 3.1. Table 1 also lists 5 most represented languages and highlights linguistic diversity. The languages include English, French, Mandarin, Spanish, German among others, confirming presence of more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes many more than two human languages; it is not restricted to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages with English as one of them, not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages, not just a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of code or programming language data entries in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains music audio files and metadata but no mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human singing voices with no biological sequences or non-human communication systems included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that constructed or fictional languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages of dataset entries are clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human languages in the audio content and metadata."
        }
      }
    }
  },
  {
    "id": "kong24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7125,
      "completion_tokens": 160,
      "total_tokens": 7285
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section explicitly mentions code availability or provides a link to code repositories.",
          "reasoning": "The paper does not mention or provide any links to code repositories or publicly available code related to data collection, preprocessing, or dataset generation. The dataset creation is described abstractly but no code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3, 3.1, 3.2, 3.3, and 3.4",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 3 and its subsections, explaining data sources, matching and processing, manual curation for annotated-strada, and limitations. This offers transparency and completeness in describing dataset construction."
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7420,
      "completion_tokens": 214,
      "total_tokens": 7634
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Data), Section 4.1 (Extraction of metrics)",
          "Reasoning": "The study recruited 38 people with Parkinson's disease and 22 controls who completed interactive sessions where speech was elicited through various speaking exercises. Audio data of participants' speech was recorded using microphones during these sessions. Hence, the audio data modality is human generated as it originates from recordings of human participants during the study."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Data), Section 4.1 (Extraction of metrics)",
          "Reasoning": "Facial metrics were extracted from audiovisual frames recorded during the interactive sessions. The paper states that facial landmark extraction was done using facial landmark detector algorithms on video recordings. These videos were recorded from participants interacting with the virtual agent, thus the video data is human generated, captured from real participants during the study."
        }
      ]
    }
  },
  {
    "id": "kothare22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8220,
      "completion_tokens": 235,
      "total_tokens": 8455
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 Data",
            "reasoning": "Participants were recruited and data was collected from 60 participants through an ongoing study with specific inclusion and exclusion criteria; this implies annotation occurred by human experts or supervised processes in a research context rather than automated or crowd-sourced annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 Data",
            "reasoning": "The paper describes a structured conversational callflow using the virtual agent Tina guiding participants through standard speaking exercises, which implies instructions and protocols for data collection were provided to the participants and possibly to annotators for consistent data gathering."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Throughout the paper",
            "reasoning": "There is no explicit mention of scoring rubrics provided for the annotation or labeling of data; rather, metric extraction was automatic or objective using software tools."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Throughout the paper",
            "reasoning": "No examples of annotation or labeling data instances are provided; the paper focuses primarily on automatic metric extraction from recorded audiovisual data rather than manual annotation with examples."
          }
        }
      ]
    }
  },
  {
    "id": "kothare22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9410,
      "completion_tokens": 409,
      "total_tokens": 9819
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert for validating the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention QA by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although automated extraction of speech and facial metrics is performed by algorithms and tools, no AI model is described as performing quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 (Extraction of metrics), Section 3 (Data)",
          "reasoning": "The dataset annotations (speech and facial metrics) were extracted automatically using software tools such as Praat, Montreal Forced Aligner, OpenCV face detector, and Dlib facial landmark detector. The paper describes standard algorithmic methods to extract these metrics in real time. However, there is no explicit mention of manual validation or review of these annotations, implying quality assurance was through automated processing pipelines."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance was performed via automated extraction and verification techniques, so the QA process is documented."
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9020,
      "completion_tokens": 413,
      "total_tokens": 9433
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Data",
          "reasoning": "The data consists of speech and facial recordings collected from 60 participants (38 with Parkinson's disease and 22 controls) who engaged in interactive sessions guided by a virtual agent. Participants performed various speaking exercises and filled out surveys, resulting in newly collected original data created by human contributors specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset consists of entirely model-generated data without reference to human contributors. The data originates from human participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human translation of data from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation in the data generation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected by aggregation or compilation of existing datasets; rather, it was newly recorded from participants specifically for the study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Extraction of metrics",
          "reasoning": "Speech and facial metrics were computationally extracted and derived from the audiovisual recordings using algorithms such as Praat, Montreal Forced Aligner, and OpenCV. These metrics represent transformations and analyses of the raw collected data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9538,
      "completion_tokens": 260,
      "total_tokens": 9798
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used exclusively for evaluation or benchmarking purposes; rather it is used in conjunction with analysis and clinical assessment."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and 5",
          "reasoning": "The dataset consists of multiple sessions of speech and facial metrics from people with Parkinson's Disease and controls, collected via a remote multimodal dialogue system. The authors analyze these metrics to identify statistically significant differences, compute minimally detectable changes (MDC), and minimal clinically important differences (MCID). The primary use is to analyze trends, patterns, and clinical relevance of these metrics rather than model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10261,
      "completion_tokens": 522,
      "total_tokens": 10783
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3. Data",
          "reasoning": "The dataset consists solely of participants who are fluent in English as per the inclusion criteria ('fluency in English' is explicitly stated). All speech data and tasks (e.g., reading, spontaneous speech, story retell) are conducted in English. No other languages are mentioned or used in the data collection."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although programming tools and algorithms (e.g., Praat, Montreal Forced Aligner) were used to extract features from the speech and video data, no programming or code snippets are included as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions are used in the methods section to define MDC and SEM, but these are not entries in the dataset. The dataset itself contains linguistic data, not mathematical or symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech and facial visual data for Parkinson's disease assessment with no biological sequences or non-human communication signals included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificial languages such as Klingon or Esperanto in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as English, with explicit inclusion criteria and task descriptions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken and facial data involving human English language communication, so language is present."
        }
      }
    }
  },
  {
    "id": "kothare22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7479,
      "completion_tokens": 150,
      "total_tokens": 7629
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or statement refers to code availability",
          "reasoning": "The paper does not provide any link, repository information, or mention of publicly available code for the dataset construction or data collection platform. The descriptions are limited to methods but no reproducible code or resources are shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Data) and Section 4 (Methods & Results)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including participant recruitment criteria, data collection protocol involving the multimodal dialogue system, task descriptions, and methods of metric extraction and analysis, allowing for a comprehensive understanding of the dataset creation process although not code."
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9075,
      "completion_tokens": 244,
      "total_tokens": 9319
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data; Abstract",
          "Reasoning": "The dataset consists of audiovisual speech recordings collected from English-speaking and Dutch-speaking people with ALS via a cloud-based multimodal dialogue system where participants interacted with a virtual guide, eliciting speech. This audio data is human-generated from direct recordings of participant speech."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data; Abstract; Section 3 Methods",
          "Reasoning": "Facial video data was collected from participants using the Modality platform, capturing video of facial movements during speech tasks. These videos were recorded in a remote naturalistic setting with human participants, thus the video data is human-generated."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3 Methods; Section 2 Data",
          "Reasoning": "Linguistic metrics were computed from automatic transcriptions of participant speech obtained using AWS Transcribe, an automated speech recognition system. These transcriptions are model-generated text from the audio recordings."
        }
      ]
    }
  },
  {
    "id": "kothare24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9875,
      "completion_tokens": 238,
      "total_tokens": 10113
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 Data, Section 3 Methods",
            "reasoning": "The paper states that speech, linguistic, and orofacial metrics were automatically extracted using Praat, Montreal Forced Aligner, MediaPipe Face Mesh, and spaCy on audiovisual data collected from patients. Thus, the annotations for the new dataset were generated by automatic computational processes rather than by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "The paper does not provide or mention any annotation guidelines, instructions, or protocols for human annotators because the annotations were produced automatically by computational tools."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "There is no mention of scoring rubrics in relation to any annotations as the data annotation is done via automatic extraction methods."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "The paper does not provide annotation examples, likely because annotations are derived automatically from raw audio and video via computational methods without human annotation guidelines or examples."
          }
        }
      ]
    }
  },
  {
    "id": "kothare24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11065,
      "completion_tokens": 340,
      "total_tokens": 11405
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for dataset annotations or content validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating the involvement of multiple human experts in quality assurance of the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance being performed by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are provided about multiple non-expert human annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses automatic extraction methods, there is no indication that an AI model was used specifically for quality assurance purposes."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 (Methods) and Section 2 (Data)",
          "reasoning": "The dataset generation and annotations are performed using automated extraction of metrics through established software tools such as Praat, Montreal Forced Aligner, MediaPipe Face Mesh, and AWS Transcribe. These automated processes serve as the quality assurance means, as there is no mention of manual correction or validation. The use of standardized and validated tools suggests automated quality control in extraction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes automated extraction processes but does not document any manual or explicit quality assurance process beyond these automatic tools, so 'N/A' is not appropriate."
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10675,
      "completion_tokens": 428,
      "total_tokens": 11103
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The paper states that audiovisual data was collected from 143 English-speaking and 26 Dutch-speaking people with ALS through ongoing studies using a cloud-based multimodal dialogue system involving virtual interactions. This indicates data acquisition directly involving human participants producing original speech and facial behavior data specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any new dataset entirely via AI or machine learning generative models without human input."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study involves data collected separately from English and Dutch participants, but there is no mention that speech or linguistic data was translated from one language to another by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was applied to generate or adapt the data from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not merely aggregated from existing sources; it was collected specifically in this study from participants using a remote platform."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 Methods",
          "reasoning": "The paper describes extracting various speech, linguistic, and facial features using tools like Praat, Montreal Forced Aligner, and MediaPipe Face Mesh. These metrics are derived from the raw collected audiovisual data by applying established computational methods, indicating a derived dataset based on original data with transformation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are clearly documented."
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11193,
      "completion_tokens": 243,
      "total_tokens": 11436
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Results and Section 5 Discussion",
          "reasoning": "The authors introduced novel datasets of audiovisual data collected from English-speaking and Dutch-speaking people with ALS (pALS) using a cloud-based multimodal dialogue platform. The datasets are utilized primarily for analyzing trends and characteristics of speech-based digital biomarkers across languages for monitoring ALS progression. The study involves extraction of features, group difference analysis, test-retest reliability, and modeling longitudinal trajectories, focusing on analysis rather than training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11916,
      "completion_tokens": 562,
      "total_tokens": 12478
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset described in the paper contains data from exactly two human languages: English and Dutch, rather than more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2 (Data), Abstract",
          "reasoning": "The dataset collected by the authors includes speech and facial data from 143 English-speaking participants and 26 Dutch-speaking participants, explicitly involving exactly two human languages: English and Dutch."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes data from Dutch-speaking participants as well, so it is not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes English-speaking participants as well as Dutch-speaking participants, so it is not a single non-English language dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset comprises speech, facial video, and linguistic features from human languages; there is no indication that programming or structured code-related content is part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although mathematical modeling is used in analysis (growth curve models), the dataset itself does not contain mathematical or logical symbolic data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech and facial movement data, but not biological sequences (e.g., DNA) or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves natural languages English and Dutch; there is no mention of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages present in the dataset are explicitly specified as English and Dutch."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries with human language (English and Dutch), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "kothare24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9134,
      "completion_tokens": 162,
      "total_tokens": 9296
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention any link, repository, or location where the code related to data collection, preprocessing, or dataset construction is made publicly available. There is no indication that such code is released alongside the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Data, Section 3 Methods, and Table 2",
          "reasoning": "The paper documents the dataset creation process in detail, including participant demographics, data collection methods using the Modality platform, tasks performed, and the specific speech, linguistic, and orofacial metrics extracted with references to tools used (e.g., Praat, Montreal Forced Aligner, MediaPipe). The methodological description provides transparency about dataset construction and preprocessing steps."
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7960,
      "completion_tokens": 211,
      "total_tokens": 8171
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data collection and Section 3.2 Data Characterization",
          "Reasoning": "The ITALIC dataset includes 16,521 crowdsourced audio samples recorded by 70 speakers from various Italian regions. The audio data was recorded by human participants using their own devices and contains utterances read from prompts derived from the MASSIVE dataset. The recordings are explicitly collected via human involvement as speech samples."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data collection and Section 3.2 Data Characterization",
          "Reasoning": "The dataset contains transcripts of the recorded audio, which are based on prompts from the MASSIVE dataset. Although the original utterance texts come from MASSIVE, the transcripts in ITALIC correspond directly to the human-read utterances and are thus considered human-generated text data aligned with the recordings."
        }
      ]
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8760,
      "completion_tokens": 241,
      "total_tokens": 9001
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 describes that the dataset was crowdsourced via a web platform with recordings by 70 speakers (crowd workers) instructed to read prompts. This indicates multiple non-expert humans performed annotation via self-recording and validation by at least two individuals who evaluated sample quality."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Participants were given a list of annotation guidelines before recording as stated in Section 3.1 ('After giving the participants a list of annotation guidelines...'). This implies instructions were provided to annotators for data collection."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "While multiple individuals reviewed samples for quality, there is no explicit mention of scoring rubrics or formal scoring criteria provided in the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper mentions annotation guidelines were provided but does not describe the inclusion of specific annotation examples in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9950,
      "completion_tokens": 386,
      "total_tokens": 10336
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by multiple human experts or annotators with subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection",
          "reasoning": "The dataset was crowdsourced from participants who recorded utterances without supervision. For quality assurance, at least two individuals reviewed each sample, validating intelligibility and coherence with the prompt. The paper does not indicate that these reviewers were experts; thus, it is reasonable to conclude that multiple human non-experts performed the quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models for quality assurance of annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process described involves human review, with no indication of automated verification techniques or algorithmic validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a quality assurance process involving multiple human reviewers, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9560,
      "completion_tokens": 450,
      "total_tokens": 10010
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection",
          "reasoning": "The ITALIC dataset was crowdsourced through a custom web platform where human participants recorded themselves reading prompts derived from MASSIVE. These audio recordings are original human-produced content, not generated by models or translated from other languages. The paper explicitly states that participants recorded utterances by reading prompts, thus creating new audio data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data in ITALIC was generated by AI or machine learning models. The dataset consists of human-recorded audio samples."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not created by translating data from another language via human translators; rather, human speakers read prompts originally from an existing dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used to produce the Italian data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the prompts were extracted from the MASSIVE dataset, the actual speech audio recordings are newly collected from human participants and not simply aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection",
          "reasoning": "The textual prompts presented to crowd workers are derived from the MASSIVE dataset, an existing multilingual NLU dataset. The authors extracted and annotated Italian prompts from MASSIVE to use as reading material, making the text data derivations from existing sources, although the audio recordings themselves are newly created."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation methodology are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10078,
      "completion_tokens": 364,
      "total_tokens": 10442
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental Setting and Section 4.2 Results on Intent Classification",
          "reasoning": "The ITALIC dataset is used to fine-tune pre-trained speech and text models for intent classification in Italian. The models include pre-trained XLSR variants for speech and pre-trained BERT and BART variants for text, which are further fine-tuned on the training split(s) of the ITALIC dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, specifically 4.2 Results on Intent Classification and 4.3 Results on Automatic Speech Recognition",
          "reasoning": "The ITALIC dataset is used for benchmarking and evaluating the performance of various state-of-the-art speech and text models on intent classification and automatic speech recognition tasks, including performance under different conditions such as noise and speaker variability."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Characterization and Section 4 Experiments",
          "reasoning": "The dataset includes rich metadata about speakers and recording conditions, enabling analysis of model robustness, generalization to unseen speakers, and impact of noise and regional linguistic variations. The authors explicitly mention the possibility for future analyses like speaker recognition, age estimation, and dialect identification."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10801,
      "completion_tokens": 542,
      "total_tokens": 11343
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ITALIC dataset is described as containing only Italian language utterances and recordings. There is no mention of multiple languages being included in the dataset instances."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages. It contains only Italian utterances and recordings."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not composed of English only language content; it is explicitly an Italian language dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 1 (Introduction), Section 3 (The ITALIC Dataset)",
          "reasoning": "The ITALIC dataset is explicitly introduced as a large-scale Italian language intent classification audio dataset. It comprises Italian audio samples recorded by speakers from various Italian regions and Italian transcripts. The dataset entries contain exactly one language, Italian, which is non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural language utterances for intent classification and speech recognition, with no mention of formal mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech recordings and transcripts; it does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves Italian language only; there is no mention or indication of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly specified as Italian; it is well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language\u2014specifically Italian utterances and transcripts."
        }
      }
    }
  },
  {
    "id": "koudounas23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8019,
      "completion_tokens": 210,
      "total_tokens": 8229
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 1 Introduction; Section 3 The ITALIC Dataset; Section 4 Experiments",
          "reasoning": "The paper explicitly states in the Abstract and Introduction that they release both the dataset, the annotation scheme, and the code for the baselines to encourage further research. The GitHub URL is provided (https://github.com/RiTA-nlp/ITALIC), which suggests public availability of the code. Section 4 references detailed information about the models and fine-tuning procedures available in the official project repository, supporting that code related to dataset processing and experimentation is openly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 The ITALIC Dataset",
          "reasoning": "Section 3 thoroughly describes the dataset collection process, including the crowdsourcing platform, participant recruitment, annotation validation procedures, metadata collection, data splits, and dataset statistics. The methodology is documented in sufficient detail to enable reproducibility and understanding of the dataset's construction."
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8433,
      "completion_tokens": 162,
      "total_tokens": 8595
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 - 3.3",
          "Reasoning": "The ClArTTS corpus audio is extracted from a pre-recorded LibriVox audiobook read by a single male human speaker. The audiobook was manually segmented and validated, indicating human-captured audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 - 3.3",
          "Reasoning": "The text transcripts for the audio segments were manually annotated and fully diacritized by a team of three Arabic annotators with multiple validation rounds, ensuring high-quality human-generated text transcriptions aligned to the audio."
        }
      ]
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9233,
      "completion_tokens": 255,
      "total_tokens": 9488
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2 Annotation process",
            "reasoning": "The paper states that a team of three Arabic annotators performed the transcription process, followed by two rounds of validation including checks by two other annotators for accuracy and consistency, indicating multiple human experts were involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation process",
            "reasoning": "The paper mentions that guidelines were provided to annotators during the annotation process, including instructions for abbreviations, numbers, special characters, punctuation, and specific tags for speech pauses and noises, implying detailed annotation instructions were given."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2 Annotation process",
            "reasoning": "The paper describes instructions and tags used during annotation but does not mention any scoring rubrics or specific evaluation scales for annotation quality or scoring, thus there is no evidence of rubrics provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2 Annotation process",
            "reasoning": "The paper does not provide any indication that annotation examples or example annotations were provided to annotators within the guidelines or appendix; no examples are described or referenced."
          }
        }
      ]
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10423,
      "completion_tokens": 442,
      "total_tokens": 10865
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "Section 3.2 describes a team of three Arabic annotators involved in the transcription process, performing multiple rounds of validation including checks by two other annotators for accuracy and consistency. Given that the dataset is Classical Arabic text-to-speech, it is reasonable to conclude annotators are subject matter experts or belong to the target demographic to ensure quality, indicating multiple human experts performed QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that QA was performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA done by multiple non-expert annotators; annotators appear to be experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model used for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "In Section 3.1, rule-based scripts using Praat and verification via energy-based VAD from Kaldi toolkit were used to mark pauses and speech segments, which acts as an automated verification step in segmenting and processing the audio prior to manual annotation, thus contributing to quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are clearly described in the paper, including manual and automatic procedures."
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10033,
      "completion_tokens": 343,
      "total_tokens": 10376
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.3",
          "reasoning": "The ClArTTS corpus is extracted from a pre-existing LibriVox audiobook recording of a classical Arabic book. The authors converted, segmented, and processed the existing audio recordings and utilized the original content without creating new text or recordings. This process represents collation from an existing audio source."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.3",
          "reasoning": "Although the audio is from an existing audiobook, the authors manually transcribed, annotated with diacritics, segmented audio clips, removed inconsistent chapters, and processed the audio to create a corpus tailored for TTS training. These modifications, including manual transcription and quality control, constitute adaptation and derivation from the original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10551,
      "completion_tokens": 316,
      "total_tokens": 10867
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5 Baseline TTS systems, Table 3",
          "reasoning": "The ClArTTS corpus is used to train neural TTS systems (Grad-TTS and Glow-TTS) from scratch, as demonstrated in Section 5 where models trained with this dataset show improved performance compared to others."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6 Evaluation and results, Table 3",
          "reasoning": "The ClArTTS dataset is used for evaluating synthesized speech quality via subjective (MOS) and objective (PESQ, MCD, Lf0 RMSE, BAP, speaker similarity) metrics to benchmark the performance of baseline TTS systems."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Corpus statistics, Figure 1, Tables 1 and 2",
          "reasoning": "The dataset is analyzed to compare phoneme and diphone coverage and distribution against existing corpora and natural text distribution, helping to understand its characteristics relevant for TTS training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11274,
      "completion_tokens": 399,
      "total_tokens": 11673
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 (Abstract), Section 3.1 (Audio pre-processing), Section 4 (Corpus statistics), Section 7 (Conclusion)",
          "reasoning": "The ClArTTS corpus is extracted from an audiobook of a classical Arabic book, specifically in Classical Arabic language. The dataset contains fully diacritized transcriptions in Arabic and is intended for Arabic TTS research. The paper explicitly mentions the corpus contains recordings and transcriptions in Classical Arabic, a non-English human language, with no indication of other languages being part of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kulkarni23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8492,
      "completion_tokens": 221,
      "total_tokens": 8713
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Entire Paper (No explicit section)",
          "reasoning": "The paper mentions the availability of the ClArTTS corpus and baseline TTS systems along with an interactive demo at www.clartts.com; however, it does not provide any explicit link or mention that the code or scripts used for data collection, preprocessing, segmentation, and annotation are publicly available. No GitHub or similar repository URLs or references to code release are given."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Corpus construction) and 4 (Corpus statistics)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including audio pre-processing steps with tools used (e.g., ffmpeg, Praat, Kaldi), annotation process with manual transcription and validation by multiple annotators using Praat TextGrid files, speech segmentation criteria including signal-to-noise ratio thresholds, removal of inconsistent chapters, and statistics on the corpus. This detailed description enables understanding and reproduction of the dataset construction procedures despite the lack of available code."
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7609,
      "completion_tokens": 116,
      "total_tokens": 7725
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Recordings; Section 2.2 Speakers",
          "Reasoning": "The new dataset consists of digitized audio recordings from the 1950s, produced by the musicologist Leon Metcalf, of Lushootseed speakers telling traditional myths and private correspondences. These recordings, although archival, are human-produced speech data captured via microphones and digitized, hence the modality is audio and the origin is human-generated recordings."
        }
      ]
    }
  },
  {
    "id": "kye24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8409,
      "completion_tokens": 236,
      "total_tokens": 8645
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The study analyzes archival recordings of Lushootseed speakers made by a musicologist and uses these recordings for acoustic measurements. The paper does not mention a team of annotators but implies that the acoustic analysis and annotations were done by the author, who is an expert conducting a scientific study."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No section specifies annotation instructions.",
            "reasoning": "The paper does not describe any detailed annotation instructions given to annotators; the analysis is done by the researcher on existing recordings without mention of instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No section specifies scoring rubrics for annotation.",
            "reasoning": "The paper provides no information about annotation rubrics or scoring guidelines; acoustic measures are extracted using Praat and statistical analysis."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No appendices or sections provide annotation examples.",
            "reasoning": "No examples of annotated data or guidelines are provided in the text or appendices; the paper directly presents acoustic analysis results."
          }
        }
      ]
    }
  },
  {
    "id": "kye24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9599,
      "completion_tokens": 302,
      "total_tokens": 9901
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert on the dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts validated or performed quality assurance on the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple human non-expert annotators conducting quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of AI models for quality assurance is described in the paper."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Measurements and Sampling Procedure",
          "reasoning": "The acoustic measurements such as duration, intensity, spectral moments, voice onset quality, f0, jitter, and intensity were extracted using the Praat software, an automated tool. The measurements and normalization processes imply automated data extraction and verification of acoustic parameters, indicating an automated quality assurance process for acoustic measurement data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a clear procedure using automated tools (Praat) for acoustic measurement extraction which functions as quality assurance; thus, QA is present."
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9209,
      "completion_tokens": 324,
      "total_tokens": 9533
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Recordings",
          "reasoning": "The data used in the study consists of archival recordings from the Metcalf Collection, recorded in the 1950s by musicologist Leon Metcalf. These recordings were not newly collected or created by the authors but were accessed and examined from existing archival sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 2.3 Measurements and Sampling Procedure, 2.4 Analysis",
          "reasoning": "The authors performed acoustic measurements and analyses on the archival recordings, extracting features such as duration, intensity, spectral moments, and voice onset quality. These measurements involved transformation and analysis of existing audio data to derive new quantitative datasets used in their study."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9727,
      "completion_tokens": 308,
      "total_tokens": 10035
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The new dataset, consisting of digitized recordings of Lushootseed speech from two elder speakers, is used primarily for analyzing acoustic properties of affricates, including VOT, intensity, spectral moments, and voice onset quality. The analyses aim to characterize linguistic trends and typological classifications rather than to train or evaluate machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10450,
      "completion_tokens": 529,
      "total_tokens": 10979
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of recordings and analyses of Lushootseed, a single Coast Salish language. No indication of inclusion of multiple languages is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is focused solely on Lushootseed language recordings, with no mention of a second human language in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper and analysis are written in English, the dataset itself consists of Lushootseed language recordings, not English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2 (Methods - Recordings and Speakers)",
          "reasoning": "The dataset contains only Lushootseed language recordings from two speakers. The dataset represents a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code-related content is included in the dataset; only acoustic speech recordings are used."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although mathematical models (linear mixed-effects) are used in analysis, the dataset itself does not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pertains to human speech data; no biological sequences or non-human communication systems are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Lushootseed is a natural, indigenous human language, not a fictional or constructed language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the recordings is clearly identified and documented as Lushootseed."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of linguistic speech recordings containing human language, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "kye24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7668,
      "completion_tokens": 151,
      "total_tokens": 7819
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention of code availability",
          "reasoning": "The paper does not mention or provide any links to code repositories or implementations related to data collection, preprocessing, or dataset construction. The methods describe using Praat software for measurements, but no code or scripts are provided or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the source and nature of recordings (archival recordings from the Metcalf Collection), speaker information, sampling procedures, and measurement methods. This information documents how the dataset was constructed and measured, enabling understanding of dataset provenance and processing."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8869,
      "completion_tokens": 139,
      "total_tokens": 9008
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2 and 2.3",
          "Reasoning": "The paper introduces a new Spoken Question Answering dataset synthesizing audio from existing textual medical multiple-choice question answering corpora using Text-To-Speech (TTS) technology via the OpenAI TTS API. The audio data is thus synthetic and model generated, not recorded from humans speaking. This is explicitly stated in Section 2.2, stating the use of TTS for synthesis, and the prompt format in Section 2.3 confirms the dataset consists of audio MCQ prompts."
        }
      ]
    }
  },
  {
    "id": "labrak24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9669,
      "completion_tokens": 301,
      "total_tokens": 9970
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 Tasks collection and description",
            "reasoning": "The new dataset is synthesized by applying Text-To-Speech (TTS) technology to existing textual multiple-choice question answering corpora (MMLU healthcare-related subset, MedQA test set, MedMCQA validation set) to generate synthetic audio. This synthesis process is automatic and not performed by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2 and Section 6 Conclusion",
            "reasoning": "The paper does not mention any annotation instructions for human annotators in the creation of the new SQA audio dataset; the dataset is synthetic audio generated from text via TTS, so no annotation instructions are applicable."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 and Section 6 Conclusion",
            "reasoning": "No scoring rubrics or evaluation criteria for manual annotations are described. Since the dataset relies on existing textual ground-truth answers and automatic audio synthesis, no rubric for human annotation was needed or provided."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.3 Audio prompt format",
            "reasoning": "Section 2.3 provides a clear example of the audio prompt format used: a question followed by options labeled A to D in spoken form, ending with the correct answer. This serves as an example guiding the synthesis and usage of the audio dataset."
          }
        }
      ]
    }
  },
  {
    "id": "labrak24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10859,
      "completion_tokens": 430,
      "total_tokens": 11289
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance performed by a single human annotator who is a subject matter expert for the new synthetic dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the new synthetic dataset derived from medical textual MCQA corpora."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that a single non-expert human performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or indication of multiple non-expert human annotators conducting quality assurance on the dataset is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses AI models for the QA task, there is no mention of an AI model performing quality assurance or validation of the new dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is synthetically generated using TTS on existing textual datasets; however, no automated quality assurance methods such as verification scripts or rule-based checks on data correctness are described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper explicitly states that the new medical spoken question-answering dataset is synthesized by Text-To-Speech (TTS) methods from textual MCQA datasets. There is no description or documentation of any quality assurance process applied to these synthetic audio annotations or content. Therefore, no quality assurance process is described or performed for the new dataset."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10469,
      "completion_tokens": 443,
      "total_tokens": 10912
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that new datasets were created from original human-written content. Instead, it explicitly mentions that the datasets are derived from existing textual multiple-choice question answering (MCQA) corpora."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While synthetic audio data is generated via Text-To-Speech (TTS) models, the textual question-answer datasets themselves are not newly generated by AI or machine learning models from scratch. The audio data is synthesized from existing textual data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given in the paper that the datasets involve human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation is made in the dataset creation process."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The new benchmark dataset is constructed by collecting existing textual MCQA datasets relevant to healthcare (MMLU, MedQA, MedMCQA) and aggregating them to form the evaluation benchmark in the medical domain."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The textual datasets are transformed into spoken audio by synthesizing the existing multiple-choice questions and options using Text-To-Speech (TTS) technology. This constitutes a derivation, as the audio data is based on existing textual data but adapted by synthesis into speech format."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the source and generation method of the dataset; thus, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10987,
      "completion_tokens": 523,
      "total_tokens": 11510
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not used for pre-training large models on general patterns. The paper focuses on zero-shot evaluation and does not indicate any pre-training usage of the dataset."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training any model from scratch on the new dataset. Only zero-shot evaluations and some fine-tuning on existing encoders are mentioned, but not training from scratch on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used to fine-tune pre-trained models with supervised learning. The study operates under zero-shot conditions without supervised fine-tuning on the new dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RLHF utilization involving the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2.2, 2.4, 4, and 4.1",
          "reasoning": "The new dataset, synthesized from existing medical MCQA corpora and converted to audio, is used exclusively as an open benchmark for zero-shot evaluation of spoken question answering systems. The evaluation metric is accuracy, and the experiments compare baseline cascade systems and proposed end-to-end methods on this dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "A subset of the dataset (MedMCQA training subset) is used for analyzing the distribution of relevant information across encoder layers in speech encoders. This analysis aims to better understand model behavior rather than training for task performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly utilized for evaluation and analysis; thus, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11710,
      "completion_tokens": 415,
      "total_tokens": 12125
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The dataset is synthesized solely from English textual medical multiple-choice question answering datasets. There is no mention of multiple languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The dataset contains only English questions and answers; no second language is involved in the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The novel dataset is constructed by synthesizing audio from existing English textual medical multiple-choice question answering corpora. The paper explicitly states all datasets are in English, and no other human languages are involved."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are all in English; no indication of non-English monolingual data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of spoken questions and options in natural language; no programming or code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are natural language questions and options related to medical knowledge; no mathematical or formal logic notation is included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech synthesized from textual medical questions; no biological sequences or non-human communication systems are present."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural English language content only, with no constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly stated and documented as English; language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken English human language (audio synthesized from English text), thus not language-free."
        }
      }
    }
  },
  {
    "id": "labrak24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8928,
      "completion_tokens": 198,
      "total_tokens": 9126
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2.2, Footnote 1",
          "reasoning": "The paper states in the abstract and Section 2.2 that the new synthetic medical spoken question answering dataset and all code and data are publicly released on GitHub and Hugging Face, with a direct URL given as https://huggingface.co/SpokenMedicalQA. This indicates that all code related to dataset construction and processing is available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2",
          "reasoning": "The paper dedicates Section 2.2 to describing the dataset construction process: it explains that the new SQA dataset is synthesized from existing textual medical multiple-choice QA corpora using an OpenAI TTS API, details the source corpora, the synthesis procedure, the audio format, speaker variety, and statistics about the resulting dataset. This demonstrates clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 13131,
      "completion_tokens": 288,
      "total_tokens": 13419
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.1, 3.2",
          "Reasoning": "The paper introduces a new African American English (AAE) dataset consisting of speech recordings. These are human-produced audio recordings of speakers exhibiting different types of voice quality, including creaky voice and modal voice. The data is clearly human-generated since it involves recordings of human speakers, explicitly analyzed for phonation types and acoustic features. There is no indication that the audio data was generated by models or derived from uncertain sources."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1; Table 2",
          "Reasoning": "The analyses presented include tabulated acoustic measurement results (e.g., Table 2) derived from processing the human-generated speech audio data. These tabular data represent measurements such as harmonic-to-noise ratios and cepstral peak prominence extracted from the audio. Although generated by computational analysis, the base data originates from human recordings. The tabular data are not model-generated synthetic data but represent summarized and measured human speech data."
        }
      ]
    }
  },
  {
    "id": "lai24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 13931,
      "completion_tokens": 231,
      "total_tokens": 14162
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.2 and Table 2",
            "reasoning": "The paper describes conducting acoustic analyses, phonation labeling, and sociolinguistic annotation based on a new African American English (AAE) dataset, which involves expertise to evaluate phonation types and their acoustic features. There is no indication that annotation was automated or by multiple non-experts; hence the annotation is best characterized as performed by single human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not explicitly describe detailed annotation instructions being provided for the annotators labeling voice quality or other aspects in the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no description of scoring rubrics or formal rubrics guiding the annotation process in the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not include annotation examples illustrating how to label or score the phonation types or creaky voice annotations."
          }
        }
      ]
    }
  },
  {
    "id": "lai24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 15121,
      "completion_tokens": 362,
      "total_tokens": 15483
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance conducted by a single human expert for the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that AI models were used as judges for quality assurance in the dataset annotation process."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using automated verification processes or algorithmic techniques as quality assurance for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "There is no explicit description or documentation of a quality assurance process applied to the new African American English (AAE) dataset introduced in the paper. The paper reports analyses using the dataset but does not mention any QA procedures."
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 14731,
      "completion_tokens": 388,
      "total_tokens": 15119
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Abstract; Sections 1 and 3",
          "reasoning": "The paper introduces a new African American English (AAE) dataset collected and analyzed by the authors. The data consists of speech recordings from young AAE speakers, with manual annotation of creaky and modal phonation types. There is no indication that the data was translated or derived from existing sources; rather, it is original speech data collected from humans for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated entirely by AI or machine learning models in this study."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not described as aggregated from existing sources; it is newly collected speech data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was derived from existing sources with modifications; it is presented as newly collected original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data collection and characteristics are clearly described, so the origin is documented."
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 15249,
      "completion_tokens": 210,
      "total_tokens": 15459
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The new African American English (AAE) dataset is primarily used for detailed acoustic and sociolinguistic analysis of creaky voice characteristics, their distribution by gender and position, and their relation to ASR error rates, rather than for model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 15972,
      "completion_tokens": 535,
      "total_tokens": 16507
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced contains speech data only from African American English speakers, with no mention of other human languages being present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes two languages; it focuses exclusively on African American English speech."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3.2",
          "reasoning": "The dataset comprises spoken African American English (AAE), a variety of English. All analyses pertain to English speech only, with no other language content mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is specifically of African American English, a dialect of English, so it is not non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions software tools for analysis and ASR systems, the dataset itself consists of spoken English data, not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain mathematical or logical notation; such notation is used in analyses but not as part of the data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human speech (African American English) and does not include biological or non-human communication sequences."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the dataset; it consists solely of a natural variety of English."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is explicitly stated as African American English, a variety of English, so the language is known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken language data in African American English; it does contain language."
        }
      }
    }
  },
  {
    "id": "lai24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 13190,
      "completion_tokens": 203,
      "total_tokens": 13393
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any links, URLs, or references to publicly available code repositories related to the dataset construction, preprocessing, or analysis. There is no indication that the code used to create or analyze the new African American English (AAE) dataset is publicly shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3 (Data, Materials, and Methods; Analyses and Results)",
          "reasoning": "The paper provides detailed information about the dataset creation, including participant demographics, recording methods, acoustic measures used, and analytical methods (e.g., mixed linear effects models in R). It describes the acoustic measures, phonation types, data points, and data processing steps such as z-score normalization, use of specific software tools (e.g., Praat, R packages). This comprehensive documentation supports reproducibility of the dataset creation and analysis process."
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8062,
      "completion_tokens": 180,
      "total_tokens": 8242
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Training sets",
          "Reasoning": "The authors introduced a new training set from the Providence corpus, consisting of 128 hours of human-annotated speech utterances from spontaneous parent-child interactions. This is a directly human-recorded dataset capturing naturalistic infant-parent interactions in audio form."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Training sets",
          "Reasoning": "The authors introduced another new training set from SEEDLingS, consisting of 1,024 hours of adult speech utterances extracted from child-centered long-form recordings collected via child-worn microphones in everyday family settings. This is human-recorded audio data capturing real-life language experiences of infants."
        }
      ]
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8862,
      "completion_tokens": 394,
      "total_tokens": 9256
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1, Section 2.1.1, Section 2.1.2",
            "reasoning": "The BabySLM benchmark minimal pairs for both lexical and syntactic evaluation were generated using automated processes. Specifically, for the lexical task, pseudo-words were generated using the Wuggy pipeline and automated post-processing steps (Section 2.1.1). For the syntactic task, templates were programmatically filled with high-frequency words from CHILDES to create minimal pairs (Section 2.1.2). The evaluation is performed by models assigning probabilities, not by human annotators, indicating an automatic annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to human annotators for the generation or evaluation of the benchmark minimal pairs. The tasks and generation processes are described as automated, with no instructions given to humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not describe any scoring rubrics for human annotation. The scoring of model performance is based on a fixed metric: models receive a score of 1 if they assign higher probability to the grammatical or real word item, else 0 (Section 2.1), which is an automated evaluation metric rather than a rubric for human annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 1 and Table 2 in Section 2.1.1 and 2.1.2",
            "reasoning": "The paper provides explicit examples of the annotation items used in the benchmark, such as minimal pairs of real and pseudo-words (Table 1) and minimal pairs of grammatical and ungrammatical sentences illustrating syntactic phenomena (Table 2), effectively serving as examples of the annotation items."
          }
        }
      ]
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10052,
      "completion_tokens": 443,
      "total_tokens": 10495
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by a single human expert on the new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper that multiple human experts performed quality assurance on the new datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator without expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided indicating that multiple non-expert humans performed quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of an AI model to perform quality assurance on the new datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Task generation), Section 2.1.2 (Task generation)",
          "reasoning": "The lexical and syntactic evaluation sets were generated procedurally: lexical minimal pairs were generated using the Wuggy pipeline for pseudoword generation with balancing steps, and the syntactic minimal pairs were generated by filling templates with high-frequency words from CHILDES. Additionally, all stimuli were synthesized using an automatic Google Text-To-Speech (TTS) system. No manual validation or annotation was described, indicating that quality assurance relied on automated processes and algorithmic generation pipelines."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the process of dataset generation in detail, including automated generation and data source curation, therefore QA is present through automated processes."
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9662,
      "completion_tokens": 523,
      "total_tokens": 10185
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors did not create original speech data or textual data entirely from scratch by human contributors. Instead, they used existing datasets (Providence, SEEDLingS) collected by others."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data was generated purely by AI or machine learning models without reference to existing data. Although they synthesize speech stimuli using Google Text-To-Speech, this is derived from existing words/templates."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data used or created was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to produce the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2, 'Training sets'; Section 3.1",
          "reasoning": "The authors constructed training sets by extracting and aggregating existing speech corpora: Providence and SEEDLingS, which are existing recorded datasets of child-centered speech and adult speech from naturalistic settings. These data were collected by previous work and simply aggregated for training."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.1 Lexical evaluation and Section 2.1.2 Syntactic evaluation",
          "reasoning": "The authors generated their benchmark data by transforming existing resources: they created synthetic minimal pairs for the lexical task using pseudo-word generation from existing word lists in CHILDES, CELEX, and CMU dictionaries, processed with the Wuggy pseudoword generator, and synthesized these with Google Text-To-Speech. For the syntactic task, templates of simple grammatical phenomena were filled with frequent words from CHILDES to generate minimal pairs of grammatical and ungrammatical sentences, which were also synthesized. Thus, these benchmark datasets are derived from existing linguistic sources and computational transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation methods are clearly described."
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10180,
      "completion_tokens": 412,
      "total_tokens": 10592
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 3.1",
          "reasoning": "The new datasets derived from the Providence and SEEDLingS corpora are used for training language models from scratch, specifically STELA and LSTM models, as described in Section 2.3 (Models) and evidenced by results reported in Section 3.1 (The BabySLM benchmark). The paper discusses language model training directly on these new datasets without pre-training, demonstrating their use as training data from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 3.1",
          "reasoning": "The BabySLM benchmark itself is a newly introduced evaluation dataset comprising lexical and syntactic tasks designed for assessing spoken language models. Section 2.1 describes the construction of these evaluation sets, and Section 3.1 demonstrates their use exclusively for evaluation of language model performance on lexical and syntactic knowledge, serving as a benchmark."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3.3",
          "reasoning": "The paper uses the new benchmark and datasets to analyze and highlight challenges in modeling language acquisition from speech data, particularly concerning data plausibility and speech conditions (clean vs. in-the-wild speech). Section 3.1 discusses trends in performance related to data plausibility, and Section 3.3 analyzes difficulties in speech variability, demonstrating use of the datasets for analytical insights beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10903,
      "completion_tokens": 579,
      "total_tokens": 11482
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only discusses datasets and benchmarks in English; no mention is made of multiple human languages included in the new datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or statement in the paper that the new datasets contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 2.1.1, 2.1.2, and 3.1",
          "reasoning": "The paper explicitly mentions that the vocabulary and syntactic phenomena are drawn from American English child-centered corpora such as CHILDES, Providence, SEEDLingS, and AO-CHILDES, all of which are English datasets. The lexical and syntactic evaluation sets are generated using English words and sentences, and the benchmark is stated to focus on English in order to align with typical children's language experience. Section 4 also acknowledges the benchmark focuses on English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets and benchmark are not constructed from any single non-English language; the focus is solely on English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-related content is included in the new datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses syntactic phenomena and linguistic structures, there is no indication that the datasets themselves contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are composed of human speech data (spoken English) and do not involve biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any constructed languages, fictional or artificial."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content is clearly specified as English throughout the dataset description."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain human language data (spoken English), so this label does not apply."
        }
      }
    }
  },
  {
    "id": "lavechin23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8121,
      "completion_tokens": 160,
      "total_tokens": 8281
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4 Conclusion and footnote 2",
          "reasoning": "The paper provides a GitHub repository link (https://github.com/MarvinLvn/BabySLM) for the BabySLM benchmark scripts, indicating availability of code for dataset creation and evaluation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 Task generation (lexical and syntactic tasks) and 2.2 Training sets",
          "reasoning": "The paper provides detailed documentation on dataset creation including vocabulary selection from CHILDES, pseudo-word generation using Wuggy, template generation for syntactic tasks, speech synthesis procedures, dataset splits, and description of training sets. This constitutes comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6080,
      "completion_tokens": 190,
      "total_tokens": 6270
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Data Collection - Real-world dataset (REAL)",
          "Reasoning": "The VietMed-Sum dataset includes real-world medical ASR data from the VietMed corpus, which is recorded speech from medical conversations, thus the modality is audio. The data is collected from real medical speech interactions involving humans, so it is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Data Collection - Simulated dataset (SIM)",
          "Reasoning": "The SIM dataset is created by simulating medical conversations from extra medical text data. The simulation incorporates hesitations and disfluencies programmatically to mimic natural speech patterns. Hence, it is text modality that is model generated through simulation, not directly recorded from humans."
        }
      ]
    }
  },
  {
    "id": "leduc24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6880,
      "completion_tokens": 230,
      "total_tokens": 7110
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.1 and Section 4",
            "reasoning": "The paper states that GPT-3.5 Turbo (ChatGPT) was used to generate summaries for the dataset transcripts, referred to as GPT-annotated summaries. Human annotators only edited GPT summaries on the GOLD subset; the initial annotation is performed by the AI model."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 and Section 4",
            "reasoning": "The paper mentions human annotators edited GPT summaries \"according to the annotation guideline,\" implying that instructions were provided for human editing, which are part of the annotation guidelines for the dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No direct mention in the paper",
            "reasoning": "There is no explicit mention of scoring rubrics or criteria used for annotation or human editing within the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No direct mention in the paper",
            "reasoning": "The paper does not provide or mention annotation examples in the annotation guidelines or editing process."
          }
        }
      ]
    }
  },
  {
    "id": "leduc24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8070,
      "completion_tokens": 431,
      "total_tokens": 8501
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that multiple human expert annotators performed QA on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention QA by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 (Labeling strategy), Section 4 (GPT for Annotation), and Acknowledgement section",
          "reasoning": "The authors used GPT-3.5 Turbo to generate initial summaries and then had human annotators edit GPT-generated summaries on the GOLD subset to create gold standard data. The acknowledgements thank individuals for helping initial annotation, implying use of multiple human annotators. However, there is no explicit claim that these annotators are subject matter experts or belong to the target demographic. Given the challenge of obtaining high-quality annotators in the medical domain and low-resource language setting discussed in Section 4.1, and the lack of explicit mention of subject matter expertise, it is reasonable to conclude that multiple human non-expert annotators performed QA by editing GPT summaries."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (Labeling strategy), Section 4 (GPT for Annotation)",
          "reasoning": "The GPT-3.5 Turbo model was used to autonomously generate summaries (GPT-annotated summaries) for every transcript, which constitutes an AI-driven annotation process contributing to quality assurance of dataset. This AI-based annotation is part of the QA process before human editing."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of automated verification of annotations by code or rule-based methods is presented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A documented QA process involving AI annotation followed by human editing is described; therefore, QA was performed and documented."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7680,
      "completion_tokens": 429,
      "total_tokens": 8109
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that new dataset content was created entirely by human contributors from scratch without reference to existing data. Instead, the data used is based on existing medical conversation datasets and simulated data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors use GPT-3.5 Turbo to generate summaries, the core datasets themselves (the speech transcripts) were not generated by AI models. The summaries generated by models are annotations, not the dataset content itself."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention human translators producing dataset content by translating from another language. The English version of the dataset is translated using machine translation (Google Translate), not human translation."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 3.5 'English-translated VietMed-Sum'",
          "reasoning": "The English version of the VietMed-Sum dataset (VietMed-Sum-en) is generated using Google Translate, a machine translation system, translating from the original Vietnamese data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 'Data Collection'",
          "reasoning": "The dataset comprises VietMed real-world medical speech transcripts collected earlier and simulated conversations generated by script imitating VietMed data; thus, it is collated from existing sources and simulated data without full human creation from scratch."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 'Data Collection' and Section 3.1 'Labeling strategy'",
          "reasoning": "The synthetic subset (SYN) is generated by GPT-3.5 Turbo summarization of the existing transcripts or simulated conversations; likewise, the GOLD set involves human editing of GPT-generated summaries, so the summary data is derived from GPT model outputs and existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly described in the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8198,
      "completion_tokens": 529,
      "total_tokens": 8727
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate any use of the VietMed-Sum dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the VietMed-Sum dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 6 (Baselines and Experimental Results), Tables 4-7, and Section 3 (Data)",
          "reasoning": "The paper presents baseline results of various state-of-the-art pre-trained models fine-tuned using the VietMed-Sum dataset with supervised learning methods on gold standard and synthetic summaries (Section 6 and Tables 4-7). This demonstrates the dataset's active use for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the VietMed-Sum dataset for reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6 (Baselines and Experimental Results), Tables 4-7",
          "reasoning": "The dataset's test sets (GOLD, SYN) are used to evaluate and benchmark baseline models' performance, as shown in multiple tables (Tables 4-7), indicating its use for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the dataset for analyzing trends or characteristics beyond its role for training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or demonstrated as serving as a knowledge base for augmenting models, such as in retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usages of the VietMed-Sum dataset in supervised fine-tuning and evaluation of baseline models."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8921,
      "completion_tokens": 615,
      "total_tokens": 9536
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains entries in Vietnamese and an English-translated version, but the original conversation data and summaries are not presented as multilingual entries containing more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although there is an English-translated version of the dataset, the main dataset entries are Vietnamese medical conversations. They are not described as bilingual conversation entries containing exactly two languages simultaneously."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.5 English-translated VietMed-Sum",
          "reasoning": "The dataset includes an English-translated version (VietMed-Sum-en) created via Google Translate, but the original new dataset introduced (VietMed-Sum) is not solely English content. The English-translated set is a derived version, not the primary dataset introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Collection; Section 3.1 Labeling strategy",
          "reasoning": "The new dataset VietMed-Sum is primarily composed of real-world Vietnamese medical speech conversations and their summaries. The dataset entries are monolingual in Vietnamese, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Section 3.2 Data Collection",
          "reasoning": "While the authors mention pseudo Python code for simulating conversations in the appendix, the dataset itself consists of speech transcripts and summaries, not entries of code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the dataset content or description that mathematical or formal logical expressions or symbolic representations are included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of human medical conversation transcripts and their summaries. No biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that any fictional or artificial languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset are clearly documented as Vietnamese and an English-translated version is explicitly described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language, specifically Vietnamese."
        }
      }
    }
  },
  {
    "id": "leduc24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6139,
      "completion_tokens": 180,
      "total_tokens": 6319
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1 Introduction",
          "reasoning": "The paper explicitly states in the abstract and introduction that all code, data (including English-translated and Vietnamese), and models are available online. This indicates that the code related to dataset construction, including data collection, preprocessing, and generation, is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Data and Section 3.2 Data Collection",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 3. It discusses the use of real-world VietMed dataset and simulated data, the labeling strategy combining GPT-generated and human-edited summaries, and the rationale for the data splits (GOLD and SYN). The paper includes explanations of data simulation approaches and annotation methods, providing sufficient transparency and completeness for reproducibility."
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7734,
      "completion_tokens": 120,
      "total_tokens": 7854
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 'Longitudinality of the Phonetic Inventory' and Conclusion",
          "Reasoning": "The paper introduces a longitudinal dataset of speech productions from children with cochlear implants, collected over 5 years post-implantation. This dataset consists of phonetic inventories derived from speech production data, including both spontaneous and imitation speech. The data involves recordings of children's speech, which is audio captured through human recording processes at clinical and therapy centers, thus involving human-generated data."
        }
      ]
    }
  },
  {
    "id": "lee22k_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8534,
      "completion_tokens": 213,
      "total_tokens": 8747
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Throughout the paper, particularly in sections describing phonetic inventory analysis and error pattern assessment",
            "reasoning": "The paper refers to 'the first author' conducting phonetic transcription and error analysis, indicating that a single human expert carried out the annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section or description provides annotation instructions.",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators or coders; only the first author performed annotations without mention of instructional guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper of formal scoring rubrics or criteria for annotations.",
            "reasoning": "The paper does not specify any rubric or scoring system used to guide annotations or evaluations of the phonetic data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No appendix or section provides annotation examples.",
            "reasoning": "No example annotations or exemplar cases are provided in the paper for the annotation task."
          }
        }
      ]
    }
  },
  {
    "id": "lee22k_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9724,
      "completion_tokens": 293,
      "total_tokens": 10017
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Throughout the paper, annotations and phonetic transcriptions are attributed to the 'first author' and mention of the 'national certificate of speech and language pathology' suggesting expert qualification.",
          "reasoning": "The paper states that phonetic annotations were conducted by the first author, who holds a national certificate in speech and language pathology, indicating subject matter expertise. There is no indication of other annotators involved."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple expert annotators conducting quality assurance or annotation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotator mentioned is qualified by certification; thus, not a non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of multiple non-expert annotators participating in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models being used for quality assurance or validation of annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of automated or algorithmic verification processes for the dataset annotations reported in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is documented as performed by an expert; therefore, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9334,
      "completion_tokens": 320,
      "total_tokens": 9654
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Throughout the paper, including 'Results' and 'Conclusion' sections",
          "reasoning": "The dataset consists of longitudinal phonetic inventories of children with cochlear implants collected via clinical sessions and speech-language therapy centers over five years. The data was generated through direct recordings and analyses of children's speech productions (both spontaneous and imitation), which represents original content created entirely from human participants without adaptation or translation from other sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study generated data from direct collection and longitudinal tracking, rather than aggregation of existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some comparison is made with previous studies, the new dataset itself is original raw data, not derived from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9852,
      "completion_tokens": 305,
      "total_tokens": 10157
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Throughout the paper, especially in the sections discussing the longitudinal phonetic inventory development and analysis of phonetic errors",
          "reasoning": "The dataset introduced consists of phonetic inventories of children with cochlear implants tracked longitudinally over 5 years. It is used primarily to analyze and understand patterns in speech and language development rather than for any form of machine learning training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10575,
      "completion_tokens": 369,
      "total_tokens": 10944
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Throughout the paper (e.g., Abstract and Results sections)",
          "reasoning": "The new dataset introduced is a phonetic inventory and speech corpus of children with cochlear implants producing Korean speech sounds. The paper explicitly describes the Korean phoneme system and analyzes speech data from Korean-speaking children, indicating the dataset contains exactly one non-English language: Korean."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human spoken language data in Korean."
        }
      }
    }
  },
  {
    "id": "lee22k_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7793,
      "completion_tokens": 174,
      "total_tokens": 7967
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about source code availability related to the dataset collection, preprocessing, or generation. There is no mention of code repositories or supplementary material containing code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections describing longitudinal study design and phonetic inventory collection processes throughout the paper (e.g., introduction and results sections)",
          "reasoning": "The paper documents the dataset creation process by detailing the longitudinal study conducted on children with cochlear implants over 5 years, including the ages of assessment, types of phonetic inventories collected (spontaneous and imitation speech), and comparison with normal hearing children. These details provide transparency on how the new phonetic inventory dataset was collected and analyzed, although there is no explicit mention of dataset publication or detailed collection protocols."
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8022,
      "completion_tokens": 123,
      "total_tokens": 8145
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Dataset; Speech Corpus of Korean-Speaking Children with CIs",
          "Reasoning": "The newly introduced dataset is a speech corpus constructed by the authors for Korean-speaking children with cochlear implants (CIs). The audio data were extracted from videos recorded during regular evaluation sessions, manually segmented, transcribed, and rated by certified speech and language pathologists. This indicates human involvement in recording and manual processing, confirming human-generated origin of audio data."
        }
      ]
    }
  },
  {
    "id": "lee24e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8822,
      "completion_tokens": 228,
      "total_tokens": 9050
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1.1 (Speech Corpus of Korean-Speaking Children with CIs)",
            "reasoning": "The speech production skills were rated by three nationally certified speech and language pathologists, all qualified experts with at least two years of relevant experience. This indicates multiple human experts conducted the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "The paper does not mention detailed annotation instructions or guidelines provided to raters for the speech production scoring."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1.1",
            "reasoning": "The rating was conducted on a scale from 1 to 5, where 1 indicates very poor and 5 indicates very good speech production skills. This scale serves as a scoring rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "The paper does not mention any annotation examples or specific exemplar utterances provided to raters during annotation."
          }
        }
      ]
    }
  },
  {
    "id": "lee24e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10012,
      "completion_tokens": 306,
      "total_tokens": 10318
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes that three nationally certified speech and language pathologists with at least two years of relevant experience rated the speech production skills; this indicates multiple experts rather than a single expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Speech Corpus of Korean-Speaking Children with CIs",
          "reasoning": "The speech production skills were rated by three nationally certified speech and language pathologists, each qualified with at least two years of experience in the rehabilitation of children with cochlear implants or with phonetics knowledge, indicating quality assurance was conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any single non-expert performed QA."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that QA was performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was performed by human experts (speech and language pathologists), not by AI models."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or algorithmic QA processes for the annotations in the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance process is documented and performed by multiple experts, thus not applicable."
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9622,
      "completion_tokens": 434,
      "total_tokens": 10056
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The paper explicitly states that the speech corpus for Korean children with cochlear implants (CIs) is being constructed because there was no available dataset previously. It involves data collected from 30 children with CIs during regular evaluation sessions, with audio extracted from videos, orthographic and phonemic transcriptions, and expert ratings by certified speech and language pathologists. This process indicates original data collection created entirely from scratch by human contributors for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the creation of any dataset generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any dataset produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate any dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset for children with CIs is not described as collected or aggregated from existing sources without modification; it was newly collected. Other datasets used are pre-existing but not introduced or created by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the new dataset is derived by modification or adaptation of an existing source; rather it was created through original data collection."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and creation method of the new dataset, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10140,
      "completion_tokens": 369,
      "total_tokens": 10509
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 3.2",
          "reasoning": "The speech corpus of Korean-speaking children with cochlear implants, which is newly constructed by the authors, is used to fine-tune their assessment model in a supervised manner. Specifically, Section 3.1 describes the dataset construction and rating by experts, while Section 3.2 describes the use of 5-fold cross-validation of this dataset for training and evaluation of the model. The dataset is used as labeled supervised data to train the prediction model of speech production scores from embeddings."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4.1",
          "reasoning": "The same speech corpus of children with CIs is used also for performance measurement, as reported in Section 4.1, by calculating Pearson correlation coefficients between predicted scores and expert-labeled scores. The experiments use 5-fold cross-validation with this dataset for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "In Section 5, the authors analyze results based on various embedding combinations and discuss the significance of using both adult and child speech patterns in the model. This indicates the dataset is used for analysis of speech characteristics and model performance nuances."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10863,
      "completion_tokens": 500,
      "total_tokens": 11363
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset introduced is exclusively Korean speech data with no mention of more than two human languages within the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The proposed new dataset contains only Korean language speech samples; no two-language dataset is introduced."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The new dataset introduced contains Korean speech data, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset",
          "reasoning": "The newly constructed speech corpus contains Korean language speech recordings from children with cochlear implants, making it monolingual non-English Korean data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries consist of human speech audio and phoneme sequences, not code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical symbolic expressions appear as entries in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech only; it does not contain biological sequences or animal/non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves natural Korean language speech, with no mention of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the speech data is clearly specified as Korean."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data and therefore is not without language."
        }
      }
    }
  },
  {
    "id": "lee24e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8081,
      "completion_tokens": 169,
      "total_tokens": 8250
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1.1 (Dataset) and throughout the paper",
          "reasoning": "The paper mentions that the speech corpus for Korean children with cochlear implants is being constructed and describes its collection and annotation process, but there is no mention or link to publicly available code or repository related to data collection, preprocessing, or dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1.1 (Dataset - Speech Corpus of Korean-Speaking Children with CIs)",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including information about the speakers, data collection procedure, audio extraction, transcription, expert rating protocols, ethical approval details, and data selection criteria, contributing to transparency and completeness of dataset documentation."
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9350,
      "completion_tokens": 112,
      "total_tokens": 9462
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 2.2, Table 1",
          "Reasoning": "Speech-MASSIVE is an audio dataset created by the authors by recruiting native speakers on Prolific to record spoken versions of MASSIVE textual data in 12 languages. The recordings are human-generated as they are performed by native human speakers. The paper details the recording and validation process confirming these data were manually collected from humans."
        }
      ]
    }
  },
  {
    "id": "lee24i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10150,
      "completion_tokens": 309,
      "total_tokens": 10459
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1",
            "reasoning": "The speech data collection and validation process was conducted by recruiting native speakers through the Prolific crowdsourcing platform. These participants, described as workers, recorded and validated speech data. There is no indication that these annotators were experts; rather, they were native speakers recruited through a crowdsourcing platform, consistent with multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1",
            "reasoning": "During the recording phase, workers were given guidelines emphasizing accurate and natural reading, proper recording conditions, and strict adherence to the text. Participants also had instructions to review their recordings. Similarly, in the validation phase, participants had specific instructions on how to validate samples, including listening and labeling as valid or invalid, with quality control checks using Common Voice samples. This indicates detailed instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper describes instructions and quality control for the annotation and validation tasks but does not mention any numeric or scoring rubrics or detailed scoring schemes being used for the labeling process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no mention of annotation examples or example annotated data given to participants in the recording or validation phases. The paper only describes guidelines and quality control procedures but no annotated examples."
          }
        }
      ]
    }
  },
  {
    "id": "lee24i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11340,
      "completion_tokens": 478,
      "total_tokens": 11818
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The paper indicates that quality assurance was performed by multiple native speakers recruited via a crowdsourcing platform, with no evidence that these were single experts. Therefore, it does not apply."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The paper states that native speakers performed recording and validation via the Prolific platform but does not indicate these annotators were subject matter experts or targeted demographic experts. Thus, this category does not apply."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The QA was not conducted by a single annotator but by multiple individuals as described in the recording and separate validation steps."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset's speech recordings were collected from native speakers via Prolific, a crowdsourcing platform, and the validation was done by a second group of native speakers. There is no indication that these individuals had subject matter expertise beyond being native speakers, implying they are non-experts. Multiple such individuals performed the quality assurance through a two-step process including re-recordings and validation. Hence, this label applies."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an automated algorithmic or rule-based verification of annotations or speech quality other than manual listening and validation by humans."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A clear quality assurance process is described in Section 2.1 involving human validation and quality control samples; thus, this label does not apply."
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10950,
      "completion_tokens": 449,
      "total_tokens": 11399
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The Speech-MASSIVE dataset was created by recruiting native speakers through the Prolific crowdsourcing platform who recorded spoken versions of the MASSIVE sentences. These recordings are original speech data generated directly by human contributors, adhering closely to the original text, thus constituting new human-created data rather than any pre-existing speech resource."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated entirely by AI or machine learning models as part of the dataset creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the underlying MASSIVE text dataset involves human translation/localization of the SLURP dataset texts to multiple languages, Speech-MASSIVE itself introduces speech data recorded via vocalization of these texts rather than producing new translated text data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is involved in the creation of the speech data in Speech-MASSIVE."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The speech data is not collected from existing speech corpora but created afresh via new human recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 and Abstract",
          "reasoning": "Speech-MASSIVE speech data is derived from the pre-existing MASSIVE textual dataset by recording spoken versions of the texts. This represents an adaptation of existing textual data into speech form, thus the speech data is based on existing source texts with a transformation applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation methods are explicitly documented and described in the paper."
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11468,
      "completion_tokens": 362,
      "total_tokens": 11830
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The Speech-MASSIVE dataset is used to fine-tune pre-trained models in supervised learning settings for Spoken Language Understanding (SLU). Section 3.3 details the use of Speech-MASSIVE for fine-tuning cascaded SLU models, and Section 3.4 reports fine-tuning of an end-to-end SLU model using Speech-MASSIVE's training sets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.3, 3.4 and 3.5",
          "reasoning": "Speech-MASSIVE is employed to evaluate SLU model performance in various training scenarios (zero-shot, few-shot, full fine-tune) across multiple languages as well as to benchmark other speech-related tasks such as language identification and speech translation, as discussed in Sections 3.3 to 3.5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Conclusion)",
          "reasoning": "The paper discusses analysis of model performances and multilingual trends, including the influence of training language on zero/few-shot SLU performance and comparison of cascaded and end-to-end models, leveraging the Speech-MASSIVE dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12191,
      "completion_tokens": 509,
      "total_tokens": 12700
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 2.2",
          "reasoning": "The dataset Speech-MASSIVE is explicitly described as multilingual, covering 12 human languages: Arabic, German, Spanish, French, Hungarian, Korean, Dutch, Polish, European Portuguese, Russian, Turkish, and Vietnamese (Abstract and Section 2.2). Therefore, it clearly includes more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Speech-MASSIVE includes 12 languages and is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages beyond English; it is not English-only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is multilingual, with multiple non-English languages included, not only one single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech data in human languages only, with no mention of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human speech in 12 natural languages and does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human languages, so it is not devoid of language."
        }
      }
    }
  },
  {
    "id": "lee24i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9409,
      "completion_tokens": 211,
      "total_tokens": 9620
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2 (footnote)",
          "reasoning": "The paper's abstract includes a URL (https://github.com/hlt-mt/Speech-MASSIVE) indicating that the dataset, models, and code are publicly available. This is further supported by the footnote in Section 2 mentioning the use of Prolific platform and detailed recruitment and validation processes. Hence, the code related to dataset collection, preprocessing, and generation can be accessed by the community."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 2.1, covering recruitment of native speakers via Prolific, instructions for recording, multi-stage validation procedures including quality control measures, and the selection criteria for languages. Section 2.2 offers overall statistics and rationale for language selection. These comprehensive details document the process sufficiently for transparency and reproducibility."
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9352,
      "completion_tokens": 228,
      "total_tokens": 9580
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Material and Section 2.3 Procedures",
          "Reasoning": "The study used the original Yanny vs. Laurel audio stimulus which was a human-recorded audio clip downloaded from a Twitter link. The stimulus was presented to participants as audio for perception, making the data modality audio. The audio clip originates from a human-recorded source, not model generated or unknown origin."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Listeners and Section 3 Results",
          "Reasoning": "The dataset consists of responses collected from 974 Swiss German listeners regarding their percept (Yanny, Laurel, mixed). These responses, along with demographic and device information (age, gender, region, musicality, device used), form structured participant metadata and labeled perceptual responses stored as tabular data. The data was manually recorded through a controlled survey procedure, indicating human generation rather than automated or model-generated data."
        }
      ]
    }
  },
  {
    "id": "leemann22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10152,
      "completion_tokens": 258,
      "total_tokens": 10410
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Listeners and Section 2.3 Procedures",
            "reasoning": "The study collected data from 974 Swiss German listeners balanced for age and gender. These participants were lay listeners recruited across multiple Swiss localities, not experts, and responses were collected from many individuals, indicating multiple human non-experts performed the percept annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Procedures, Figure 4",
            "reasoning": "Listeners were instructed to carefully listen to the supervisor's explanations before playing the audio stimulus, and they were told to play the file only once and then record their answer. This indicates detailed annotation instructions were provided to participants."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.3 Procedures",
            "reasoning": "Response data was coded into specific categories: Yanny, Laurel, or mixed percept (including responses like 'both' or other variants). This coding scheme functions as a rubric for annotation categorization."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or description of example annotations or annotation examples being provided to participants or annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "leemann22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11342,
      "completion_tokens": 356,
      "total_tokens": 11698
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a single non-expert human annotator performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Procedures",
          "reasoning": "Listener responses were recorded and coded as Yanny, Laurel, or mixed. Since the participants were 974 Swiss German listeners (non-expert in annotation terms) providing perceptual judgments, and the responses were coded into categories, it implies multiple human non-experts generated and provided the perceptual data. The paper does not describe an expert annotation or validation process beyond the participants' own perception reports, which can be considered annotation by multiple non-expert humans (listeners). Thus, quality assurance arises through multiple non-expert human contributions inherent in the data collection."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any AI model used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process or algorithmic quality assurance is described for the data annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes collection and categorization of perceptual responses from multiple listeners, implying some form of data validation by multiple human non-experts (listeners); therefore, quality assurance is present in this form."
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10952,
      "completion_tokens": 456,
      "total_tokens": 11408
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Listeners and Section 2.3 Procedures",
          "reasoning": "The dataset consists of responses from 974 Swiss German listeners who participated in the study by listening to the original Yanny v. Laurel stimulus and reporting their percepts. These participants were human subjects providing original data through their auditory perception and responses, not translations or adaptation of existing datasets. The paper explicitly states the collection of new perceptual data from these human participants using a bespoke app and controlled procedures."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data in the paper is described as being generated by AI or machine learning models from scratch."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses the existing original Yanny v. Laurel audio stimulus, the main dataset consists of perceptual responses collected new from human participants, not an aggregation of existing datasets without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 Material",
          "reasoning": "The original Yanny v. Laurel stimulus audio was retrieved from an existing online source (a Twitter post) and then normalized in amplitude with Praat for presentation in the study. This indicates the audio stimulus was derived from existing content with some transformation applied, but the participant response data constitute new data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11470,
      "completion_tokens": 245,
      "total_tokens": 11715
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Results, especially 3.1 Overall distribution, 3.2 Age, 3.3 Gender, 3.4 Device",
          "reasoning": "The dataset collected by the authors from 974 Swiss German listeners is analyzed to understand the factors affecting perception of the Yanny v. Laurel stimulus. The authors use the dataset primarily for statistical analysis of trends and patterns in perceptual responses (e.g., effects of age, gender, device), not for any model training or evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12193,
      "completion_tokens": 297,
      "total_tokens": 12490
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Material; Section 2.2 Listeners; Sections 3.1-3.4 Results",
          "reasoning": "The dataset consists of responses from Swiss German listeners to an audio clip originally in English (the ambiguous recording of the word Yanny or Laurel). The perception data collected is based on this English stimulus only, and the listeners are Swiss German speakers responding to English auditory content. The paper does not mention multiple languages in the dataset responses or data collection; only English words are in the stimuli. Hence, the dataset entries contain monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "leemann22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9411,
      "completion_tokens": 168,
      "total_tokens": 9579
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories related to the data collection, preprocessing, or dataset generation. There is no mention of code releases or supplementary materials that contain such code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including participant recruitment and balancing (Section 2.2), the use of the SDATS project framework, the stimulus material description (Section 2.1), and data collection procedures (Section 2.3). It describes the demographic balancing, recording setup, devices used, response coding, and analysis approach, which constitutes adequate documentation for reproducibility."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7803,
      "completion_tokens": 132,
      "total_tokens": 7935
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4, Section 5.2",
          "Reasoning": "The authors introduced newly pre-trained Wav2Vec 2.0 models based on speech audio data collected from public sources for German and Czech languages, including VoxPopuli dataset and self-crawled podcasts and audiobooks, totaling 50k hours each; also the English base model was pre-trained previously on Libri-light. This audio speech data is human-generated, recorded from natural sources, and is not generated by any models or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8603,
      "completion_tokens": 232,
      "total_tokens": 8835
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.2 MALACH",
            "reasoning": "The MALACH dataset is described as an oral history archive with transcripts that were released under the Linguistic Data Consortium (LDC) (see references [18] and [19]). The paper mentions careful preprocessing, including segmentation and cleaning of transcripts, implying expert annotation of transcripts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5.2 MALACH",
            "reasoning": "The paper does not explicitly mention detailed annotation instructions provided to annotators for the MALACH dataset newly introduced or used. It only describes data selection and preprocessing steps."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5.2 MALACH",
            "reasoning": "There is no mention in the paper of scoring rubrics or formalized guidelines for annotation quality or evaluation for the MALACH dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5.2 MALACH",
            "reasoning": "The paper does not provide examples or mention example transcripts or annotation examples for the MALACH dataset."
          }
        }
      ]
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9793,
      "completion_tokens": 282,
      "total_tokens": 10075
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts conducted quality assurance on the datasets used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any quality assurance conducted by a single non-expert annotator on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple non-expert annotators performing quality assurance on the dataset annotations or content."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that an AI model was used as a judge for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic processes used for quality assurance of the new dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not provide any details or documentation about performing quality assurance on the new datasets introduced. Quality assurance processes are neither described nor referenced for the pre-processed or balanced subsets used for training and evaluation."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9403,
      "completion_tokens": 477,
      "total_tokens": 9880
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any original speech or transcript data entirely from human contributors from scratch. All datasets used (MALACH and CommonVoice) are pre-existing."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any dataset generated by AI or machine learning models. The focus is on training and evaluating ASR models, not generating new data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation being used to generate any part of the datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4, Section 5",
          "reasoning": "The datasets used for pre-training and fine-tuning (e.g., CET Czech and German speech from VoxPopuli and self-crawled podcasts, CommonVoice dataset, and MALACH oral history recordings) are collected or aggregated from existing public sources without mention of original content creation or substantial modification. The authors assembled these existing data sources to create balanced subsets for experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "For the MALACH dataset, although the underlying data is from an existing archive, the authors performed some modifications such as balancing the dataset across languages, segmenting long recordings into shorter segments, cleaning transcripts (removing non-speech events and punctuation, converting Czech transcripts to formal Czech). These transformations classify the resulting datasets as derived from original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies dataset origins and any modifications applied, so the data generation method is documented."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9921,
      "completion_tokens": 451,
      "total_tokens": 10372
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4 (Pre-trained models)",
          "reasoning": "The authors pre-trained new monolingual, bilingual, and trilingual Wav2Vec 2.0 models from scratch using large-scale unlabeled speech data, applying self-supervised learning as described in Section 4."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Pre-trained models)",
          "reasoning": "The new Wav2Vec 2.0 models for German and Czech, including bilingual and trilingual combinations, were trained from randomly initialized parameters using publicly collected data, thus representing training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments)",
          "reasoning": "The authors fine-tuned all pre-trained Wav2Vec and Whisper models supervisedly on labeled MALACH and CommonVoice datasets to adapt models for ASR tasks as detailed in Section 6."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of reinforcement learning or RL-based post-training techniques with the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 6 (Experiments) and Table 2",
          "reasoning": "The MALACH and CommonVoice datasets were used for evaluation of different models by measuring word error rates (WER) on test sets, as shown in Table 2 and described in experimental sections."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While analysis of model performance is conducted, the datasets themselves are not used primarily for analysis of trends or characteristics outside the scope of training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as a knowledge base to augment models or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new datasets introduced in the paper (pre-training data and the balanced subsets for fine-tuning) are utilized extensively for pre-training, fine-tuning, and evaluation; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10644,
      "completion_tokens": 612,
      "total_tokens": 11256
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 1 Introduction, Section 5 Fine-tuning datasets",
          "reasoning": "The paper introduces the MALACH dataset which contains interviews in 32 languages, including English, German, and Czech. The authors also constructed balanced subsets containing these three languages for their experiments, indicating the dataset entries include more than two human languages (English, German, and Czech). They fine-tuned models on multilingual combinations and on mixed-language data, confirming the dataset is multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although bilingual language pairs are used in training the models (e.g., English+German), the dataset itself is not described as having exactly two languages in entries. Instead, the dataset MALACH contains many languages (32), and the experimental subsets include three languages, making the dataset multilingual rather than strictly bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain not only English but also multiple other languages. The paper does not introduce any new dataset that is only English monolingual."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Similarly, the datasets include multiple languages. No new dataset introduced contains exactly one non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described are speech and text transcription datasets with human languages; no programming or code-related data is introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain speech and textual transcripts without mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets comprise human speech and transcripts from oral history archives; no biological or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any constructed or fictional language data is part of the introduced datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the datasets are clearly stated and documented (English, German, Czech, and in the case of MALACH more languages)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets involve human language speech and text data; therefore, they do contain language."
        }
      }
    }
  },
  {
    "id": "lehecka24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7862,
      "completion_tokens": 171,
      "total_tokens": 8033
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any link, mention, or reference to code repositories or other resources that would indicate that code related to data collection, preprocessing, or generation of datasets introduced by the authors is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 'Pre-trained models' and Section 5 'Fine-tuning datasets'",
          "reasoning": "The paper describes in detail the process of creating the datasets used for fine-tuning, including sources of data (e.g., public datasets like VoxPopuli and self-crawled podcasts and audiobooks), balancing procedures to select equal training data per language, cleaning and preprocessing steps such as transcript cleaning and segmentation, and train/dev/test splits."
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7952,
      "completion_tokens": 259,
      "total_tokens": 8211
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 2.2",
          "Reasoning": "The new dataset introduced is an extension of the Blizzard Challenge 2013 corpus (2013-EH2), which consists of uncompressed WAV audio files of a single female speaker reading audiobooks. This data was originally human recorded speech, thus human generated audio. It is explicitly mentioned that the corpus comprises 9733 utterances with durations varying between 0.5 s and 35.3 s, and includes new samples as well as original challenge samples extended with modern synthetic speech systems. The core speech data originates from human recorded sources."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The paper introduces four new synthetic speech systems representing modern speech synthesis technologies combining two acoustic models (Tacotron and FastPitch) and two neural vocoders (WaveNet and Parallel WaveGAN). The audio generated by these systems is model generated synthetic speech. This is part of the new dataset extension, providing synthetic audio data generated automatically by speech synthesis models trained on the original human recorded data."
        }
      ]
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8752,
      "completion_tokens": 244,
      "total_tokens": 8996
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3 and 3.1",
            "reasoning": "The evaluation was conducted with 68 native English listeners recruited online via Prolific, which indicates multiple non-expert human participants performed the naturalness annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "The paper states that the evaluation protocol followed the Blizzard Challenge 2013 protocol and ITU-T P.800 recommendation, and that the total duration and instructions to use headphones were given, indicating instructions were provided to the listeners."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3 and Table 1 & 2",
            "reasoning": "Listeners rated naturalness using Mean Opinion Scores (MOS) on a 5-point Likert scale per the Absolute Category Rating (ACR) evaluation method, which serves as a scoring rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No explicit mention or description of annotation examples or training samples given to annotators is present in the paper; thus, annotation examples were not provided."
          }
        }
      ]
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9942,
      "completion_tokens": 328,
      "total_tokens": 10270
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single human expert for the dataset annotations or evaluation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that multiple human experts performed quality assurance for dataset annotation or validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Recruitment and evaluation protocol, and Section 3.1 Listeners",
          "reasoning": "The subjective evaluation of naturalness was performed by 68 native English speakers recruited online via Prolific. These listeners conducted the listening test and provided MOS ratings. Although they are native speakers, there is no indication they are speech synthesis experts. Their role was to rate synthetic speech naturalness, serving as annotators validating the dataset. Hence, QA was performed by multiple human non-expert annotators (native lay listeners)."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance is described as being performed by an AI model as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance via automated verification of annotations or formulas for the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process for the dataset through human evaluation by multiple human non-experts is documented."
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9552,
      "completion_tokens": 461,
      "total_tokens": 10013
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new data created entirely from scratch by human contributors. The datasets used (Blizzard Challenge 2013) were pre-existing and not newly created human-generated content."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The authors generated new synthetic speech samples using four newly built modern speech synthesis systems combining acoustic models (Tacotron, FastPitch) and neural vocoders (WaveNet, Parallel WaveGAN). These synthetic speech samples are original content generated entirely by machine learning models from the same text and corpus, representing new data from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data produced by human translations from other languages is made in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of machine translation to generate data is mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The authors used existing publicly available Blizzard Challenge 2013 datasets, which are collected and aggregated corpora of audiobook speech data. These data were taken without significant modification except downsampling (to 16 kHz) for consistency."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.2 and Section 2.2",
          "reasoning": "The authors derived new datasets by selecting a subset of sentences from the Blizzard Challenge 2013 test sets and produced synthetic speech via transformation of original text using modern TTS models. This involved adaptation of existing data through model-based transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data origin and generation methods."
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10070,
      "completion_tokens": 301,
      "total_tokens": 10371
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The authors trained four modern speech synthesis systems using Tacotron and FastPitch acoustic models on the extended Blizzard Challenge 2013 dataset. This indicates that the constructed dataset is used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3, especially Sections 3.2 and 3.4",
          "reasoning": "The paper extensively describes using the extended Blizzard Challenge 2013 dataset for conducting a large-scale subjective evaluation comparing historical and modern speech synthesis systems. The dataset serves as a benchmarking tool to assess naturalness of synthetic speech."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 and 4",
          "reasoning": "The authors analyze trends such as the relative nature of Mean Opinion Scores (MOS) over time, comparing past and present systems, highlighting how MOS scores changed. This dataset enables analysis of evaluation metrics and perception patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10793,
      "completion_tokens": 410,
      "total_tokens": 11203
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset introduced extends the Blizzard Challenge 2013 which contains data from a single English speaker, and no mention of multiple languages is made."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper only discusses English speech data; no second language is indicated or used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 2.1.1",
          "reasoning": "The dataset comprises English speech from a single female American English speaker reading English audiobooks, as explicitly described in Section 2.1. The text input to the synthesis systems is phonetic sequences derived from English data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains any non-English language speech."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While code implementations are referenced, the dataset itself does not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No description or inclusion of mathematical or logical symbolic notations in the dataset is present in the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset pertains to human speech only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or evidence of constructed or fictional language data in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English speech data and thus contains language entries."
        }
      }
    }
  },
  {
    "id": "lemaguer22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8011,
      "completion_tokens": 197,
      "total_tokens": 8208
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 and Conclusion",
          "reasoning": "The paper states that the resulting dataset, models, and configurations to reproduce them, as well as the subjective evaluation results, are freely available for research purposes and provides a link to a GitHub repository (https://github.com/sigmedia/bc_2013_extension). This indicates that the code related to data preparation, model training, and evaluation is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 2.2",
          "reasoning": "The paper provides comprehensive documentation about the dataset creation process, including details on dataset selection, data preprocessing (e.g., downsampling to 16 kHz), selection criteria of historical systems, and the construction of modern systems with specific acoustic models and neural vocoders. The evaluation protocol and listener recruitment are detailed as well. This thorough description serves as suitable documentation for dataset creation and usage."
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8730,
      "completion_tokens": 83,
      "total_tokens": 8813
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Material and Methods",
          "Reasoning": "The paper introduces and uses a subset of the Donate Speech Corpus, which consists of spontaneous Finnish speech recorded from thousands of volunteers via a mobile or desktop app, thus these audio recordings are human-generated data captured from human speakers."
        }
      ]
    }
  },
  {
    "id": "lennes23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9530,
      "completion_tokens": 235,
      "total_tokens": 9765
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2, Material and Methods",
            "reasoning": "The annotations correspond to self-reported background information such as gender and age group provided by thousands of volunteer speakers via the Donate Speech app. These volunteers are non-experts providing metadata about themselves, rather than expert annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2, Material and Methods",
            "reasoning": "The paper does not mention any detailed instructions provided to the annotators (volunteer speakers) about how to provide or annotate their background information. They simply provided self-reported metadata via the app."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2, Material and Methods",
            "reasoning": "No scoring rubrics or criteria were provided for the annotation of the self-reported metadata, as it was simply user-provided demographic info without formal annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2, Material and Methods",
            "reasoning": "No examples or sample annotations were mentioned in relation to the collection of the demographic metadata from participants."
          }
        }
      ]
    }
  },
  {
    "id": "lennes23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10720,
      "completion_tokens": 442,
      "total_tokens": 11162
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert or annotator. There is no description of manual inspection or validation performed by a subject matter expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset. No manual annotation or expert validation process is described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert annotator. There is no mention of manual checks or validation performed by non-experts."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using an AI model as a judge or for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2, Material and Methods",
          "reasoning": "Quality assurance was implicitly performed through automated pitch analysis using Praat scripts with predefined parameters for pitch detection. The process of selecting data, excluding inconsistent metadata, and using consistent algorithmic parameters constitutes an automated verification and filtering process ensuring data reliability. Although no explicit QA step is described, the paper specifies automated procedures to handle and clean the data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the paper does not describe explicit manual quality assurance, there is evidence of automated processing and data cleaning steps indicating that some form of quality assurance was applied."
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10330,
      "completion_tokens": 436,
      "total_tokens": 10766
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Material and Methods",
          "reasoning": "The dataset analyzed is a subset of the Donate Speech Corpus, which contains speech recordings collected directly from thousands of volunteer human speakers in Finland via a dedicated mobile or desktop app. The speech samples are spontaneous, created entirely by human speakers themselves, reflecting original human-generated speech content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was generated by AI or machine learning models. The speech data is collected from human speakers, not synthesized or automatically generated."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the data being produced by translation from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the Donate Speech Corpus aggregates many recordings from different speakers, the corpus itself is a primary collection of newly recorded human speech, not aggregated existing speech data. The paper does not indicate using existing corpora or collating data from other sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not derived from existing sources with modifications; it is original speech data collected from volunteers. The paper describes analysis and pitch extraction, but the dataset itself is not described as derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is explicitly documented in the paper as new human-generated speech collected via the Donate Speech campaign."
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10848,
      "completion_tokens": 213,
      "total_tokens": 11061
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3: Results",
          "reasoning": "The Donate Speech Corpus subset is used primarily to analyze trends and patterns in the pitch distributions of spontaneous Finnish speech across a large number of speakers, focusing on age and gender differences. The paper conducts detailed statistical and comparative analyses on this dataset, without reference to using it for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11571,
      "completion_tokens": 496,
      "total_tokens": 12067
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only Finnish speech samples, collected from Finnish speakers, so it is not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes exactly two languages; only Finnish data is used."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain English speech; it only contains Finnish."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 (Material and Methods)",
          "reasoning": "The Donate Speech Corpus subset used in the study contains spontaneous speech samples only in Finnish, confirmed by statements that the speakers are Finnish and the speech is in Finnish language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech recordings; no programming or code-related data entries are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains speech audio and pitch values, not mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech only, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Constructed or fictional languages are not mentioned or included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly specified as Finnish."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data (Finnish speech), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "lennes23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8789,
      "completion_tokens": 184,
      "total_tokens": 8973
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2: Material and Methods",
          "reasoning": "The paper discusses the use of a Praat script for pitch analysis and processing of the Donate Speech Corpus data, but does not provide any link or reference to publicly accessible code repositories containing the used code for data collection, preprocessing, or generation. There is no mention of code availability for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3 (Material and Methods and Results)",
          "reasoning": "The paper provides a detailed description of the dataset used (a subset of the Donate Speech Corpus), including information about data collection via a dedicated app, recording conditions, pitch analysis method with parameters, preprocessing criteria for speaker inclusion, and statistical analysis steps. Although some limitations and uncertainties are acknowledged, the dataset creation process is transparently documented in the paper itself."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9386,
      "completion_tokens": 263,
      "total_tokens": 9649
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 and Section 4.1",
          "Reasoning": "The new data used for text-to-dysarthric-speech synthesis is derived from the TORGO dysarthric speech database, which consists of human-recorded dysarthric speech audio. The paper describes training Grad-TTS models from scratch on this dysarthric human speech to create synthesized dysarthric audio. Therefore, the original TORGO dysarthric speech data is human generated and audio modality."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Section 3.2, and Section 4.1",
          "Reasoning": "The paper introduces new synthesized dysarthric speech audio generated by the authors' trained Grad-TTS diffusion probabilistic model (text-to-speech synthesis) on the TORGO dysarthric data. This synthetic data is model generated audio produced via the Grad-TTS model trained from scratch by the authors. This synthetic audio is then used for data augmentation to fine-tune ASR models. Thus, the synthesized dysarthric speech data is audio modality with model-generated origin."
        }
      ]
    }
  },
  {
    "id": "leung24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10186,
      "completion_tokens": 237,
      "total_tokens": 10423
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.4.2",
            "reasoning": "Subjective evaluation of synthesised data was conducted by a Speech and Language Therapist (SLT) with more than 10 years of experience assessing and diagnosing speech disorders, indicating a single human expert performed the subjective annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.4.2",
            "reasoning": "The SLT used a defined 5-point scale (0 for none to 4 for severe) to rate dysarthria severity, which implies instructions for annotation were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.4.2",
            "reasoning": "Ratings were performed on a 5-point severity scale, indicating the presence of a scoring rubric for severity assessment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.4.2",
            "reasoning": "No mention of providing annotation examples or example audio samples as annotation guidelines; although 20 audio samples were randomly selected per speaker, these were evaluation samples rather than examples for annotator training."
          }
        }
      ]
    }
  },
  {
    "id": "leung24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11376,
      "completion_tokens": 381,
      "total_tokens": 11757
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.4.2",
          "reasoning": "Subjective evaluation of the synthesized dysarthric speech data was conducted by a single expert Speech and Language Therapist (SLT) with more than 10 years of relevant experience assessing and diagnosing speech disorders. This human expert directly evaluated the severity of dysarthria in both real and synthesized speech samples."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only mentions evaluation by a single expert listener and does not report quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The human evaluator was a subject matter expert, thus this label does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that quality assurance involved multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance was performed by an AI model serving as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic QA specifically for the dataset annotations or synthesized content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented via the expert subjective evaluation conducted by an SLT; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10986,
      "completion_tokens": 433,
      "total_tokens": 11419
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any newly created data entirely by humans from scratch. The study uses the existing TORGO dysarthric speech dataset for training and validation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 3.2",
          "reasoning": "The paper introduces synthetic dysarthric speech data generated by training Grad-TTS models from scratch on existing dysarthric speech data (TORGO) to synthesize new speech samples with dysarthric characteristics. This synthetic data is newly generated by neural network models (diffusion probabilistic models) and used for data augmentation in ASR model fine-tuning."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The synthetic data is generated by models rather than collected or aggregated directly from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 3.2",
          "reasoning": "The synthetic data is derived from existing TORGO dysarthric dataset used to train Grad-TTS models. The models transform the original speech into new synthesized speech samples, thus the synthetic data is derived via transformation of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of generation of the new dataset is explicitly detailed in the paper."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11504,
      "completion_tokens": 554,
      "total_tokens": 12058
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper trains Grad-TTS models for speech synthesis from scratch, but these models are trained on existing TORGO dysarthric data rather than a newly introduced dataset. The paper does not introduce a new dataset used for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 and Section 4.2",
          "reasoning": "The synthetic dysarthric speech data generated by the authors' trained Grad-TTS models (text-to-dysarthric-speech synthesis) is used as augmented training data to fine-tune existing pre-trained Whisper ASR models for dysarthric speech recognition. This supervised fine-tuning approach is explicitly described and evaluated."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the dataset in reinforcement learning based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2.1",
          "reasoning": "The synthetic dysarthric speech data and real TORGO data are also used for evaluation purposes, measuring synthesis quality via mean cepstral distortion and human subjective evaluation, and assessing ASR model performance on dysarthric speech via word error rate metrics."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper analyzes the characteristics of the synthesized dysarthric speech data (e.g., severity ratings and synthesis quality) and studies the effect of augmented synthesized data on ASR performance for different speaker severity groups."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the synthesized or real datasets are used as a knowledge base for retrieval-augmented or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced (synthesized dysarthric speech data and splits from TORGO) have clear practical uses in fine-tuning ASR models and evaluation as detailed in the paper."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12227,
      "completion_tokens": 665,
      "total_tokens": 12892
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "No explicit mention of multiple human languages in the dataset.",
          "reasoning": "The paper describes dysarthric speech data from the TORGO dataset used for training and augmentation. All text and speech data are in English; no evidence of more than two human languages is present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "No section describes datasets with exactly two languages.",
          "reasoning": "The dataset used and synthesized data are exclusively in English. The paper does not mention the presence of a second human language or bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 and throughout the paper; also in Abstract and Introduction.",
          "reasoning": "The TORGO dysarthric speech dataset, on which the new text-to-dysarthric-speech synthesis models are trained from scratch, contains English speech only. All experimental evaluations including ASR adaptation are performed with English speech data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "No indication of any non-English single-language dataset.",
          "reasoning": "The dataset used is English speech only; no non-English datasets are mentioned or introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "While method section mentions code repositories, no dataset entries containing code are introduced.",
          "reasoning": "The new datasets introduced are speech and text pairs, not programming or structured code content. The paper only references speech and text data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Mathematical equations appear to describe model details but are not part of the dataset.",
          "reasoning": "Mathematical notation appears in the methodology sections describing model architecture, but the datasets themselves do not contain entries of mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No mention in the paper of biological or non-human communication data.",
          "reasoning": "The datasets introduced are speech data from human speakers with dysarthria, all human speech-based."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No mention of fictional or artificial languages in the datasets.",
          "reasoning": "All linguistic data used is natural English speech; no constructed languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Dataset language is clearly specified as English (Sections 3.2 and 4).",
          "reasoning": "The language of the dataset is clearly specified as English, so the language origin is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced contains speech and textual content, so language is present."
        }
      }
    }
  },
  {
    "id": "leung24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9445,
      "completion_tokens": 191,
      "total_tokens": 9636
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.1 Dysarthric Speech Synthesis and Section 2.1 Methodology",
          "reasoning": "The paper explicitly states that the Grad-TTS code adapted from https://github.com/huawei-noah/Speech-Backbones is used, and the implementation is available at https://github.com/WingZLeung/TTDS, indicating public availability of the code related to the dataset synthesis process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1 Dysarthric Speech Synthesis and 3.2 Text-to-Speech Synthesis",
          "reasoning": "The paper describes in detail the dataset creation process for the synthesized dysarthric speech data, including the training splits, conditions (all-speaker, single-speaker, severity-group models), data splits creation, and model training details, providing comprehensive documentation of the dataset generation procedure."
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6442,
      "completion_tokens": 175,
      "total_tokens": 6617
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1, 3.1, 3.2, 3.3, 3.4",
          "Reasoning": "The DDS dataset consists of real audio recordings of speech played through a high-quality loudspeaker and re-recorded using three different microphone devices in nine realistic environments, with multiple microphone positions. This is clearly stated in Section 1 (Introduction) describing the dataset as parallel recordings of high-quality and device-degraded speech, in Sections 3.1-3.4 detailing data collection from playback and re-recording with physical devices, thus human-involved recording. It is audio data modality because the dataset consists specifically of speech waveforms sampled at audio sampling rates (44.1 or 48 kHz)."
        }
      ]
    }
  },
  {
    "id": "li22e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7242,
      "completion_tokens": 218,
      "total_tokens": 7460
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and Figure 1",
            "reasoning": "The DDS dataset recordings were obtained by playing clean speech materials through a monitor loudspeaker and re-recording using different consumer-grade recording devices in various environments and positions. This process involves a deterministic, physical recording setup rather than human annotation. No mention of human annotators or manual labelers for the recordings is present in the paper."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any instructions provided for annotators, as the dataset was created by automatic replay and recording setup rather than human labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "Since there were no human annotators involved, no scoring rubrics were applied or described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are present because the dataset creation involved automatic recording procedures, not human labels or manual annotations."
          }
        }
      ]
    }
  },
  {
    "id": "li22e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8432,
      "completion_tokens": 317,
      "total_tokens": 8749
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert to validate the dataset annotations or recordings."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assuring the dataset or its annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's quality assurance process does not involve AI models; no such process is described in the paper."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4",
          "reasoning": "The paper describes using a cross-correlation algorithm to align recorded speech with the original clean speech, an automated process to ensure alignment quality. Additionally, objective measures (PESQ and ESTOI) were computed to analyze dataset quality. These automated verification steps serve as quality assurance processes for the dataset recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While no human annotation quality assurance is described, the paper does report automated alignment and objective analysis, so the dataset is not without any quality assurance."
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8042,
      "completion_tokens": 412,
      "total_tokens": 8454
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not consist of original speech content created entirely anew by humans; rather, it uses existing high-quality speech recordings played back and re-recorded."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data were generated entirely by AI or machine learning models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any data created through translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that machine translation was involved in creating the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset builds upon existing datasets (DAPS and VCTK), it does not simply collate existing data without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3 Dataset Overview",
          "reasoning": "The DDS dataset is constructed by playing existing high-quality speech datasets (DAPS and VCTK) through speakers and re-recording them in diverse realistic environments with multiple devices and microphone positions, thereby producing new realistic degraded versions of previously existing clean speech. This process transforms the original data through re-recording, adding environmental and device degradations, which qualifies as deriving data based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the dataset is clearly documented."
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8560,
      "completion_tokens": 304,
      "total_tokens": 8864
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 Baseline Experiments",
          "reasoning": "The DDS dataset is used as training data in multiple subsets (D1 to D5) to train speech enhancement models from scratch. The models are trained on the dataset to enhance device-degraded speech."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Baseline Experiments",
          "reasoning": "The DDS dataset is used as a test set containing 500 utterances recorded in a specific environment and device (livingroom1 by Uber Mic) to evaluate the performance of different speech enhancement baseline models trained on various subsets of DDS."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.5 Initial Analysis of DDS",
          "reasoning": "The authors perform analysis using objective speech quality scores (PESQ, ESTOI) across environments, devices, and positions on the DDS dataset to study the effects of recording factors on speech quality, demonstrating the dataset's diversity and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9283,
      "completion_tokens": 506,
      "total_tokens": 9789
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset DDS is based on English speech data from DAPS and VCTK datasets only, with no mention of other languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English speech recordings, so no two languages are included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The DDS dataset uses clean speech materials selected from DAPS and VCTK datasets, both of which contain professional English speech recordings only. The paper mentions the use of speech from 48 English speakers and no other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content is explicitly English speech; no other single non-English language is mentioned."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech recordings only, with no programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are audio speech recordings; there is no mathematical or logical notation included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech recordings only; no biological sequences or non-human communication systems are involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are present; only natural English speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly states and documents the dataset's language as English; no ambiguity or unknown language content exists."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken English language audio recordings, so it does contain language data."
        }
      }
    }
  },
  {
    "id": "li22e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6501,
      "completion_tokens": 181,
      "total_tokens": 6682
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "There is no explicit mention in the paper of code related to data collection, preprocessing or generation being made publicly available.",
          "reasoning": "The paper only provides a link to the dataset itself (https://doi.org/10.5281/zenodo.5464104) but does not mention the availability of any code repository or software used in the dataset construction or preprocessing. No sections or references discuss code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and subsections 3.1 to 3.4",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process in Section 3 Dataset Overview, including speech materials selection, environmental setup, devices and recording positions, and alignment procedure. There are detailed tables describing dataset settings and initial analysis, indicating thorough documentation of the dataset construction."
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7354,
      "completion_tokens": 214,
      "total_tokens": 7568
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Section 3.2",
          "Reasoning": "The TALCS corpus is described as a new dataset introduced by the authors, recorded from online one-to-one English teaching scenes with teachers' speech captured using personal computer microphones (human-involved recording). It is a mono-channel Mandarin-English code-switching speech corpus with natural dialogue, not read aloud, indicating real human speech recordings. This confirms the audio modality data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The TALCS corpus includes 370,000 manually transcribed sentences with careful annotation and quality control, performed by human annotators who passed CET-4. The transcriptions involve manual segmentation, text normalization, and quality inspection, indicating that the text data is human generated and linked to the audio recordings for speech recognition tasks."
        }
      ]
    }
  },
  {
    "id": "li22j_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8154,
      "completion_tokens": 291,
      "total_tokens": 8445
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 3.2 Corpus Transcription",
            "reasoning": "The annotation was performed by data annotators who have passed CET-4, which is an English proficiency test; they were responsible for dividing audio into sentences and transcribing each sentence. The paper states that only one annotator was assigned per class, indicating single human annotator per segment, and they are not described as experts but rather as qualified annotators with CET-4 level English."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Corpus Transcription",
            "reasoning": "Detailed instructions were provided regarding text normalization (capitalization of English words, spacing rules, normalization of digits to Chinese numerals, transcription rules about omitting sentences with mispronunciations), indicating instructions were given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2 Corpus Transcription",
            "reasoning": "There is no mention of scoring rubrics or explicit quantitative rubrics guiding annotation judgments; quality is ensured by manual checks and defined pass/fail thresholds rather than rubric-based scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2 Corpus Transcription",
            "reasoning": "The paper provides specific instructions and examples in text normalization but does not explicitly mention providing annotation examples as part of the guidelines to the annotators."
          }
        }
      ]
    }
  },
  {
    "id": "li22j_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9344,
      "completion_tokens": 276,
      "total_tokens": 9620
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.2 - Corpus Transcription",
          "reasoning": "The paper states that data annotation was done by annotators who have passed CET-4, implying they have sufficient expertise in English language to accurately transcribe code-switching utterances. Each recording (approximately 2 hours) was annotated by a single annotator, who divided the audio into sentences and transcribed them. This annotator is treated as an expert due to language proficiency requirements and their role."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 - Corpus Transcription",
          "reasoning": "After the annotation stage, a quality checking stage was conducted where data quality inspectors checked 20% of the data for consistency between speech and transcription. If errors exceeded 10%, reannotation was required. This represents a systematic, partially automated quality control process ensuring annotation reliability."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8954,
      "completion_tokens": 448,
      "total_tokens": 9402
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The TALCS corpus is described as collected from real online one-to-one teaching scenes involving teachers and students from different regions of China. The speech utterances were recorded by personal computer microphones during real classes, and the transcriptions were manually annotated by human annotators who passed CET-4, ensuring high-quality original human-generated content. The data is not translated or adapted from other datasets but recorded and transcribed from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any part of the TALCS corpus was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation from another language by human translators; the dataset consists of natural code-switching speech rather than translated content."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of any machine translation being used in producing the TALCS corpus."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The TALCS corpus is not described as being collected or aggregated from existing sources without modification. It is newly collected from a specific scenario."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The TALCS corpus is stated as a subset of the TAL-ASR corpus, indicating it is derived from this larger existing dataset, implying adaptation or selection from a pre-existing corpus to focus on code-switching scenarios."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is sufficiently specified and documented; thus this category does not apply."
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9472,
      "completion_tokens": 324,
      "total_tokens": 9796
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiments",
          "reasoning": "The TALCS corpus is used to train speech recognition models from scratch using popular ASR toolkits such as ESPnet and WenetSpeech. The paper describes training end-to-end ASR models on this corpus, indicating its use for model training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using TALCS corpus specifically for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning based post-training methods using the TALCS corpus are described."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiments",
          "reasoning": "The TALCS corpus is divided into training, development and test sets, with the test set used for evaluation and benchmarking the performance of the baseline speech recognition systems, as shown by the comparison of Mixture Error Rates (MER)."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not focus on analyzing trends or characteristics of the dataset beyond reporting baseline ASR results."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10195,
      "completion_tokens": 493,
      "total_tokens": 10688
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains exactly two human languages: Mandarin and English. There are no indications of more than two languages present."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 (Corpus profile) and Abstract",
          "reasoning": "The TALCS corpus is explicitly described as a Mandarin-English code-switching speech corpus, containing speech utterances mixing these two languages. It includes intra-sentence and inter-sentence code-switching between Mandarin and English, making it bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not English only; it specifically contains Mandarin and English utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not monolingual non-English; it includes both Mandarin and English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any programming or structured code-related content in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are mentioned in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are part of the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include artificial or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are specified and well documented as Mandarin and English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural human language data and thus is not language-free."
        }
      }
    }
  },
  {
    "id": "li22j_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7413,
      "completion_tokens": 147,
      "total_tokens": 7560
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not mention or provide any link or repository to code related to the TALCS corpus data collection, preprocessing, or generation. There is no explicit indication that such code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Corpus description, especially Section 3.1 and 3.2",
          "reasoning": "The paper documents the TALCS dataset creation process in detail, including recording procedure, corpus profile, data transcription and annotation procedures. Details include recording environment, speakers, transcription protocol, quality checking, and dataset splits, thus providing comprehensive documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9319,
      "completion_tokens": 106,
      "total_tokens": 9425
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Table 1",
          "Reasoning": "The LittleBeats (LB) dataset consists of day-long home audio recordings collected from families with children under 5 years old using the LB infant wearable multi-modal device. These recordings were manually labeled by human coders for speaker diarization and vocalizations, indicating human involvement in data generation as the audio is captured from real family interactions."
        }
      ]
    }
  },
  {
    "id": "li23e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10119,
      "completion_tokens": 247,
      "total_tokens": 10366
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 (Labeled data)",
            "reasoning": "The paper states that human coders manually labeled the in-domain LittleBeats (LB) home recordings with cross-coder validation and reports inter-coder reliability (Cohen's kappa), indicating multiple human annotators were involved for the new LB labeled dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not explicitly mention or provide detailed annotation instructions in the guidelines for annotators on the LB labeled dataset; it only describes the labels and process but does not specify instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not provide evidence of any scoring rubrics or criteria used during annotation for the LB labeled dataset; it only reports inter-coder reliability but no rubric details."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "No annotation examples are provided or referenced in the paper for the LB labeled data; the paper only summarizes annotation categories and process without example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "li23e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11309,
      "completion_tokens": 325,
      "total_tokens": 11634
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Labeled data",
          "reasoning": "The paper states that human coders manually labeled vocalizations and that 10% of segments were double-coded to ensure inter-coder reliability, with reported Cohen's kappa scores. This implies multiple human annotators were involved in the annotation process, suggesting multiple human experts performed quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that annotators lacked subject matter expertise; the annotators are referred to as human coders performing manual, precise annotation with cross-coder validation, implying expertise."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using AI models to judge or validate the annotations in a quality assurance capacity."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or quality assurance methods for validating the annotation labels in the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a quality assurance process involving human coders and cross-validation, so QA cannot be considered absent."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10919,
      "completion_tokens": 445,
      "total_tokens": 11364
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper states that human coders manually annotated in-domain LB home recordings by selecting 10-min segments with highest vocalization rates and labeling vocalizations with Praat, with cross-coder validation to ensure quality. This indicates original content created entirely from scratch by human contributors, not derived or translated from existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by translating content using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The unlabeled data consists of large amounts of day-long home audio recordings collected from families wearing LB or LENA devices, aggregated without significant modification. These recordings were collected from multiple families and aggregated into datasets used for pretraining."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The preprocessing involved resampling audio to 16kHz, applying voice activity detection to remove silent portions, dividing audio into 10s segments, and applying energy thresholding to out-of-domain labeled data. These modifications represent transformations of existing audio sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents data origin and processing clearly, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11437,
      "completion_tokens": 541,
      "total_tokens": 11978
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 3",
          "reasoning": "The authors collected a new large-scale unlabeled dataset of 1100h of in-domain LittleBeats (LB) home audio recordings from families and used this dataset exclusively for unsupervised pretraining of the wav2vec 2.0 (W2V2) model, as described in sections 2.1 and 3. This pretraining aims to learn family audio representation from unlabeled data before supervised fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study does not describe training models from randomly initialized parameters using the proposed dataset; rather, it uses pretraining plus fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3",
          "reasoning": "The authors manually labeled a portion of the LB in-domain dataset and used this labeled data to fine-tune the pretrained W2V2 model in a supervised manner for speaker diarization and vocalization classification tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that reinforcement learning or RL-based post-training methods (such as RLHF) are used with the proposed dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 4",
          "reasoning": "The labeled LB dataset is used as a test set for evaluating model performance in speaker diarization and vocalization classification, with leave-one-family-out splits."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper primarily uses the dataset for training and evaluation; no primary focus on using the dataset solely for analyzing data trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documented utility of the dataset in pretraining, fine-tuning, and evaluation."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12160,
      "completion_tokens": 536,
      "total_tokens": 12696
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the proposed dataset contains multiple human languages. It focuses on recordings from family audio but does not mention multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described to contain exactly two human languages; no mention of bilingual content is found."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (Data), particularly 2.1 and 2.2 Data description and annotation process",
          "reasoning": "The paper states recordings were collected from families in the U.S., and the annotations include vocalizations such as child-directed speech, adult-directed speech, which are in English. There is no mention of other languages, indicating the dataset is monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset contains only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is natural audio recordings; no code or programming language content is present."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio recordings and annotations; it does not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While infant cries and vocalizations are biological in origin, the dataset is annotated and treated as human language data, not as biological sequences or non-human communication systems like DNA or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is specified and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language in audio recordings."
        }
      }
    }
  },
  {
    "id": "li23e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9378,
      "completion_tokens": 157,
      "total_tokens": 9535
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided for code related to dataset construction.",
          "reasoning": "The paper mentions that code and model weights related to model training are available but does not provide any link or information about code for dataset collection or preprocessing; thus, code for constructing the dataset is not made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data) including subsections 2.1, 2.2, and 2.3.",
          "reasoning": "The paper provides detailed descriptions of data collection procedures, ethical approvals, participant recruitment, labeling protocols, inter-coder reliability statistics, preprocessing steps, and data partitioning. This comprehensive documentation sufficiently describes the dataset creation process."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8171,
      "completion_tokens": 113,
      "total_tokens": 8284
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Experimental datasets",
          "Reasoning": "The paper introduces two new datasets, NS-100 and LS-100, which are constructed by selecting samples from publicly available audio corpora NSynth and LibriSpeech, respectively. These corpora consist of audio recordings captured from real human sources (musical instruments and speakers). Therefore, the modality is audio, and the data is human generated through recording of real sounds."
        }
      ]
    }
  },
  {
    "id": "li23w_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8971,
      "completion_tokens": 272,
      "total_tokens": 9243
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 Experimental datasets",
            "reasoning": "The paper states that the datasets NS-100 and LS-100 are constructed by choosing samples from existing public audio corpora NSynth and LibriSpeech. The construction details are provided at the GitHub repository. There is no mention of human annotation, but selection and splitting are done programmatically, thus annotations are effectively performed by an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1 Experimental datasets",
            "reasoning": "The paper does not provide any description of annotation instructions or guidelines for annotators, as it uses samples selected from public corpora rather than human annotation tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 Experimental datasets",
            "reasoning": "There is no mention of scoring rubrics or criteria used for annotating or labeling data; the datasets are constructed by selection from existing labeled corpora, thus no rubrics are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1 Experimental datasets",
            "reasoning": "No examples of annotation or labeling are provided in the paper or appendix; the datasets are created by choosing samples from public corpora with existing labels, so no annotation examples are discussed."
          }
        }
      ]
    }
  },
  {
    "id": "li23w_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10161,
      "completion_tokens": 373,
      "total_tokens": 10534
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for the datasets NS-100 or LS-100."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any involvement of multiple human experts in quality assurance of the constructed datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating a single non-expert human performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details are provided regarding multiple non-expert human annotators conducting quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an automated verification process or rule-based techniques used for quality assurance of the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces two datasets, NS-100 and LS-100, constructed by selecting samples from publicly available NSynth and LibriSpeech corpora. However, the paper does not describe any quality assurance process or validation for the dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9771,
      "completion_tokens": 333,
      "total_tokens": 10104
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The datasets NS-100 and LS-100 are constructed by selecting samples from two publicly available audio corpora, NSynth and LibriSpeech, respectively. Thus, the data is collected and aggregated from existing publicly available sources without indications of significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "While the datasets are created from publicly available corpora, the authors have performed selection and splitting of classes and samples to create NS-100 and LS-100 datasets with specific class splits for incremental learning experiments, indicating the datasets are derived via transformation and adaptation of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states the data origin and construction method, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10289,
      "completion_tokens": 550,
      "total_tokens": 10839
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new datasets NS-100 and LS-100 exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2 (Method), Section 3.1 (Experimental datasets)",
          "reasoning": "The datasets NS-100 and LS-100 are constructed from NSynth and LibriSpeech corpora respectively by selecting classes and samples, and are used for training the embedding extractor (EE) and the stochastic classifier (SC) from scratch in the base session; the EE is trained supervised episodically and the SC is trained afterwards as described in Section 2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2 (Method), Section 3.1 (Experimental datasets)",
          "reasoning": "The datasets are used in incremental sessions where the stochastic classifier (SC) is incrementally expanded to recognize new classes using few samples, effectively serving as supervised fine-tuning of the classifier component on new classes."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of reinforcement learning or RL-based post-training methods such as RLHF with the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 (Experimental datasets), Section 3.3 (Experimental results)",
          "reasoning": "The datasets NS-100 and LS-100 are also used for evaluation and benchmarking the proposed method and baselines, as the paper reports accuracy and performance metrics on testing splits of these datasets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not primarily used for trend or characteristic analysis; no such analysis is reported."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases to augment models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes and demonstrates the use of the introduced datasets for training from scratch, supervised fine-tuning, and evaluation."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11012,
      "completion_tokens": 634,
      "total_tokens": 11646
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced, NS-100 and LS-100, are derived from NSynth (musical instrument sounds) and LibriSpeech (speech audiobooks) respectively. The speech corpus LibriSpeech consists of audiobooks in English only. There is no indication of multiple languages used in the datasets."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of exactly two human languages in the proposed datasets. LS-100 is based on English audiobooks solely and NS-100 includes musical instrument sounds which are non-linguistic audio."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental datasets",
          "reasoning": "The LS-100 dataset is derived from the LibriSpeech corpus which contains English audiobooks, implying the speech data is English only. Thus, the dataset contains only English content for language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The NS-100 dataset is based on NSynth musical instrument sounds, which are non-linguistic sounds, not in any human language. There is no mention of any non-English human language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are audio sample datasets and do not contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical expressions and formulas, the datasets themselves consist of audio recordings, not mathematical or logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 3.1 Experimental datasets",
          "reasoning": "The NS-100 dataset includes audio recordings of musical instrument sounds (non-human communication), which can be considered non-linguistic audio signals. While not biological sequences, they do represent non-human communication sounds."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages and audio types in the datasets are clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain language data (spoken English) in LS-100 and non-human audio in NS-100, so they cannot be considered as having no language."
        }
      }
    }
  },
  {
    "id": "li23w_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8230,
      "completion_tokens": 216,
      "total_tokens": 8446
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3.1",
          "reasoning": "The abstract mentions a code repository at 'https://github.com/vinceasvp/meta-sc' for reproducing experiments and dataset construction details. Also, in Section 3.1, the paper states that the construction details and explanations of the two new datasets (NS-100 and LS-100) are introduced at this GitHub link, indicating that code for dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and Abstract",
          "reasoning": "Section 3.1 provides detailed information about how the NS-100 and LS-100 datasets were constructed by selecting samples from the public NSynth and LibriSpeech corpora, including class splits, number of samples per class, and information about training and testing splits. The paper also refers readers to the GitHub repository for additional construction details and metadata, indicating that the dataset creation process is documented both in the paper and via supplementary online resources."
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8285,
      "completion_tokens": 119,
      "total_tokens": 8404
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1: Data description",
          "Reasoning": "The CN-Celeb-AV dataset is composed of video segments collected from public media (Bilibili), involving real-world audio-visual content of people. Videos are recorded by humans or captured from human-operated devices, making the data human-generated. The dataset contains over 420k video segments from 1,136 individuals, explicitly described as collected in the wild from a publicly accessible platform."
        }
      ]
    }
  },
  {
    "id": "li23y_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9085,
      "completion_tokens": 247,
      "total_tokens": 9332
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3, 'Step 3. Human check with online auxiliary system'",
            "reasoning": "The paper states human annotators check video segments and label acceptance, and a senior annotator double-checks all accepted segments. The mention of a crowd-sourcing platform implies multiple annotators who are likely non-experts rather than declared experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3, Step 1 and Step 3 descriptions",
            "reasoning": "The annotation process includes specific acceptance criteria (humans can tell POI existence from at least one modality) and use of an online platform, indicating instructions were provided to guide annotators' labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No annotation rubrics explicitly described",
            "reasoning": "The paper does not mention detailed scoring rubrics or grading scales; annotation involves binary accept/reject labels without an explicit rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples explicitly provided",
            "reasoning": "The paper does not provide examples of annotated segments or labels in the guidelines or appendices."
          }
        }
      ]
    }
  },
  {
    "id": "li23y_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10275,
      "completion_tokens": 534,
      "total_tokens": 10809
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance was conducted by a single human annotator who is an expert. Annotation and checking were performed by humans but no evidence indicates the individual was a subject matter expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human annotators with subject matter expertise performed quality assurance. Human check is described but there is no indication the annotators were experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information suggests that quality assurance was done by a single human non-expert. Human checks involved multiple annotators rather than a single person."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "Section 3.3 describes that multiple human annotators performed the human check and labeling of video segments using an online platform. The annotators worked to label segments as accepted or rejected and a senior annotator performed double-checking. There is no indication that these annotators possessed subject matter expertise; the paper explicitly mentions human check to confirm valid segments without claiming expert status."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "Automated processing models including MTCNN for face detection, InsightFace for face verification, and ECAPA-TDNN for speaker verification were used in Step 2 of the collection pipeline to automatically process and score candidate video segments prior to human checking. These AI models acted as automated judges in the quality assurance process."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "An automatic process using algorithmic models such as MTCNN, InsightFace, and ECAPA-TDNN was used to automatically extract candidate segments and filter out low-scoring ones before human annotation. This represents an automated verification step as part of the QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a clear quality assurance process involving both automatic processing and human checking, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9885,
      "completion_tokens": 404,
      "total_tokens": 10289
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of video segments collected from public media (Bilibili) featuring real-world videos of persons, not data created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data consists of real-world videos collected from public media, not generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation systems is present in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.3",
          "reasoning": "The dataset was collected from existing public videos on the Bilibili platform (public media), as stated in Section 3.1 and detailed in the collection pipeline in Section 3.3. The data is aggregated from these existing sources without significant alteration to the original content."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset involves some processing (automatic extraction, human checking), the core data itself is collected videos rather than data derived or transformed from previously existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and collection method of the data is specified and well documented in the paper."
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10403,
      "completion_tokens": 512,
      "total_tokens": 10915
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the CN-Celeb-AV dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the CN-Celeb-AV dataset is used to train models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using CN-Celeb-AV to fine-tune pre-trained models in a supervised manner."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training techniques are mentioned in relation to CN-Celeb-AV."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The CN-Celeb-AV dataset is primarily used as an evaluation benchmark to test the performance of audio-visual person recognition systems under real-world, unconstrained conditions. Section 4 details the evaluation experiments conducted using CN-Celeb-AV evaluation sets, comparing performance with other datasets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 (Results) and Section 3.2 (Features of CN-Celeb-AV)",
          "reasoning": "The paper analyzes the characteristics and challenges presented by CN-Celeb-AV, discussing the complexity of the dataset and its suitability for real-world AVPR research. The results also analyze system performances, illustrating dataset challenges."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base for retrieval-augmented or similar model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the CN-Celeb-AV dataset, particularly for evaluation and analysis purposes."
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11126,
      "completion_tokens": 274,
      "total_tokens": 11400
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data description",
          "reasoning": "The CN-Celeb-AV dataset contains video segments collected from Bilibili, a Chinese public media platform, and features mostly Chinese celebrities. It covers 11 genres and is intended for Chinese speaker recognition research (see reference to CN-Celeb datasets). There is no indication of multiple languages or bilingual content. Thus, the dataset entries are only in one language, which is Chinese, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li23y_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8344,
      "completion_tokens": 206,
      "total_tokens": 8550
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.3 Collection pipeline",
          "reasoning": "The paper describes the data collection pipeline in detail and mentions that the source code of a crowd-sourcing platform for video annotation and human check will be published on the dataset webpage. However, no direct link or repository for the full code used for the entire dataset construction, including data collection, preprocessing, and generation, is provided within the paper. Therefore, there is no evidence that all code related to dataset construction is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.3 Collection pipeline",
          "reasoning": "The paper thoroughly documents the dataset creation process in Section 3.3, providing detailed descriptions of the manual video selection, automatic data processing, and human check steps. It also explains the rationale behind the pipeline design to handle partial modality data and discusses considerations like avoiding modality bias. Hence, the dataset creation process is transparently and comprehensively documented in the paper."
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7885,
      "completion_tokens": 243,
      "total_tokens": 8128
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, 'The PunCantonese Corpus'; especially Sections 2.1 'Cantonese Speech Transcripts' and 2.2 'Written Text from Cantonese Wikipedia'",
          "Reasoning": "The paper introduces the PunCantonese corpus, a new dataset primarily composed of text data. This corpus is collected from annotated spoken Cantonese transcripts and written-style Wikipedia sentences. The spoken transcripts originate from Common-Voice and PyCantonese datasets, which are human-generated speech transcripts that have been manually punctuated. The written text data is obtained from Cantonese Wikipedia, which is also human-generated content authored by Wikipedia contributors. Thus, the dataset modality is text. The data is human-generated because the spoken transcripts are transcriptions of human speech, and the Wikipedia text is authored by humans. The paper does not indicate any automatic or model-based generation of this data, nor does it treat the data origin as unknown or derived from web crawl without clarification. Therefore, the data is of text modality, human-generated, not model-generated, and not of unknown origin."
        }
      ]
    }
  },
  {
    "id": "li23z_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8685,
      "completion_tokens": 370,
      "total_tokens": 9055
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 (PunCantonese Corpus) and 2.1, 2.2 (Cantonese Speech Transcripts and Written Text from Cantonese Wikipedia)",
            "reasoning": "The paper states that the PunCantonese corpus is compiled from three sources: Common-Voice and PyCantonese spoken transcripts with punctuations, and the written Cantonese Wikipedia. The Wikipedia data are publicly available, cleaned and pre-processed automatically (e.g., filtering sentences by length, punctuation presence). The paper only mentions filtering and processing steps for data collection, and does not mention any human annotation process for these datasets or for additional punctuation annotation. Since these datasets were collected from publicly annotated or existing resources and processed automatically, the annotations are likely from automatic or existing sources rather than manual new annotations by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2 (PunCantonese Corpus)",
            "reasoning": "No mention of annotation instructions or guidelines given to annotators in the paper. The paper describes data collection and preprocessing but does not specify any set of instructions provided for annotation in this new corpus creation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 (PunCantonese Corpus)",
            "reasoning": "The paper does not mention scoring rubrics or guidelines for labeling or quality assessment of annotations in the new corpus."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2 (PunCantonese Corpus) and Table 1",
            "reasoning": "Table 1 shows example sentences from the PunCantonese corpus, but these are examples of the data itself, not examples provided as annotation guidelines to annotators. The paper does not mention providing annotation examples as guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "li23z_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9875,
      "completion_tokens": 375,
      "total_tokens": 10250
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert or subject matter expert for the PunCantonese corpus."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts or annotators with subject matter expertise validated the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide details about multiple human non-experts performing quality checking or annotation validation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any AI model being used as a judge or for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance processes applied to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process applied on the PunCantonese corpus, nor any validation or annotation correction procedures. The dataset is compiled from existing sources and Wikipedia text with pre-processing, but no QA process is described."
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9485,
      "completion_tokens": 445,
      "total_tokens": 9930
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being fully created from scratch by human contributors; it is assembled primarily through sourcing existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any part of the dataset was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made that the dataset involves translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate parts of the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (The PunCantonese Corpus), Sections 2.1 and 2.2",
          "reasoning": "The PunCantonese corpus is compiled by collecting and aggregating data from existing sources: the publicly available spoken Cantonese datasets Common-Voice and PyCantonese, and Cantonese Wikipedia. These data sources were collected without significant modifications beyond filtering and cleaning, indicating the dataset is a collation of existing data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 (Written Text from Cantonese Wikipedia)",
          "reasoning": "The dataset from Cantonese Wikipedia underwent preprocessing including sentence division, filtering of sentences without full punctuation, replacing or removing certain punctuation marks, and removing sentences with certain length criteria and non-Cantonese/English languages. This shows the data is derived from an existing source through modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the dataset are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10003,
      "completion_tokens": 208,
      "total_tokens": 10211
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3 Experimental Results; Section 3 Approach",
          "reasoning": "The PunCantonese dataset is used to fine-tune a pre-trained multilingual Transformer-based language model for the downstream task of Cantonese punctuation restoration, as described in Section 3 and evaluated with experimental results in Section 4.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10726,
      "completion_tokens": 508,
      "total_tokens": 11234
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The PunCantonese corpus contains only Cantonese and English (code-switched sentences), no more than two human languages are present."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2, Table 1, and Section 2.3",
          "reasoning": "The PunCantonese dataset contains sentences in Cantonese and English, including code-switched examples with both languages per sentence. The dataset only covers these two human languages explicitly, as stated in Section 2 and exemplified in Table 1."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not solely English; it contains Cantonese as the primary language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though Cantonese is present, the dataset also includes English code-switched sentences, so it is not purely monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only human language transcripts, no biological or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are present in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the dataset are clearly specified as Cantonese and English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data in Cantonese and English."
        }
      }
    }
  },
  {
    "id": "li23z_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7944,
      "completion_tokens": 175,
      "total_tokens": 8119
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention of code availability in the paper.",
          "reasoning": "The paper does not contain any links, URLs, or sections discussing the public availability of code used for data collection or dataset construction. There is no indication that related code has been released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 - The PunCantonese Corpus (pages 1-4).",
          "reasoning": "The paper provides detailed documentation on the dataset creation process including data sources (Common-Voice, PyCantonese, Cantonese Wikipedia), preprocessing steps, filtering criteria, splitting into train/validation/test, statistics and challenges related to the dataset. This is comprehensively documented within Section 2 and sub-sections 2.1, 2.2, and 2.3."
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6482,
      "completion_tokens": 292,
      "total_tokens": 6774
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2 and 2.2",
          "Reasoning": "The subcategory taxonomy textual descriptions were generated with the assistance of large language models, specifically GPT-4, as described in Sections 2.1.2 and 2.2. The text descriptions for subcategories and adjectives used for similarity calculations are outputs from the LLM, thus model generated."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2, paragraph 'Step 1: Connect text and image features' and Figure 4",
          "Reasoning": "Images are visual frames randomly selected from original audio-visual datasets (such as VGGSound) and are matched with text and audio features. These images are captured data involving human-recorded content, hence human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2, especially the description of automatic matching and End Product paragraph",
          "Reasoning": "The audio clips originate from existing datasets (e.g., VGGSound), which consist of recordings of real-world sounds captured via human-involved processes (e.g., recordings of animals, tools, environment). Therefore, the audio modality in the dataset is human generated."
        }
      ]
    }
  },
  {
    "id": "li24ha_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7282,
      "completion_tokens": 197,
      "total_tokens": 7479
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 and 2.2",
            "reasoning": "The paper describes that the DiveSound dataset is constructed via an automatic pipeline involving LLMs (GPT-4) for taxonomy construction and pretrained models CLIP and CLAP for multimodal matching without manual human annotation intervention."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide any explicit annotation instructions for human annotators, as the dataset is constructed by an automatic pipeline."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria provided for annotators, since annotation is automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not include annotation examples or guidelines demonstrating annotations, as the data matching is fully automated via model computations."
          }
        }
      ]
    }
  },
  {
    "id": "li24ha_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8472,
      "completion_tokens": 308,
      "total_tokens": 8780
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human expert performing quality assurance on the dataset or its annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about a single non-expert human annotator conducting quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any multiple human non-expert annotators involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of an AI model as a judge for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Automatic Text-Audio-Image Data Matching Process",
          "reasoning": "The dataset construction uses an automatic matching process involving pretrained models CLIP and CLAP to align text, audio, and image data, and large language model GPT-4 to generate taxonomy and subcategories. This automated pipeline validates data quality by matching features and discarding subcategories with insufficient audio samples, constituting an automated quality assurance process rather than manual annotation verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented and performed automatically during dataset construction."
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8082,
      "completion_tokens": 501,
      "total_tokens": 8583
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new data was created entirely from scratch by human contributors. Instead, it discusses automatically constructing a dataset based on existing sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While large language models and pre-trained models are used to generate subcategory labels and assist data matching, the resulting dataset contains paired audio, text, and image clips from existing sources rather than data generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation of data from other languages by human translators in the dataset construction."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify use of machine translation for dataset creation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Automatic Construction of Diverse Subcategory Taxonomy), especially 2.2 (Automatic Text-Audio-Image Data Matching Process)",
          "reasoning": "The dataset is constructed by automatically matching audio, image, and text features from the existing VGGSound dataset using pretrained models (CLIP, CLAP) and LLM-based taxonomy generation. This involves collecting and aggregating data from an existing source without significant modification of the audio or images themselves."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 (Characterizing Sound Diversity Taxonomy with LLM)",
          "reasoning": "The new taxonomy with diverse subcategories is created by reclassifying and regrouping existing VGGSound labels using GPT-4, generating new class groupings and subcategory labels which transform and adapt the existing label set into a more diverse taxonomy oriented toward multimodal distinguishability. This represents modifications and adaptations of existing data labels to form the new taxonomy."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation process is well documented in the paper; therefore, N/A category does not apply."
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8600,
      "completion_tokens": 359,
      "total_tokens": 8959
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3. Diverse Audio Generation Model",
          "reasoning": "The newly constructed DiveSound dataset is used to train a latent diffusion model for text-to-audio generation from scratch; the paper describes model training details including the use of the dataset to train the latent diffusion model (LDM) with modality fusion input."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3. Evaluation Metrics and Section 3.4. Results",
          "reasoning": "The DiveSound dataset is used to evaluate and benchmark audio generation quality and diversity through both objective metrics (e.g. FAD, MSD) and subjective Mean Opinion Score (MOS), demonstrating improvements in diversity and quality when training with the dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9323,
      "completion_tokens": 544,
      "total_tokens": 9867
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is constructed based on textual descriptions and labels in English only, with no indication of entries in more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or statement indicating the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 2.2",
          "reasoning": "The dataset's labels and textual descriptions are processed and generated using GPT-4 in English, and the constructed taxonomy with subcategories and matching text-audio-image data pairs are primarily represented in English. There is no mention of any other human language used in data entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly based on English labels and descriptions; no non-English languages are used exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code snippets are present as entries in the dataset; code is only described as used in methodology but is not part of dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulas or logical notation appear only in method descriptions, not as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some sounds are from animals, the dataset entries are human language text labels describing these sounds; they are not biological sequences or non-human symbolic communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of dataset entries is explicitly English; it is clearly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries include text labels and descriptions in English, so it does contain language."
        }
      }
    }
  },
  {
    "id": "li24ha_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6541,
      "completion_tokens": 158,
      "total_tokens": 6699
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit reference to code availability is provided in the paper.",
          "reasoning": "The paper mentions that the DiveSound framework constructs the new dataset automatically using LLMs and pretrained models, but there is no explicit mention or link to publicly available code or repository for reproducing the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, especially subsection 2.1 and 2.2.",
          "reasoning": "The paper provides a detailed description of the dataset construction process using GPT-4 for clustering and subcategory reasoning, and utilizes CLIP and CLAP for automatic text-audio-image data matching. Steps are described clearly with figures and methodology, enabling understanding of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6498,
      "completion_tokens": 93,
      "total_tokens": 6591
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data",
          "Reasoning": "The paper uses the MUStARD++ dataset composed of audiovisual utterances extracted from American TV sitcoms, which involves human speech recordings. The data is human-generated audio collected from television shows, manually labeled for sarcasm and emotions, indicating human involvement in data creation and annotation."
        }
      ]
    }
  },
  {
    "id": "li24ma_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7298,
      "completion_tokens": 226,
      "total_tokens": 7524
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Annotation",
            "reasoning": "The annotation process was conducted by two graduate students specializing in linguistics who independently labeled utterances; their inter-annotation agreement (Kappa score: 0.6042) indicates multiple expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Annotation",
            "reasoning": "Annotators were instructed to identify lexically meaningful words and phrases and to label utterances as neutral or non-neutral guided by video and context, indicating that specific instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 Annotation",
            "reasoning": "There is no explicit mention or description of detailed scoring rubrics or criteria specifying graded scoring; only a binary labeling (neutral or non-neutral) is mentioned without detailed rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 Annotation",
            "reasoning": "No examples of annotations or sample labels were provided or described in the paper for the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "li24ma_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8488,
      "completion_tokens": 478,
      "total_tokens": 8966
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotation",
          "reasoning": "The annotation process involved two graduate students specializing in linguistics, indicating subject matter expertise. They independently labeled the utterances, achieving substantial inter-annotator agreement (Kappa score: 0.6042). This suggests that multiple experts annotated, but since the rubric does not allow both 'Single Human Expert' and 'Multiple Human Experts', and they had two annotators, the appropriate category is 'Multiple Human Experts'. Hence, 'Single Human Expert' is not applicable."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotation",
          "reasoning": "Two graduate students specializing in linguistics performed the annotation independently, indicating multiple human experts were involved. Their expertise and the measurement of inter-annotator agreement support the reliability of the annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators were graduate students in linguistics, indicating expertise, so single non-expert labeling does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotators were experts (graduate students in linguistics), so this category does not apply."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model used for quality assurance of annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "Section 2.3 Acoustic analyses",
          "reasoning": "While some automatic tools were used for audio processing and alignment (e.g., Gentle), these were for audio processing rather than quality assurance of annotations. The paper does not describe automated verification as part of annotation quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process for the dataset annotations is documented and described, including expert multiple annotators and agreement measurement."
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8098,
      "completion_tokens": 421,
      "total_tokens": 8519
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used in the paper is MUStARD++, which is an existing dataset compiled from American TV sitcoms and not newly created entirely by human contributors from scratch by the authors of this paper."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new data was generated entirely by AI or machine learning models. The authors analyze an existing dataset rather than generating new data automatically."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or statement in the paper that the data was produced through human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of machine translation for data generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data",
          "reasoning": "The MUStARD++ dataset used was compiled from audiovisual utterances extracted from American TV sitcoms, indicating data is collected or aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotation",
          "reasoning": "The authors derived a new labeled subset by selecting key phrases within the existing sarcastic utterances, and further searched for neutral utterances containing these key phrases spoken by the same speaker, then manually annotated these for sarcasm or neutrality, applying transformations and annotations on the existing dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8616,
      "completion_tokens": 266,
      "total_tokens": 8882
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3",
          "reasoning": "The paper uses the MUStARD++ dataset, which includes audiovisual sarcastic and non-sarcastic utterances annotated with sarcasm types and acoustic features, primarily for analyzing prosodic and semantic cues of sarcasm. The dataset is employed for statistical analysis of prosodic patterns and their interplay with semantic cues rather than for model training or evaluation, as indicated in Sections 2.2 (Annotation) and 3 (Results and Discussion). This supports that the dataset's utility is primarily for analysis of trends, patterns, and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9339,
      "completion_tokens": 413,
      "total_tokens": 9752
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data",
          "reasoning": "The dataset used is MUStARD++, containing utterances from American TV sitcoms, all in English language; no indication of additional languages is given."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data",
          "reasoning": "The paper does not mention the dataset containing exactly two human languages; only English content is described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data",
          "reasoning": "The MUStARD++ dataset is described as comprised of utterances extracted from American TV sitcoms; all examples and contexts are in English, with no indication of other languages involved."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2.1 Data",
          "reasoning": "The dataset is from American TV shows and contains English utterances; no non-English single language dataset is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audiovisual utterances and annotations; there is no mention of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of mathematical or formal logical expressions as content in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset entries are human speech from TV shows, no biological sequences or non-human communication included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves natural English from TV shows; constructed or fictional languages are not mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly identified as English from American TV sitcoms; language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken utterances and annotations, clearly involving human language."
        }
      }
    }
  },
  {
    "id": "li24ma_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6557,
      "completion_tokens": 208,
      "total_tokens": 6765
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 (Methodology)",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to the dataset construction, data cleaning, annotation tools, or preprocessing steps. The paper references external tools (e.g., voice fix tools and Gentle) but does not share any custom scripts or code repositories developed for the dataset creation or annotation process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Data) and 2.2 (Annotation)",
          "reasoning": "The paper documents the use of the MUStARD++ dataset, including its origin, size, and annotation categories. It also details the annotation methodology for key phrases with procedures, annotator agreement, and the filtering process for selecting neutral utterances. Acoustic preprocessing steps and tools are described, along with the statistical analyses performed. While the dataset itself is pre-existing, the paper clearly documents the additional annotation and processing steps undertaken by the authors."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9759,
      "completion_tokens": 221,
      "total_tokens": 9980
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data collection and 2.2 ASR corpus construction",
          "Reasoning": "The MSR-86K corpus comprises 86,300 hours of transcribed ASR audio derived from publicly accessible YouTube videos. The audio is from real human speech captured in these videos, downloaded as single-channel wav files sampled at 16 kHz. Since the videos are originally recorded by humans and publicly accessible, the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data collection and 2.2 ASR corpus construction",
          "Reasoning": "The transcriptions in MSR-86K are obtained from manually uploaded video subtitles from the same publicly accessible YouTube videos. The subtitles are manually created by humans and then downloaded along with the audio. The corpus includes normalized text obtained by processing these subtitles, hence the text modality is human generated."
        }
      ]
    }
  },
  {
    "id": "li24s_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10559,
      "completion_tokens": 215,
      "total_tokens": 10774
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 (Corpus Construction)",
            "reasoning": "The MSR-86K corpus annotations are generated automatically by processing publicly accessible YouTube videos with manually uploaded subtitles. The annotations involve automatic transcription alignment and filtering steps using pre-trained ASR and language identification models as described in Section 2. There is no indication of human annotators creating or verifying annotations manually."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention providing detailed annotation instructions to annotators, as the annotation process is automatic without human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics for annotation quality are described, since the annotation is conducted by automated processes rather than human judgments."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention of annotation examples provided for human annotators because the annotation is performed automatically through subtitle collection and forced alignment."
          }
        }
      ]
    }
  },
  {
    "id": "li24s_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11749,
      "completion_tokens": 409,
      "total_tokens": 12158
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of QA by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple non-expert humans conducting QA."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.2, ASR filter and LID filter",
          "reasoning": "The paper describes using a language identification (LID) model as a filter to remove mismatched subtitles and audio, and an ASR model trained to calculate word error rates to filter low-quality segments. Both these models serve as AI models used for automatic quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2, Forced alignment and Duration balance",
          "reasoning": "Quality assurance involves automated forced alignment using a pretrained CTC-based ASR model to re-align subtitles with audio, and voice activity detection (VAD) for duration balancing. These are algorithmic processes that automatically verify and improve the quality and alignment of transcriptions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents several quality assurance steps involving automated processes and AI models."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11359,
      "completion_tokens": 445,
      "total_tokens": 11804
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset MSR-86K is not created entirely from scratch by human contributors; it is collected from existing publicly accessible YouTube videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models; it is collected from real-world audio and subtitle data available on YouTube."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of human translation being used to produce the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset construction process does not involve machine translation of content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data collection",
          "reasoning": "The MSR-86K corpus is derived from publicly accessible videos on YouTube, collected by retrieving video IDs, downloading available audio and subtitles, and aggregating them without significant manual modification. The paper describes the use of keyword searching, video ID deduplication, filtering, and downloading of public data, indicating aggregation of existing data sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 ASR corpus construction",
          "reasoning": "The original subtitles from YouTube videos are further processed, including text normalization, forced alignment using a pre-trained ASR model, language identification filtering, word error rate-based filtering, and voice activity detection-based segmentation. These processing steps apply transformations and adaptations to the aggregated raw data, producing a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the dataset's source and generation process, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11877,
      "completion_tokens": 456,
      "total_tokens": 12333
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 2.3; Section 3.2",
          "reasoning": "Section 2.3 describes the collection of unsupervised audio data used for unsupervised pre-training, and Section 3.2 explains the use of unsupervised pre-training with the MSR-86K and other data to train a model (HuBERT). This shows MSR-86K data contributes to pre-training."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe training a model from randomly initialized parameters solely on MSR-86K; training involves pre-training then fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "Section 3.2 details fine-tuning the pre-trained model with the MSR-86K supervised transcribed data and other open-source corpora to build a robust multilingual ASR model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or demonstration of reinforcement learning post-training techniques such as RLHF using MSR-86K is provided in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Section 3.1 uses the MSR-86K development set to evaluate the quality of the corpus and monolingual models, thus it is used for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Section 3.1 analyzes the corpus quality, language distribution, and error rates, indicating use of MSR-86K for analysis of dataset characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the MSR-86K dataset serving to augment models as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes multiple practical uses of MSR-86K for pre-training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12600,
      "completion_tokens": 570,
      "total_tokens": 13170
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Section 2.2, Table 2",
          "reasoning": "The MSR-86K corpus introduced in the paper contains transcribed ASR data for 15 human languages including Spanish, Korean, English, French, German, Hindi, Vietnamese, Italian, Dutch, Portuguese, Thai, Russian, Indonesian, Japanese, and Arabic. This is explicitly stated in the abstract, detailed in Section 2.2 where the language durations are listed, and shown in Table 2. Therefore, the dataset is multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the dataset containing more than two languages, specifically 15 languages; hence, it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MSR-86K corpus includes multiple languages; it is not restricted to only English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises multiple languages and is not confined to exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication in the paper that the dataset contains programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of transcribed speech data; there is no indication of mathematical or logical notation included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication systems are mentioned or included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned as part of the MSR-86K corpus."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included in the dataset are explicitly listed, so the language(s) are not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language transcriptions; therefore, this category does not apply."
        }
      }
    }
  },
  {
    "id": "li24s_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9818,
      "completion_tokens": 185,
      "total_tokens": 10003
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided",
          "reasoning": "The paper mentions that the MSR-86K corpus will be publicly released on HuggingFace but does not provide any link or statement indicating that the code used for data collection, processing, and generation is publicly available. There is no indication that the scripts or codebases used to construct the dataset have been shared in a repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Corpus Construction",
          "reasoning": "The paper provides a detailed description of the dataset construction process in Section 2, including data collection methods (keyword creation, video retrieval, subtitle detection), transcription and filtering, forced alignment, duration balancing, language ID filtering, ASR filtering, data split, and unsupervised corpus creation. This documentation is fairly comprehensive to allow understanding of how the dataset was built."
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8270,
      "completion_tokens": 79,
      "total_tokens": 8349
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Speech recordings",
          "Reasoning": "The authors recorded speech signals from 86 participants reading aloud a German standard passage using a head microphone in a controlled environment, implying the raw audio data is human-generated through human speech production and human recording."
        }
      ]
    }
  },
  {
    "id": "liebig22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9070,
      "completion_tokens": 255,
      "total_tokens": 9325
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Listening experiment",
            "reasoning": "The paper states that a group of 28 laypersons (non-expert listeners) rated the femininity/masculinity of the speech samples. These laypersons performed the annotation for the new speech samples recorded in this study."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Listening experiment",
            "reasoning": "The listening experiment was designed using Praat with a defined 6-point Likert scale from 1 (very masculine) to 6 (very feminine), which implies structured instructions were given to annotators for the rating task."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Listening experiment",
            "reasoning": "The use of a specific 6-point Likert scale (1 to 6) for rating femininity/masculinity acts as a scoring rubric guiding how annotators assigned ratings."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not provide explicit mention or description of example annotations or annotated instances provided to raters in the guidelines or experimental setup."
          }
        }
      ]
    }
  },
  {
    "id": "liebig22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10260,
      "completion_tokens": 413,
      "total_tokens": 10673
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that quality assurance (QA) was performed by a single human expert annotator on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Listening experiment; Section 3.1 Rater reliability",
          "reasoning": "The listening experiment involved 28 laypersons and 13 experts, including speech therapists, who rated the femininity/masculinity of speech samples. The expert ratings (from multiple human experts) were used to assess rater reliability. This indicates QA was performed by multiple human experts who validated the ratings and hence the annotation quality."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided of QA conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Listening experiment; Section 3.1 Rater reliability",
          "reasoning": "A group of 28 laypersons, who are non-expert human annotators, rated the femininity/masculinity of the speech samples. Multiple non-expert human raters contributed evaluations, indicating QA by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any AI model was used as a judge or for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses automatic feature extraction methods and automated regression models, it does not describe automated verification or algorithmic quality assurance of the annotation labels or rating data themselves. Therefore, 'Automatic Process' QA is not applicable."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a clear QA process involving multiple human raters (experts and non-experts) for the annotation of femininity/masculinity ratings, so labeling 'N/A' is not appropriate."
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9870,
      "completion_tokens": 229,
      "total_tokens": 10099
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Speech recordings",
          "reasoning": "The paper states that 86 participants were recruited and recorded reading aloud a German standard passage ('Der Nordwind und die Sonne'). This data was collected by the authors specifically for this study, involving human speakers reading a set text, constituting original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 Feature Extraction",
          "reasoning": "The acoustic features (e.g., fundamental frequency, vocal tract length) were derived from the recorded speech samples using signal processing and analysis methods, representing data transformations applied to the original recorded data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10388,
      "completion_tokens": 286,
      "total_tokens": 10674
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.4",
          "reasoning": "The paper describes using the newly collected speech dataset of 86 speakers as training data for three regression models to predict femininity/masculinity based on extracted acoustic features. The models were trained from scratch on these data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The dataset was also used for model evaluation through k-fold cross-validation and leave-one-out cross-validation approaches to assess model performance on predicting femininity/masculinity."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, 4",
          "reasoning": "The dataset was analyzed for rater reliability, feature correlations, and to understand the importance of acoustic features for gender perception, as discussed in the correlation and discussion sections."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11111,
      "completion_tokens": 351,
      "total_tokens": 11462
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Speech recordings",
          "reasoning": "The new dataset introduced consists of recordings of 86 speakers reading aloud a German standard passage \u201cDer Nordwind und die Sonne\u201d. All participants were native German speakers. Hence, the dataset is monolingual in a non-English language, specifically German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liebig22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8329,
      "completion_tokens": 164,
      "total_tokens": 8493
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper regarding code availability.",
          "reasoning": "The paper does not mention any publicly available code or repository related to the dataset collection, preprocessing, or generation. There is no indication of code release or link to external resources."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method) with subsections 2.1 Speech recordings and 2.3 Feature Extraction.",
          "reasoning": "The dataset creation is documented in detail in Section 2, describing the recruitment of participants, recording setup, ethical approval, and acoustic feature extraction methods. The process of collecting speech samples from 86 participants and the methodology for feature extraction is transparently documented, enabling reproducibility of data collection and processing steps."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8461,
      "completion_tokens": 117,
      "total_tokens": 8578
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 (Corpus Description)",
          "Reasoning": "The NMSQA dataset introduced in the paper consists of spoken question answering data. The training and development sets are synthesized using Amazon Polly TTS service (model generated audio), while the test set contains real human speech recorded from 60 human speakers (human generated audio). Thus, the dataset's audio modality is both human-generated (test set) and model-generated (train/dev sets)."
        }
      ]
    }
  },
  {
    "id": "lin22c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9261,
      "completion_tokens": 214,
      "total_tokens": 9475
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "In Section 3.1, it is stated that the answer intervals were annotated by Montreal Forced Aligner [30], which is an automatic alignment tool, implying annotation was performed by an automatic process rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide any details or mention of annotation instructions given to annotators; since the alignment was performed by an automatic process, instructions for manual annotation are not applicable."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No scoring rubrics or evaluation criteria used by annotators are mentioned for the annotation process as it was automatic."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No annotation guideline examples are given or referenced in the paper, consistent with use of an automatic annotation method."
          }
        }
      ]
    }
  },
  {
    "id": "lin22c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10451,
      "completion_tokens": 291,
      "total_tokens": 10742
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single human expert annotator for the new NMSQA dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple human experts performed quality assurance on the NMSQA dataset or its annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the involvement of multiple human non-expert annotators in QA of the NMSQA dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that an AI model was used as a judge for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The answer intervals in the NMSQA dataset were annotated by Montreal Forced Aligner, which is an automatic forced alignment tool, indicating automated quality assurance of the answer timing annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is documented through the usage of an automatic forced alignment tool for annotations, thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10061,
      "completion_tokens": 530,
      "total_tokens": 10591
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description, Abstract, and Introduction",
          "reasoning": "The NMSQA test set was produced by 60 human speakers reading the SQuAD-dev-2 passages and questions naturally, which constitutes original spoken data created by human contributors. This data is not translated, adapted, or derived from pre-existing spoken data but is new recordings of reading existing textual questions and passages."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The training and validation sets of NMSQA were synthesized from Amazon Polly Text-to-Speech (TTS) service, which is an AI model generating spoken audio from text. Thus, parts of the released NMSQA data are newly generated by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using machine translation for data generation in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The NMSQA dataset leverages the existing SQuAD v1.1 dataset's textual questions and passages, thus aggregating and repurposing this existing textual QA dataset as spoken data. This counts as collating existing data sources (SQuAD dataset) with minimal modification but generating speech versions."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The spoken versions of SQuAD's textual QA dataset are derived by generating audio versions via TTS synthesis and human reading, with forced alignment to annotate answer spans. Hence, the spoken dataset is based on existing sources with modifications and transformations applied (e.g., speech synthesis, human reading, alignment), so it is derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the origin and generation method of the data clearly, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10579,
      "completion_tokens": 540,
      "total_tokens": 11119
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new dataset (NMSQA) being used for pre-training large models on general patterns. The pre-training described uses other existing datasets or unlabeled speech data but not NMSQA."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the NMSQA dataset is used to train models from scratch without pre-training. The paper focuses on fine-tuning pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4 (Implementation Details of DUAL), Section 4 (Results)",
          "reasoning": "The NMSQA dataset is used for supervised fine-tuning the pre-trained language model (Longformer) adapting it to the SQA downstream task by predicting answer time spans using spoken questions and passages. The paper explicitly states that DUAL is fine-tuned on the NMSQA data."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning post-training techniques or RLHF being used with the NMSQA dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 (Evaluation), Section 4 (Results)",
          "reasoning": "The NMSQA dataset is utilized for evaluation purposes, specifically the human speaker test set is used to benchmark model performance and robustness, as described in the evaluation and results sections."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some ablation analyses are performed in the paper, these analyses focus on model aspects rather than primarily using NMSQA for analyzing data trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the NMSQA dataset as a knowledge base to augment models through retrieval or other means."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents use of the NMSQA dataset for supervised fine-tuning and evaluation; thus, 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11302,
      "completion_tokens": 567,
      "total_tokens": 11869
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces the NMSQA dataset which is based on the SQuAD v1.1 dataset, an English language question answering corpus. The spoken forms in the NMSQA dataset are produced via Amazon Polly TTS and recordings by human speakers, all of which are in English. There is no mention of other human languages used."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English speech data; no evidence or mention of a second human language is present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Corpus Description",
          "reasoning": "The NMSQA dataset is described as the spoken version of SQuAD v1.1, which is an English question answering dataset. The speech data (both synthetic from Amazon Polly and human spoken) are all in English, as supported by the detailed descriptions and references to English Wikipedia content in the passage."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of any non-English language being included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken question answering data; it does not include programming languages or code snippets as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the dataset contains mathematical or formal logical expressions as part of the content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is focused on spoken human language questions and passages; no biological or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages such as Klingon or Esperanto are mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain spoken English language data."
        }
      }
    }
  },
  {
    "id": "lin22c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8520,
      "completion_tokens": 181,
      "total_tokens": 8701
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link for dataset construction code is provided in the paper.",
          "reasoning": "The paper discusses the release of the NMSQA dataset and the use of external tools like Amazon Polly for TTS and Montreal Forced Aligner for alignment, but does not mention any publicly available code repository or provide links for the data collection, preprocessing, or generation code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Corpus Description)",
          "reasoning": "The paper provides a detailed description of the dataset creation process: The training and development sets were synthesized from Amazon Polly TTS with 12 speakers; the test set was recorded by 60 human speakers; answer intervals were annotated by Montreal Forced Aligner. This constitutes documentation of the dataset creation process, although no full procedural details or code are supplied."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7995,
      "completion_tokens": 231,
      "total_tokens": 8226
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 (Corpus Collection); Table 1",
          "Reasoning": "The paper introduces a new large-scale parallel English-Pidgin corpus consisting of 29,737 sentence pairs derived mainly from the Holy Bible, where each English verse was mapped to its corresponding Pidgin verse, with manual processing to ensure quality for some chapters. This is human-generated text data created by human translators and processed manually."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 (Corpus Collection); Table 1",
          "Reasoning": "The paper introduces a corpus of 5 million synthetic English-Pidgin sentence pairs generated using back-translation methods applied to existing English datasets (ISWLT'15 and WMT14) and Pidgin monolingual data, used for data augmentation in training. Although generated via model back-translation, these are considered synthetic data generated by automated model methods rather than manually created, hence model-generated text."
        }
      ]
    }
  },
  {
    "id": "lin23e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8795,
      "completion_tokens": 284,
      "total_tokens": 9079
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2 and Abstract",
            "reasoning": "The paper describes collecting the parallel English-Pidgin dataset primarily from existing religious texts such as the Holy Bible and JW300 corpora as well as other sources, and 5 million synthetic sentence pairs generated via an automated back-translation process. There is no explicit mention of humans performing manual annotation apart from limited manual processing for quality assurance, but no annotation per se. The data generation from these sources and synthetic generation can be classified as an Automatic Process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any detailed annotation instructions for the data collection process of the new parallel English-Pidgin dataset or synthetic data generation. The data mainly derived from existing resources and automatic synthesis, thus no instructions for annotation are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No mention of any scoring rubrics for human annotation in relation to the new parallel dataset or synthetic corpus; the dataset is constructed automatically or by aligning existing texts."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No examples are provided showing annotation guidelines or annotated samples, since the dataset involves existing texts and automatically generated synthetic data, not newly human-annotated data requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "lin23e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9985,
      "completion_tokens": 405,
      "total_tokens": 10390
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any instance where a single human expert conducted quality assurance on the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or description in the paper indicating that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance done by a single human non-expert on the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert annotators performing quality assurance on the dataset in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that AI models were used as judges or for validating the dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based algorithmic quality assurance process applied to the new datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the dataset collection process but does not provide any information on a quality assurance process, annotation validation, or quality checks applied to the new English-Pidgin parallel dataset or the synthetic datasets. Apart from mentioning a 'limited number of chapters' that required manual processing to ensure quality for the Bible dataset, there is no explicit QA procedure described; thus, no formal QA process is documented."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9595,
      "completion_tokens": 591,
      "total_tokens": 10186
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset was created entirely from scratch by human contributors without translation or adaptation. The human effort mentioned involves manual processing and ensuring quality of existing data, but no claim of fully original human-generated data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Abstract; Section 2; Section 4.2",
          "reasoning": "The authors generated a large synthetic dataset of 5 million English-Pidgin sentence pairs using back-translation with a trained machine translation system (Section 2 and Abstract). This synthetic data is newly generated by a model without direct human creation or translation, thus qualifies as new data from model generation."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The paper describes the collection of a parallel English-Pidgin corpus mainly from the Holy Bible where each English verse was mapped to its Pidgin counterpart; manual processing was involved to ensure quality, implying human translation. Additionally, the Naija Treebank includes transcribed spoken Pidgin texts with English translations, indicating human translation efforts."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While back-translation is used to generate synthetic data, this is a form of model generation rather than machine translation of existing data by MT systems. The paper does not mention data produced by applying machine translation to existing text corpora to form datasets for training or release."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The authors aggregated multiple existing datasets from various sources such as JW300, NaijaSenti, Afri-BERTa, BBC Pidgin, ASR, PidginUNMT, and others to compile their corpus. This is data collection from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2; Section 4.2",
          "reasoning": "The synthetic parallel corpus generated via back-translation is derived from existing monolingual corpora and parallel corpora like WMT14 and ISWLT, with modification (machine translation and data augmentation). Additionally, task adaptive training and continual adaptive training use augmented or adapted data that modifies original data for specific tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies clearly the origins of the introduced datasets; therefore, no dataset origin is undocumented."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10113,
      "completion_tokens": 562,
      "total_tokens": 10675
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3 (Cross-Lingual Adaptive Training), Figure 1, and Section 4.1",
          "reasoning": "The newly collected large-scale English-Pidgin parallel corpus and associated unlabeled Pidgin monolingual data are used to continually adapt a pre-trained base model via masked language modeling (MLM) on Pidgin data, constituting continual adaptive training before fine-tuning. This represents an unsupervised pre-training stage to adapt the model to Pidgin language patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper exclusively discusses adapting existing pre-trained models rather than training models from randomly initialized parameters using the new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Cross-Lingual Adaptive Training), Section 4.1 (Results), Table 3 and Table 4",
          "reasoning": "The collected datasets including the English-Pidgin parallel corpus are used to fine-tune pre-trained language models for downstream tasks such as sentiment classification and machine translation using supervised learning methods, e.g., the task adaptive training with back-translation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or demonstration of reinforcement learning post-training methods, such as RLHF, using the new datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not used exclusively for evaluation or benchmarking; rather they are primarily used for training and adaptation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Corpus Collection) and Orthographic Analysis subsection",
          "reasoning": "The newly collected corpus is analyzed for orthographic variations and linguistic characteristics to understand the data's nature and to motivate the need for adaptation techniques."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not described as serving as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes multiple practical uses of the collected dataset for both training (pre-training and fine-tuning) and analysis."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10836,
      "completion_tokens": 530,
      "total_tokens": 11366
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces datasets primarily consisting of English and Nigerian Pidgin language pairs, not exceeding two languages, so it is not multilingual."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 2 (Corpus Collection); Table 1",
          "reasoning": "The new dataset introduced by the authors is a large-scale English-Nigerian Pidgin parallel corpus containing 29.73k sentence pairs, clearly bilingual involving exactly two human languages: English and Nigerian Pidgin."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is parallel English-Pidgin; there is no new monolingual English dataset introduced by the authors."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses existing monolingual Pidgin data, no new monolingual (non-English) dataset is introduced by the authors."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the authors introduce datasets containing programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset containing mathematical or formal logical expressions is introduced in the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced do not include biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns natural languages, specifically English and Nigerian Pidgin, not any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages of the dataset are clearly specified as English and Nigerian Pidgin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data, so it is not applicable to mark as not containing any language."
        }
      }
    }
  },
  {
    "id": "lin23e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8054,
      "completion_tokens": 150,
      "total_tokens": 8204
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention of code availability or link is provided in the paper text for dataset construction.",
          "reasoning": "The paper describes collecting and combining various datasets and generating synthetic sentence pairs, but does not provide any links, repositories, or mentions of making code used for constructing or processing the dataset publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Corpus Collection",
          "reasoning": "The paper provides a detailed description of the corpus collection process, listing the sources of the parallel English-Pidgin corpus and monolingual datasets, describing manual processing for some parts, and analysis of orthographic variations. This documentation covers the dataset creation process clearly in Section 2."
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8334,
      "completion_tokens": 115,
      "total_tokens": 8449
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Overview; Section 2.2 Data Acquisition",
          "Reasoning": "The paper introduces the SimuSOE dataset consisting of 4428 simulated snoring audio recordings from 82 adult participants. These audio recordings were captured using a lavalier microphone placed 3 cm from the participant's mouth while they intentionally emitted simulated snoring sounds. This direct human recording confirms the data is audio modality and human generated."
        }
      ]
    }
  },
  {
    "id": "lin24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9134,
      "completion_tokens": 258,
      "total_tokens": 9392
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1 and Section 2.2",
            "reasoning": "The paper states that after simulated snoring sounds were recorded, each participant underwent a standard full PSG diagnosis, and experienced experts who are RPSGT certified scored sleep stages and respiratory events to generate PSG reports. This implies that the annotation of the gold standard clinical labels (e.g., AHI) was performed by multiple certified experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specifically described",
            "reasoning": "The paper does not provide explicit mention or detailed description of annotation instructions or guidelines given to the experts for scoring PSG data or labeling the simulated snoring dataset."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 and citation [11]",
            "reasoning": "The paper references the AASM Manual V2.6 for PSG scoring rules and definitions, indicating that standardized scoring rubrics defined by the American Academy of Sleep Medicine manual were used by experts for annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples mentioned",
            "reasoning": "The paper does not mention or provide any annotation examples or sample labeled instances used to train or guide annotators."
          }
        }
      ]
    }
  },
  {
    "id": "lin24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10324,
      "completion_tokens": 629,
      "total_tokens": 10953
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Overview and Section 2.2 Data Acquisition",
          "reasoning": "The paper states that after recording simulated snoring sounds, each participant underwent standard full PSG diagnosis, and three experienced experts who are RPSGT certified scored sleep stages and respiratory events based on the recorded PSG data (Section 2.2). Although multiple experts performed the PSG scoring, this refers to labeling PSG diagnosis rather than directly annotating the simulated snoring audio samples in the dataset. The simulated snoring audio samples themselves are raw data recordings. The quality assurance over the main annotations linking snoring audio to AHI labels is based on the experts' PSG reports, which were generated by multiple experts; however, the paper does not explicitly state how conflicts or consensus were resolved or how many annotations per sample were available. Since PSG reports were produced by multiple experts, but the paper does not clarify whether the PSG report labels were consolidated by a single expert or multiple experts collectively, and considering the usual practice, this can be interpreted as quality assurance performed by experienced human experts. The PSG reports act as the authoritative ground truth labels linked as annotations to the snoring samples."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Acquisition",
          "reasoning": "The paper specifies that \"three experienced experts who got RPSGT certified scored sleep stages and respiratory events based on the recorded PSG data\" (Section 2.2). This indicates that multiple human experts were involved in the quality assurance process for the PSG-derived annotations which label the severity of OSAHS associated with the snoring samples. Thus, the annotation that links the audio data to the disease severity labels was generated and validated by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating any quality assurance conducted by a single non-expert annotator in the dataset preparation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance being performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of any AI model as a judge or quality assurance mechanism in the annotation process for the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic QA process is described for verifying dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance procedures are described and involve expert annotators, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9934,
      "completion_tokens": 402,
      "total_tokens": 10336
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Datasets), particularly 2.1 (Data Overview) and 2.2 (Data Acquisition)",
          "reasoning": "The SimuSOE dataset consists of 4428 simulated snoring audio samples recorded from 82 adult human participants. The data is newly created by humans who intentionally produce simulated snoring sounds during wakefulness. The study describes the recording setup and procedure in detail and involves human participants simulating snoring in various positions. Therefore, the data is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by any AI or machine learning model; all snoring data is recorded from human participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not aggregated from existing datasets or sources; it was newly collected and recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not derived or adapted from existing datasets; it is original data collected specifically for this study."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10452,
      "completion_tokens": 317,
      "total_tokens": 10769
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4 Conclusions",
          "reasoning": "The SimuSOE dataset is used to train classification models for OSAHS diagnosis and severe OSAHS screening through supervised learning methods, as shown by the conducted binary classification experiments in Section 3.2 demonstrating effectiveness of simulated snoring data for these tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4 Conclusions",
          "reasoning": "The dataset is employed to evaluate and benchmark the performance of models in OSAHS screening tasks, shown by reported metrics like accuracy and sensitivity in Section 3.2 to measure the effectiveness of simulated snoring signals for OSAHS evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2.2 Classification effects in different sleeping positions",
          "reasoning": "The dataset is used to analyze and understand the impact of sleeping positions on the classification performance, indicating usage for analyzing trends and characteristics within the data related to OSAHS evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11175,
      "completion_tokens": 614,
      "total_tokens": 11789
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described, SimuSOE, contains audio recordings of simulated snoring sounds from human participants but does not specify any human languages present in the audio or metadata."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes two human languages; the focus is on snoring sounds, not spoken language content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio snoring recordings and physiological measurements; there is no evidence of English language content within the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain spoken or written content in any single non-English language; it consists of audio snoring sounds without linguistic content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include programming or structured code content; it only includes recorded simulated snoring audio and associated physiological data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes statistical analysis and metrics, the dataset entries themselves do not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Overview, Section 2.2 Data Acquisition",
          "reasoning": "The proposed dataset, SimuSOE, consists of audio recordings of human snoring, which is a biological non-verbal sound related to human upper airway functioning. Although snoring is human in origin, it is categorized as biological and non-linguistic communication signals rather than human linguistic data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages; it only contains simulated snoring sounds."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language or types of linguistic content in the dataset entries are explicitly specified as simulated snoring sounds without any language spoken."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio data of human-generated snoring sounds, which are vocalized sounds; thus, they constitute a form of non-linguistic communication and so cannot be categorized as containing no language at all."
        }
      }
    }
  },
  {
    "id": "lin24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8393,
      "completion_tokens": 183,
      "total_tokens": 8576
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention found about code availability or link to code repositories.",
          "reasoning": "The paper does not provide any URLs, links, or explicit statements about releasing the code used for data collection, preprocessing, or dataset generation. There is no section dedicated to code availability or reproducibility via code in the text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Datasets) and subsections 2.1 (Data Overview) and 2.2 (Data Acquisition)",
          "reasoning": "The paper provides detailed documentation of the data collection methods, including equipment used (microphone model and audio interface), sampling frequency and resolution, participant demographics and informed consent, recording conditions and posture, and the PSG procedure for diagnosis labeling. These details thoroughly document the dataset creation and acquisition process enabling understanding and potential replication."
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5528,
      "completion_tokens": 259,
      "total_tokens": 5787
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Auditory stimuli",
          "Reasoning": "The dataset contains 40 short Chinese stories narrated by male or female professional speakers, which are human-generated audio stimuli presented to participants."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Data acquisition",
          "Reasoning": "The EEG signals were recorded from 20 participants using a 64-channel EEG system during auditory stimuli presentation. These EEG signals are human-generated data recorded with human participants using EEG sensors."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Auditory stimuli",
          "Reasoning": "Multiple-choice question answers and self-rated attention scores were collected from participants as behavioral indices, representing human-generated textual data associated with the EEG recordings."
        }
      ]
    }
  },
  {
    "id": "lin24f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6328,
      "completion_tokens": 303,
      "total_tokens": 6631
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1 and Section 2.4",
            "reasoning": "Participants (20 individuals) served as subjects who answered multiple-choice questions after each auditory stimulus trial to assess attention and compliance. The paper mentions use of multiple-choice questions and self-rated attention scores, suggesting that these behavioral annotations were provided by the participants themselves (non-expert humans). There is no mention of expert annotators labeling or annotating data separately."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "Section 2.4 describes the ASAD task design, instructing participants during trials to concentrate on one story among competing stimuli. The instructions specify focusing on a single story and answering questions about comprehension, indicating instructions were provided to participants for annotation (i.e., their own attention and responses)."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention any specific scoring rubrics provided to annotators or participants to score or judge annotations for behavior or EEG data. The multiple-choice questions have correct answers, but no rubrics or scoring guidelines for annotations are described explicitly."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not describe providing explicit annotation examples or example trials to annotators or participants in the dataset construction process. No example annotations or guidance beyond task instructions are detailed."
          }
        }
      ]
    }
  },
  {
    "id": "lin24f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7518,
      "completion_tokens": 299,
      "total_tokens": 7817
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert to validate the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper about multiple human experts conducting quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that an AI model was used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although preprocessing steps and automated data handling are described, these constitute data processing rather than quality assurance of dataset annotations, and the paper does not explicitly describe automated verification of annotations or dataset content as QA."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process performed on the new ASA dataset. There is no mention of annotation validation, expert review, or automated QA procedures. The focus is on data collection and experimental design with no explicit QA process described."
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7128,
      "completion_tokens": 353,
      "total_tokens": 7481
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Description of the ASA Dataset)",
          "reasoning": "The paper describes the creation of the ASA dataset, which involved recruiting 20 human participants with normal hearing to record EEG signals while they listened to auditory stimuli (Chinese stories narrated by professional speakers). These auditory stimuli are original in the sense that the experiment was conducted by the authors, and the EEG data were recorded afresh from human subjects under controlled experimental conditions. Thus, this dataset is generated from scratch by human participants and human experimentation, not adapted or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of EEG recordings from human subjects and auditory stimuli played to them; there is no indication that any data were generated by AI or machine learning models directly for this dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention in the paper of translating data from another language by humans for this dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that machine translation was used to generate or transform data for this dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not an aggregation from existing sources but rather generated newly via experimentation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that this dataset was based on existing data with modifications; it is presented as a newly recorded dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data source and how the data was generated, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7646,
      "completion_tokens": 308,
      "total_tokens": 7954
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Training and evaluation",
          "reasoning": "The ASA dataset is utilized in training the CA-CNN model from scratch under subject-independent conditions, as described in Section 4.2. The model is trained iteratively with binary cross-entropy loss and the Adam optimizer on this dataset, indicating direct use for training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Training and evaluation; Section 5 Results and Discussion",
          "reasoning": "The ASA dataset is used to benchmark and evaluate different models including CA-CNN, GCN, STAnet, and CNN-KUL under subject-independent conditions. Evaluation metrics like ASAD accuracy and standard deviation are reported to assess model performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 Effect of channel attention",
          "reasoning": "The dataset enables analysis on the effect of channel attention on model performance, demonstrating trends and patterns in EEG responses and robustness across subjects."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8369,
      "completion_tokens": 523,
      "total_tokens": 8892
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The ASA dataset contains auditory stimuli in only one language, Mandarin, and does not include more than two languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset uses auditory stimuli only in Mandarin; no presence of exactly two languages is indicated."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset auditory stimuli are in Mandarin, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (Auditory stimuli)",
          "reasoning": "The dataset auditory stimuli consist solely of short Chinese stories narrated by Mandarin speakers, indicating it is monolingual in a non-English language (Mandarin)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains EEG signal data and associated auditory stimuli, but does not include programming code entries as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Mathematical notation appears in the model description but not as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although EEG data is biological in origin, the dataset does not contain biological sequences or non-human communication systems as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes any fictional or artificially constructed language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the auditory stimuli is clearly indicated as Mandarin Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (Mandarin) in the auditory stimuli and therefore is not without language content."
        }
      }
    }
  },
  {
    "id": "lin24f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5587,
      "completion_tokens": 140,
      "total_tokens": 5727
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 6 Conclusion",
          "reasoning": "The paper states that the ASA dataset and the source code of the reference model are to be publicly released for research purposes, indicating future availability but does not provide a current link or location for this code; thus, the code is not presently made publicly available in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Description of the ASA Dataset",
          "reasoning": "The paper provides detailed documentation about the dataset creation process including participant demographics, auditory stimuli details, data acquisition setup, and experimental task design in Section 2, indicating comprehensive documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6960,
      "completion_tokens": 207,
      "total_tokens": 7167
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Table 1",
          "Reasoning": "The VoxBlink2 dataset comprises approximately 9,904,382 high-quality utterances sourced from 111,284 users on YouTube. These utterances represent human-spoken audio data collected from videos. The data are naturally human-generated speech captured in the wild from user-uploaded videos, not model-generated or synthetic."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, Table 1, Section 2.2.1",
          "Reasoning": "The VoxBlink2 dataset consists of about 2,097,062 video clips from YouTube users. These videos are human-recorded clips uploaded by users and extracted for dataset construction. Thus, the video data is human-generated, as it is captured by human-operated cameras and uploaded online."
        }
      ]
    }
  },
  {
    "id": "lin24j_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7760,
      "completion_tokens": 279,
      "total_tokens": 8039
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 Data mining and Section 2.2.1 Collection Pipeline",
            "reasoning": "The VoxBlink2 dataset is collected through a data mining pipeline involving automatic processes such as candidate collection via user retrieval, face detection with MobileNet, face recognition using a pre-trained ArcFace classifier, and audio-visual speaker diarization models to filter speech segments. This indicates that annotations (i.e., labeling of speaker identity and speech segments) are performed by automated, algorithmic procedures rather than by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention of instructions in the paper regarding annotation guidelines for VoxBlink2 data mining process.",
            "reasoning": "The paper does not describe any human annotation instructions or guidelines because the labels are obtained via automated data mining methods, not human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of rubrics for annotation in the paper.",
            "reasoning": "No discussion of scoring rubrics for annotation is present since annotations are generated automatically without human scoring or grading."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention of examples provided for annotation in the paper.",
            "reasoning": "No annotation examples or sample annotations are included because the annotation is fully automated through the mining pipeline without manual examples."
          }
        }
      ]
    }
  },
  {
    "id": "lin24j_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8950,
      "completion_tokens": 400,
      "total_tokens": 9350
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert human annotator performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple non-expert human annotators conducted quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Step IV (Active Speaker Detection & Overlap Speech Detection)",
          "reasoning": "The dataset quality is enhanced by integrating audio-visual speaker diarization and overlap detection models, thus using AI models as judges to detect active speech segments and eliminate overlapping speech, contributing to data quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Data mining overall description",
          "reasoning": "The data mining pipeline employs automated techniques including face detection, face recognition with a pre-trained classifier, IOU thresholds for track generation, and sampling based on cosine similarity scores, representing algorithmic and rule-based quality assurance processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes quality assurance measures via automated and AI model methods; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8560,
      "completion_tokens": 460,
      "total_tokens": 9020
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset VoxBlink2 is collected from user-uploaded videos on YouTube and is not created entirely from scratch by human contributors; rather, it is aggregated from existing content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the VoxBlink2 dataset includes data generated by AI or machine learning models; it consists of videos sourced from YouTube users."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from other languages via human translators for dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to machine translation involvement in the dataset creation or data transformation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2.1",
          "reasoning": "The VoxBlink2 dataset is collected by aggregating existing user-uploaded videos from YouTube, using an optimized data collection pipeline that collects over 6 million one-minute videos and extracts utterances and video clips. The data is aggregated without indication of significant content modification, thus it is considered collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2.1 and 2.2.2",
          "reasoning": "The dataset is processed by applying face detection, face recognition using a trained classifier, active speaker detection, and overlap speech detection to refine and filter the collected raw data, indicating transformations and adaptations applied to the collected source data, making it derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and the methods of data processing and generation, so the origin is specified."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9078,
      "completion_tokens": 607,
      "total_tokens": 9685
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4 Experimental Settings and Section 5.1 Speaker Verification",
          "reasoning": "The VoxBlink2 dataset is primarily used for pre-training speaker recognition models before fine-tuning on other datasets (e.g., VoxCeleb2). The paper explicitly mentions pre-training models on VoxBlink2 to improve their generalization and achieve state-of-the-art results. This is discussed in Section 4 and demonstrated in Section 5.1 where experiments analyze the effect of varying training data scale."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe training models from randomly initialized parameters solely on the VoxBlink2 dataset without any pre-training or fine-tuning on other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 Experimental Settings and Section 5.1 Speaker Verification",
          "reasoning": "The VoxBlink2 dataset is used for supervised fine-tuning of pre-trained models. The paper details fine-tuning pre-trained models on a mixed set involving VoxBlink2 or VoxCeleb2, showing effectiveness in boosting performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of VoxBlink2 in reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Open-Set Speaker-Identification and Section 5.2 Open-Set Speaker-Identification",
          "reasoning": "A subset of VoxBlink2, the VoxBlink-clean set, is used to create benchmark evaluation protocols for the new Open-Set Speaker-Identification task, serving as the evaluation dataset to assess model performance in challenging authentication scenarios."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 and Section 5.2 Results",
          "reasoning": "The paper analyzes the influence of data scale and model complexity on performance, and studies open-set speaker-identification performance using VoxBlink2 subsets, thus using the dataset for analytical purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset being used as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The VoxBlink2 dataset is actively used throughout the paper for pre-training, fine-tuning, evaluation, and analysis, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9801,
      "completion_tokens": 532,
      "total_tokens": 10333
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Data mining, Step I. Candidate Collection",
          "reasoning": "The paper states that a long keyword list spanning 18 languages was used for user retrieval when collecting videos from YouTube (Section 2.2.1). This indicates that the VoxBlink2 dataset contains entries from more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than two languages, not exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages and is not limited to only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages, not exclusively a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes equations and explanations, these are part of the paper's content and evaluation protocols, not the dataset itself. The dataset consists of audio-visual utterances, not mathematical or formal symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human speech data and videos, with no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the dataset includes fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included are explicitly referenced (18 languages), so the language contents are specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken human languages in the utterances, so it contains languages."
        }
      }
    }
  },
  {
    "id": "lin24j_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7019,
      "completion_tokens": 192,
      "total_tokens": 7211
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention of code availability",
          "reasoning": "The paper provides a URL (http://voxblink2.github.io) for data and model resources but does not explicitly mention or link to the code used for the dataset construction or data mining pipeline. There is no text indicating that code for data collection, preprocessing, or generation has been publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (VoxBlink2 Dataset), subsections 2.1 (Data Description) and 2.2 (Data mining)",
          "reasoning": "The paper describes the dataset creation process in detail within Section 2, including the data mining pipeline, data collection steps, face detection and recognition methods, active speaker and overlap speech detection, as well as classifier training procedures. This provides a clear and comprehensive documentation of how the VoxBlink2 dataset was constructed."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6215,
      "completion_tokens": 243,
      "total_tokens": 6458
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Corpus Production Process and Section 2.2, Corpus Details",
          "Reasoning": "The MinSpeech new dataset consists of 2237 hours of unlabeled audio and 1778 hours of labeled audio derived from 36 diverse TV dramas and programs. The audio data is sourced from YouTube videos with human-generated subtitles, subsequently processed and segmented into audio files. The audio represents speech recordings captured from real-world human speech, therefore human generated and not model generated. The dataset creation process is explicitly described in Section 2."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.3 Label Generation and Section 2.1.1 Screening and Downloading Videos",
          "Reasoning": "The labels used for the labeled audio component are Mandarin text subtitles extracted from human-generated YouTube subtitles in '.vtt' files, representing human-generated text transcription. These labels are not generated by models but are sourced from humans providing subtitles, as described in the corpus production process in Section 2."
        }
      ]
    }
  },
  {
    "id": "lin24m_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7015,
      "completion_tokens": 305,
      "total_tokens": 7320
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1.2 Audio Segmentation and Section 2.1.3 Label Generation",
            "reasoning": "The labeled audio segments are generated by cropping audio files based on timestamp information in human-generated subtitle files from YouTube videos, using deterministic methods such as silero-vad toolkit for segmentation and timestamp-based cropping. The label texts are extracted from subtitle files following explicit filtering rules. There is no mention of human annotators manually labeling or transcribing the data; the process is automated utilizing subtitle timings and texts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1.3 Label Generation",
            "reasoning": "The paper describes filtering and processing rules applied programmatically to the subtitle text and audio data, but does not specify any detailed instructions provided to human annotators, as the annotation is derived automatically from subtitle files."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1.3 Label Generation",
            "reasoning": "There is no indication of scoring rubrics or quality assessment criteria provided to annotators; quality is assessed through signal-to-noise ratio and signal energy thresholds applied programmatically rather than human scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper (no section mentions annotation examples)",
            "reasoning": "No examples of annotation or labeling instructions are provided in the paper. The labeling utilizes existing subtitles and automated processing without example-based guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "lin24m_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8205,
      "completion_tokens": 330,
      "total_tokens": 8535
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of multiple human experts performing quality assurance or annotation validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of any AI models as judges or for quality assurance of the labels or data."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.4 Audio Validation",
          "reasoning": "The paper describes an automatic audio validation process using signal-to-noise ratio (SNR) and signal energy criteria to screen audio files for quality. This verification is an automated, rule-based algorithmic approach to ensure audio quality, constituting an automatic quality assurance process. Additionally, in Section 2.1.2 Audio Segmentation, the segmentation of unlabeled audio uses the silero-vad toolkit with defined thresholds, which also constitutes automated processing related to data quality and segmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an explicit automatic QA process, so 'N/A' is not applicable."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7815,
      "completion_tokens": 436,
      "total_tokens": 8251
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus was collected from existing YouTube videos with human-generated subtitles; it is not original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is sourced from real audio recordings and subtitles, not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by translating content from another language through human translators. The labels are in Mandarin, but were originally human-generated subtitles for Southern Min speech videos rather than translations."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that machine translation was used to create the data labels or content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and Section 2.1.3",
          "reasoning": "The data was collected by downloading existing YouTube videos with human-generated subtitles and extracting audio and text. The corpus was formed by aggregating these existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.2 and Section 2.1.3",
          "reasoning": "The corpus was processed by segmenting audio according to subtitle timestamps, applying filtering rules (e.g., removing special characters, excluding certain data), and validating audio files using SNR and energy criteria. This indicates the data was derived from existing sources with modifications and transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the data are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8333,
      "completion_tokens": 421,
      "total_tokens": 8754
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.4 SSL-Transformer Baseline System",
          "reasoning": "The MinSpeech unlabeled corpus of 2237 hours is explicitly used for the pre-training phase of self-supervised learning models Wav2vec 2.0 and HuBERT, as stated in Section 3.4."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training any model from randomly initialized parameters without pre-training on MinSpeech."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.4 SSL-Transformer Baseline System, Section 4.2",
          "reasoning": "The labeled MinSpeech corpus (1778 hours) is used to fine-tune pre-trained SSL models (Wav2vec 2.0 and HuBERT) with supervised learning methods, as detailed in Sections 3.4 and 4.2."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of using MinSpeech for reinforcement learning post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Experiments, Tables 5-7",
          "reasoning": "MinSpeech subsets Dev and Test are utilized for evaluating and benchmarking ASR models' performance, as shown in multiple experimental results in Section 4."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using MinSpeech primarily for analysis of trends or characteristics beyond training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use MinSpeech as a knowledge base or for augmentation in retrieval-augmented models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple practical usages of the MinSpeech corpus including pre-training, fine-tuning, and evaluation, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9056,
      "completion_tokens": 472,
      "total_tokens": 9528
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset described in the paper focuses on the Southern Min dialect with Mandarin text labels; only two human languages are mentioned, so it is not multilingual in the sense of containing more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.1.3 Label Generation / Section 2.2 Corpus Details",
          "reasoning": "The labeled audio consists of Southern Min dialect speech with Mandarin text labels used for alignment. Therefore, the dataset contains exactly two human languages: Southern Min (spoken) and Mandarin (text labels)."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention the dataset containing English content; rather, English is explicitly excluded from subtitle text (Section 2.1.3)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While Southern Min is a non-English language, the dataset is paired with Mandarin text labels, so it is not strictly monolingual in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of programming or code content in the dataset; it solely consists of speech audio and corresponding Mandarin text labels."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains mathematical or logical expressions; it is a speech corpus with text labels."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech only, with no non-human biological communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of constructed or fictional languages; the dataset involves a natural dialect and Mandarin text."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages involved are specified as Southern Min speech and Mandarin text labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (Southern Min speech and Mandarin text labels); thus, it is not language-free."
        }
      }
    }
  },
  {
    "id": "lin24m_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6274,
      "completion_tokens": 156,
      "total_tokens": 6430
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not mention or provide any links or references to code repositories related to the data collection, preprocessing, or dataset construction procedures. It only mentions that the MinSpeech dataset is available at a URL, but there is no indication that the code to build the dataset is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (MinSpeech Corpus) and its subsections",
          "reasoning": "The paper provides detailed documentation on the dataset creation process, including corpus production process (Section 2.1), audio segmentation, label generation, and audio validation. Tables and descriptions of corpus segmentation and utterance length distribution also support dataset documentation."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6979,
      "completion_tokens": 184,
      "total_tokens": 7163
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Dataset collection and Section 4.1.1",
          "Reasoning": "The Macaque Voiceprint Verification Dataset (MVVD) is newly constructed by the authors through 21 days of recording macaque calls using a dedicated microphone at 44.1kHz. The data is explicitly recorded by human operators using equipment, indicating human involvement in data collection."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1, Dataset collection and Section 4.1.1",
          "Reasoning": "The MVVD also includes video data recorded simultaneously with audio by a professional camera operated during the data collection process. The video data is captured by humans using recording devices, reflecting human generation."
        }
      ]
    }
  },
  {
    "id": "liu22d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7779,
      "completion_tokens": 219,
      "total_tokens": 7998
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1.2 Dataset labelling",
            "reasoning": "The paper states that start and end times of each macaque call were manually marked based on auditory perception, and human ears can distinguish macaque calls well, indicating manual annotation performed by humans but no indication these annotators were experts in macaque calls."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1.2 Dataset labelling",
            "reasoning": "No mention or description of detailed annotation instructions or protocols provided to annotators in the paper."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1.2 Dataset labelling",
            "reasoning": "The paper does not describe any scoring rubrics or criteria for annotation quality or call boundary decisions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1.2 Dataset labelling",
            "reasoning": "There is no indication that example annotations or call segments were provided as guidelines to annotators."
          }
        }
      ]
    }
  },
  {
    "id": "liu22d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8969,
      "completion_tokens": 308,
      "total_tokens": 9277
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset labeling or quality assurance was performed by a single human expert. It only mentions that human ears distinguish macaque calls well but does not specify expert status or single annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human annotators with subject matter expertise performing quality assurance of the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.1.2 Dataset labelling",
          "reasoning": "The dataset labeling is described as being done manually by marking start and end times of macaque calls using human ears to distinguish calls. The paper does not clarify annotators' expertise, so it is reasonable to infer the labeling was done by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model is indicated as being used for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based quality checks for dataset annotation are described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A human manual labeling process is described in detail, so quality assurance is present and documented."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8579,
      "completion_tokens": 391,
      "total_tokens": 8970
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper states that the authors cooperated with a primate research center to record and manually label audio and video data from 144 macaques. They spent 21 days recording and manual labeling to construct the Macaque Voiceprint Verification Dataset (MVVD). Each macaque's calls were manually marked by humans to accurately identify call start and end times, thus this dataset is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by any AI or machine learning model; it was collected from actual macaque calls."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources; it was directly recorded and labeled by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of derivation or transformation from existing datasets. The dataset was created from original recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and collection method are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9097,
      "completion_tokens": 518,
      "total_tokens": 9615
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Experimental setup",
          "reasoning": "The MVVD dataset is used as training data to train the proposed macaque voiceprint verification models from scratch. Specifically, the basic training set (MacaqueT) and its variants (MacaqueT1, MacaqueT5, MacaqueT10) are used to train the model with different audio lengths, as described in Section 4.2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the MVVD dataset to fine-tune a pre-trained model; the models are trained directly on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning (RL) or RL-based fine-tuning methods applied to the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and Section 4.2 Experimental setup",
          "reasoning": "The MVVD dataset is also split into evaluation sets (MacaqueE and variants) used for testing and benchmarking the voiceprint verification models. The experimental results demonstrate the model performance on these evaluation sets."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some analyses of call characteristics (e.g., Fig.5 spectrogram comparisons, duration statistics in Fig.6) are performed, these are done on the dataset rather than the dataset being used as an analysis tool per se. The primary use is for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base or retrieval resource to augment models in this work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the MVVD dataset in both model training and evaluation."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9820,
      "completion_tokens": 620,
      "total_tokens": 10440
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1.1 Dataset collection",
          "reasoning": "The dataset consists of audio and video data recorded from 144 macaques; there is no mention of human languages or multiple human languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1.1 Dataset collection",
          "reasoning": "The dataset contains calls of macaques only, not entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper is written in English, but the dataset contains macaque call audio data, not English human language speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset entries are audio calls of macaques, which are non-human vocalizations, not human language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section describes dataset entries containing programming or structured code content.",
          "reasoning": "The dataset includes audio and video recordings of macaque calls without programming code or data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No section describes dataset entries containing mathematical or formal logical expressions.",
          "reasoning": "The dataset entries are recordings of macaque calls, not mathematical or symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset, especially 4.1.1 Dataset collection and 4.1.3 Dataset characteristics",
          "reasoning": "The proposed MVVD dataset contains audio recordings of macaque calls, which are non-human biological communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No mention in the paper of any fictional or artificially created languages in the dataset.",
          "reasoning": "The dataset contains natural macaque vocalizations, not constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Dataset details in Section 4.1 specify the nature of the audio entries as macaque calls.",
          "reasoning": "The language content (macaque calls as non-human animal vocalizations) is explicitly documented and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries consist of audio vocalizations, which are a form of communication (non-human) thus contain language-like signals."
        }
      }
    }
  },
  {
    "id": "liu22d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7038,
      "completion_tokens": 168,
      "total_tokens": 7206
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper provides a URL 'https://github.com/PengLiu2022/MacaqueDataset' where the Macaque Voiceprint Verification Dataset (MVVD) can be downloaded, which indicates that the dataset code related to data collection or at least the dataset itself is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper provides detailed documentation on dataset collection (microphone and camera setup, number of macaques, recording duration), labeling process (manual labeling steps, rationale for not using VAD), and dataset characteristics (duration statistics, segment counts, splits into training and testing sets), thereby offering thorough transparency about the dataset creation process."
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6595,
      "completion_tokens": 109,
      "total_tokens": 6704
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Source of data",
          "Reasoning": "The paper introduces new dataset consisting of audio recordings of isolated vowels and a read passage collected from 60 patients diagnosed with Spasmodic Dysphonia (SD) or Vocal fold Palsy (VP). These are real speech audio recordings captured in a clinical setting using a microphone in a quiet clinic room, thus human generated and not model generated."
        }
      ]
    }
  },
  {
    "id": "liu22q_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7395,
      "completion_tokens": 189,
      "total_tokens": 7584
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.3",
            "reasoning": "The phonetic annotation of the speech recordings was performed using the Montreal forced aligner, which is an automatic forced alignment tool. This process produced segmentation and phonetic labelling automatically without mention of human annotators performing the phonetic annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide or mention any detailed annotation instructions for human annotators related to phonetic annotation; the alignment was automatic."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No scoring rubrics for annotation are described or implied for the automatic forced alignment annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No annotation examples are provided or referenced; the process was automated and no manual annotation examples are described."
          }
        }
      ]
    }
  },
  {
    "id": "liu22q_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8585,
      "completion_tokens": 425,
      "total_tokens": 9010
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper that a single human expert validated the dataset annotations or content. The manual corrections to transcripts were made, but no indication that it was done by a single expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for forced alignment and classification, there is no indication that AI models were used specifically as judges for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Alignment and annotation",
          "reasoning": "The dataset was annotated via automated forced alignment using the Montreal forced aligner. To establish accuracy, a manual check of a random sample of annotations (493 checked, 2% in significant error) was conducted, but no corrections were made based on this. This suggests that the primary quality assurance of phonetic annotation was conducted through this automated process, supported by algorithmic techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documented quality assurance via automated forced alignment and error checking."
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8195,
      "completion_tokens": 397,
      "total_tokens": 8592
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset comprises audio recordings collected from 60 patients diagnosed with Spasmodic Dysphonia and Vocal fold Palsy during clinical assessments. These recordings were made directly from human patients reading a fixed passage and producing sustained vowels, explicitly gathered for this study. Therefore, the data represent original content created entirely from scratch by human contributors, not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data were generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any translation of data from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data from other languages is reported in the dataset preparation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not collected or aggregated from existing sources without modification; rather, it was newly recorded specifically for this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the study uses feature extraction and processing techniques, the raw data themselves are original recordings, not derived or adapted from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies data origin, thus this category does not apply."
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8713,
      "completion_tokens": 273,
      "total_tokens": 8986
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.5 Classification and Section 3 Results",
          "reasoning": "The paper describes the use of the collected dataset of speech recordings from patients with spasmodic dysphonia and vocal fold palsy to train Support Vector Machine classifiers from scratch, with leave-one-out cross-validation for pathology discrimination."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Phonetic analysis and Section 3.3 Phonetic fusion",
          "reasoning": "The dataset is used primarily for analyzing the effects of phonetic context in discriminating voice pathologies, investigating differences in classification accuracy across phone categories, and studying phonetic fusion to improve classification, thus analyzing patterns and characteristics rather than solely training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9436,
      "completion_tokens": 532,
      "total_tokens": 9968
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains speech recordings only from British English speakers reading an English passage, with no indication of other languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain entries with exactly two human languages; only English speech recordings are described."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Source of data",
          "reasoning": "The dataset consists of recordings of British English speakers reading the 'Arthur the Rat' passage and producing isolated vowels. The passage and speech are exclusively in English as stated in Section 2.1."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The data is described as English speech only; no non-English single language dataset is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists solely of speech audio recordings; no programming or code data entries are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech recordings and phonetic annotations; no mathematical or formal logical expressions are noted in the data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the study concerns human voice pathology, the dataset is composed of human speech recordings, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset entries is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken English language content, so it does not lack language."
        }
      }
    }
  },
  {
    "id": "liu22q_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6654,
      "completion_tokens": 201,
      "total_tokens": 6855
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention or provide any link or reference to publicly available code related to data collection, preprocessing, or generation of the dataset. There is no indication that code used to construct the dataset or perform alignments and feature extraction is available publicly."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Materials and Methods) and especially Sections 2.1 to 2.3",
          "reasoning": "The paper provides detailed documentation on the dataset creation and collection process in Section 2.1, including information about the patient groups, recording hardware, recording conditions, and speech materials collected. Further details on preprocessing such as resampling and manual editing are described in Section 2.2 and 2.3, including the transcript creation and forced alignment method. This provides a reasonably clear description of the dataset creation process, although no mention is made of dataset sharing or public availability."
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6610,
      "completion_tokens": 173,
      "total_tokens": 6783
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4 Dataset description",
          "Reasoning": "The MSDWild dataset contains 3143 video clips collected from public videos of daily casual conversation in the wild, which are naturally shot videos without over-editing. This indicates human involvement in creating and recording the video data, as they originate from human-recorded public videos of real conversations."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4 Dataset description",
          "Reasoning": "MSDWild includes audio data from the same public video clips of daily casual conversations. These audios are naturally recorded in real-world environments, involving human speech captured by human-operated recording devices in the original source videos."
        }
      ]
    }
  },
  {
    "id": "liu22t_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7410,
      "completion_tokens": 184,
      "total_tokens": 7594
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4, Dataset description",
            "reasoning": "Section 4 describes that videos with over two speakers talking in turn were filtered manually without using pre-trained audio-visual algorithms to avoid dataset bias. Manual filtering implies multiple human annotators with expertise did the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not mention specific annotation instructions provided to annotators in creating the MSDWild dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "There is no description in the paper of any scoring rubrics or formal criteria used during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not provide any examples or guidelines for annotation in the text or appendix."
          }
        }
      ]
    }
  },
  {
    "id": "liu22t_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8600,
      "completion_tokens": 393,
      "total_tokens": 8993
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No explicit mention or description of quality assurance performed by multiple human experts in the dataset annotation or validation process is provided in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single human non-expert performed any quality assurance on the dataset annotations according to the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 1 and Section 4",
          "reasoning": "The paper states that to filter videos with more than two speakers talking in turn, the filtering was done manually without using pre-trained audio-visual algorithms to avoid bias. Although the expertise level of the annotators is not explicitly described, the manual labeling process implies multiple human annotators were involved. Since there is no specific information that annotators were experts or members of the target demographic, it is reasonable to conclude that quality assurance was conducted by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using any AI model to perform quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper mentions using some automatic face detection and tracking methods such as S3FD and IoU tracking, they are part of the data processing pipeline rather than quality assurance of annotation correctness. No mention of automated verification to assure annotation annotation quality is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented and described in the paper, specifically the manual filtering of the data to control speaker count, so the QA process cannot be considered as not documented or absent."
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8210,
      "completion_tokens": 481,
      "total_tokens": 8691
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 1 (Introduction), 4 (Dataset description), and Abstract",
          "reasoning": "The dataset MSDWild was collected by the authors from public videos that are naturally shot, focusing on daily casual conversations in the wild. They describe a manual filtering process to remove videos with over two speakers and emphasize the naturalness of the data without over-editing. The dataset includes over 3000 video clips with 80 hours of labeled data, annotated to identify speaker diarization. This indicates original content creation from real-world recordings and manual annotation by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected from natural videos and manually labeled by humans rather than generated by AI or machine learning models. There is no indication that data itself was generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data or annotations from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 (Dataset description), Introduction",
          "reasoning": "The dataset is collected from existing public videos (e.g., Vlogs) available online. The authors curated and aggregated these videos to form the dataset, but the clips are natural recordings, not generated or heavily altered. The process involves collecting and filtering video clips from these external sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some data processing occurs (such as face detection and tracking), the data itself is not described as being substantially modified or transformed from pre-existing datasets. The core content remains naturally recorded videos."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin is clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8728,
      "completion_tokens": 577,
      "total_tokens": 9305
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Dataset Description); Section 5.3 (Visual-based pipeline); Section 5.3.1 (Experiment setup); Section 5.3.2 (Implementation of visual-based pipeline)",
          "reasoning": "The paper describes using the MSDWild dataset for training purposes, e.g., the few-talker training set of MSDWild used to train models for audio-visual speaker diarization tasks as described in Sections 4 and 5. Training setups including training visual only, two-stream audio-visual, and fused audio-visual models on this dataset are explained, indicating training from scratch or supervised training on this new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe using the dataset to fine-tune pre-trained models via supervised fine-tuning; rather, the experiments describe training models on this data or evaluation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence of reinforcement learning based post-training methods such as RLHF being used with this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 (Evaluation Metrics); Section 5.4 (Result and Analysis)",
          "reasoning": "The dataset is widely used for evaluation and benchmarking, with detailed results on DER and JER metrics reported for several audio-only, visual-only, and audio-visual baseline methods, particularly on few-talker and many-talker testing sets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Dataset Description); Section 5.4 (Result and Analysis)",
          "reasoning": "The paper provides detailed analyses of dataset characteristics such as the speaker count, overlapped speech, distribution differences compared with other datasets, and discusses the challenge and patterns in the data influencing algorithm performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as or used as a knowledge base for augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple practical uses of the dataset for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9451,
      "completion_tokens": 268,
      "total_tokens": 9719
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Introduction, Section 4 Dataset description",
          "reasoning": "The dataset MSDWild is collected from public videos covering rich real-world scenarios and languages (Abstract). It contains daily casual conversations in various real-world environments, which implies multiple languages are present. Although exact languages are not listed, the paper specifies the dataset includes multiple languages rather than being restricted to English or a single language (Abstract, Introduction). Thus, MSDWild is considered multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "liu22t_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6669,
      "completion_tokens": 210,
      "total_tokens": 6879
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or part in the paper mentions or provides a link to code for dataset construction.",
          "reasoning": "The paper mentions a dataset release URL (https://x-lance.github.io/MSDWILD) but does not provide any direct link or mention of publicly available code related to the data collection, preprocessing, or annotation pipeline. There is no explicit section that states the availability of code or software for reproducing the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Dataset description) and Section 1 (Introduction) discuss the dataset collection and characteristics.",
          "reasoning": "The paper provides a detailed description of the dataset collection pipeline, data source selection (keywords used for video collection), manual filtering criteria, dataset structure, statistics (number of clips, duration, speaker counts, overlaps, etc.), and baseline experimental setup. These components convey good transparency and documentation about how the dataset was constructed and its properties, sufficient for understanding the dataset creation process."
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8772,
      "completion_tokens": 159,
      "total_tokens": 8931
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Data Simulation",
          "Reasoning": "The SLURP-S and LT-S datasets are newly developed multi-channel noisy-reverberant datasets derived from the original SLURP and Libri-Trans datasets. They contain audio data that is both human-generated original speech (from SLURP and Libri-Trans) and model-generated augmentations that simulate reverberation, multiple noises, and microphone array effects using Pyroomacoustics and other noise sources to simulate smart-speaker scenarios. Therefore, the audio data is a mixture of human-generated speech and model-generated simulated acoustic conditions, as explicitly described in Section 4.1."
        }
      ]
    }
  },
  {
    "id": "lu22c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9572,
      "completion_tokens": 233,
      "total_tokens": 9805
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1",
            "reasoning": "Section 4.1 describes the creation of two multi-channel noisy-reverberant datasets based on SLURP and Libri-Trans (SLURP-S and LT-S) using simulation methods with Pyroomacoustics and noise samples. This indicates data was generated through an automatic simulation process, not by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "The paper does not mention any human annotation instructions provided for the newly created SLURP-S and LT-S datasets, as these are simulated datasets generated automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "There is no mention of scoring rubrics or annotation guidelines for these datasets, since the data was physically simulated rather than human annotated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1",
            "reasoning": "No annotation examples are provided or needed because the dataset creation involved simulation and not manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "lu22c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10762,
      "completion_tokens": 358,
      "total_tokens": 11120
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by single human annotators who are experts or members of the target demographic for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper indicating that multiple human experts performed quality assurance on the introduced datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by a single non-expert human annotator on the introduced datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance involving multiple human non-expert annotators for the introduced datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description is provided in the paper about AI models being used as judges for quality assurance of the dataset annotations or contents."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "The introduced multi-channel noisy-reverberant datasets (SLURP-S and LT-S) are created by augmenting original datasets using simulation tools, such as Pyroomacoustics for room impulse responses, and noises from FSD50k and SINS datasets. The data preparation process is automatic and algorithmic. There is no mention of manual verification, implying automated processes govern the validity and consistency of the data simulation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes an automatic data simulation process for the new datasets, indicating a quality assurance process by automatic means is present, thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10372,
      "completion_tokens": 290,
      "total_tokens": 10662
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "The paper describes development of two multi-channel noisy-reverberant datasets, SLURP-S and LT-S, which are derived from existing datasets SLURP and Libri-Trans by applying simulated room impulse responses, adding noises from existing noise corpora, and simulating realistic smart-speaker capture conditions. This process constitutes modifications and transformations applied to original sources, making these datasets derived rather than new from scratch."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10890,
      "completion_tokens": 299,
      "total_tokens": 11189
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4, especially Sections 4.2 and 4.4",
          "reasoning": "The newly created multi-channel noisy-reverberant datasets SLURP-S and LT-S are used to fine-tune pre-trained speech enhancement and downstream models such as ASR, ST, and SLU in a supervised manner. For example, in Section 4.4, the authors fine-tune SE and SLU models together on the SLURP-S dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The SLURP-S and LT-S datasets are used for evaluating how well speech enhancement front-ends combined with back-end tasks such as ST and SLU perform. The datasets serve as benchmark corpora for assessing multi-channel noisy-reverberant scenarios."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11613,
      "completion_tokens": 648,
      "total_tokens": 12261
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "The paper introduces two new datasets (SLURP-S and LT-S) based on SLURP and Libri-Trans, but these are designed for English speech tasks (ASR, ST, SLU) only, with no indication of multiple languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "There is no mention of any bilingual data or two specific human languages present in the new datasets. Only English language speech data is used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Simulation and Section 4 (Experiments)",
          "reasoning": "The new multi-channel noisy-reverberant datasets SLURP-S and LT-S are simulations based on the SLURP and Libri-Trans datasets, which are English language speech datasets. The paper explicitly mentions ASR, ST, and SLU tasks with English content. No other human languages are mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "No non-English language dataset or content is introduced or described in the paper's proposed new datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper discusses new datasets for speech tasks; there is no mention that the new datasets include code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the paper includes math notation in methodology, the datasets themselves do not contain mathematical or formal symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The new datasets consist of simulated human speech in noisy reverberant environments and do not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced are based on real English speech corpora and do not include any fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper explicitly states the linguistic origin of the new datasets as English; therefore, the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new datasets clearly contain human spoken language data, so the label 'N/A' for no language does not apply."
        }
      }
    }
  },
  {
    "id": "lu22c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8831,
      "completion_tokens": 179,
      "total_tokens": 9010
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The paper explicitly states that the multi-channel ST and SLU datasets developed by the authors are released on HuggingFace, and the code for ESPnet-SE++ is available on GitHub at https://github.com/ESPnet/ESPnet, which indicates the code for data preparation is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Data Simulation",
          "reasoning": "Section 4.1 provides detailed documentation on the data simulation process for the two newly developed multi-channel noisy-reverberant datasets (SLURP-S and LT-S), including the simulation parameters such as room size, reverberation time, noise sources, microphone array setup, and data subsets selection, which is transparent and adequate for reproducibility."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8384,
      "completion_tokens": 191,
      "total_tokens": 8575
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data details and Section 2.2 The Generation Process of codec-based fake audio",
          "Reasoning": "The Codecfake dataset consists of real audio samples collected from two existing corpora: VCTK, which contains speech from 110 English speakers, and AISHELL3, which contains Mandarin speech from 218 speakers. These samples are human-produced speech recordings captured by microphones. The dataset also contains fake audio samples generated by seven neural codec models applied to these real samples, resulting in new audio samples synthesized by the models. Therefore, the dataset includes both human generated (original recordings) and model generated (neural codec synthesized) audio samples, with origin explicitly described in Section 3.1. The modality is audio as the data consists of speech waveforms and their codec-based synthetic versions."
        }
      ]
    }
  },
  {
    "id": "lu24f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9184,
      "completion_tokens": 257,
      "total_tokens": 9441
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 and Section 3.1",
            "reasoning": "The Codecfake dataset is constructed by re-encoding and decoding real speech datasets (VCTK and AISHELL3) using seven neural codec models as described in Sections 2.2 and 3.1. This process is fully automated via trained neural codec models without human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide any information regarding annotation instructions provided to annotators for the Codecfake dataset. Since data are generated algorithmically, no human annotation instructions are relevant or described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "There is no mention of scoring rubrics or criteria used for annotation or evaluation in constructing the Codecfake dataset; data generation is automated without subjective scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide any annotation examples or samples for the Codecfake dataset construction or labeling guidelines, as the dataset is generated automatically without human annotations."
          }
        }
      ]
    }
  },
  {
    "id": "lu24f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10374,
      "completion_tokens": 386,
      "total_tokens": 10760
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about any quality assurance process performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of AI models for supervising or verifying the dataset quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 The Generation Process of codec-based fake audio",
          "reasoning": "The dataset is generated through automated processes by neural audio codec models which re-encode and decode existing datasets (VCTK and AISHELL3). This suggests the dataset quality relies on the correctness of the automated codec generation without explicit manual quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although no explicit manual quality assurance process is described, an automatic generation process is used, so 'N/A' does not fully apply."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9984,
      "completion_tokens": 450,
      "total_tokens": 10434
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not consist of original content created entirely by humans from scratch. Instead, it is generated through automatic processes involving neural codec models."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2 The Generation Process of codec-based fake audio",
          "reasoning": "The Codecfake dataset fake audio samples are generated entirely by AI models \u2014 seven representative neural codec models are used to re-encode and decode existing real speech data to create fake audio. This process involves automatic data generation by machine learning models without direct human content creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that any data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although real speech data from public datasets like VCTK and AISHELL3 are used as source audio, the fake data is not merely aggregated or collected from existing sources; it is transformed through neural codec models."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 The Generation Process of codec-based fake audio",
          "reasoning": "The fake audio data is derived from existing datasets (VCTK and AISHELL3) by applying seven different neural codec models to re-encode and decode the audio. This constitutes data derived from existing sources with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is clearly documented in Section 2.2, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10502,
      "completion_tokens": 513,
      "total_tokens": 11015
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the Codecfake dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the Codecfake dataset is used to train a model from randomly initialized parameters; rather, it is used to train or fine-tune ADD models which may already incorporate pretrained components."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 (Codec-trained ADD models results)",
          "reasoning": "The paper describes training ADD models (e.g., W2V2-AASIST) supervised by the Codecfake dataset to improve detection performance on codec-based audio. This indicates usage of the dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using reinforcement learning or RL-based post-training methods with the Codecfake dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2 (Results and Discussion)",
          "reasoning": "The Codecfake dataset is used to evaluate performance of vocoder-trained and codec-trained ADD models across various test conditions (C1-C7). The dataset's test subsets serve as the evaluation benchmark."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use Codecfake for analysis of trends or characteristics beyond performance evaluation; the focus is on training and evaluating ADD detectors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base or used for retrieval augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the Codecfake dataset for training and evaluation of audio deepfake detection models."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11225,
      "completion_tokens": 740,
      "total_tokens": 11965
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 1 (Introduction), Section 2.2 (The Generation Process of codec-based fake audio), and Section 3.1 (Data details)",
          "reasoning": "The Codecfake dataset contains audio samples from two different languages: English (from VCTK dataset) and Chinese Mandarin (from AISHELL3 dataset). Specifically, in Section 1, it states Codecfake spans two languages. Section 2.2 details the usage of VCTK (English) and AISHELL3 (Mandarin Chinese) as source datasets for generating fake audio with seven neural codecs. Section 3.1 further enumerates the number of samples from both datasets. Therefore, the dataset includes exactly two human languages, English and Mandarin Chinese, making it bilingual (two languages); however, since the rubric specifies 'Multilingual' as more than two human languages and 'Bilingual' as exactly two languages, the dataset fits the Bilingual category rather than Multilingual."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 1 (Introduction), Section 2.2 (The Generation Process of codec-based fake audio), and Section 3.1 (Data details)",
          "reasoning": "The proposed Codecfake dataset includes audio samples from English (VCTK corpus) and Chinese Mandarin (AISHELL3 corpus) only, with no mention of additional languages. Sections 1, 2.2, and 3.1 explicitly mention these two languages and the usage of their datasets to generate the fake samples. Thus, since the dataset contains exactly these two human languages, it qualifies as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not monolingual English because it also contains Mandarin Chinese (as part of AISHELL3)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains English and Mandarin Chinese, so not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises audio data samples; there is no indication of containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio and fake audio generated from neural codecs; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved in the dataset are clearly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio samples of human languages (English and Mandarin Chinese), so language is present."
        }
      }
    }
  },
  {
    "id": "lu24f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8443,
      "completion_tokens": 226,
      "total_tokens": 8669
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or portion of the paper provides a link or mention of publicly available code for dataset construction.",
          "reasoning": "The paper does not include any URLs, footnotes, or mentions indicating that the code used to generate the Codecfake dataset or associated preprocessing/generation scripts are publicly available. There is no repository link or supplementary material reference for code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 'The Generation Process of codec-based fake audio' and Section 3.1 'Data details'",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including the training process of neural codec models (Section 2.2), datasets used (LibriTTS for training codecs; VCTK and AISHELL3 for inference), the implantation details for generating fake audio, the specific neural codec models selected with description (Section 2.1), and the detailed dataset composition and splits (Section 3.1). This documentation is sufficient and transparent regarding how the dataset was constructed and its characteristics."
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10176,
      "completion_tokens": 308,
      "total_tokens": 10484
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 2.2 Data Processing Pipeline; Table 1",
          "Reasoning": "The paper introduces FLEURS-R, a new speech corpus derived by applying a speech restoration model (Miipher) to the original FLEURS corpus's speech recordings. The original FLEURS corpus consists of human-recorded speech data in 102 languages. FLEURS-R maintains the same speech samples but with improved audio quality via processing. Therefore, the audio data modality is present, and it originates from human-generated recordings that have been processed but not generated anew by models. This is explicitly stated in Abstract and Section 2.2, where FLEURS-R is described as a restored version of the original human-recorded speech corpus, not newly generated audio."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Table 1",
          "Reasoning": "The FLEURS-R corpus contains parallel speech and text data in 102 languages, consistent with the original FLEURS corpus. The text data corresponds to the transcriptions derived from human-generated text content (Wikipedia text sentences read aloud). No mention is made of generating or synthesizing text artificially; thus, text data is human-generated. Therefore, text modality is present and originates from human-generated sources as per the paper's description."
        }
      ]
    }
  },
  {
    "id": "ma24c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10976,
      "completion_tokens": 227,
      "total_tokens": 11203
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 Data Processing Pipeline",
            "reasoning": "The creation of the FLEURS-R corpus was done by applying the Miipher speech restoration model automatically on the existing FLEURS dataset. The paper describes the processing pipeline as an automatic method for speech restoration and filtering, with no mention of human annotators or manual annotation involved in the new dataset creation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit instructions described",
            "reasoning": "The paper does not mention any annotation instructions provided to annotators, since no human annotations were involved in creating the FLEURS-R dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric descriptions",
            "reasoning": "Since the process is automated and no manual annotation is reported, no scoring rubrics or evaluation criteria for annotators are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples provided",
            "reasoning": "No annotation examples or guidelines are presented in the paper for this dataset; the dataset was created via a deterministic automatic process."
          }
        }
      ]
    }
  },
  {
    "id": "ma24c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12166,
      "completion_tokens": 336,
      "total_tokens": 12502
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert humans conducting quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 3.1",
          "reasoning": "The paper describes the use of ASR-based filtering to identify successfully restored samples (Section 2.2 Data Processing Pipeline). Additionally, in Section 3.1, the Maestro-U grapheme ASR model is used to evaluate consistency in semantic content between the original and restored speech. This indicates that AI models were used as judges for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While speech restoration involved an automated pipeline, the quality assurance described specifically involved AI model judgments rather than purely algorithmic or rule-based automated verification. The paper does not detail automated verification of code or formulas as QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance via AI models is described; thus, it is incorrect to say no QA process is applied or documented."
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11776,
      "completion_tokens": 412,
      "total_tokens": 12188
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper indicates that the FLEURS-R corpus uses the same constituent samples as the original FLEURS corpus, which was recorded data, but does not state that any new speech recordings were made by humans for FLEURS-R."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Processing Pipeline",
          "reasoning": "The FLEURS-R corpus was generated by applying the Miipher speech restoration model to the existing FLEURS corpus, thereby generating new, restored speech samples entirely by a machine learning model without new human recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the speech data was produced by human translation activities."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate the data; the paper focuses on speech restoration, not translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not merely collected or aggregated from existing sources but processed using a restoration model to improve audio quality."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Processing Pipeline",
          "reasoning": "FLEURS-R is based on the existing FLEURS corpus with modifications applied via speech restoration modeling, representing a transformed version of the original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data generation process for the new dataset."
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12294,
      "completion_tokens": 337,
      "total_tokens": 12631
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3.1",
          "reasoning": "The FLEURS-R corpus is used to train multi-speaker Virtuoso 2 text-to-speech (TTS) baseline models from scratch, as described in Section 3.3.1, where the same hyperparameters are used for training on either FLEURS or FLEURS-R to enable consistent comparison."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The corpus is used for automatic speech recognition (ASR) based intelligibility evaluations and speech naturalness evaluations (SQuId MOS) to benchmark quality improvements of the restored speech over the original FLEURS corpus, as explained in Sections 3.1 and 3.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The dataset is analyzed for trends and patterns including character error rates by language and naturalness scores across different languages and utterance durations, as reported in Sections 3.1 and 3.2."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13017,
      "completion_tokens": 530,
      "total_tokens": 13547
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Introduction; Table 1",
          "reasoning": "The paper introduces FLEURS-R, a speech-restored version of the FLEURS corpus that contains N-way parallel speech data in 102 languages. It explicitly states that FLEURS-R maintains the linguistic scope of FLEURS, covering 102 languages spanning over 17 language families (Abstract, Introduction). Table 1 compares FLEURS-R as containing 102 locales, confirming the high multilingual nature of the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains 102 languages, not exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes 102 languages rather than only English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages, not only one single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as speech and text data for multiple human languages, with no mention of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech and text data spanning 102 human languages, not biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains natural human languages; there is no indication that constructed or fictional languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included are explicitly specified as 102 locales, hence not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset explicitly contains human language speech and corresponding text."
        }
      }
    }
  },
  {
    "id": "ma24c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10235,
      "completion_tokens": 173,
      "total_tokens": 10408
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not explicitly provide or mention any link or location where the code used for constructing the FLEURS-R dataset or the speech restoration process is publicly available. While the dataset itself is publicly released on Hugging Face, there is no statement or reference indicating the availability of the code used for data processing or restoration."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 2.1",
          "reasoning": "The dataset creation process is documented in detail in Sections 2 and 2.1, where the authors explain the speech restoration pipeline, including specific modifications to the Miipher model to accommodate multilingual data, and describe the data processing pipeline such as filtering restored samples. This provides sufficient transparency and detail about the dataset construction."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8343,
      "completion_tokens": 265,
      "total_tokens": 8608
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset construction",
          "Reasoning": "The dataset Propicto-eval was created by recording 3,011 sentences spoken by 62 unique speakers over six months, involving a recording campaign. This audio data involves human speech recordings explicitly stated in the paper."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset construction",
          "Reasoning": "The transcriptions of the audio recordings were derived from the recorded sentences and their original text sources such as children's stories, everyday situations, and medical domain sentences. This text originates from human-created sentences used for the recording campaign."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset construction and Table 1",
          "Reasoning": "The pictogram token sequences aligned with the audio and text were generated automatically by applying the method from Macaire et al. [22], which involves algorithmic mapping rules and a restricted lexicon to produce a 'pictographic language' representation from the transcriptions. This indicates that the pictogram token sequences are model generated from text."
        }
      ]
    }
  },
  {
    "id": "macaire24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9143,
      "completion_tokens": 222,
      "total_tokens": 9365
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.5 Human evaluation",
            "reasoning": "The human evaluation in Section 4.5 describes that 100 randomly chosen sentences from the Propicto-orf\u00e9o test data were annotated by two expert annotators from the project. The use of multiple expert annotators is explicitly stated here."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.5 Human evaluation",
            "reasoning": "The paper states that the evaluation is performed by adapting the analytical framework MQM which provides guidelines and procedures for measuring translation quality, indicating detailed annotation instructions were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.5 Human evaluation",
            "reasoning": "The human evaluation uses the MQM framework which involves assigning error types and severity levels, effectively constituting a rubric for scoring translations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention providing annotation examples for the human evaluation process in any section."
          }
        }
      ]
    }
  },
  {
    "id": "macaire24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10333,
      "completion_tokens": 480,
      "total_tokens": 10813
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance of the dataset annotations was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 4.5 Human evaluation and Section 3.1 Dataset construction",
          "reasoning": "Human evaluation was performed by two expert annotators from the project who are experts, as stated in Section 4.5. The evaluation involved annotation on 100 randomly chosen sentences from the Propicto-orf\u00e9o test set and Propicto-eval data. Although the human evaluation is described mainly for translation outputs, these annotators are expert members of the project, implying expert human involvement in quality assessment related to the data and task."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-experts conducted quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was performed by an AI model acting as a judge."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction and Section 4.5 Human evaluation",
          "reasoning": "The pictogram translations were generated automatically using the method of Macaire et al. [22], an automated alignment and generation process following specific rules and a restricted lexicon. This indicates an automated process was used to generate dataset annotations, which constitutes a form of automated quality control of the generation. However, explicit verification or correction is not described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents quality assurance steps involving automated processes and expert human evaluation, thus quality assurance is present."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9943,
      "completion_tokens": 545,
      "total_tokens": 10488
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The authors construct two new datasets: Propicto-orf\u00e8o and Propicto-eval. Propicto-eval was created through a recording campaign involving 62 unique speakers reading sentences compiled from publicly available ARASAAC PDFs, over six months. This process was overseen by human contributors (annotators and recorders), indicating original human-generated data in terms of audio recording and curation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve human speech recordings and associated human-curated pictogram translations; no description indicates the data was generated entirely by AI models independently."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data results from human translation from one language to another; pictogram translations are generated from French text but not described as a human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The pictogram translations are generated using a method described by Macaire et al. [22], which applies specific rules and a restricted lexicon to French transcriptions. This is a rule-based mapping rather than a machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "Propicto-orf\u00e8o is built upon the Corpus d'Etude pour le Fran\u00e7ais Contemporain (CEFC) [21], which is an existing speech/text corpus. The authors extracted audio segments and their transcriptions from it. This constitutes data aggregated from existing sources without significant modification to the source audio or text."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The pictogram terms translations in both datasets are generated by applying the method of Macaire et al. [22], which converts the textual transcriptions into pictogram sequences using rules and a restricted lexicon. This constitutes data derived from existing transcriptions with modifications/adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and construction process are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10461,
      "completion_tokens": 576,
      "total_tokens": 11037
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new datasets for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions training an NMT model from scratch, but this model is trained on the Propicto-orf\u00e8o dataset, which is derived from pre-existing data and generated via a method from prior work [22]. The datasets themselves are constructed for aligned speech-text-pictogram sequences but not explicitly described as enabling training from randomly initialized parameters independently from pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Training details",
          "reasoning": "The paper explicitly states that several models (Wav2Vec2.0 ASR, mBART, T5-large, NLLB-200, ConST) are fine-tuned on the Propicto-orf\u00e8o training data, adapting pre-trained models for the speech-to-pictogram task. This demonstrates the use of the dataset for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of the datasets for reinforcement learning post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction, Section 4 Results and Discussion",
          "reasoning": "The Propicto-eval dataset is constructed specifically for evaluation purposes, containing recordings from unique speakers and relevant contexts. The paper uses this dataset and the test splits of Propicto-orf\u00e8o to benchmark and measure performance of models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.5 Human evaluation",
          "reasoning": "The datasets are used for in-depth analysis, including human evaluations analyzing error types and overall quality scores to better understand model behavior on pictogram translation beyond automatic metrics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets are used as external knowledge bases or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets have multiple practical usages described in the paper for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11184,
      "completion_tokens": 632,
      "total_tokens": 11816
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The proposed datasets, Propicto-orf\u00e8o and Propicto-eval, contain French speech, text transcriptions, and pictogram translations only. There is no mention of multiple languages present in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets contain French speech and their aligned transcriptions plus pictogram representations, which are considered as a target sequence of terms but not a second human natural language. Therefore, the dataset entries do not contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The input speech and transcripts are in French, not English. The textual content is explicitly stated to be French speech and text."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The datasets are constructed from French speech corpora and their French transcriptions aligned with pictogram sequences. The language used is French, which is a non-English human language. This applies to both Propicto-orf\u00e8o and Propicto-eval."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries consist of audio (speech), text transcriptions, and pictogram term sequences. There is no indication of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or formal logical expressions or symbolic notations are described as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset focuses on human speech (French) and pictogram sequences. There is no mention of biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The pictogram sequences represent graphic symbols matched to concepts; they are not described as a constructed or artificial language such as Klingon or Esperanto."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is clearly documented as French, so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Since the dataset contains human language data (French speech and text), it does not apply as non-language data."
        }
      }
    }
  },
  {
    "id": "macaire24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8402,
      "completion_tokens": 219,
      "total_tokens": 8621
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 3.1 Dataset construction; Footnote 3",
          "reasoning": "The paper explicitly states in the abstract that they constructed and released two freely available corpora for the speech-to-pictogram task. In Section 3.1 and the beginning of the paper, they mention that the code is released at https://github.com/macairececile/speech-to-pictograms, indicating public accessibility of the code associated with dataset creation and processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset construction",
          "reasoning": "The paper provides a detailed description of the dataset construction process in Section 3.1, including the source corpora used (Corpus d'Etude pour le Fran\u00e7ais Contemporain), the steps for extracting audio segments, methods for generating pictogram translations, the domains covered, and the evaluation dataset creation process involving recording and translation. It also notes compliance with data protection rights overseen by a Data Protection Officer, reflecting transparency and ethical considerations."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7536,
      "completion_tokens": 316,
      "total_tokens": 7852
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Corpus Overview",
          "Reasoning": "The paper states the Santa Barbara Corpus comprises 60 recordings (24 hours) of naturally-occurring speech recorded from 1987\u20131996 using stereo microphones. These recordings capture conversational spoken American English from diverse speakers, indicating human involvement in recording natural speech audio."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Corpus Overview and Section 3.1 Transcript Processing and Correction",
          "Reasoning": "The corpus includes detailed human-created transcripts at the level of intonation units, containing utterance-level time marks, speaker labels, overlap labels, and other annotations. These transcripts were originally created for linguistic and human viewing purposes, indicating they are human-generated text."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Corpus Overview",
          "Reasoning": "The corpus includes metadata about participants\u2019 gender, age, hometown, education, occupation, ethnicity, and recording details. This metadata is structured informational data about the recordings and speakers, produced with human involvement during corpus preparation."
        }
      ]
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8336,
      "completion_tokens": 219,
      "total_tokens": 8555
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1, Transcript Processing and Correction",
            "reasoning": "The paper states that typographical and annotation errors were extensively checked and many annotation errors were corrected manually. The complexity and quality of the original annotations suggest they were initially prepared by multiple human experts (linguists) as the corpus was originally collected for linguistic study."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section",
            "reasoning": "The paper does not provide explicit evidence of detailed annotation instructions given to annotators in the preparation of the original Santa Barbara Corpus or during their manual corrections."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section",
            "reasoning": "There is no mention of scoring rubrics or guidelines used for annotation evaluation or consistency checks in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit section",
            "reasoning": "The paper does not mention providing annotation examples as part of the guidelines or documentation for the Santa Barbara Corpus annotations or their processing."
          }
        }
      ]
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9526,
      "completion_tokens": 445,
      "total_tokens": 9971
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper that multiple human experts performed quality assurance on the Santa Barbara corpus annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Transcript Processing and Correction",
          "reasoning": "The paper describes manual corrections and extensive checking of annotation errors performed by humans to fix typographical inconsistencies and annotation errors in the transcripts. The mention of 'many annotation errors were corrected manually' and 'the results were then extensively checked for processing errors' indicates that multiple human annotators without explicit mention of subject matter expertise conducted these corrections to make the dataset usable for evaluation."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models were used for forced alignment and diarization (Section 3.3), there is no indication that these AI models were used for quality assurance (QA) of annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Sections 3.2 Anonymization Filter Detection; 3.3 Alignment",
          "reasoning": "The paper describes development of a manually-tuned algorithm to detect anonymized regions (Section 3.2), which included automated spectral analysis and manual correction\u2014suggesting automated detection augmented by human correction. Also, an automated forced alignment pipeline involving three forced alignment models was used to re-align segments for better temporal accuracy (Section 3.3). They also computed agreement metrics between models and manually inspected disagreements. These processes exemplify automated quality assurance steps through algorithmic and rule-based techniques combined with some manual review, indicating an Automatic Process QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes quality assurance processes involving both manual correction by multiple humans and automated processes, so an absence of QA does not apply."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9136,
      "completion_tokens": 467,
      "total_tokens": 9603
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any new data created entirely from scratch by human contributors. Instead, it utilizes an existing corpus collected from 1987\u20131996."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was generated solely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation systems as per the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 2 Corpus Overview; Section 3 Corpus Preparation",
          "reasoning": "The Santa Barbara Corpus of Spoken American English is an existing corpus collected historically; the authors repurpose and assemble it for evaluation purposes. The data is collected from original recordings made between 1987 and 1996, thus it is aggregated from existing recordings and transcripts."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 Corpus Preparation, especially Sections 3.1 Transcript Processing and Correction, 3.2 Anonymization Filter Detection, and 3.3 Alignment",
          "reasoning": "The authors apply modifications such as correcting and reformatting transcripts, detecting anonymized audio regions, re-aligning segments using forced alignment, and re-segmenting. These are transformations and adaptations of the existing original data to make it suitable for automatic processing and evaluation, indicating the data is derived from original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and processing of the dataset, so N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9654,
      "completion_tokens": 395,
      "total_tokens": 10049
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 1 Abstract; Section 4 Experimental Configuration; Section 5.2 Speech Recognition Results",
          "reasoning": "The Santa Barbara Corpus of Spoken American English (SBCSAE) is repurposed by the authors primarily for evaluation of multi-talker speech technology including speaker diarization and speaker-attributed ASR. The paper describes preparing the corpus for automatic processing and demonstrates the failure of state-of-the-art systems on it, showing inconsistent system performance (Section 1 Abstract, Section 4, and Section 5). No indication is given that the dataset is used for training or fine-tuning models, only for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 1 Abstract; Section 5.1 Performance Analysis (implied in the text); Section 5.2 Speech Recognition Results",
          "reasoning": "The paper includes detailed analysis of system performance on the corpus, such as analyzing diarization errors across speaker demographics and conversation types (Section 5), and investigating performance trends such as difficulties with elderly and teenage speakers. This use of the dataset to analyze trends and system weaknesses shows it is used for analysis purposes as well as evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents extensive preparation and use of the SBCSAE corpus for evaluation and analysis of speech technology; thus, the dataset has documented and demonstrated utility."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10377,
      "completion_tokens": 584,
      "total_tokens": 10961
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper introduces the Santa Barbara Corpus of Spoken American English (SBCSAE) as a dataset comprising spoken American English only. There is no mention of multiple languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only American English speech and transcripts; there are references to code-switching marks but no indication that a second language is present systematically or separately."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (Corpus Overview), Section 3.1 (Transcript Processing and Correction)",
          "reasoning": "The paper clearly states that the Santa Barbara Corpus contains naturally-occurring speech from American English speakers and is designed for linguistic study of spoken American English. The transcripts and annotations are detailed and pertain solely to English speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of the dataset containing any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves spoken language recordings and transcripts; although processing scripts are mentioned, the dataset itself does not include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of mathematical or formal logical expressions or symbolic representations in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is speech recordings and transcripts of human spoken language; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains naturally occurring American English speech; no fictional or artificially created languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language is explicitly specified as American English; there is no uncertainty about the language content of the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language and transcripts; therefore, language is present in the dataset."
        }
      }
    }
  },
  {
    "id": "maciejewski24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7595,
      "completion_tokens": 130,
      "total_tokens": 7725
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 1 (Introduction) and Section 3 (Corpus Preparation)",
          "reasoning": "The paper states that all corpus processing scripts and results will be released and integrated into the Lhotse audio preparation library for ease of use, indicating that the preprocessing code is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Corpus Preparation)",
          "reasoning": "The paper provides detailed descriptions of the dataset preparation process including transcript processing and correction, anonymization filter detection, and alignment procedures, demonstrating thorough documentation of the dataset construction and processing steps."
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9307,
      "completion_tokens": 111,
      "total_tokens": 9418
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection",
          "Reasoning": "The MASCFLICHT Corpus is a novel dataset introduced by the authors, containing speech samples recorded from 30 German participants using a smartphone microphone. The data consists of human speech recorded under five different face mask conditions, collected in a controlled environment with human participants reading or describing tasks. This confirms it is audio data, recorded by humans, hence human-generated."
        }
      ]
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10107,
      "completion_tokens": 300,
      "total_tokens": 10407
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1 Data Collection",
            "reasoning": "The data was collected by having 30 participants (15 female, 15 male) record speech samples wearing different types and coverage of face masks. Participants themselves followed instructions to produce speech under different conditions. Therefore, the annotated conditions (i.e., mask worn and coverage) come from controlled data collection involving human participants rather than expert annotation or AI-based labeling."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Data Collection",
            "reasoning": "Participants were given a set of guided and free speech tasks with detailed experimental instructions, including conditions of mask type and coverage (e.g., surgical or FFP2 mask, partial or full coverage). The data collection protocol specifies reading sentences or describing pictures under specified mask conditions, indicating instructions for participants to produce labeled data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "There is no mention in the paper of scoring rubrics or criteria used by annotators to assess or label data. The dataset labels correspond to experimental conditions set by the data collection protocol rather than subjective annotation requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not provide or mention annotation examples or sample labeled data in the guidelines. The labels are derived from controlled recording conditions rather than annotation processes needing examples."
          }
        }
      ]
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11297,
      "completion_tokens": 330,
      "total_tokens": 11627
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of multiple human experts performing quality assurance on the dataset annotations or content in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single non-expert human performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple non-expert humans conducting quality assurance for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model is described as being used for quality assurance purposes in the paper."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "While the paper does not explicitly mention manual quality assurance of annotations, it describes an automated pre-processing pipeline for voice activity detection and segmentation (Section 2.2). This can be considered an automated process for verifying voiced segments, indirectly contributing to quality control of the data segments used. However, no explicit quality assurance of annotation labels (such as mask type or coverage area) by automated methods is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents pre-processing and data partitioning procedures, but it does not describe any explicit human or automated quality assurance of dataset annotations. Automated pre-processing is present, thus N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10907,
      "completion_tokens": 367,
      "total_tokens": 11274
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The paper explicitly states that the MASCFLICHT Corpus consists of speech samples collected from 30 German participants who recorded speech under five different face mask conditions using a smartphone. The data was recorded in a controlled environment with human participants performing speech tasks, which constitutes original content created entirely from human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected from human speakers and not generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data being produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated through machine translation as per the paper content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was created through direct recording from human participants rather than aggregation from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is original speech recorded from participants; thus, it is not based on existing sources with modification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the dataset is clearly specified."
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11425,
      "completion_tokens": 397,
      "total_tokens": 11822
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results) and Section 3.2 (Model Description)",
          "reasoning": "The dataset is used to train various models from scratch to recognize face mask type and coverage area from speech. Models such as CNNscratch and RN18scratch are explicitly trained from randomly initialized weights on the MASCFLICHT dataset, as described in Section 3.2 and evaluated in Section 4."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Model Description) and Section 4 (Experimental Results)",
          "reasoning": "The RN18tuned model uses transfer learning by tuning a pre-trained ResNet18 model on the MASCFLICHT dataset to perform classification tasks, indicating supervised fine-tuning on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results)",
          "reasoning": "The dataset is partitioned into train, development, and test sets and used to evaluate baseline model performances quantitatively via metrics such as Unweighted Average Recall (UAR). Evaluation results are reported in detail in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results) and Section 5 (Conclusions)",
          "reasoning": "The dataset enables analysis of feature representations and model architectures for the specific paralinguistic tasks of face mask type and coverage area recognition, including observation of suitability of features and model choices, as discussed in Section 4 and 5."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12148,
      "completion_tokens": 543,
      "total_tokens": 12691
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The dataset contains speech samples from only German speakers; no multiple languages are present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "Only one human language, German, is spoken by all participants; no bilingual content exists."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The corpus consists only of German speech; the text and speech data are not in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The dataset is collected from 30 German participants speaking German, with all sentences and speech samples in German exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section specifies code or programming language content in the dataset.",
          "reasoning": "The dataset consists solely of speech recordings; no programming or code samples are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No section indicates presence of mathematical or logical symbolic expressions in the dataset.",
          "reasoning": "The dataset features speech audio samples without embedded mathematical or logical notations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No description suggests inclusion of biological sequences or non-human communication forms in the dataset.",
          "reasoning": "The dataset comprises human German speech samples only."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "All speech is in standard German; no fictional or artificial languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Not applicable as the language of dataset entries is clearly documented as German.",
          "reasoning": "The paper clearly specifies the language spoken by participants."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data (German speech), hence language is present."
        }
      }
    }
  },
  {
    "id": "mallolragolta23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9366,
      "completion_tokens": 225,
      "total_tokens": 9591
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention",
          "reasoning": "The paper does not provide any explicit reference, link, or mention to publicly available code repositories related to the dataset collection, preprocessing, or generation. There is no indication that the code used to construct the MASCFLICHT corpus is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (The MASCFLICHT Corpus), specifically subsection 2.1 (Data Collection) and 2.2 (Data Pre-Processing and Partitioning)",
          "reasoning": "The paper provides a detailed description of the data collection procedure, participant demographics, recording conditions, mask types and configurations, and the recording setup (smartphone model, environment). It also describes the preprocessing steps (voice activity detection, segmentation, normalization), partitioning strategy (speaker-independent train/development/test splits following a LOSO-CV approach), and rationale behind data balancing and participant distribution. Ethical approval and informed consent are also mentioned. Overall, these details form a thorough documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8723,
      "completion_tokens": 285,
      "total_tokens": 9008
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.1; Section 2.2",
          "Reasoning": "The SOMOS dataset consists of 20,100 synthesized speech utterances produced by 200 different neural text-to-speech systems based on Tacotron models, all synthesized using the LPCNet vocoder. The speech samples are entirely synthetically generated by AI acoustic models, not manually recorded by humans."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The synthesis corpus includes 2,000 sentences derived from multiple human-generated textual sources such as: 100 sentences from the LJ Speech scripts (excluded from LJ Speech training), sentences from Blizzard Challenges 2007-2016, semantically unpredictable sentences, Wikipedia and general public domain sentences from the web. These texts were manually created by humans and curated by the authors for coverage."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3; Section 3.1",
          "Reasoning": "The SOMOS dataset includes crowdsourced human evaluations of synthesized utterances' naturalness collected via Amazon Mechanical Turk. The subjective ratings are human-generated data, representing human listener judgments of audio samples."
        }
      ]
    }
  },
  {
    "id": "maniati22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9523,
      "completion_tokens": 239,
      "total_tokens": 9762
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 specifies that subjective evaluations were conducted online with crowd-sourced naive listeners via Amazon Mechanical Turk (AMT). A total of 987 workers participated, indicating multiple annotators who are non-experts recruited from crowdsourcing platforms."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Explicit annotation instructions were provided to listeners: they were asked to rate naturalness on a 5-point Likert scale with detailed scale options and meaning provided, ensuring annotators understood the task."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The 5-point Likert scale with defined categories from 1 (very unnatural) to 5 (completely natural) served as a rubric guiding annotators on how to score naturalness."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not mention providing annotation examples or sample annotated data to the raters in the crowdsourcing task."
          }
        }
      ]
    }
  },
  {
    "id": "maniati22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10713,
      "completion_tokens": 494,
      "total_tokens": 11207
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper states that an experienced US native linguist, who is an expert and native speaker, evaluated approximately 100 test pages covering all synthetic TTS systems. His evaluations showed 100% consistency and strong positive correlation with crowdsourced ratings, indicating expert quality assurance contribution alongside non-expert data."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only documents a single human expert's involvement for quality assurance; multiple experts performing QA is not mentioned."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that quality assurance was conducted by a single non-expert; only crowdsourced non-expert annotations and a single expert are described."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "Multiple non-expert annotators participated as crowd workers on Amazon Mechanical Turk to rate utterances' naturalness. The paper describes recruiting nearly 1000 listeners across three locales who provided ratings, constituting multiple human non-expert QA of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any AI model being used as a judge or quality assessor for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper describes using automated quality control procedures including validation questions, ground truth samples, enforcement of audio playback, and filtering based on patterns in annotations to automatically detect and exclude suspicious or low-quality responses, constituting an automated QA process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a detailed and multi-faceted quality assurance process involving multiple non-expert annotators, a single expert annotator, and automated filtering, so QA is present and documented."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10323,
      "completion_tokens": 511,
      "total_tokens": 10834
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Sentences",
          "reasoning": "The dataset's 2,000 sentences include 100 sentences randomly selected from the LJ Speech script and 1,900 sentences composed from various public domain sources (Blizzard Challenges scripts, Wikipedia, and 'general' public domain sentences). This indicates that the textual data - at least partially - were newly composed or freshly selected by human contributors to ensure domain, length, and phoneme coverage."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.1 Systems",
          "reasoning": "The speech data consists of 20,100 utterances synthesized by 200 different neural TTS systems derived from Tacotron-based acoustic models. These synthetic utterances were generated entirely by AI models, i.e., neural TTS systems, using the designed sentences as input, producing novel audio data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper of any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence that any part of the dataset was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 Sentences",
          "reasoning": "The inference corpus sentences include publicly available sentences from Blizzard Challenges (2007-2016), Wikipedia, and other public domain sources. These were collected and aggregated to form the textual dataset used for synthesis."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The synthetic speech data are generated by models, but this is distinct from being 'derived' from existing audio sources with modifications; the synthetic speech is newly generated by TTS models, not adaptations of prior audio. The text corpus was collated rather than derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and generation method of the dataset, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10841,
      "completion_tokens": 481,
      "total_tokens": 11322
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the SOMOS dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 MOS Prediction Baselines",
          "reasoning": "The paper describes training MOS prediction models from scratch on the SOMOS dataset (Section 4), indicating its use for training models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 MOS Prediction Baselines",
          "reasoning": "Pretrained MOS prediction models are fine-tuned on the SOMOS dataset validation set, demonstrating supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the SOMOS dataset is used for reinforcement learning post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 MOS Prediction Baselines",
          "reasoning": "The dataset is used to evaluate model performance, serving as a benchmark for state-of-the-art MOS prediction models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2 Quality of Ratings, Section 3.3 Results, Section 4 MOS Prediction Baselines",
          "reasoning": "The dataset is analyzed to assess subjective evaluation quality, listener rating consistency, and model prediction challenges, indicating usage for analysis of patterns and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the SOMOS dataset serving as a knowledge base for augmentation or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple practical usages of the SOMOS dataset; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11564,
      "completion_tokens": 422,
      "total_tokens": 11986
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2 (Speech Dataset)",
          "reasoning": "The dataset consists solely of English sentences synthesized by TTS models; only English is used throughout."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2 (Speech Dataset)",
          "reasoning": "There is no mention of any language other than English; hence no bilingual content is present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (Speech Dataset) and Section 3.1 (Listening Test Design)",
          "reasoning": "The dataset uses only English sentences, sampled from English corpora (LJ Speech, Blizzard Challenge, Wikipedia etc.), and listeners were restricted to native English speakers from English locales. Hence, all dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset only comprises English utterances, no other single non-English languages are present."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech utterances and subjective scores; there is no programming code content included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mathematical or logical symbolic data are part of the proposed dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech only and no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or evidence of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the entries is explicitly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of audio speech utterances containing English language, so it does contain language content."
        }
      }
    }
  },
  {
    "id": "maniati22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8782,
      "completion_tokens": 198,
      "total_tokens": 8980
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention the availability of any code repository or link for the data collection, preprocessing, or synthesis code used to generate the SOMOS dataset. There is a URL provided for dataset samples and information (https://innoetics.github.io/publications/somos-dataset/index.html), but no indication that code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process, including the design of the speech dataset with descriptions of TTS systems used (Section 2), the composition of the sentences (Section 2.2), the vocoder and acoustic models (Section 2.1), and the crowdsourced MOS evaluation setup and quality control measures (Section 3). This documentation is thorough and transparent for reproducibility and understanding."
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6411,
      "completion_tokens": 193,
      "total_tokens": 6604
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3",
          "Reasoning": "The BRA VE-MASKS corpus contains audio recordings of continuous Russian speech from 30 native speakers wearing various types of masks and without masks. The recordings were made using human participants and professional recording equipment, explicitly described in Section 3.1 and Section 3."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. and 3.2",
          "Reasoning": "The BRA VE-MASKS corpus contains multi-angle images and video recordings of persons' faces wearing different masks and without masks. These videos were recorded from multiple devices including smartphones and a tablet with human participants performing various tasks and head rotations in controlled conditions. This is explicitly stated in Sections 3 and 3.2."
        }
      ]
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7211,
      "completion_tokens": 221,
      "total_tokens": 7432
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.3",
            "reasoning": "The corpus annotation involved using the RetinaFace detector to perform region-of-interest (mask bounding boxes) annotation automatically, followed by manual checking for false positives. Thus, primary annotation was performed by an automatic detector with manual correction, effectively an automatic process with verification."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "No explicit mention of detailed annotation instructions provided to annotators is made in the paper's description of corpus annotation; only the method using the RetinaFace detector and manual checking is described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "There is no mention of any scoring rubrics or formal guidelines provided for annotation or verification stages in the corpus construction."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not describe any example annotations or guideline examples given to annotators or used during annotation."
          }
        }
      ]
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8401,
      "completion_tokens": 315,
      "total_tokens": 8716
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human expert conducted quality assurance of the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of multiple human experts or subject matter experts in the quality assurance process of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single human non-expert in the dataset creation or annotation process."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge to perform quality assurance on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Corpus annotation",
          "reasoning": "The corpus annotation utilized the RetinaFace detector to generate bounding boxes automatically; this automatic process was later manually corrected by removing false positives. Thus, the initial quality assurance of face region annotations involved an automatic process complemented by manual verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automatic annotation process and manual cleaning of errors, indicating that some form of quality assurance was performed; hence, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8011,
      "completion_tokens": 415,
      "total_tokens": 8426
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 BRAVE-MASKS corpus",
          "reasoning": "The BRA VE-MASKS corpus is described as newly created multimodal data containing audio and video recordings of 30 Russian native speakers wearing various masks. The data was recorded specifically for this study using three devices in office conditions, with participants performing various speech tasks. This indicates the data was collected entirely from human participants anew, not translated or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated purely by AI or machine learning models; all data pertains to human-collected audio-visual recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved translation from another language done by humans. The corpus speech is in Russian and is recorded, not translated."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is described or implied in the data creation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as an aggregation of existing sources; it was newly recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though data augmentation is applied during model training (e.g., Specaugment, Mosaic), the raw dataset itself is not derived or significantly transformed from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as newly recorded multimodal data from human participants."
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8529,
      "completion_tokens": 291,
      "total_tokens": 8820
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Mask Type Recognition Task) and Section 5 (Experimental Results)",
          "reasoning": "The BRA VE-MASKS corpus was used to fine-tune pre-trained deep neural networks for the supervised learning task of mask type recognition. The authors used pre-trained audio neural networks and Yolov5 object detector, replacing or tuning the output layers with BRA VE-MASKS data, which indicates supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Experimental Results) and the description of dataset splitting in Section 3.3",
          "reasoning": "The BRA VE-MASKS corpus was split into Train, Development, and Test sets, with Development and Test sets used to measure and report performance of the developed models, thereby serving an evaluation/benchmarking role."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9252,
      "completion_tokens": 252,
      "total_tokens": 9504
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3, e.g., 3.1 Bimodal part",
          "reasoning": "The BRA VE-MASKS corpus contains audio-visual recordings of continuous Russian speech by native Russian speakers, explicitly stating 30 native Russian speakers participated in the data collection. This indicates the dataset entries contain only one human language, which is non-English (Russian)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "markitantov22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6470,
      "completion_tokens": 187,
      "total_tokens": 6657
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not contain any links or mentions of publicly available code repositories for the dataset construction, data collection, preprocessing, or annotation processes. The only code-related reference is to open-source codes for Yolov5 training (footnote 1), which is unrelated to the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (BRAVE-MASKS corpus) including subsections 3.1, 3.2, 3.3",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 3, covering recording devices, data collection procedures, participant demographics, mask types, data partitioning, annotation procedures including manual checking of face bounding boxes, and data statistics. Such detailed description facilitates transparency and reproducibility in dataset construction."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7203,
      "completion_tokens": 198,
      "total_tokens": 7401
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 (Saxophone recordings for A2S) and Abstract",
          "Reasoning": "The paper introduces a novel collection of 1026 real saxophone recordings used specifically for audio-to-score transcription. These recordings are real-world saxophone performances captured in a home studio by human musicians, representing the audio modality with direct human generation through performance and recording."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 (Saxophone recordings for A2S)",
          "Reasoning": "The dataset includes the corresponding scores for the saxophone recordings in the Kern format, which is a digital symbolic music notation representation (textual format). These scores are human-generated annotations of the performances, created manually or derived from existing musical notations, and are used as the ground truth for transcription."
        }
      ]
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8003,
      "completion_tokens": 181,
      "total_tokens": 8184
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2",
            "reasoning": "Section 2 describes that the pieces were recorded by musicians proficiently trained in the instrument, implying expert musicians performed the recordings and provided corresponding score annotations; this indicates that multiple human experts annotated the data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not explicitly describe the presence of detailed annotation instructions provided to the annotators in Section 2 or elsewhere."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No mention of scoring rubrics or similar evaluative guidelines for annotations is found in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "No examples or example annotations provided to annotators or described in annotation guidelines are mentioned in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9193,
      "completion_tokens": 316,
      "total_tokens": 9509
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was performed by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description in the paper about multiple expert annotators performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains no information about quality assurance performed by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using an AI model as a judge or for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors applied transposition and encoding transformations to unify pitch references and encode the scores, no explicit automated quality verification or validation procedure is described as quality assurance for dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a novel dataset of real saxophone recordings and their corresponding scores, but it does not document any quality assurance process or validation procedure regarding the accuracy or correctness of the score annotations or recordings. The dataset is described as recorded by proficient musicians and the scores are given in Kern format, yet no explicit QA methodology is reported."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8803,
      "completion_tokens": 437,
      "total_tokens": 9240
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The authors created a novel corpus of 1026 real saxophone recordings performed by proficient human musicians in a home studio. These recordings are original musical performances rather than adaptations or translations of existing data. The associated digital scores are aligned with these recordings, constituting original human-generated data created from scratch."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "Synthetic audio data for saxophone performances was generated using two synthesis methods: FluidSynth with a SoundFont bank and the MIDI-DDSP neural synthesis model. This synthetic data is generated by models from the annotated scores to produce artificial audio and thus qualifies as new data created by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from one language to another through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources; it was newly recorded for this work."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The real recordings' scores were transposed to a unified pitch reference to standardize data for transcription, applying modifications (transposition) to existing score annotations. Furthermore, synthetic data is derived from these existing score annotations through synthesis processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and generation methods of its datasets."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9321,
      "completion_tokens": 532,
      "total_tokens": 9853
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the saxophone recordings dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2, 4.4",
          "reasoning": "The dataset of real saxophone recordings (Tr) introduced by the authors is used to train a model from scratch, i.e., training the neural network using the recordings and their score annotations as supervised training data in an end-to-end audio-to-score transcription task."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2, 4.4",
          "reasoning": "The combined dataset (Tc) scenario includes a fine-tuning policy where the model is first trained on synthetic data and then fine-tuned on the real saxophone recordings dataset introduced, constituting supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of reinforcement learning based post-training techniques such as RLHF applied to this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The newly introduced real saxophone recordings dataset is also used as test data for evaluation of transcription models' performance, serving as a benchmark for evaluating end-to-end audio-to-score transcription systems."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4, 6",
          "reasoning": "The dataset is used to analyze patterns in transcription performance, the benefits of synthetic data, and to provide insights into challenges of transcribing real recordings, which is primarily analytical."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described or used as a knowledge base to augment models via retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced is demonstrably used in multiple practical ways including training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10044,
      "completion_tokens": 622,
      "total_tokens": 10666
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced in the paper comprises recorded saxophone performances and their corresponding music scores, without any mention of linguistic content in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries in exactly two human languages; there is no indication of such linguistic content."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset primarily consists of musical audio recordings and their corresponding symbolic music scores; no English textual content is described as part of the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any entries in a single non-English human language; the focus is on music recordings and symbolic music notation."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper references software tools and programming for analysis, the dataset itself does not contain programming code or structured programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 2 and 3.1",
          "reasoning": "The dataset includes digital music scores encoded in Kern format, a symbolic representation including musical notation, pitch, rhythm, and other annotations, which can be considered as a formal symbolic language. Additionally, the representation involves subword encodings of music tokens (duration, pitch, accidentals), which are formal symbolic notations. These symbolic music annotations are akin to a formal logical or mathematical notation system used to encode music."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human music recordings and symbolic music scores; there are no biological sequences or non-human communication systems included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any fictional or artificially created languages such as Klingon or Esperanto; it contains musical recordings and scores only."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages or symbolic systems used in the dataset are explicitly described (e.g., Kern format music notation); thus, the linguistic nature is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains symbolic representations of music scores (Kern format), which is a formal symbolic system, thus language or notation is present."
        }
      }
    }
  },
  {
    "id": "martinezsevilla23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7262,
      "completion_tokens": 152,
      "total_tokens": 7414
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or explicit mention",
          "reasoning": "The paper does not mention or provide any link or reference to source code related to the dataset creation, collection, preprocessing, or generation. There is no indication that the code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Saxophone recordings for A2S",
          "reasoning": "The paper documents the dataset creation process in Section 2 by describing the collection of 1026 real saxophone recordings with digital scores in Kern format. It details the instrument types, recording conditions, music styles, annotation formats, transposition process, and metadata included. This provides transparency about the dataset creation and characteristics."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9240,
      "completion_tokens": 182,
      "total_tokens": 9422
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 and 4.3",
          "Reasoning": "The new 60-hour and extended up to 300-hour artificial training dataset was created from clean Czech TV/R broadcast recordings. The authors artificially created overlapped speech by summing two recordings with a random shift, adding noise via augmentation from CHiME-4 dataset, and appending non-speech frames. Thus, the base audio is human-generated broadcast recordings, while the overlapped speech training data is model generated through mixing and augmentation as described in Section 4.2. Additionally, the new evaluation dataset of about 10 hours of Czech and Slovak broadcast shows is human generated, recorded from multiple TV, radio, and internet sources, manually annotated by humans (Section 4.3)."
        }
      ]
    }
  },
  {
    "id": "mateju22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10040,
      "completion_tokens": 202,
      "total_tokens": 10242
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.3",
            "reasoning": "Section 4.3 states the authors prepared a new dataset for evaluation with over 10 hours of Czech and Slovak TV, radio, and internet shows and manually annotated the data by hand. The plural wording and manual annotation indicate multiple human experts likely performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "The paper mentions manual annotations with labels for four acoustic events but does not describe or provide annotation instructions or guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "There is no mention of scoring rubrics or detailed evaluation criteria within the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "The paper does not mention providing specific examples of annotations or annotated data segments as guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "mateju22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11230,
      "completion_tokens": 355,
      "total_tokens": 11585
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was performed by a single human expert annotator on the new broadcast dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple human experts were involved in quality assurance or annotation of the new dataset; annotators are not described as experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert annotator performed the quality assurance or annotation on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.3 Data and metrics, paragraph 3",
          "reasoning": "The paper describes that over 10 hours of Czech and Slovak broadcast data were collected and manually annotated by hand. However, it does not specify that annotators were experts or single annotators but implies the use of multiple annotators without expert qualification. The data is labeled with four acoustic event types by manual annotation, indicating a human annotation process likely conducted by multiple annotators who are not specifically characterized as experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance process is not described as being performed by an AI model for the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or algorithmic quality assurance for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents that the new dataset was manually annotated and made public, indicating a quality assurance process by human annotators; thus, it is not N/A."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10840,
      "completion_tokens": 451,
      "total_tokens": 11291
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The authors created a new evaluation dataset comprised of over 10 hours of Czech and Slovak TV, radio, and internet shows which were manually annotated by hand with four acoustic event labels (speech, non-speech, overlapped speech, and cross-noise). This data was created entirely from original broadcast recordings and manually labeled by humans, indicating original content not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new evaluation dataset consists of original broadcast recordings manually annotated rather than being a simple collection or aggregation of existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The authors prepared an artificial training dataset for x-vector classifier training by mixing clean Czech recordings from existing TV/radio broadcasts to create overlapped speech examples. This dataset is derived from existing clean recordings with modifications such as time shifts, mixing of speakers, noise augmentation and addition of non-speech frames, indicating a derived data source."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and creation process of the new datasets."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11358,
      "completion_tokens": 343,
      "total_tokens": 11701
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 and Section 5",
          "reasoning": "The authors introduce a new Czech/Slovak broadcast dataset with hand-annotated overlapped speech segments which they use exclusively for evaluation and benchmarking of their proposed overlapped speech detection approach. This is evidenced in Section 4.3 where they describe the preparation and annotation of the dataset, and in Section 5 where the dataset is used to evaluate the method's performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The newly introduced Czech/Slovak broadcast dataset is clearly used for evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12081,
      "completion_tokens": 523,
      "total_tokens": 12604
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4.3 Data and metrics",
          "reasoning": "The new evaluation dataset introduced by the authors consists of Czech and Slovak TV, radio, and internet shows, which implies the use of two different, but closely related, human languages. Hence, the dataset is bilingual but also can be considered multilingual as it contains data in more than one language."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 4.3 Data and metrics",
          "reasoning": "The authors explicitly mention the evaluation dataset contains recordings in Czech and Slovak languages only, exactly two languages, making it bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any new dataset containing English-only material introduced by the authors."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset has entries from two languages (Czech and Slovak), so it is not monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described consist of audio recordings with speech, no programming or structured code-related content is involved."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of datasets containing mathematical or logical expressions is provided."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data is introduced as a new dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or artificial languages in the new datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the new dataset are explicitly stated and clearly described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language speech, so it cannot be non-linguistic."
        }
      }
    }
  },
  {
    "id": "mateju22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9299,
      "completion_tokens": 180,
      "total_tokens": 9479
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.3",
          "reasoning": "The paper states that the authors have prepared and manually annotated a new evaluation dataset of Czech and Slovak broadcast recordings and that they decided to release these annotated data to the speech research community, but there is no mention or link to source code or scripts related to the data collection, preprocessing, annotation, or generation being publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.3",
          "reasoning": "The paper documents the dataset creation process in Section 4.3, describing in detail how the dataset was gathered (from Czech and Slovak TV, radio, internet shows), its total duration, the hand-annotation process with defined labels, and the format of annotation files provided. This documentation provides transparent and complete information regarding dataset creation for downstream use."
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8856,
      "completion_tokens": 169,
      "total_tokens": 9025
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Automated data-harvesting process",
          "Reasoning": "The new dataset is automatically harvested by processing Swedish public sources with audio recordings (broadcasts, parliament sessions, YouTube, audiobooks) involving human-spoken speech audio, which is recorded and provided originally by humans."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Automated data-harvesting process",
          "Reasoning": "The harvesting uses associated textual data from subtitles, transcripts, summaries, and e-books, which are human-generated texts manually created for these media; thus, the text data are human created and not model generated or of unknown origin."
        }
      ]
    }
  },
  {
    "id": "mateju23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9656,
      "completion_tokens": 233,
      "total_tokens": 9889
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1, Section 4.3",
            "reasoning": "Section 4.1 states test sets were transcribed automatically then edited by a native speaker, indicating human expert involvement for transcription correction. Section 4.3 describes data harvesting and clean chunk selection using ASR outputs but does not mention human annotators for harvesting annotations, implying automatic selection for training data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "The paper does not mention providing any annotation or transcription instructions to annotators or guidelines for editing transcripts; the transcription correction by native speakers is mentioned but no detailed instructions are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "No mention of scoring rubrics or any formal evaluation criteria for annotations or corrections is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.3",
            "reasoning": "The paper does not reference annotation examples or sample annotations in any section, including appendices."
          }
        }
      ]
    }
  },
  {
    "id": "mateju23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10846,
      "completion_tokens": 419,
      "total_tokens": 11265
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance of dataset annotations was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information that a single non-expert conducted QA for the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document QA by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.3 Automated data-harvesting process",
          "reasoning": "The harvesting scheme uses an existing ASR system (an AI model) to transcribe audio chunks and compares these transcriptions with reference fragments. Chunks with a character error rate below 2% are added to the training set, effectively an AI model performing QA by judging alignment quality."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.3 Automated data-harvesting process",
          "reasoning": "The alignment between audio chunks and reference texts uses Levenshtein distance (an algorithmic technique), and a voice activity detector is employed for splitting audio into chunks. The selection criterion based on character error rate < 2% is an automatic, rule-based QA process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described, including automated and AI-model-based verification steps."
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10456,
      "completion_tokens": 456,
      "total_tokens": 10912
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that original content was created entirely from scratch by human contributors for the training datasets. Instead, publicly available datasets were used and additional data were harvested automatically using ASR systems."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the new datasets were generated entirely by AI or machine learning models without referencing or transforming existing data. The harvested data comes from publicly available recordings and associated text, not model-generated content."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that any data was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The new training data was collected from existing public sources such as broadcast, parliament, YouTube, and audiobook archives. These sources provided audio and associated text, which were harvested by processing and alignment but the content was collected as-is from existing archives without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The data harvesting process involved aligning ASR output with reference texts, splitting into chunks, and only selecting segments with low character error rate. This processing and selection represents modifications and transformation of the original data sources to create a usable training corpus, thus the data can be considered derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and the method of generation are well documented in the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10974,
      "completion_tokens": 406,
      "total_tokens": 11380
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2, 4.4, 4.5",
          "reasoning": "The newly harvested Swedish dataset (over 1,000 hours collected via automatic harvesting combining Swedish and Norwegian data/models) is used to train E2E ASR models, including training from scratch or fine-tuning initialized from Norwegian models, demonstrating improved model performance with increasing amounts of this data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.3, 4.4, 4.5",
          "reasoning": "The harvested Swedish datasets are used to fine-tune pre-trained models (initialization from Norwegian models) iteratively as part of the data harvesting and model improvement process to reduce word error rates in speech recognition."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning based post-training techniques such as RLHF applied to these datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.5",
          "reasoning": "The authors prepared and publicly released multiple Swedish test sets covering various applications and domains which are used exclusively for evaluation and benchmarking of the developed ASR models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The paper analyzes the impact of increasing amounts of harvested training data on ASR model performance, investigating trends in word error rate reduction relating to multilingual training and transfer learning to understand bootstrapping efficiency."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of the dataset serving as a knowledge base for augmenting models in retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11697,
      "completion_tokens": 630,
      "total_tokens": 12327
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes datasets primarily focused on Swedish and Norwegian languages for training and evaluation purposes. Although there is mention of English models used for transfer learning, the proposed datasets introduced do not contain entries with more than two human languages simultaneously."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Sections 4.2, 4.3, and 4.4",
          "reasoning": "The new datasets introduced for training combine Swedish and Norwegian languages, as described extensively in Sections 4.2 and 4.3, and detailed in Table 4 (Section 4.4). The harvested datasets include Swedish audio-text pairs supported by Norwegian data/models to improve training and the final ASR model. The bilingual nature is explicitly leveraged for multilingual training and transfer learning, making these datasets bilingual (Swedish and Norwegian)."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new datasets introduced in the paper contain only English-language content. The English model mentioned is pre-existing and used for transfer learning, not part of a new dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not solely monolingual in Swedish or any other single non-English language, as they incorporate bilingual (Swedish-Norwegian) data combinations for model training and harvesting."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce datasets containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are part of the introduced datasets."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets pertain solely to human speech and text alignments; there is no mention or inclusion of biological or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not include fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the datasets is clearly specified as Swedish and Norwegian; there is no ambiguity about the language(s) used."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced contain human language data (Swedish and Norwegian) and thus cannot be labeled as containing no language."
        }
      }
    }
  },
  {
    "id": "mateju23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8915,
      "completion_tokens": 170,
      "total_tokens": 9085
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.3 and 5",
          "reasoning": "The paper does not include any link or mention of publicly available code repositories for the data harvesting or dataset construction process. It only mentions that test sets or links to their sources and detailed logs are released on their cloud, but no code for dataset creation or data harvesting is provided or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 4.3 and 5",
          "reasoning": "The paper provides a detailed description of the dataset creation and automated data harvesting process, including splitting of audio files, aligning with text, criteria for including data chunks, and iterative retraining of models. This documentation is comprehensive and precise, enabling understanding of the dataset construction process, although no publicly shared code is mentioned."
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6487,
      "completion_tokens": 272,
      "total_tokens": 6759
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3: DocSpeech Dataset",
          "Reasoning": "DocSpeech dataset consists of text tokens extracted from semi-structured Word documents (from ReadingBank dataset) which are originally human-created documents. The text is used with their reading order and bounding boxes. The source documents are explicitly human-generated Word documents."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3: DocSpeech Dataset",
          "Reasoning": "DocSpeech contains 10K audio clips synthesized by concatenating mel-spectrograms of word-level audio mappings obtained from forced alignment on LJSpeech dataset. The audio clips are generated by a process involving forced alignment and concatenation with mel-spectrogram tokens to produce the speech audio clips, thus the audio modality is model generated synthetic audio, not originally human recorded speech."
        },
        {
          "Modality": "image",
          "Human Generated": false,
          "Model Generated": false,
          "Unknown Origin": true,
          "Reference": "Section 4.1: Problem Formulation",
          "Reasoning": "Document images used as input containing the layout and bounding boxes are mentioned but their origin is not specified or detailed in terms of being human-generated or model-generated in the paper; thus origin is unknown."
        }
      ]
    }
  },
  {
    "id": "mathur22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7287,
      "completion_tokens": 269,
      "total_tokens": 7556
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3 (DocSpeech Dataset)",
            "reasoning": "The dataset is created synthetically by repurposing the open-source ReadingBank dataset and generating audio clips by concatenating word-level mel-spectrograms aligned from LJSpeech using the Gentle Forced Aligner. Therefore, the annotation, specifically the correct reading order of text tokens, bounding boxes, and alignments, stems from automated processes rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 (DocSpeech Dataset)",
            "reasoning": "There is no indication in the paper that detailed annotation instructions were provided to human annotators, as the data creation relies on repurposing existing datasets and automatic alignment methods."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 (DocSpeech Dataset)",
            "reasoning": "No scoring rubrics or formal evaluation criteria for annotation quality are mentioned in the context of data annotation since the dataset was constructed automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 (DocSpeech Dataset)",
            "reasoning": "The paper does not mention providing annotation examples or guidelines; the dataset creation process is an automatic pipeline integrating OCR, reading order from DocX files, and forced alignment."
          }
        }
      ]
    }
  },
  {
    "id": "mathur22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8477,
      "completion_tokens": 369,
      "total_tokens": 8846
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information about multiple human experts performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single human non-expert was involved in quality assurance of the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance involving multiple human non-expert annotators for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any AI model being used as a judge or performing quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3. DocSpeech Dataset",
          "reasoning": "The dataset DocSpeech is created synthetically by re-purposing the ReadingBank dataset that provides structured Word documents with known correct reading orders. The authors use forced alignment tools (Gentle Forced Aligner, a Kaldi-based audio alignment tool) to obtain word-level alignments from LJSpeech, and combine the mel-spectrograms corresponding to each token in the correct reading order. This process relies on automated audio alignment and merging methods rather than manual validation. There is no explicit mention of human verification, but an automatic process is used to ensure correctness."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper does document the use of an automatic process for dataset construction and audio alignment, so it is not the case that no quality assurance process is described."
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8087,
      "completion_tokens": 316,
      "total_tokens": 8403
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3, DocSpeech Dataset",
          "reasoning": "The DocSpeech dataset is created by re-purposing the existing open-source ReadingBank dataset which contains semi-structured Word documents with annotated reading sequences and bounding boxes. The authors then map audio clips from the LJSpeech dataset to each unique word using forced alignment. They combine mel-spectrograms of these word-level audio snippets in the correct reading order of the document to synthesize speech clips for documents. This process involves modifying, transforming, and adapting existing datasets (ReadingBank and LJSpeech) to construct the new DocSpeech dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8605,
      "completion_tokens": 506,
      "total_tokens": 9111
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.3 (Training Details), Section 4 (Our Approach), Section 6 (Results and Analysis)",
          "reasoning": "The DocSpeech dataset is used to train the proposed DocLayoutTTS model from scratch in a supervised manner, as described in Section 5.3 where the models are trained using the dataset. The dataset provides paired documents and speech, allowing the model to learn text reordering and speech synthesis end-to-end. This is evidenced by descriptions of curriculum learning with DocSpeech data and performance evaluation on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using DocSpeech for fine-tuning pre-trained models. Instead, DocSpeech is used primarily for training and evaluation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of reinforcement learning or RL-based post-training methods applied on the DocSpeech dataset is present in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.2 (Evaluation), Section 6 (Results and Analysis)",
          "reasoning": "DocSpeech is used for evaluation purposes where 100 samples are selected from DocSpeech test set to evaluate models and collect MOS scores comparing the proposed method with baselines. This shows the dataset serves as a benchmark for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or characteristics; rather, it is used for training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not serve as a knowledge base or retrieval augmentation resource."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The DocSpeech dataset is actively used for both training and evaluation as documented in multiple sections."
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9328,
      "completion_tokens": 551,
      "total_tokens": 9879
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper only mentions English language data, with no indication of other human languages present in the DocSpeech dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only English text and audio; no mention is made of a second human language in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3, DocSpeech Dataset; Introduction, Abstract",
          "reasoning": "The DocSpeech dataset is created from Word documents with English text tokens and audio clips of a single English speaker reading the documents. The paper specifies the use of LJSpeech (an English speech dataset) and Gentle forced aligner for English word-level alignment, indicating that the content is in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention inclusion of any non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or structured code-related content is indicated within the dataset; the dataset consists of natural language text and speech."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is based on document text and audio recordings, with no indication of mathematical or logical symbolic content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only human speech data; no biological sequences or non-human communication data is involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper explicitly states the language content as English; language is well specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes natural language text and corresponding speech, hence language is present."
        }
      }
    }
  },
  {
    "id": "mathur22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6546,
      "completion_tokens": 191,
      "total_tokens": 6737
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "The paper does not provide any section or link referring to the dataset construction code",
          "reasoning": "The paper mentions the creation of the DocSpeech dataset by re-purposing the open-source ReadingBank dataset and describes the data processing steps in Section 3, but it does not provide any link or mention making the code for dataset construction (such as audio concatenation, forced alignment, or preprocessing scripts) publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: DocSpeech Dataset",
          "reasoning": "The paper provides detailed documentation about the dataset creation process in Section 3, explaining how DocSpeech was created by combining ReadingBank documents with LJSpeech forced alignment, the method of concatenating mel-spectrograms with m-tokens to ensure natural pauses, and the dataset statistics such as number of files, average duration, and train/test splits."
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7354,
      "completion_tokens": 122,
      "total_tokens": 7476
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 2.3, Section 3",
          "Reasoning": "SiDi KWS is a dataset consisting of 24.3 million labeled audio recordings of single-spoken keywords. These audio samples were created by applying forced alignment to existing public transcribed speech datasets (LibriSpeech, Mozilla Common Voice, MLS) that contain human speech recordings. Therefore, the audio samples originate from human recordings captured in the original datasets, not generated by models or synthetic procedures."
        }
      ]
    }
  },
  {
    "id": "meneses22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8154,
      "completion_tokens": 242,
      "total_tokens": 8396
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 and Section 2.3",
            "reasoning": "The SiDi KWS dataset was created automatically by applying forced alignment with the Montreal Forced Aligner tool on already transcribed public speech datasets, using the open-source KeywordMiner framework developed by the authors to segment single-spoken keywords. There is no mention of human annotators manually labeling or reviewing the data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to annotators, since the annotation was done automatically by a forced alignment algorithm rather than by human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No scoring rubrics or quality assessment criteria for manual labeling are provided or described, given that the dataset creation was automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide annotation examples from guidelines because the segmentation and labeling were done automatically without human annotators following instructions or examples."
          }
        }
      ]
    }
  },
  {
    "id": "meneses22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9344,
      "completion_tokens": 372,
      "total_tokens": 9716
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator for the SiDi KWS dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper of multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset creation process does not mention any single human non-expert annotator performing QA."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence suggesting that multiple human non-experts conducted quality assurance on the annotations or data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA process did not involve an AI model serving as a judge; the aligner tool used an ASR model for alignment, but not for quality assurance assessment."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 2.2",
          "reasoning": "The dataset SiDi KWS was generated by automatically applying forced alignment using the Montreal Forced Aligner (MFA) tool, which automatically aligns speech and transcriptions. KeywordMiner framework automates processing with MFA to segment and label single-spoken keywords. There is no mention of human verification; thus, the QA relies on an automatic forced alignment process, which is an algorithmic verification of timing and segmentation. Therefore, the quality assurance can be considered as an automatic process via forced alignment and tooling."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described in terms of automatic forced alignment and segmentation using MFA and the KeywordMiner framework, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8954,
      "completion_tokens": 324,
      "total_tokens": 9278
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by human contributors; instead, it is derived from existing public transcribed speech datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated entirely by AI or machine learning models creating new content; it is created by segmenting existing speech recordings using forced alignment."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 2.3, Section 3",
          "reasoning": "While the dataset uses existing public transcribed speech datasets as inputs, the final single-word audio clips are not just aggregated unchanged; they are segmented based on forced alignment, which is a transformation process."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 2.2",
          "reasoning": "The SiDi KWS dataset is derived from existing public datasets (LibriSpeech, Common Voice, MLS) by applying forced alignment (using the Montreal Forced Aligner) to segment and label single spoken keywords. This process is a modification and adaptation of existing data to produce the new dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9472,
      "completion_tokens": 205,
      "total_tokens": 9677
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3, Results and Discussion",
          "reasoning": "The dataset is analyzed in Section 3 in terms of distribution of keywords, recording counts, length of keywords and audio, and language characteristics. These analyses demonstrate the use of the dataset primarily to understand characteristics and trends rather than specific training or evaluation tasks."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10195,
      "completion_tokens": 533,
      "total_tokens": 10728
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract and Section 3 Results and Discussion",
          "reasoning": "The paper explicitly states that the SiDi KWS dataset is a public large-scale multilingual dataset composed of audio recordings in four human languages: English, French, German, and Spanish. Section 3 details the dataset statistics per language and Figure 2 shows spectrograms of keywords in all four languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two languages (English, French, German, Spanish), thus it is not a bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages, not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than one language and includes English, so it is not monolingual in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of spoken keyword audio recordings in human languages; there is no mention of any programming code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains no mathematical or formal logical notations or symbolic representations; it is composed of speech audio samples."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech only, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any fictional or artificially created languages included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are all explicitly identified and documented as English, French, German, and Spanish."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language speech data and thus does contain language content."
        }
      }
    }
  },
  {
    "id": "meneses22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7413,
      "completion_tokens": 170,
      "total_tokens": 7583
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.2 Keyword Miner and Section 4 Conclusion",
          "reasoning": "The paper explicitly states that KeywordMiner, the framework used to generate the SiDi KWS dataset, is open-source and publicly shared to benefit the research community (Section 2.2 and Conclusion). This indicates that the code for data preprocessing, forced alignment, segmentation, and export is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Sections 2 (Methodology) and 3 (Results and Discussion), explaining forced alignment, the KeywordMiner framework components, input datasets, lexicons, infrastructure used, and dataset statistics. This detailed documentation supports reproducibility."
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6394,
      "completion_tokens": 193,
      "total_tokens": 6587
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2, 2.3",
          "Reasoning": "The dataset includes microphone audio recordings of isolated phonemes uttered by two human participants during the voice-based cursor control task. These audio recordings were captured in real time using a human-operated microphone device as part of the experimental protocol designed by the authors."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, 2.3",
          "Reasoning": "The dataset contains intracranial neural recordings obtained via stereotactic electroencephalography (SEEG) depth electrode arrays implanted in two human patients. The neural data are human-generated in the sense that they are acquired from human brain activity using clinical recording equipment during the experimental task as introduced and collected by the authors."
        }
      ]
    }
  },
  {
    "id": "meng22d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7194,
      "completion_tokens": 281,
      "total_tokens": 7475
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.3 and 2.4",
            "reasoning": "The paper describes that the dataset was created through a clinically supervised experiment involving two patients implanted with intracranial electrodes, with subsequent post-hoc analyses conducted by the researchers. The annotation of speech and phoneme labels was based on instructed isolated phoneme utterances during the voice-based cursor control task, implying expert oversight in labeling the speech events."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 and 2.4",
            "reasoning": "Participants were given explicit instructions to produce sustained utterances of specific phonemes (/a/, /i/, /o/, /m/, /t\u0283/) during calibration and the main experiment, indicating that annotation was guided by a detailed protocol on stimulus production."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "The paper does not mention any explicit scoring rubrics or criteria used for annotation beyond binary labeling (speech vs silence) or phoneme classes based on isolated utterances, suggesting no use of rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention in the paper",
            "reasoning": "There is no indication that example annotations or example guidelines were provided for the annotation process in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "meng22d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8384,
      "completion_tokens": 378,
      "total_tokens": 8762
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single human expert annotator for the newly collected dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving multiple human expert annotators for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert human performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information suggests that multiple non-expert human annotators conducted quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used to perform quality assurance as a judge of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the study uses algorithmic processing for decoding and feature extraction, the paper does not describe automated verification processes specifically aimed at quality assurance of the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process for the newly introduced intracranial recordings and phoneme annotations. There is no mention of any human or automated verification or validation of dataset content or labels."
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7994,
      "completion_tokens": 383,
      "total_tokens": 8377
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.3 and 4 Discussion",
          "reasoning": "The dataset was collected from two patients who produced isolated phonemes in a voice-based cursor control task, generating original intracranial neural recordings and corresponding audio data. This data was created entirely from scratch by human participants during the described experiment and is not derived, translated, or adapted from any existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was generated purely by AI or machine learning models. The data collected represents human neural and audio signals."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of data being produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any machine translation was performed on the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from existing sources but generated newly in the experimental sessions."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper performs analyses derived from the raw data, the dataset itself is not described as derived or transformed from existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of data generation are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8512,
      "completion_tokens": 260,
      "total_tokens": 8772
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 and Figure 1",
          "reasoning": "The dataset is used to train SVM decoders for voice activity tracking and phoneme classification, and the performance is measured with accuracy values compared to chance levels, indicating usage for evaluation of decoding accuracy."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1, 3.2, 4; Figures 2 and 3",
          "reasoning": "The dataset is primarily used to analyze correlations between intracranial neural signals and phoneme utterances to understand neural encoding of speech sounds, focusing on spatiotemporal and spectral characteristics for speech onset and sustained responses."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9235,
      "completion_tokens": 649,
      "total_tokens": 9884
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that the participants are native speakers of Australian English and produces isolated phonemes from English language sounds only. There is no mention of any other human language data present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains phoneme utterances exclusively in English, from native Australian English speakers only. No two languages are referenced or recorded."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (Study participants), Section 2.3 (Model calibration and data collection), Abstract",
          "reasoning": "The dataset consists of isolated phoneme utterances produced by native speakers of Australian English without any other language context. The phonemes used (/a/, /i/, /o/, /m/, /t\u0283/) correspond to English vowel and consonant sounds. The paper explicitly mentions participants as native Australian English speakers and analyzes English phonemes."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language utterances are recorded or presented in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes the implementation via Python code for data processing and modeling, the dataset itself consists of neural recordings and corresponding phoneme utterances, not entries of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data entries are neural signal time series and audio phoneme utterances. Mathematical descriptions are used in analysis, but the dataset itself does not contain mathematical or symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2.1 (Study participants), Section 3 (Results), Abstract",
          "reasoning": "The dataset includes intracranial neural recordings (SEEG) capturing brain activity related to speech production. These neural signals are biological data underlying human communication, thus qualifying the dataset as containing biological non-linguistic communication system data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are involved in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language context and identity are explicitly stated as English; thus the dataset language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset does contain linguistic content in the form of English phoneme utterances and corresponding neural recordings linked to speech."
        }
      }
    }
  },
  {
    "id": "meng22d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6453,
      "completion_tokens": 185,
      "total_tokens": 6638
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any link or reference to a public repository or website hosting the code related to data collection, preprocessing, or dataset generation. There is no mention of code availability anywhere in the text."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Materials and Methods) specifically subsections 2.1 to 2.5",
          "reasoning": "The paper thoroughly documents the dataset creation process including participant recruitment (2.1), the voice-based cursor control task and audio processing (2.2), model calibration and data collection procedure (2.3), methods for neural feature identification (2.4), and classification approaches (2.5). The experimental setup, synchronization of data streams, tasks, and analyses are well described to enable reproducibility in principle."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11354,
      "completion_tokens": 80,
      "total_tokens": 11434
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Speech material",
          "Reasoning": "The dataset consists of speech recordings collected from 11 Polish preschool children via a picture naming test using an electret microphone. The recordings are of real children's speech, captured directly, thus human generated audio signals."
        }
      ]
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12154,
      "completion_tokens": 255,
      "total_tokens": 12409
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 (Speech material)",
            "reasoning": "The paper describes that speech examination was performed by an experienced speech-language pathologist (SLP), and an independent annotator who is a linguist and an SLP conducted an additional auditory assessment to ensure the speech diagnostic description matched the recordings. This indicates the annotations were carried out by single human experts specialized in speech-language pathology."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1 (Speech material)",
            "reasoning": "The paper does not provide any information about detailed annotation instructions for the annotators. Only the role of the SLP and linguist-annotator is mentioned without explicit mention of instructions or guidelines for the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 (Speech material)",
            "reasoning": "There is no mention in the paper of specific scoring rubrics or scoring schemes provided for the annotation or speech examination process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "No examples of annotated data or annotation exemplars are provided or referenced in the paper for this new dataset."
          }
        }
      ]
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13344,
      "completion_tokens": 284,
      "total_tokens": 13628
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 Speech material",
          "reasoning": "The auditory assessment was conducted by an independent annotator who is described as both a linguist and a speech-language pathologist, indicating subject matter expertise. The paper explicitly states this assessment was performed by a single human expert to ensure diagnostic descriptions matched the recorded signals."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention involvement of multiple human experts in quality assurance or annotation processes."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA was performed by an expert annotator, as specified, so single human non-expert QA is not applicable."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of multiple non-expert annotators performing QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of AI models for QA purposes."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of algorithmic or rule-based automated verification procedures being applied for quality assurance of annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance procedures are explicitly described; thus, a lack of QA is not applicable."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12954,
      "completion_tokens": 384,
      "total_tokens": 13338
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 (Speech material)",
          "reasoning": "The dataset consists of speech recordings collected from 79 Polish preschool children through a picture naming test conducted in kindergartens. The recordings were made specifically for this study as part of a larger project and involved human participants uttering words selected by the researchers. This data was not translated or adapted from existing datasets but created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated entirely by AI or machine learning models; all data are human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced through human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data were collected specifically for this study and not aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although acoustic features are extracted from the raw audio data, this is a standard signal processing step, not a transformation of pre-existing data sources. The underlying dataset itself is original."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection method are clearly stated in the paper."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13472,
      "completion_tokens": 355,
      "total_tokens": 13827
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 Materials and Method; Section 3 Results & Discussion",
          "reasoning": "The dataset consists of speech samples from 11 Polish children used to extract acoustic features and conduct statistical analysis to investigate covert contrasts in sibilant substitutions. The data is analyzed to detect acoustic differences and patterns rather than used to train, fine-tune, or evaluate machine learning models. The study explicitly describes usage of linear mixed-effects models for acoustic analysis, indicating the dataset serves primarily for linguistic and acoustic analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used for the analysis of speech patterns through acoustic feature extraction and statistical modeling, as detailed in the paper."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14195,
      "completion_tokens": 403,
      "total_tokens": 14598
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and Table 1",
          "reasoning": "The dataset consists of recordings of Polish children's speech and uses only Polish language stimuli. The words for the picture naming test are all Polish words including Polish phonemes /\u00f9/ and /s/. The paper explicitly describes the study as focused on Polish phonology and child speech acquisition in Polish. Therefore, the dataset contains entries with exactly one language which is non-English (Polish)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken language samples from Polish children and thus contains language data, so it is not N/A."
        }
      }
    }
  },
  {
    "id": "miodonska23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11413,
      "completion_tokens": 143,
      "total_tokens": 11556
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link or reference to code repositories or other sources where the data collection, preprocessing, or analysis code associated with the dataset can be accessed."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Materials and Method",
          "reasoning": "The paper provides detailed documentation on dataset creation, including participant selection criteria, recording equipment and settings, the picture naming test and word selection, annotation and preprocessing steps, and feature extraction methods within Section 2. This thorough description ensures a reasonable level of transparency and completeness about how the dataset was constructed."
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8577,
      "completion_tokens": 215,
      "total_tokens": 8792
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Materials",
          "Reasoning": "The paper describes a new speech corpus collected by the authors, recording audio data from 106 children pronouncing Polish dental voiceless sibilants. The recordings were conducted in kindergartens and a primary school, capturing natural speech audio. This is a newly introduced dataset, explicitly collected by the authors for the study, thus the modality is audio, and it is human generated (recorded from children)."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Materials",
          "Reasoning": "The paper mentions that the data were collected using a multimodal acquisition device that records multiple audio channels and video streams with a stereovision module. Although the current study uses mainly audio data, the newly collected dataset includes video modality. The video data were recorded from children during speech tasks, thus human generated and captured with human involvement."
        }
      ]
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9377,
      "completion_tokens": 311,
      "total_tokens": 9688
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 1.1 and Section 2",
            "reasoning": "The speech corpus was collected and annotated by a team consisting of speech engineers and speech and language pathologists (experts). Multiple experts were involved in speech examination and annotation as stated in Section 1.1 ('collected and annotated by our team consisting of speech engineers and speech and language pathologists') and Section 2 describing the speech examination performed by SLPs."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 and Section 3.1",
            "reasoning": "The speech examination was conducted using a dedicated questionnaire covering anatomical and articulation features, suggesting detailed procedural instructions. Also, the segmentation and annotation used spectrograms and waveforms following a described procedure, implying annotation instructions were available."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2 and Section 3.2",
            "reasoning": "The speech diagnoses were prepared based on articulation evaluated methodically (normative, interdental, other) implying rubric standards. Furthermore, statistical modeling (LME models) was based on clear group definitions indicating rubric usage to score/discriminate articulation types."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 1 and Section 2",
            "reasoning": "Table 1 presents the six-word subset of linguistic material used for annotation indicating example stimuli were provided. Also, the paper specifies the word examples used in annotation processes, supporting example provision."
          }
        }
      ]
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10567,
      "completion_tokens": 425,
      "total_tokens": 10992
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 1.1 (Contribution of the paper) and Section 2 (Materials)",
          "reasoning": "The speech corpus was collected and annotated by a team including speech and language pathologists (SLP) and speech engineers. The speech examination was performed by an SLP (a subject matter expert), who prepared the speech diagnoses based on a dedicated questionnaire. This indicates that at least a single human expert was involved in the quality assurance of the dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions a team consisting of speech engineers and SLPs who collected and annotated data, but does not specify multiple experts conducting QA independently or collaboratively to validate annotations. Hence, it does not explicitly support QA by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any non-expert single annotator conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting multiple non-expert annotators were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of AI models to validate or perform QA on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic rules for QA of dataset annotations are described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a QA process involving human experts; therefore, QA is considered applied."
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10177,
      "completion_tokens": 417,
      "total_tokens": 10594
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Materials",
          "reasoning": "The paper clearly states that the speech data were collected from 106 children aged 4;11\u20138;0 in kindergartens and a primary school using audio recordings obtained via a multimodal device. The data collection involved human participants who produced original utterances in Polish, recorded specifically and solely for this study. The authors emphasize that the corpus includes active vocabulary words and was annotated by their team. There is no indication that the dataset was translated, adapted from existing corpora, or generated by models. Therefore, the dataset is original data created entirely by human contributors from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by AI or machine learning models without human speech input."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was created by new audio recordings from children, not aggregated from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset was derived by modification or adaptation of existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the origin and collection method of the dataset, so origin is documented."
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10695,
      "completion_tokens": 221,
      "total_tokens": 10916
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results & Discussion)",
          "reasoning": "The data collected and analyzed from the speech corpus of 106 Polish children are used primarily to analyze acoustic differences in the frication noise of dental voiceless sibilants and the influence of interdental articulation. Statistical modeling and acoustic feature analysis were performed to investigate trends and characteristics rather than for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11418,
      "completion_tokens": 500,
      "total_tokens": 11918
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of Polish child speech data. There is no mention of more than two human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only Polish speech; there are no two human languages represented."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English content; it is based entirely on the Polish language."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 2 Materials; Section 4 Results & Discussion",
          "reasoning": "The new dataset is composed of speech samples collected from Polish children speaking Polish, which is a non-English language. The paper explicitly states the speech materials and words used are in Polish."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-related content is included in the dataset; it is strictly speech recordings."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical expressions appear only in the analysis section but do not form entries of the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech samples only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of inclusion of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as Polish."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset comprises spoken language data (Polish), so language is present."
        }
      }
    }
  },
  {
    "id": "miodonska24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8636,
      "completion_tokens": 158,
      "total_tokens": 8794
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any links, references, or indications to publicly available code repositories for the dataset collection, preprocessing, annotation, or feature extraction. No code release or sharing information is provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 (Materials) and 3 (Methods)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including recruitment, recording setup (multimodal acquisition device), setting (kindergartens and school), ethical approval, speech examination and diagnosis by speech-language pathologists, language material details, the annotation procedure, and acoustic feature extraction methods. This documentation offers transparency and completeness about dataset construction supporting reproducibility."
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8317,
      "completion_tokens": 162,
      "total_tokens": 8479
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Spontaneous dialogue corpus",
          "Reasoning": "The authors recorded spontaneous audio dialogues between human speakers in controlled environments to create the dataset for training the dialogue TTS system. This data consists of recorded speech capturing natural conversations, indicating direct human involvement in data creation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2: Spontaneous dialogue corpus",
          "Reasoning": "The recorded speech was automatically transcribed using ASR, then manually corrected and annotated with time information by humans, producing text transcripts aligned with audio. This process confirms human-generated text data derived from human-produced speech recordings."
        }
      ]
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9117,
      "completion_tokens": 180,
      "total_tokens": 9297
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2",
            "reasoning": "Section 2 describes that speech transcripts were manually modified and given time information after automatic speech recognition. The manual modification of transcripts and time annotations implies human expert involvement for annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper states manual transcription correction and timing annotation but does not provide details of any specific annotation instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "There is no mention in the paper of scoring rubrics or standardized scales for the transcription or timing annotation tasks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No examples of annotation guidelines or example annotations are provided or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10307,
      "completion_tokens": 512,
      "total_tokens": 10819
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information that quality assurance of the spontaneous dialogue corpus transcription or annotation was conducted by a single human annotator with subject matter expertise or a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any process where multiple human experts performed quality assurance for the dataset. No details on expert annotators or their involvement in QA are provided."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit information that a single non-expert human performed any quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2, Spontaneous dialogue corpus",
          "reasoning": "The paper states that automatic speech recognition (ASR) was used to transcribe recorded speech, and then transcripts were manually modified and given time information to produce final transcripts with time information. Although the paper does not mention the expertise level of the transcribers, manual modification likely involved multiple human annotators. There is no explicit mention that these annotators were experts, so it is reasonable to assume they were non-experts performing manual correction and time alignment."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2, Spontaneous dialogue corpus",
          "reasoning": "The dataset transcription process involved the use of an automatic speech recognition (ASR) system to automatically transcribe the recorded speech. This AI model was used as an initial step before manual correction to generate transcripts."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated verification of dataset annotations or content beyond ASR for transcription. No algorithmic or rule-based quality assurance processes for verifying annotation correctness or consistency are described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper does describe a quality assurance process involving manual correction of ASR transcripts, so it is not the case that no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9917,
      "completion_tokens": 416,
      "total_tokens": 10333
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Spontaneous dialogue corpus",
          "reasoning": "The paper states that the authors recorded and transcribed actual spontaneous dialogues between human speakers in a controlled environment without predefined transcripts. The dialogues were freely conducted on a given topic, recorded on independent channels, and manually transcribed with time information added based on ASR outputs that were manually corrected. This indicates that the speech data is original content created entirely from scratch by human contributors and is not derived or translated from existing materials."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the generation of new datasets entirely by AI or machine learning models without references to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of data being produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation was made in the data preparation or dataset description."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from existing sources; it was explicitly recorded anew by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While certain derived data such as latent representations are produced by the model training, the original dataset (speech corpus) itself is not described as derived from existing sources via modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data creation process; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10435,
      "completion_tokens": 416,
      "total_tokens": 10851
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2",
          "reasoning": "The spontaneous dialogue corpus recorded by the authors is used directly to train the proposed VAE/GMV AE-VITS text-to-speech models from scratch, as described in Sections 4.1 and 4.2. The training uses 15,880 utterances from the recorded dataset to optimize the model parameters. Thus, the dataset serves as training data starting from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The spontaneous dialogue corpus is also used for evaluation purposes. Specifically, subsets of the recorded dialogues are utilized as evaluation sets to measure the quality of synthesized speech using objective metrics (e.g., MCD, MSD, DUR) in Section 4.2 and subjective mean opinion score (MOS) tests for utterance- and dialogue-level naturalness in Section 4.3. This shows the dataset\u2019s use for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2.4",
          "reasoning": "The paper uses the spontaneous dialogue dataset to analyze the learned latent space of the proposed GMVAE-VITS model. In Section 4.2.4, analyses such as latent distribution visualization and characterization of latent classes in terms of pitch, duration, and loudness are performed using the dataset. Hence, the dataset is used to analyze characteristics and trends relevant to speaking styles."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11158,
      "completion_tokens": 403,
      "total_tokens": 11561
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1.1",
          "reasoning": "The dataset described consists of Japanese dialogues only, as explicitly stated that the dialogues are in Japanese between two female speakers."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 4.1.1",
          "reasoning": "There is no mention of any dataset containing exactly two human languages; the dialogues are in Japanese only."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 4.1.1",
          "reasoning": "The dataset is not in English but in Japanese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1.1",
          "reasoning": "The proposed spontaneous dialogue corpus consists of Japanese dialogues only, as indicated by the description of 'Japanese dialogues' between two female speakers in isolated soundproof chambers."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The dataset consists of speech recordings and transcripts; no programming or code segments are part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "While the paper contains mathematical expressions to explain the methods, the dataset itself does not contain mathematical or logical notation as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The dataset pertains to human spoken language dialogues; no biological or animal communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The dataset does not include any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The language used in the dataset is clearly specified as Japanese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language data (Japanese), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "mitsui22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8376,
      "completion_tokens": 148,
      "total_tokens": 8524
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not mention or provide any link or reference to publicly available code related to data collection, preprocessing, or dataset generation. No repositories or URLs are cited for dataset construction code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "Section 2 'Spontaneous dialogue corpus' describes the dataset construction process, including recording method, transcription process, and characteristics of the data. It details how dialogues were recorded freely on given topics without predefined transcripts, ASR usage, manual transcription with time information, and processing to split audio into utterances. This provides substantial documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 12489,
      "completion_tokens": 167,
      "total_tokens": 12656
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Dataset Curation",
          "Reasoning": "The authors curated an audio-visual dataset by leveraging meta-data and open-source databases (FluencyBank) and conducted new annotations for audio segments labeling disfluencies. The audio data are sampled from human speech recordings, indicating human-generated origin."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Dataset Curation",
          "Reasoning": "The dataset includes video recordings of individuals speaking from FluencyBank, which contains video recordings captured from human subjects. The videos are segmented based on metadata aligned to audio annotations, indicating human-generated origin of the video modality."
        }
      ]
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 13289,
      "completion_tokens": 253,
      "total_tokens": 13542
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1 Dataset Curation",
            "reasoning": "The paper states that Lea et al. [13] released an annotated dataset of 3-second audio segments for the FluencyBank database with 3 annotators labeling each audio segment for disfluencies, and the authors conducted a distillation using majority vote to finalize the dataset used in their work."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Dataset Curation",
            "reasoning": "The mention of systematic labeling by 3 annotators and adherence to taxonomic recommendations implies that instructions or annotation guidelines were provided to ensure consistency among annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.1 Dataset Curation",
            "reasoning": "The use of majority vote to address inter-annotator disagreement and the taxonomy with five disfluency classes plus fluent speech suggest a rubric or defined criteria were used to guide annotation decisions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 Dataset Curation",
            "reasoning": "The paper does not explicitly mention the provision of annotation examples or sample annotations in the guidelines for annotators."
          }
        }
      ]
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 14479,
      "completion_tokens": 426,
      "total_tokens": 14905
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify quality assurance being conducted by a single human annotator with subject matter expertise or from the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The dataset annotations were performed by three annotators who systematically labeled each audio segment into disfluency classes. While it is not explicitly stated that the annotators are experts, they follow the taxonomic recommendations from prior work. The presence of multiple annotators implies multiple human experts were involved in quality assurance through majority vote distillation to handle inter-annotator disagreement."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human non-experts performed quality assurance; annotators are suggested to have some expertise implied by systematic labeling and adherence to taxonomic standards."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model used as a judge for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic rule-based techniques are described as part of quality assurance for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes an annotation process conducted by three annotators with majority vote distillation, indicating quality assurance was performed and documented."
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 14089,
      "completion_tokens": 327,
      "total_tokens": 14416
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The authors curate their audio-visual disfluency dataset by leveraging meta-data and existing open-source databases from previous works, specifically from FluencyBank and SEP28k datasets. They construct a paired multimodal dataset by segmenting videos based on existing audio annotation meta-data without creating original content independently."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The authors apply modifications such as filtering samples via majority vote distillation for annotation consistency and segmenting video clips aligned with audio annotations to build a multimodal dataset. This involves transforming and adapting pre-existing datasets to suit their multimodal disfluency detection task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 14607,
      "completion_tokens": 288,
      "total_tokens": 14895
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation; Abstract; Section 2 Approach",
          "reasoning": "The curated audio-visual disfluency dataset is used to train and fine-tune a multimodal disfluency detection model. The paper describes using this paired and annotated dataset to supervise the learning of their proposed multimodal fusion model that improves detection performance compared to unimodal baselines, indicating supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Experiments; Table 2; Figure 2",
          "reasoning": "The curated audio-visual dataset is used for benchmarking the performance of their proposed multimodal disfluency detection approaches and comparison with baselines across multiple disfluency tasks. Results are reported quantitatively, demonstrating use for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 15330,
      "completion_tokens": 526,
      "total_tokens": 15856
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The curated dataset is based on FluencyBank which contains only English language speech recordings. No evidence of multiple human languages is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English speech data; no mention of exactly two human languages being included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The dataset is curated from the FluencyBank corpus, which contains video recordings from 32 individuals with disfluent speech in the English language. This is explicitly stated in Section 2.1."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any non-English language data; only English is indicated."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio and video data with annotations for disfluency. There is no indication of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to define loss functions and models, the dataset itself does not contain entries with mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech recordings only; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of constructed (fictional or artificial) languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language (English) and thus cannot be classified as not containing any language."
        }
      }
    }
  },
  {
    "id": "mohapatra24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 12548,
      "completion_tokens": 218,
      "total_tokens": 12766
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The paper mentions that they curate a custom audio-visual dataset by leveraging metadata and raw datasets from previous works, and that they release this curated dataset along with training and evaluation splits for reproducibility. However, there is no mention or provided link to the release of the code used for the dataset curation, including processing, annotation distillation, or segmentation. Thus, the paper does not provide the code for constructing the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Dataset Curation",
          "reasoning": "The paper provides a detailed description of the dataset curation process, including leveraging meta-data and open-source databases from past works, adhering to taxonomic recommendations, recruiting annotators with majority vote distillation to address inter-annotator disagreement, segmenting videos based on meta-data to construct paired multimodal dataset, and sampling rates of audio and video. Therefore, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8449,
      "completion_tokens": 437,
      "total_tokens": 8886
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 FluencyBank",
          "Reasoning": "The FluencyBank dataset consists of video recordings (audio content) of people who stutter in reading and interview settings, recorded from real human subjects. The data was re-annotated by trained speech-language pathologists, indicating human involvement in both collection and annotation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 FluencyBank and 2.3 Data Augmentation for Disfluent Speech",
          "Reasoning": "Text transcripts in CHAT format accompany the audio data in FluencyBank, created by human annotators with expertise in stuttering. Additionally, ground truth transcripts are used as a base for synthetic disfluency insertion during data augmentation."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Data Augmentation for Disfluent Speech",
          "Reasoning": "Synthetic speech data is created using the OpenAI TTS API with inserted artificial disfluencies (word repetitions, phrase repetitions, interjections), generated during training to augment the small FluencyBank dataset, thus generated by a model."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Non-Disfluent Speech Datasets",
          "Reasoning": "FluencyBank-N is a synthetic dataset generated from FluencyBank transcripts using the SpeechT5 text-to-speech model combined with pre-trained speaker vectors, producing disfluency-free speech samples. This is model-generated audio data."
        }
      ]
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9249,
      "completion_tokens": 215,
      "total_tokens": 9464
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 FluencyBank",
            "reasoning": "The paper states that the FluencyBank dataset was re-annotated in CHAT format by trained speech-language pathologists with expertise in stuttering, ensuring inter-annotator agreement. This indicates that multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2 FluencyBank",
            "reasoning": "The paper mentions the re-annotation by experts but does not provide or mention any detailed annotation instructions or guidelines given to the annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 FluencyBank",
            "reasoning": "There is no mention or description of scoring rubrics provided to annotators or used during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 FluencyBank",
            "reasoning": "The paper does not mention any annotation examples provided to annotators as part of the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10439,
      "completion_tokens": 315,
      "total_tokens": 10754
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single human expert for any newly introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 FluencyBank and Section 2.2 FluencyBank (paragraph 2)",
          "reasoning": "The FluencyBank dataset's original CHAT transcripts had temporal misalignment and inaccuracies. To address this, the dataset was re-annotated in CHAT format by trained speech-language pathologists with expertise in stuttering while ensuring inter-annotator agreement. This implies multiple human experts conducted quality assurance on the annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not performed by an AI model as a judge for the dataset annotation or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance applied to dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes quality assurance involving expert re-annotation; hence, QA was performed and documented."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10049,
      "completion_tokens": 492,
      "total_tokens": 10541
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 FluencyBank",
          "reasoning": "The FluencyBank dataset consists of video recordings of people who stutter and corresponding transcripts re-annotated in CHAT format by trained speech-language pathologists. This indicates original content created by human contributors specifically for research purposes, not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.3 Data Augmentation for Disfluent Speech",
          "reasoning": "The authors generated new speech data by applying a novel data augmentation method involving synthesized disfluencies through the OpenAI TTS API, creating synthetic speech samples with controlled disfluencies. This generation is model-based and creates original augmented data without direct derivation from original speech audio."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any machine translation used to produce the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 FluencyBank",
          "reasoning": "The FluencyBank dataset is a collection of video recordings and transcripts aggregated from existing sources of people who stutter, albeit with re-annotation. Thus, it represents collated data gathered for the study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Non-Disfluent Speech Datasets",
          "reasoning": "The FluencyBank-N dataset was derived by synthesizing a disfluency-free version of the FluencyBank dataset using SpeechT5 TTS model with pre-trained speaker vectors, indicating data transformation based on existing source transcripts."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10567,
      "completion_tokens": 531,
      "total_tokens": 11098
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper fine-tunes a pre-trained wav2vec 2.0 model but does not train any model from scratch using the new datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.5 Implementation, Section 3 Results & Discussion",
          "reasoning": "The FluencyBank dataset and the augmented disfluent speech datasets introduced by the authors are used to fine-tune a pre-trained wav2vec 2.0 ASR model using supervised learning, as explicitly outlined in the methodology and confirmed by experimental results showing performance improvements after fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the datasets are used in any reinforcement learning based post-training approaches such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2.2 FluencyBank, 3 Results & Discussion",
          "reasoning": "The FluencyBank dataset (re-annotated by the authors) is also used as an evaluation benchmark for assessing ASR performance on disfluent speech and its augmented version for non-disfluent speech, as detailed in their experiments and tables reporting word error rates and BERTScore."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 Results & Discussion",
          "reasoning": "The authors analyze the impact of various fine-tuning and data augmentation strategies on ASR accuracy bias, speaker-level and disfluency-type specific results, providing a detailed characterization and understanding of ASR behavior on disfluent speech."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the datasets as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly demonstrates multiple practical usages including fine-tuning, evaluation, and analysis of the introduced datasets."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11290,
      "completion_tokens": 619,
      "total_tokens": 11909
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's introduced datasets, including FluencyBank and the augmented versions, contain only English content. There is no mention of data involving multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as containing English speech only, with no mention or evidence of exactly two languages being present in the data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (FluencyBank); Sections 2.3 and 2.5 (Data augmentation and dataset creation); throughout descriptions of the datasets.",
          "reasoning": "The FluencyBank dataset, which is the basis for the new data annotated and augmented by the authors, contains video and audio recordings of people who stutter speaking in English. The transcripts and audio samples are in English, and the synthetic speech generation (for FluencyBank-N) is also based on English transcripts. There is explicit reference to reading passages and interviews in English, and no mention of any other language. Hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or statement in the paper that the dataset contains any non-English language speech."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset entries containing programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While mathematical notation is present in the paper for describing evaluation metrics and formulas, the dataset entries themselves are speech and transcripts, which do not contain mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech recordings and transcripts related to stuttering; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly stated and identified as English; not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language speech and transcripts, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "mujtaba24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8508,
      "completion_tokens": 168,
      "total_tokens": 8676
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any links or references to public code repositories for the data augmentation, dataset construction, or preprocessing code. No URLs or mentions of hosting platforms like GitHub were found for the dataset construction or augmentation code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.3 Data Augmentation for Disfluent Speech and Section 2.2 FluencyBank",
          "reasoning": "The paper documents the dataset creation process including the use and re-annotation of the FluencyBank dataset, details on data augmentation methodology including how disfluencies are introduced and the parameters used, and the overall dataset characteristics. This provides transparency on how the data was prepared and augmented for training."
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 12282,
      "completion_tokens": 252,
      "total_tokens": 12534
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Datasets, paragraph describing the new dataset starting with 'In order to evaluate our models on realistic unseen data in-the-wild...'",
          "Reasoning": "The new dataset consists of 37.9 hours of audio clips from English-speaking celebrities and politicians. The 'authentic' or 'real' data clips are manually collected from publicly available sources such as podcasts and speeches, which are human generated recordings. These are directly captured human speech audio files."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3 Datasets, paragraph describing the new dataset starting with '...The fake clips are created by segmenting 219 of publicly available video and audio files that explicitly advertise audio deepfakes...'",
          "Reasoning": "The fake data clips in the new dataset are generated by audio deepfake algorithms as the segmentations come from publicly available video and audio files advertising audio deepfakes (e.g., synthesized clips of celebrities speaking absurd or out-of-character content). This implies they are model generated audio deepfake syntheses rather than authentic human recordings."
        }
      ]
    }
  },
  {
    "id": "muller22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 13082,
      "completion_tokens": 229,
      "total_tokens": 13311
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 (Datasets)",
            "reasoning": "The 'in-the-wild' dataset was manually collected and verified by the authors who are researchers (experts) in the field. The collection involved manually segmenting publicly available audio/video files and verifying authenticity, indicating expert human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 (Datasets)",
            "reasoning": "The paper does not describe any detailed instructions provided to annotators for labeling or segmenting the data; the dataset was curated manually by the authors without mention of formal annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 (Datasets)",
            "reasoning": "No scoring schemes or rubrics are mentioned regarding annotation or labeling of the 'in-the-wild' dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 (Datasets)",
            "reasoning": "No examples of annotated data or guidelines with example labels are provided in the paper; only general description of data collection is given."
          }
        }
      ]
    }
  },
  {
    "id": "muller22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 14272,
      "completion_tokens": 308,
      "total_tokens": 14580
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance of the dataset by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single non-expert performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that multiple non-expert human annotators conducted quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that automatic AI models were used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an automated verification or algorithmic/rule-based quality assurance process used for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new audio deepfake dataset collected from publicly available videos explicitly advertising audio deepfakes and corresponding genuine material. While they 'take care to include clips where the type of speaker, style, emotions, etc., are similar to the fake,' there is no explicit statement about any formal quality assurance process applied to validate or annotate the dataset beyond manual collection. Therefore, no quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 13882,
      "completion_tokens": 478,
      "total_tokens": 14360
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset consists of audio clips collected by segmenting publicly available video and audio files that explicitly advertise audio deepfakes, and manually collecting genuine instances from the same speakers using publicly available material. The data is not created from scratch by humans, but rather collected from existing sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset does not include data generated entirely by AI or machine learning models by the authors themselves. Although the fake clips are audio deepfakes created by TTS or voice synthesis models, these were sourced from publicly available files rather than generated by models the authors trained or created."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "There is no indication in the paper that data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "There is no indication in the paper that machine translation was used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The new dataset is composed by collecting and segmenting publicly available video and audio files advertising audio deepfakes for fake data, and collecting corresponding genuine audio from publicly available material such as podcasts and speeches. This constitutes data collation from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset is not described as a derived dataset with modifications or adaptations applied to existing sources. The authors segmented and selected clips but did not apply transformations to create new data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the origin and method of data collection for the new dataset; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 14400,
      "completion_tokens": 319,
      "total_tokens": 14719
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 'Datasets', Section 5.3 'Evaluation on in-the-wild data', and Table 1",
          "reasoning": "The new audio deepfake dataset is introduced explicitly to evaluate the generalization of existing models on real-world, in-the-wild deepfake audio data. The paper reports results of various models tested on this dataset to assess performance degradation compared to the ASVspoof benchmark. There is no indication that this dataset is used for model training or fine-tuning; it is solely used for benchmarking and performance measurement purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3 'Evaluation on in-the-wild data' and Conclusion",
          "reasoning": "The dataset is also used for analyzing model performance gaps and challenges in generalization from lab-based benchmarks to real-world scenarios. The authors analyze trends and discrepancies in model behavior on this dataset, highlighting its utility for understanding audio deepfake detection challenges."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 15123,
      "completion_tokens": 529,
      "total_tokens": 15652
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "The new dataset contains English-speaking celebrities and politicians only; there is no evidence of more than two human languages being present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "The dataset entries are explicitly described as English-speaking content only, with no mention of precisely two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 Datasets, first paragraph; Abstract",
          "reasoning": "The new dataset consists of English-speaking celebrities and politicians, both real and deepfake audio clips. The paper repeatedly states that the dataset features English-speaking content, e.g., clips of Barack Obama and Donald Trump, with no mention of other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "No indication that the dataset contains any non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "The dataset consists of audio files, no programming or code-related content is present."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "The dataset is audio data, no mathematical or logical symbolic expressions are included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "General",
          "reasoning": "The dataset concerns human voice audio deepfakes, no biological sequences or animal communication involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "The dataset contains real and synthetic English speech, no constructed fictional languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3 Datasets",
          "reasoning": "Language of dataset entries is clearly stated as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset entries contain spoken English language audio."
        }
      }
    }
  },
  {
    "id": "muller22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 12341,
      "completion_tokens": 225,
      "total_tokens": 12566
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 Datasets",
          "reasoning": "The paper states that the new real-world audio deepfake dataset is collected and published, with a URL provided to access the data (deepfake-demo.aisec.fraunhofer.de/inthe-wild). However, the paper does not mention or provide any link to publicly available code related to the data collection, preprocessing, or construction of this dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Datasets",
          "reasoning": "The paper provides a detailed description in Section 3 about the creation of the new 'in-the-wild' audio deepfake dataset. It explains the process of collecting 17.2 hours of forged audio and 20.7 hours of authentic audio from 58 celebrities and politicians, including how fake clips are obtained from publicly available deepfake videos and how corresponding genuine instances are manually collected with matching style and conditions. The paper also mentions the average clip length and that the data is publicly accessible. This constitutes clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8355,
      "completion_tokens": 180,
      "total_tokens": 8535
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "KSC2 is an industrial-scale speech corpus comprising transcribed audio recordings collected from multiple sources including crowdsourced read speech, professional speakers, television news, televised talk shows, radio programs, parliament speeches, and podcasts. These are manually transcribed recordings of human speech, indicating human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The corpus includes manually created and verified transcriptions of the speech audio data, produced by native Kazakh and Russian speakers (human transcribers). These transcriptions are human-generated text data corresponding to the speech audio, confirming the existence of human-generated text modality."
        }
      ]
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9155,
      "completion_tokens": 229,
      "total_tokens": 9384
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1 (Structure) and Section 3.1.1 to 3.1.7",
            "reasoning": "The KSC2 corpus, including newly added data (television news, television and radio programs, parliament speeches, podcasts), was manually transcribed by native Kazakh and Russian speakers, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper states that data was manually transcribed by native speakers but does not explicitly mention any detailed annotation instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "There is no mention of any scoring rubrics or rubric-based guidelines for annotations for the new datasets in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide information or reference about annotation examples given to the annotators for the newly introduced data."
          }
        }
      ]
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10345,
      "completion_tokens": 404,
      "total_tokens": 10749
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The manual transcription was performed by native Kazakh and Russian speakers, which can be considered as human experts or members of the target demographic. Transcribers are likely experts due to their native speaker status and task specificity."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Multiple native Kazakh and Russian speakers are mentioned as involved in manual transcription, indicating that quality assurance was performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA being performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention QA by multiple non-expert annotators; annotators are native speakers and thus treated as experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "For the crowdsourced KSC recordings, an ASR system automatically accepted utterances with zero character error rate, which indicates an AI model was used as a judge in quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of purely algorithmic or rule-based automatic verification of annotations or content is given."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented, combining human transcription and AI-assisted verification."
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9955,
      "completion_tokens": 480,
      "total_tokens": 10435
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.3 to 3.1.7",
          "reasoning": "The new datasets, including television news, television programs, radio programs, parliament speeches, and podcasts, were manually transcribed by native Kazakh and Russian speakers, created from scratch via original recordings of live or broadcasted speech, not derived from existing datasets or translations. The authors explicitly state manual transcription and new data collection (Sections 3.1.3-3.1.7)."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any of the new datasets were generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced via translation from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1.3 to 3.1.7",
          "reasoning": "The additional data consists of recordings extracted from existing broadcasting sources such as television news (from Khabar Agency), television and radio programs, parliament YouTube channels, and podcasts. This data was collected and aggregated from publicly available or permission-granted sources without significant modification, constituting collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The KSC portion includes crowdsourced data partially checked and verified by an ASR system (automatic acceptance of clear recognitions; others manually transcribed), representing derived data since it was modified and validated from initially raw volunteer submissions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides explicit information regarding the origin and generation of all dataset components."
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10473,
      "completion_tokens": 350,
      "total_tokens": 10823
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper describes training Transformer-based ASR models from scratch using the KSC2 corpus as the primary training data (Section 4.1). The experimental results in Section 4.2 demonstrate model training and evaluation on this newly introduced corpus, indicating use for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention fine-tuning of pre-trained models using this corpus. Instead, it discusses training models from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RLHF techniques applied to this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The paper performs speech recognition experiments where the KSC2 dataset is split into training, validation, and test sets; evaluation metrics such as WER and CER are reported on these splits (Section 4.2), demonstrating the dataset\u2019s use for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some data statistics and code-switching analysis are discussed, the primary purpose of the dataset described is not analysis but training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of the dataset as a knowledge base for retrieval or augmentation is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11196,
      "completion_tokens": 677,
      "total_tokens": 11873
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Sections 3.1.4-3.1.7, 3.2",
          "reasoning": "The KSC2 dataset contains utterances with Kazakh-Russian code-switching, a conversational practice common among Kazakh speakers. The sections describing television programs, radio programs, parliament speeches, and podcasts explicitly mention that the utterances include Kazakh and Russian languages, including intra-sentential and intra-word code-switching (Section 3.1.4 to 3.1.7). This means the dataset includes at least two human languages used mixed within utterances, fulfilling requirements for bilingual/multilingual content. Since entries contain both Kazakh and Russian languages, it is bilingual. There is no indication of additional human languages beyond Kazakh and Russian, so only these two are documented."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Abstract, Sections 3.1.4-3.1.7, 3.2",
          "reasoning": "KSC2 contains utterances from Kazakh and Russian languages, especially annotated code-switching between Kazakh and Russian. All descriptions and experimental setups mention these two languages; no other human language is referenced in the dataset. Therefore, the dataset is bilingual with Kazakh and Russian languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English-only entries; the paper focuses on Kazakh and Russian languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not monolingual but bilingual Kazakh-Russian; monolingual non-English only does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset contains programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No presence of mathematical or formal logical symbolic expressions is described in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data is included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain fictional or constructed languages; only natural Kazakh and Russian languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present are explicitly identified as Kazakh and Russian; there is no uncertainty about language annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain spoken human languages (Kazakh and Russian), thus not applicable."
        }
      }
    }
  },
  {
    "id": "mussakhojayeva22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8414,
      "completion_tokens": 214,
      "total_tokens": 8628
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 4.1; Conclusion",
          "reasoning": "The paper explicitly states that the KSC2 corpus, training recipes, and pretrained models have been made publicly available through their GitHub repository. While the exact URL is not given within the provided excerpt, the mention of shared code and resources indicates that the code related to dataset usage and model training is accessible to the public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (KSC2 Specifications); Section 4 (Speech Recognition Experiments)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the sources of data, data collection methods, manual transcription procedures, and dataset statistics in Section 3. Sections 3.1 and its subsections thoroughly describe the origin and nature of the dataset components. Furthermore, the paper details experimental setup and results in Section 4, facilitating understanding and use of the dataset. Ethical approval and transcription validation are also mentioned, contributing to transparency."
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7524,
      "completion_tokens": 82,
      "total_tokens": 7606
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Dataset), Section 3.2 (Data Collection)",
          "Reasoning": "The dataset RAOFFENSE consists of speech segments extracted from 28 movies and TV series in Arabic, which are scripted media recorded by human actors, thus human generated audio recordings."
        }
      ]
    }
  },
  {
    "id": "nafea24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8324,
      "completion_tokens": 214,
      "total_tokens": 8538
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.4 Annotation Procedure",
            "reasoning": "Section 3.4 states that three of the authors, fluent in Arabic and familiar with dialects, annotated the data separately and held review sessions to resolve disagreements, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Annotation Guidelines",
            "reasoning": "Section 3.3 explicitly describes developed annotation guidelines, including definitions of offensiveness and instructions for dealing with ambiguous cases and quality filtering, indicating detailed instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3 Annotation Guidelines",
            "reasoning": "The paper describes annotation guidelines and instructions but does not mention any scoring rubrics or numerical scales used during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire Paper",
            "reasoning": "There is no mention or presentation of annotation examples given to annotators or included in annotation guidelines anywhere in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "nafea24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9514,
      "completion_tokens": 400,
      "total_tokens": 9914
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being conducted by a single human expert annotator. Instead, multiple annotators are involved."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.4 Annotation Procedure",
          "reasoning": "The annotation was conducted by three of the authors who were selected as annotators due to their fluency in Arabic and familiarity with different dialects, suggesting subject matter expertise. They labeled the dataset independently and met to resolve disagreements, indicating multiple expert human annotators performing quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators were selected for their fluency in Arabic and familiarity with dialects, indicating expertise; no indication of a single non-expert annotator performing QA."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotators are described as experts; thus, multiple non-expert annotators did not perform QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by AI models as judges is described in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or rule-based processes used for quality assurance of the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving multiple expert annotators and review sessions is clearly documented, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9124,
      "completion_tokens": 403,
      "total_tokens": 9527
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Data Collection and Annotation",
          "reasoning": "The dataset consists of more than 2000 spoken segments from 28 Arabic movies and TV series, manually selected, filtered, and annotated by the authors. These are original audio clips from scripted media, not translations or derivatives, annotated by the authors themselves with specific offensiveness labels."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by AI or machine learning models; it originates from actual media clips."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data produced by translating from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine translation being used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 Data Collection",
          "reasoning": "The authors have collected publicly available audio clips from existing Arabic movies and TV series on YouTube. Thus, they aggregated data from existing media sources without modifying the original content."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of extracts from existing media, but it does not describe modifications or transformations to the source material beyond filtering and labeling. The data is not substantially transformed or adapted beyond annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the source and method of dataset creation."
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9642,
      "completion_tokens": 312,
      "total_tokens": 9954
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5, Experiments and Section 6, Results & Discussion",
          "reasoning": "The RAOFFENSE dataset is used to fine-tune pre-trained models such as Wav2vec-Arabic and AraBERTv2 for the binary classification task of offensive speech detection. The paper describes training and hyperparameter tuning of classifiers on this dataset for supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3 Evaluation Setup and Section 6 Results & Discussion",
          "reasoning": "The dataset is split into training, validation, and test sets, and model performance is evaluated and benchmarked using metrics like Matthews Correlation Coefficient (MCC). The dataset serves as a benchmark for offensive speech detection approaches."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.5 Dataset Analysis",
          "reasoning": "The dataset is analyzed in terms of offensive instance proportions across loudness levels, gender disparities, and length distributions, providing insights into data characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10365,
      "completion_tokens": 544,
      "total_tokens": 10909
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 Dataset",
          "reasoning": "The dataset contains entries from various Arabic dialects (Egyptian, Gulf, Levantine, Iraqi, Maghriby) but all are varieties of Arabic, a single language, so it is not considered multilingual with more than two distinct human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 Dataset",
          "reasoning": "The dataset does not contain exactly two human languages; it contains only Arabic dialects."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3 Dataset",
          "reasoning": "The dataset entries are not in English; the data is exclusively Arabic speech from media rated for adult audiences."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3 Dataset",
          "reasoning": "The dataset entries consist solely of Arabic speech across different dialects (Egyptian, Gulf, Levantine, Iraqi, Maghriby), which is a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no indication that the dataset contains programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset does not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset does not contain biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset only includes natural Arabic dialects, no constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3 Dataset",
          "reasoning": "The language of the dataset is clearly described as Arabic and its dialectal varieties."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain language in the form of spoken Arabic; thus, it is not language-free."
        }
      }
    }
  },
  {
    "id": "nafea24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7583,
      "completion_tokens": 187,
      "total_tokens": 7770
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit reference to code availability or code repository for dataset construction in the paper.",
          "reasoning": "The paper does not mention any code release or provide a link to any repository containing code used for data collection, preprocessing, or dataset generation. There is a reference to model configurations hosted on Zenodo, but this pertains to model baselines and not to the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 3.1 to 3.5",
          "reasoning": "The paper includes detailed descriptions of the dataset creation process, including data collection (Section 3.2), defining offensiveness (Section 3.1), annotation guidelines (Section 3.3), annotation procedure (Section 3.4), and dataset analysis (Section 3.5). These sections provide transparency and completeness regarding the dataset development."
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10711,
      "completion_tokens": 163,
      "total_tokens": 10874
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Materials and Methods",
          "Reasoning": "The paper describes collecting acoustic data from five Japanese speakers reading sentences with English laterals. The audio was recorded using a microphone and processed for analysis, indicating human involvement in data generation and usage of real human speech recordings."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Materials and Methods",
          "Reasoning": "The articulatory data were obtained using high-speed ultrasound tongue imaging with the Telemed MicrUs system while participants spoke. The ultrasound probes are human-operated sensors capturing tongue movements, thus human generated data from sensor measurements."
        }
      ]
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11511,
      "completion_tokens": 244,
      "total_tokens": 11755
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 Methods, 2.1 Participants, 2.2 Materials",
            "reasoning": "The annotation and data collection (acoustic and articulatory data) were conducted by researchers as part of the experimental setup involving five Japanese speakers; the study is focused on speech production analysis requiring expertise in phonetics and articulatory methods, indicating evaluation by expert humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit description of annotation instructions for annotators is provided in the text.",
            "reasoning": "The paper does not mention or provide any detailed instructions given to annotators for annotation tasks or data labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of scoring rubrics or structured criteria for annotation is found in the paper.",
            "reasoning": "There is no evidence that formalized rubrics or scoring guidelines were used in the data annotation or evaluation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples or guidelines containing examples are described in the paper.",
            "reasoning": "The paper does not provide sample annotations or examples of how the data should be annotated or labeled."
          }
        }
      ]
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12701,
      "completion_tokens": 375,
      "total_tokens": 13076
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human annotator who is an expert or a member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper indicating that multiple experts performed quality assurance on the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance being conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used as judges for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes automated data collection and analysis methods, there is no indication that automated verification or rule-based checks constituted a quality assurance process for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any specific quality assurance process for the newly collected dataset. There is no evidence or discussion about annotation validation, reliability checks, or other QA steps."
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12311,
      "completion_tokens": 373,
      "total_tokens": 12684
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.",
          "reasoning": "The paper reports acoustic and articulatory data obtained directly from five Japanese speakers recorded under controlled experimental conditions (including simultaneous audio and high-speed ultrasound tongue imaging). This data was elicited using specifically designed stimuli and carrier phrases developed by the authors, indicating that the dataset was newly created from original human production, not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by AI or machine learning models without human production."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not based on machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from existing sources but was collected directly from human participants in a new study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data does not appear to be based on existing data with modifications but represents novel recordings collected specifically for this research."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and collection methods are described explicitly in the paper."
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12829,
      "completion_tokens": 248,
      "total_tokens": 13077
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2.2, 3.1, and 3.2",
          "reasoning": "The dataset constructed by the authors consists of acoustic and articulatory recordings from Japanese speakers producing English laterals in different syllabic positions and vowel contexts. This dataset is used primarily to analyze phonetic and articulatory patterns and variations in second language speech acquisition, not for training or evaluating machine learning models. The paper uses sophisticated statistical modeling (linear mixed effects and GAMMs) to analyze the data trends and articulatory characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13552,
      "completion_tokens": 547,
      "total_tokens": 14099
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study involves two human languages, Japanese and English, but does not have entries with more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Abstract; Sections 2.1 and 2.2",
          "reasoning": "The dataset includes speech productions by native speakers of Japanese (L1) producing English (L2) laterals. The dataset contains entries in both Japanese (participants' L1) and English (L2 speech productions being analyzed). Although the speech data for analysis focuses on English laterals, the participants' first language (Japanese) is also pertinent to the study, making the dataset bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While English laterals constitute the analyzed speech data, the participants are native Japanese speakers and the study discusses L1 influence, implying the dataset is not exclusively monolingual English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes English language material for analysis; thus, it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code content is involved in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though statistical methods (e.g., linear mixed-effects modeling) are used, the dataset entries themselves do not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are present."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or constructed languages are involved."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present (Japanese and English) are explicitly described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes linguistic data in human languages (Japanese and English)."
        }
      }
    }
  },
  {
    "id": "nagamine22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10770,
      "completion_tokens": 129,
      "total_tokens": 10899
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention or provide a link to any code repository or scripts related to data collection, preprocessing, or analysis for the newly introduced dataset of articulatory and acoustic measurements from Japanese speakers."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Methods",
          "reasoning": "The paper provides detailed documentation of the data collection procedures including participant information, stimuli materials development, recording setup for simultaneous acoustic and ultrasound data, data preprocessing criteria, and analysis methods, as described primarily in the Methods section."
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5517,
      "completion_tokens": 146,
      "total_tokens": 5663
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, The WHiSER Corpus and its subsections",
          "Reasoning": "The WHiSER corpus is newly introduced by the authors and consists of speech files obtained from declassified Nixon's Oval Office recorded conversations (1971-1973). These are authentic, natural emotional speech recordings captured via distant microphones during real-life political meetings. The data modality is audio because the data consists of speech recordings. The origin is human generated since the recordings are directly captured from human conversations, not simulated or generated by models. The paper explicitly states the source as Nixon's tapes, confirming human origin."
        }
      ]
    }
  },
  {
    "id": "naini24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6317,
      "completion_tokens": 212,
      "total_tokens": 6529
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1",
            "reasoning": "The paper states that the WHiSER corpus was annotated by 33 student workers at an anonymous institution, who are likely non-experts. Each speaking turn was annotated by at least five annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Annotators used self-assessment manikins (SAMs) as visual references to help assess dimensional attributes, indicating the presence of instructions or guidance for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "Emotional attributes were rated on a seven-point Likert scale and primary/secondary emotions were selected from predefined categorical labels, demonstrating the presence of scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not explicitly mention providing annotation examples or example annotations in the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "naini24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7507,
      "completion_tokens": 383,
      "total_tokens": 7890
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that annotations were performed by 33 student workers, indicating multiple annotators and no mention of any being single experts."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators were 33 student workers at an anonymous institution; there is no indication they have subject matter expertise or belong to the target demographic. Therefore, they cannot be considered experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotations were done by multiple annotators, not a single one, thus this category does not apply."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Pipeline and Data Annotation",
          "reasoning": "The WHiSER corpus was annotated by 33 student workers. The paper does not indicate that these annotators had subject matter expertise or belonged to the target demographic, so they are considered non-expert annotators. Each utterance was annotated by at least five annotators, indicating multiple non-expert human annotators were involved in the quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models were used to select emotionally rich samples, the actual annotation process (quality assurance) was done by human annotators. No mention is made of AI models being used as judges in the QA process."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or QA checks of annotations through automatic processes. The AI models were used in data filtering prior to annotation, but not for quality assurance of the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes a quality assurance process involving multiple human annotators providing labels. Therefore, QA was performed and documented."
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7117,
      "completion_tokens": 440,
      "total_tokens": 7557
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not original content created entirely from scratch by human contributors. Instead, the data consists of pre-existing recorded conversations from President Nixon's Oval Office recordings."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated by AI or machine learning models; it originates from real-world audio recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was produced by translation from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 Introduction and 3 The WHiSER Corpus",
          "reasoning": "The WHiSER corpus is collected from existing sources, specifically the declassified President Nixon\u2019s Oval Office recorded conversations from 1971 to 1973. The recordings were archived and then selected and annotated for emotional content by the authors, but the base audio data itself is aggregated from these existing archival recordings without creation from scratch."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Pipeline and Data Annotation",
          "reasoning": "The dataset is derived in that the authors applied extensive processing steps including filtering for quality, segmenting, and emotional content selection using machine learning models, followed by manual annotation for emotional labels. These represent modifications and transformations applied to the original recordings, producing a dataset derived from the raw data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of generation of the dataset is clearly documented and specified."
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7635,
      "completion_tokens": 293,
      "total_tokens": 7928
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 (The WHiSER Corpus), Section 4 (Experimental Results), and Section 5 (Conclusions)",
          "reasoning": "The WHiSER corpus is explicitly described as intended to be used as an independent test set for evaluating speech emotion recognition (SER) systems in complex and realistic acoustic environments. The paper reports experimental results evaluating SER models trained on other datasets and tested on WHiSER, illustrating its role for benchmarking and performance measurement rather than training or fine-tuning."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 (Emotional Content Analysis)",
          "reasoning": "The paper performs detailed analyses of the emotional content and statistical properties of the WHiSER corpus, including distributions of emotional attributes and categorical emotions, signifying its use primarily for analyzing emotion trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8358,
      "completion_tokens": 512,
      "total_tokens": 8870
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The WHiSER corpus is based on President Nixon's Oval Office recordings and no mention is made of multiple languages; only English is referenced."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The WHiSER corpus entries are not described as containing exactly two languages; there is no indication of bilingual content."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (The WHiSER Corpus) and Abstract",
          "reasoning": "The paper describes the dataset as consisting of speech recordings from President Nixon's Oval Office conversations, which are in English. There is no mention of any other language in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains English speech only; there is no indication that the entries are in any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio files and annotated emotional labels; there is no content related to programming or code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset contains mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech only; it does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken English language content, so it is not applicable to mark as having no language."
        }
      }
    }
  },
  {
    "id": "naini24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5576,
      "completion_tokens": 172,
      "total_tokens": 5748
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 3",
          "reasoning": "The abstract states that all data and emotional labels are released on their GitHub repository (https://github.com/msplabresearch/WHiSER). This indicates that the associated code and data are publicly available. While the paper does not explicitly state the availability of all code for data collection and preprocessing, the link to the repository and mention of data and labels release strongly suggests that relevant code for dataset construction is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3",
          "reasoning": "Section 3 thoroughly documents the dataset creation process, including pipeline stages such as audio preparation, filtering, selection criteria, annotation methodology, and statistics on dataset size and emotional content. This detailed description constitutes comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11382,
      "completion_tokens": 123,
      "total_tokens": 11505
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1: Collecting Recognition Errors for a Keyword List with Synthetic Audio",
          "Reasoning": "The authors generate speech audio for a list of keywords using a Text-to-Speech (TTS) system, which is an AI model, to create synthetic audio data. This audio is then used with a recognition model to obtain pairs of correct labels and recognition errors. Since the audio data is generated via TTS without direct human recording, it is model generated synthetic audio."
        }
      ]
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12182,
      "completion_tokens": 243,
      "total_tokens": 12425
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.1 Data",
            "reasoning": "Section 4.1 describes manual steps in generating bias keywords: manual removal of clear morphological analysis errors, selection of proper nouns and personal names based on morphology, and word segmentation using MeCab. This implies that a single or multiple human experts with domain knowledge performed annotation and verification on the dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Data",
            "reasoning": "The paper does not provide any explicit mention or description of detailed annotation instructions or guidelines for the human annotator(s). The description focuses on the process flow rather than guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Data",
            "reasoning": "No scoring rubrics or criteria for evaluation of annotations are described in the text. Only the process of extraction and manual cleaning is mentioned."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Data",
            "reasoning": "The paper does not present or mention any annotation examples or samples given to annotators during the creation or cleaning of the bias keyword list."
          }
        }
      ]
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13372,
      "completion_tokens": 411,
      "total_tokens": 13783
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on any newly introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human experts evaluated or performed QA on any datasets; no such information is provided."
        },
        "Single Human Non-Expert": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The paper describes that after extracting misrecognized words via morphological analysis and automatic methods, proper nouns and personal names were manually filtered to remove clear morphological analysis errors from the extracted set of bias keywords. This indicates that a manual review by at least one human (with likely non-expert status, as no expertise or qualification for the reviewer is specified) was performed to improve the dataset quality."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that multiple non-expert annotators were involved in quality assurance or curation of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a quality assurance judge for the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "Most of the dataset creation and annotation involve automated procedures, e.g., generating synthetic speech from a keyword list using Text-to-Speech (TTS); recognition error collection by running the recognition model on synthesized speech; morphological analysis using MeCab; and automatic extraction of misrecognized words and labels. These algorithmic and rule-based steps serve as an automated quality control and annotation generation process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A combination of manual curation (albeit by unknown human expertise) and automated methods is documented; thus, it is incorrect to say no QA is performed or documented."
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12982,
      "completion_tokens": 471,
      "total_tokens": 13453
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper explicitly states that synthetic speech audio was generated for the keyword list using Text-to-Speech (TTS) synthesis, which is a model-based generation method. This synthetic speech data was then used to create pairs of correct labels and recognition errors by feeding it into the recognition model. Since this synthetic audio is generated entirely by a machine learning model and not derived from existing natural recordings, this content qualifies as new data from a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper describes using existing test sets such as CSJ eval1, eval2, eval3, JSUT, Common Voice, and TEDxJP-10K for evaluation purposes, where keywords were extracted by analyzing recognition errors from prior recognition runs. Specifically, keywords were extracted from evaluation sets through morphological analysis and manual filtering, thus these keyword lists are collated from existing sources without significant content creation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "The keyword lists themselves are extracted from recognition errors on existing evaluation sets, meaning they are derived from existing data with some filtering, morphological analysis, and manual corrections applied. Additionally, the pairs of correct labels and recognition error instances were created by processing synthesized speech through the recognition model, representing a transformation of original data for adaptation to their InterBiasing method."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Data sources and generation methods are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13500,
      "completion_tokens": 240,
      "total_tokens": 13740
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.3",
          "reasoning": "The datasets created by the authors consist of pairs of correct labels and recognition error instances generated using Text-to-Speech (TTS) and the recognition model, which are used to bias intermediate predictions in the acoustic encoder. These datasets are then utilized to evaluate and measure improvements in recognition accuracy of unknown words through experiments reported in Section 4.3. Thus, the dataset is used exclusively for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14223,
      "completion_tokens": 441,
      "total_tokens": 14664
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data",
          "reasoning": "The dataset introduced is based on the Corpus of Spontaneous Japanese (CSJ) and other Japanese speech-related test sets such as JSUT-basic 5000 and TEDxJP-10K. The paper's experiments and evaluations are conducted strictly in Japanese, indicating the dataset entries are in one non-English language, Japanese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Equations and formulas in Sections 2.1, 3.1, and 3.2",
          "reasoning": "The paper includes multiple mathematical expressions and formulas used to describe model computations and algorithmic operations that pertain to the dataset construction and usage descriptions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data, specifically Japanese speech and text labels and associated synthetic speech, so it is not language-free."
        }
      }
    }
  },
  {
    "id": "nakagome24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11441,
      "completion_tokens": 181,
      "total_tokens": 11622
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No link or mention of code availability related to dataset construction.",
          "reasoning": "The paper does not provide any URLs, footnotes, or text indicating that code for constructing the dataset (such as generation of misrecognized keyword pairs or intermediate prediction correction) is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 and Section 4.1",
          "reasoning": "The paper documents the dataset creation process by describing the generation of synthetic speech from keywords using Text-to-Speech (TTS), the collection of recognition error pairs through model predictions on TTS audio (Section 3.1), and details the identification and manual cleaning of misrecognized words from evaluation sets to generate bias keyword lists (Section 4.1). These descriptions provide transparency about dataset construction even though the code is not provided."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7443,
      "completion_tokens": 146,
      "total_tokens": 7589
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, 'Bilingual speaker recognition test set'; Section 2.2, 'VoxCeleb1-B Evaluation list'",
          "Reasoning": "The VoxCeleb1-B evaluation set is a newly introduced large-scale bilingual speaker recognition test protocol derived from the VoxCeleb1 dataset. It consists of audio data (utterances) annotated with speaker and language labels, manually checked by human annotators. The data originates from human speech recordings as part of the original VoxCeleb1 dataset and further manual annotation, as described in the paper, thus is human generated audio data."
        }
      ]
    }
  },
  {
    "id": "nam23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8243,
      "completion_tokens": 195,
      "total_tokens": 8438
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1",
            "reasoning": "The paper states that the language annotations of VoxCeleb1 dataset used in the new evaluation protocol VoxCeleb1-B were manually checked by annotators of various nationalities, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not mention providing any specific detailed annotation instructions for the annotators who manually checked the language labels."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no mention of scoring rubrics or formal scoring criteria being provided for the language label annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not describe providing any annotation examples or exemplars for language label annotation."
          }
        }
      ]
    }
  },
  {
    "id": "nam23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9433,
      "completion_tokens": 333,
      "total_tokens": 9766
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert for dataset annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "In Section 2.1, the paper states that language annotations of the VoxCeleb1 dataset were manually checked by annotators of various nationalities, implying multiple human annotators with relevant expertise validated language labels."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any single non-expert human quality assurance for dataset annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that multiple non-expert annotators conducted the quality assurance; rather annotators are implied to be knowledgeable."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "In Section 2.1, spoken language pseudo-labels were initially obtained by a Spoken Language Recognition (SLR) model pretrained on VoxLingua107 dataset, indicating the use of an AI model for initial label generation, although these were then manually checked."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated verification process or algorithmic/rule-based QA for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes quality assurance involving both AI model (SLR) and manual annotation by multiple humans, so QA is documented and applied."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9043,
      "completion_tokens": 612,
      "total_tokens": 9655
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Bilingual speaker recognition test set), especially 2.2 VoxCeleb1-B Evaluation list",
          "reasoning": "The paper introduces VoxCeleb1-B, a large-scale bilingual evaluation set derived from the existing VoxCeleb1 dataset. The authors manually annotated language labels of VoxCeleb1 utterances (Section 2.1), involving human annotators from various nationalities who manually checked and excluded some utterances with unrecognizable language. The evaluation protocol VoxCeleb1-B was constructed based on these manual annotations and careful trial design to simulate bilingual scenarios. Thus, the language labels and evaluation protocol are original content created by human contributors for the purposes of this work."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was generated purely by AI or machine learning models. While pseudo-labels from a spoken language recognition model were used to assist annotations, these are not presented as new datasets generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was produced by translating content from one language to another via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data generation by machine translation is mentioned in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Bilingual speaker recognition test set), especially 2.2 VoxCeleb1-B Evaluation list",
          "reasoning": "VoxCeleb1-B evaluation set is derived from VoxCeleb1 dataset, which is an existing dataset. The authors aggregated and filtered the existing speech samples, added newly annotated language labels, and constructed a new evaluation trial list to simulate bilingual scenarios. This involves data collection and aggregation from an existing source with limited modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 Obtaining language labels and Section 2.2 VoxCeleb1-B Evaluation list",
          "reasoning": "The VoxCeleb1-B test protocol is based on existing VoxCeleb1 data but with significant adaptations such as manual language annotation, exclusion of ambiguous utterances, and the creation of a large number of cross-lingual trial pairs. These constitute transformations and adaptations of existing data, making it a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and method of generation are documented in the paper, specifically in Section 2 and subsections."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9561,
      "completion_tokens": 444,
      "total_tokens": 10005
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly introduced dataset for pre-training large models on general patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset VoxCeleb1-B is not used for training models from scratch; it is described as an evaluation benchmark rather than a training resource."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using VoxCeleb1-B to fine-tune pre-trained models with supervised methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using the dataset in reinforcement learning based post-training such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "VoxCeleb1-B is introduced explicitly as a large-scale bilingual speaker recognition evaluation set to benchmark speaker recognition models under bilingual scenarios. It is used exclusively to evaluate and analyze model performance, not for training."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses the dataset for analyzing bilingual speaker recognition performance, this is a use of the benchmark for evaluation rather than primary data analysis in isolation. The dataset's main purpose is evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as a knowledge base for augmentation or retrieval-augmented generation in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used in evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10284,
      "completion_tokens": 564,
      "total_tokens": 10848
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The newly proposed test protocol VoxCeleb1-B is constructed from the VoxCeleb1 dataset and explicitly includes utterances labeled with 15 different human languages including English, French, Hindi, German, Spanish, Italian, Afrikaans, Portuguese, Dutch, Korean, Urdu, Swedish, Russian, Chinese, and Arabic. This dataset contains cross-lingual trials involving multiple languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains entries from more than two languages (15 languages are used), so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to only English; multiple other languages are included."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not restricted to a single non-English language; it contains multiple languages including English and others."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are audio utterances labeled with language identities; there is no mention of mathematical or logical notations in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech recordings without biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language labels have been manually annotated for the dataset, and 883 utterances without recognized language were excluded, so the dataset languages are specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of speech utterances in multiple human languages, so it contains language entries."
        }
      }
    }
  },
  {
    "id": "nam23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7502,
      "completion_tokens": 145,
      "total_tokens": 7647
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2",
          "reasoning": "The paper does not provide any link or mention of publicly available code or scripts for constructing or generating the VoxCeleb1-B evaluation dataset or the language label annotations."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (2.1 Obtaining language labels and 2.2 VoxCeleb1-B Evaluation list)",
          "reasoning": "The paper documents the dataset creation process including how language labels were obtained using a Spoken Language Recognition model and manual checking, the selection criteria for languages and samples, and how evaluation trials were constructed, detailing the scale and composition of VoxCeleb1-B."
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10755,
      "completion_tokens": 123,
      "total_tokens": 10878
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. Versatile Voice Dataset",
          "Reasoning": "The Versatile Voice Dataset (VVD) is introduced as a new dataset collected by the authors comprising audio recordings from 3 trans-feminine voice teachers. The dataset consists of these speakers reading the 6 CAPE-V sentences in various pitch, resonance, and weight configurations. Audio clips were recorded by humans (voice teachers) and provided directly by the authors, with no other preprocessing besides sampling rate standardization."
        }
      ]
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11555,
      "completion_tokens": 299,
      "total_tokens": 11854
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3: Versatile Voice Dataset",
            "reasoning": "The dataset consists of recordings by three trans-feminine gender-affirming voice teachers who themselves produced the voice clips. These voice teachers are experts in the field of voice modification and gender-affirming voice training, indicating that annotation or labeling was performed by single human experts (the speakers themselves), rather than crowd workers or models."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2: Data Collection",
            "reasoning": "Section 3.2 describes that voice teachers produced voices according to self-interpreted configurations of low, medium, and high pitch, resonance, and weight, implying that some form of instructions or guidelines were given to specify these allowed configurations even if ambiguous."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2: Data Collection",
            "reasoning": "There is no explicit mention of scoring rubrics or criteria for evaluating the voice recordings provided to the voice teachers or annotators. The instructions appear ambiguous and allow for personal interpretation rather than defined rubric-based scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2: Data Collection",
            "reasoning": "No examples or exemplar samples are described as provided to the voice teachers in the paper. The voice teachers interpreted instructions differently, indicating that no standardized examples were given as part of annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12745,
      "completion_tokens": 486,
      "total_tokens": 13231
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected from three trans-feminine gender-affirming voice teachers who are subject matter experts; however, the authors do not report any quality assurance process conducted by a single human expert annotator to validate or verify dataset content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the dataset speakers are themselves experts in voice training, there is no description of a quality assurance process involving multiple human experts reviewing or validating the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance carried out by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.2.2 Error Analysis and Difficulty",
          "reasoning": "The authors conducted perceptual experiments involving six non-expert listeners from Amazon Mechanical Turk performing speaker verification tasks on the dataset. This indicates that multiple non-expert humans engaged in a form of quality assurance or validation of the dataset's properties with respect to perceptual speaker verification."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2 Speaker Verification and Section 4.1 Gender Classification",
          "reasoning": "The dataset was evaluated using AI models such as ECAPA-TDNN from SpeechBrain and NeMo for tasks like gender classification and speaker verification, serving as automatic judges of dataset content consistency, effectively performing quality assurance from a model perspective."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification of dataset content or annotations via algorithmic or rule-based techniques acting as a quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some form of quality assurance is described and performed by both multiple human non-experts and AI models, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12355,
      "completion_tokens": 403,
      "total_tokens": 12758
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Versatile Voice Dataset), especially Section 3.2 (Data Collection)",
          "reasoning": "The Versatile Voice Dataset (VVD) was created by three trans-feminine gender-affirming voice teachers who produced recordings of themselves reading six CAPE-V sentences in multiple voice configurations along pitch, resonance, and weight. The speakers deliberately modified their voices following instructions and their own interpretations, producing original audio content. The dataset was collected anew and is not derived, translated, or aggregated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any part of the dataset was generated solely by AI or machine learning models. All audio data originated from human speakers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation was used as the data consists of original English speech recordings."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The VVD was not collated or aggregated from existing datasets; it is newly recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not derived or transformed from existing recorded materials; it is newly produced."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are specified clearly."
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12873,
      "completion_tokens": 450,
      "total_tokens": 13323
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The Versatile Voice Dataset (VVD) is explicitly used to evaluate current speaker embeddings on gender classification and speaker verification tasks. The paper presents experiments measuring equal error rate (EER) and classification accuracy of state-of-the-art models on the VVD, demonstrating limitations due to vocal variability. These evaluations illustrate how existing models fail to reliably identify speaker identity or gender across diverse voice modifications, hence the VVD serves a benchmarking and performance measurement role."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The dataset is used primarily for analyzing trends and characteristics related to vocal modifications, gender perception, and speaker identity modeling. The paper uses the VVD to analyze sensitivity of gender classifiers to intra-speaker vocal changes, speaker verification errors, human vs model performance comparisons, and the relationship between vocal texture and categorical gender labels. Additionally, Section 5 discusses using the dataset for the future modeling of vocal texture, emphasizing the analysis of perceptual voice qualities."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the VVD as a knowledge base for retrieval-augmented generation or similar augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13596,
      "completion_tokens": 551,
      "total_tokens": 14147
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The dataset consists of three trans-feminine voice teachers each reading sentences in English only; no mention is made of multiple languages being present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The dataset entries are in English only; there is no indication of a second language being used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The Versatile Voice Dataset (VVD) contains recordings of three speakers reading the CAPE-V sentences, which are English sentences. The paper consistently refers to the data as English speech without mention of other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The dataset consists only of English speech samples; no non-English language is described or included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset consists of spoken audio recordings, not code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset entries are speech audio samples; mathematical or logical notation is not part of the dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset consists of human speech; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "There is no indication that constructed or fictional languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The language of the dataset entries is clearly specified as English and detailed; thus, the language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains spoken language (English), so the N/A category does not apply."
        }
      }
    }
  },
  {
    "id": "netzorg24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10814,
      "completion_tokens": 185,
      "total_tokens": 10999
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention found",
          "reasoning": "The paper does not include any links or mentions of publicly available code or repositories related to the Versatile Voice Dataset or its construction. There is no indication that code used for data collection, preprocessing, or generation has been made publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Versatile Voice Dataset), specifically Sections 3.1 and 3.2",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including descriptions of pitch, resonance, and weight as defined by the voice teachers, the data collection procedure with the three trans-feminine voice teachers producing voices in different configurations, and information on preprocessing (only sample rate standardization). The dataset characteristics, speaker information, and recording conditions are clearly described, making the dataset creation transparent."
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6945,
      "completion_tokens": 279,
      "total_tokens": 7224
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data collection; Section 3 Feature Extraction and Data Preprocessing",
          "Reasoning": "The audio data was recorded from human participants during a structured conversation with a virtual agent, using their end devices, captured at 44.1 kHz sampling rate as stated in Section 2. This constitutes human-generated audio data."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data collection; Section 3 Feature Extraction and Data Preprocessing",
          "Reasoning": "Video data consists of facial video recorded from participants during the conversation with the virtual agent, as facial landmarks and orofacial features were extracted. The video was captured from human participants using their devices, making it human-generated video data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Data collection; Section 3 Feature Extraction and Data Preprocessing",
          "Reasoning": "Text data arises from automatic speech recognition transcriptions of human speech during the conversational protocol. Though transcription is machine-produced, the speech content is created by human participants. The linguistic and cognitive features, such as word recall and digit span, are based on these human-generated spoken utterances."
        }
      ]
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7745,
      "completion_tokens": 293,
      "total_tokens": 8038
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 Data collection",
            "reasoning": "The study collected data using crowdsourced participants recruited via Prolific, who interacted with a virtual agent. These participants provided speech, video, and questionnaire data. The annotation in this context refers to participant responses and task completions, provided by non-expert laypeople, not experts or AI models."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 Data collection",
            "reasoning": "The paper states that after informed consent, a virtual agent guided participants through a set of structured speaking exercises and open-ended questions, indicating the presence of instructions to participants for producing the data relevant to the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2 Data collection and Section 3 Feature Extraction and Data Preprocessing",
            "reasoning": "Cognitive tasks such as word and digit recall were scored automatically using defined criteria (e.g., digit span scoring from 0 to 2), indicating the use of rubrics to assess cognitive performance. This scoring represents rubrics associated with the new dataset tasks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide explicit examples of annotations, participant responses, or scoring examples within the guidelines or text for the new dataset. Thus, there is no evidence of annotation examples being provided."
          }
        }
      ]
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8935,
      "completion_tokens": 405,
      "total_tokens": 9340
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts conducted quality assurance for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model being used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 (Feature Extraction and Data Preprocessing)",
          "reasoning": "The dataset processing pipeline includes automated scoring of cognitive tasks based on ASR output, automatic extraction of acoustic, visual, and linguistic features using established software (Praat, Kaldi, OpenCV, Dlib, Amazon Transcribe, spaCy), and outlier removal via a distribution-based algorithm. This indicates that automated verification and preprocessing techniques were used to ensure data quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents and describes automated quality assurance and verification processes applied to the dataset; hence, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8545,
      "completion_tokens": 436,
      "total_tokens": 8981
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2. Data collection",
          "reasoning": "The paper describes a crowdsourced study where human participants engaged in structured conversations with a virtual agent, producing original speech, video, and cognitive task responses. This data was collected directly from human subjects and is original content created entirely from scratch by human contributors. The participants generated the data themselves, and it is not translated, adapted, or derived from pre-existing material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new dataset was generated entirely by AI or machine learning models. The data consists of human speech and video recordings, along with cognitive task performance."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used in the data generation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from existing sources; it was collected directly from new human participants."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3. Feature Extraction and Data Preprocessing",
          "reasoning": "The features extracted (e.g., acoustic metrics, visual metrics, linguistic features) represent derived data because they are computed based on the raw audio and video recordings of the human participants. These features are modifications and transformations applied to original human-generated data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are specified clearly in the paper."
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9063,
      "completion_tokens": 268,
      "total_tokens": 9331
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Analyses and Observations and Section 5 Discussion",
          "reasoning": "The paper describes the collection of a new crowdsourced multimodal dataset which is primarily used to analyze the relationships between speech, text, cognitive, and facial video features with depression severity and antidepressant use. Statistical analyses such as Kruskal-Wallis tests and effect size calculations are conducted to observe trends and differences. Classification experiments are performed to explore discriminative power but no model training from scratch or fine-tuning is central. Overall, the dataset's utility is focused on analysis of patterns and characteristics rather than direct training or evaluation of models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9786,
      "completion_tokens": 515,
      "total_tokens": 10301
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Data collection and Section 3 Feature Extraction and Data Preprocessing",
          "reasoning": "The paper explicitly states use of English language for tasks such as word recall, digit span, verbal fluency, and cognitive assessments, and the automatic transcription is done using Amazon Transcribe tailored for English. There is no mention or indication of any other languages in the dataset. All linguistic features are computed on English transcriptions."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions processing techniques and software packages (e.g., Praat, Kaldi, spaCy, OpenCV), the dataset entries themselves do not contain programming or code language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech, video, and cognitive task data; there is no indication that the dataset entries contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes human speech and facial video data only, with no biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly documents the language as English; therefore, language is specified and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken and transcribed language data; therefore, it is not language-free."
        }
      }
    }
  },
  {
    "id": "neumann23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7004,
      "completion_tokens": 159,
      "total_tokens": 7163
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any links or statements about public availability of code related to data collection, preprocessing, feature extraction, or dataset generation. There is no mention of repositories or code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper documents the dataset creation process extensively in Section 2 (Data collection) describing participant recruitment, data collection procedures including speech tasks and questionnaires, and Section 3 (Feature Extraction and Data Preprocessing) detailing modalities, metrics extracted, preprocessing steps such as outlier removal and missing data handling. Thus, the dataset creation process is reasonably well documented in the paper."
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7520,
      "completion_tokens": 173,
      "total_tokens": 7693
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1, 4.1",
          "Reasoning": "The new dataset is a synthetic parallel audio corpus created by the authors using a voice conversion (VC) model. It consists of pairs of audio utterances with the same content and voice but different accents. The original recordings come from human-generated datasets (CMU-ARCTIC and L2-ARCTIC corpora) and the synthesized audio is then generated by AI voice conversion models (FragmentVC or VQMIVC). Therefore, the resulting dataset includes audio data that is both human recorded and model generated (synthesized) as described explicitly in Section 3.1.1 and the dataset composition in Section 4.1."
        }
      ]
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8320,
      "completion_tokens": 247,
      "total_tokens": 8567
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 and Section 4.1",
            "reasoning": "The synthetic training data for the accent conversion model is generated using a voice conversion (VC) model automatically, as described in Sections 3.1.1 and 3.1.2. This process uses VC architectures (FragmentVC and VQMIVC) to synthesize pairs of audios differing in accent but with the same speaker voice, suggested to be automatic and not involving human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instruction section provided",
            "reasoning": "The paper does not describe any instructions provided for annotators, as the synthetic dataset generation is automatic using VC models."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of rubrics in relation to synthetic data generation",
            "reasoning": "No scoring rubrics or evaluation rubrics are described regarding annotation or evaluation of the synthetic dataset creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples described for annotation guidelines",
            "reasoning": "No examples of annotations or annotation guidelines relevant to the dataset creation are provided in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9510,
      "completion_tokens": 412,
      "total_tokens": 9922
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert or annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts or annotators performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA process carried out by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance by multiple non-expert human annotators is described or implied in the paper."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4.2.2 Objective tests",
          "reasoning": "The paper describes using an accent classifier (a 4-LSTM-layer network) trained on their own training data to assess accent classification accuracy (ACC) on the converted and original audios. This accent classifier AI model acts as a judge for evaluating accent characteristics of the dataset samples, thus performing an AI-model based quality assessment."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of algorithmic or rule-based automated verification of dataset annotations or content as part of QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a quality assessment conducted by an AI model (accent classifier), so QA was performed and is described; thus 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9120,
      "completion_tokens": 465,
      "total_tokens": 9585
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was created entirely from scratch by humans for this work. Instead, existing corpora were used as source data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (Generating synthetic data) and Section 4.1 (Dataset and training description)",
          "reasoning": "The authors generate synthetic parallel datasets by applying voice conversion (VC) models (FragmentVC and VQMIVC) to existing audio corpora to create pairs of audios with the same speaker voice but different accents. This synthetic data is newly generated by AI models without direct referencing or simple modification of existing data, fulfilling the criteria for new data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involves translation through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any generation of data via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is derived from existing corpora but not simply collected or aggregated without modification; the authors transform the data via voice conversion models."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "The starting point for synthetic data generation is existing speech corpora (CMU-ARCTIC and L2-ARCTIC). The authors apply voice conversion models to these existing audios to generate new audio pairs that differ in accent but maintain voice identity. This represents a transformation of existing data to produce derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and generation process of the datasets used."
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9638,
      "completion_tokens": 245,
      "total_tokens": 9883
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 - 3.2, Section 4.1",
          "reasoning": "The authors generate a synthetic dataset by voice conversion to create paired audios with the same voice but different accents, which is then used to train (fine-tune) the seq2seq Accent Conversion (AC) model using supervised learning methods. Training details with this synthetic dataset are described in Sections 3.1 and 3.2, and training setup is elaborated in Section 4.1."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10361,
      "completion_tokens": 482,
      "total_tokens": 10843
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 4.1 Dataset and training description",
          "reasoning": "The datasets used and synthesized in the paper involve English spoken with different accents (American and Indian accented English). No indication of any other human languages beyond these accented English variants is provided."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and training description",
          "reasoning": "The synthesized dataset consists of audio pairs containing English spoken with two distinct accents: native American English and Indian accented English. These represent two different accents of the same language (English), effectively making the dataset bilingual in terms of accent varieties."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset and training description",
          "reasoning": "The original and synthesized datasets contain English speech only, albeit with different accents (American and Indian). The content in the speech is in English language, so the dataset entries are monolingual English audio entries with accent variation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains English speech only, with no indication of any non-English language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists purely of speech audio data; there is no programming or code content included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication of mathematical or logical symbolic data entries in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech audio only; there is no biological sequence or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any constructed, fictional, or artificial languages; only natural English speech is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper clearly specifies the languages and accents involved (American and Indian accented English), so the language identity is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language audio data, specifically English speech; thus, it is language-relevant."
        }
      }
    }
  },
  {
    "id": "nguyen22d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7579,
      "completion_tokens": 198,
      "total_tokens": 7777
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link present",
          "reasoning": "The paper does not provide any link or mention of publicly available code repositories related to the synthetic data generation process or dataset construction. The only URLs given are to third-party implementations like tacotron2 and Wav2vec, not to the authors' own code for dataset synthesis."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Generating synthetic data) and Section 4.1 (Dataset and training description)",
          "reasoning": "The paper describes in detail how the synthetic dataset is generated using voice conversion models, including the process overview, architectures used (FragmentVC and VQMIVC), how data from existing corpora (CMU-ARCTIC and L2-ARCTIC) are utilized, and the total size and composition of the synthesized dataset. This provides a clear documentation of dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7667,
      "completion_tokens": 139,
      "total_tokens": 7806
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Data Collection and Section 3.1 Dataset",
          "Reasoning": "The dataset is derived from the MediaSum corpus, which consists of transcripts of media interviews (dialogues) originally created and recorded by humans as they are actual interview transcripts from NPR and CNN. The text modality here is the transcript text itself, human-generated through original media production and transcription processes. The authors sample and process these transcripts to create their SpeakerID training data by anonymizing speaker names and matching name mentions, but the underlying data originates from human-produced dialogue transcripts."
        }
      ]
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8467,
      "completion_tokens": 221,
      "total_tokens": 8688
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2",
            "reasoning": "The dataset is constructed automatically from the MediaSum corpus by a method involving NER detection, speaker anonymization, and name-to-speaker mapping using Levenshtein distance for text matching, thus the annotations are generated by an automatic process rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper describes an automatic data processing pipeline but does not provide or mention any human annotation instructions for the new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "No scoring rubrics or evaluation criteria for human annotation guidance are described; data generation is fully automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "Although an example of the MediaSum data format is shown (Figure 1), no annotation examples or guidelines for data annotation are provided as the dataset labels are generated automatically."
          }
        }
      ]
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9657,
      "completion_tokens": 317,
      "total_tokens": 9974
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human expert manually validating or performing quality assurance on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance carried out by any single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that multiple non-expert human annotators performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information suggests that an AI model was used as a judge or for QA of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 Data processing",
          "reasoning": "The dataset annotations were derived through an automatic procedure involving named entity recognition using the Trankit model and fuzzy text matching via Levenshtein Distance algorithm to map speaker names. This automatic process serves as the core method for generating the dataset labels, implying an automated quality assurance via algorithmic verification and matching without manual intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the dataset is automatically constructed, the paper explicitly describes automated methods for labeling and matching, which constitutes a form of automated quality assurance. Therefore, QA process is present and documented."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9267,
      "completion_tokens": 438,
      "total_tokens": 9705
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creating any new dataset from human contributors producing entirely original content from scratch. Instead, it uses existing transcripts from the MediaSum corpus."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating a dataset entirely using AI or machine learning models without reference to existing data. The data originates from existing transcripts."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involves human translations from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any machine translation was used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Collection",
          "reasoning": "The dataset is derived from the existing MediaSum corpus, which contains transcripts of media interviews. The authors sample and extract data from this existing dataset without fundamentally altering the transcripts themselves, indicating that the data is aggregated or collected from an existing source."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Collection",
          "reasoning": "The authors process the MediaSum transcripts by detecting person names using an NER model, anonymizing speaker identities, and mapping detected person names to speakers via text matching with Levenshtein Distance. These steps constitute modifications and transformations of existing data to create their SpeakerID dataset, thus the dataset is derived from original sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly states the data source and its processing method; hence the data origin is documented."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9785,
      "completion_tokens": 516,
      "total_tokens": 10301
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the proposed dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses pretrained language models like RoBERTa and does not indicate training from randomly initialized parameters using the proposed dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1: Dataset and Section 3.4: Results",
          "reasoning": "The paper states that they randomly sample data from MediaSum to create a SpeakerID dataset used to fine-tune pretrained language models (e.g., RoBERTa) with supervised learning methods, as evidenced in Sections 2.3 and 3. Experiments and results show the model performance fine-tuned on this dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training methods using the proposed dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1: Dataset and Section 3.4: Results",
          "reasoning": "The proposed dataset sample is split into train/dev/test subsets and used to evaluate and benchmark the models, with metrics reported in Table 3 for precision, recall, and F1 on the test set."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not focus on analyzing trends or patterns in the dataset independently of its use for training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmentation or retrieval-augmented generation according to the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents practical use of the proposed dataset for supervised fine-tuning and evaluation of models; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10508,
      "completion_tokens": 506,
      "total_tokens": 11014
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is derived solely from the MediaSum corpus, which consists of interviews from sources like NPR and CNN in English. There is no mention of multiple languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as containing exactly two languages. It only references English-language data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The authors explicitly state that they sample 200 meetings from the MediaSum dataset in English language for their SpeakerID dataset. MediaSum is composed of English transcripts from NPR and CNN."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of dialogue transcripts and speaker annotations with no mention of programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Despite equation-like notation in model descriptions, these do not appear as dataset entries but are part of the methodological exposition."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human spoken dialogue transcripts, with no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses natural English language only; no fictional or constructed languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language transcripts and thus is not void of language."
        }
      }
    }
  },
  {
    "id": "nguyen24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7726,
      "completion_tokens": 212,
      "total_tokens": 7938
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 1 Introduction",
          "reasoning": "The paper explicitly states in the Abstract that \"The data and code are publicly available here: https://github.com/adobe-research/speaker-identification\", indicating that the code related to dataset creation, preprocessing, and model implementation is available in this repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 Data Collection",
          "reasoning": "Section 2.2 provides a detailed description of how the new SpeakerID dataset is constructed from the MediaSum corpus, including steps such as detecting person names, anonymizing speaker identities, mapping detected names to speaker identities using fuzzy text matching with Levenshtein Distance, and usage of a state-of-the-art NER model. The explanation is thorough and clear, reflecting transparent and complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8164,
      "completion_tokens": 112,
      "total_tokens": 8276
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Database Construction",
          "Reasoning": "The UY/CH-CHILD is an audio speech database composed of recordings of Uyghur children pronouncing Chinese words. The recordings were made using a laptop and microphone in controlled environments, involving direct human capture of speech data from 106 children, as detailed in Section 3.2 (Speech Recording). Thus, the modality is audio captured by human participants."
        }
      ]
    }
  },
  {
    "id": "nijat24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8964,
      "completion_tokens": 368,
      "total_tokens": 9332
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3 and 3.4",
            "reasoning": "Section 3.3 states that students from the International Cultural Exchange College at Xinjiang University performed the target speech segmentation. Section 3.4 states that students from Xinjiang University conducted phonetic annotations. There is no indication these annotators were experts; they are described as university students, which aligns with non-expert human annotators working in multiple to perform these tasks."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 and 3.4",
            "reasoning": "Section 3.2 describes the recording procedure including tutor supervision and guidelines that suggest a controlled and guided collection environment. Section 3.4 details the phonetic annotation process, specifying annotation format and usage of special labels (e.g., '*') when classification was difficult, indicating that instructions about the annotation guidelines and procedures were provided to the annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.4",
            "reasoning": "The phonetic annotation involved labeling pronunciations in a structured format: Initial~Final~Tone, with specific notation for uncertain cases (e.g., '*'). This structured labeling implies a rubric or a defined scoring or labeling standard to distinguish correct and incorrect pronunciations and uncertainty, facilitating consistent annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.4",
            "reasoning": "An explicit example is provided in Section 3.4: for the character \u2018\u5149\u2019 with canonical pronunciation \u2018guang1\u2019, the annotation \u2018g~uan*\u2019 is used to illustrate a specific mispronunciation and tone uncertainty. Providing such examples demonstrates that annotation guidelines included examples to assist annotators."
          }
        }
      ]
    }
  },
  {
    "id": "nijat24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10154,
      "completion_tokens": 318,
      "total_tokens": 10472
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being conducted by a single human annotator who is an expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that multiple expert annotators performed quality assurance. The annotators are described as students from Xinjiang University without indication of subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify quality assurance by a single non-expert annotator; rather, multiple annotators were involved."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.4",
          "reasoning": "The target speech segmentation and phonetic annotations were performed by multiple students from Xinjiang University and the International Cultural Exchange College; there is no indication these annotators are subject matter experts, implying QA was conducted by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge for quality assurance in the dataset annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification or computational rule-based quality assurance for the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving multiple human non-experts is described, so it is not the case that no QA was applied or documented."
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9764,
      "completion_tokens": 407,
      "total_tokens": 10171
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 3.1 to 3.4",
          "reasoning": "The paper explicitly describes the collection of new speech data from 106 Uyghur children who pronounced predefined Chinese words. The data includes newly recorded audio samples and carefully annotated pronunciations at the syllable level done by human annotators. The data was obtained through direct recording sessions under tutor supervision, following a vocabulary design tailored for this study. The resulting database, UY/CH-CHILD, is a brand new dataset created from scratch without deriving from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the dataset involves any translation of content from other languages performed by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of machine translation to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected or aggregated from pre-existing sources; the authors recorded new speech data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not created by modifying or adapting existing data; it is original data collected from recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10282,
      "completion_tokens": 209,
      "total_tokens": 10491
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Pronunciation Bias Analysis)",
          "reasoning": "The UY/CH-CHILD dataset is primarily used for analyzing pronunciation skill development and error patterns among Uyghur children learning Chinese as a second language, as evidenced by the detailed vowel, consonant, and tone bias analyses presented in Section 4."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11005,
      "completion_tokens": 610,
      "total_tokens": 11615
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on Uyghur children learning Chinese as their second language and contains Chinese words spoken by these children. There is no indication that more than two human languages are included in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the children are native Uyghur speakers learning Chinese as L2, the dataset entries contain only Chinese word pronunciations and their phonetic annotations in Pinyin. The dataset does not contain spoken samples or annotations in two languages per entry. Hence, this is not a bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain English content. The target words are in Chinese, and the annotations are in Pinyin, a Romanization of Chinese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3 (Database Construction)",
          "reasoning": "The UY/CH-CHILD dataset consists of audio recordings of Chinese words spoken by Uyghur children learning Chinese as a second language. The words are exclusively Chinese, and all phonetic annotations are in Pinyin (Romanized Chinese). The dataset is focused on Chinese language pronunciation data without inclusion of Uyghur or other languages as audio or annotated content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech only; there is no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains Chinese word pronunciations and is not related to any fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are clearly specified as Chinese words spoken by Uyghur children; the language content is well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech data with linguistic content (Chinese words) and is therefore not applicable for the non-language category."
        }
      }
    }
  },
  {
    "id": "nijat24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8223,
      "completion_tokens": 156,
      "total_tokens": 8379
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link mentions code availability",
          "reasoning": "The paper details the dataset construction process but does not provide any links, repository references, or mention of publicly releasing the code used for data collection, preprocessing, segmentation, or annotation. The focus is on data availability rather than on code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 4 (Database Construction and Pronunciation Bias Analysis)",
          "reasoning": "The paper provides a detailed and clear description of the dataset creation process, including vocabulary design, speech recording methodology, segmentation, annotation, and then presents analysis based on the dataset. This documentation is thorough and sufficient to understand the construction of the new dataset."
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7320,
      "completion_tokens": 174,
      "total_tokens": 7494
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 PRIORI dataset, Section 3.1.1 Emotional MisMatch (EMM)",
          "Reasoning": "The PRIORI dataset is described as a longitudinal collection of audio recordings from individuals with bipolar disorder, recorded using a secure app on participants' smartphones capturing participants' side of conversations. This indicates human-generated audio data captured by humans via smartphones."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Emotional MisMatch (EMM)",
          "Reasoning": "Text transcripts were obtained using the Amazon Mechanical Turk (AMT) platform where crowd workers transcribed the audio recordings, indicating human-generated text data obtained via human transcription."
        }
      ]
    }
  },
  {
    "id": "niu23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8120,
      "completion_tokens": 211,
      "total_tokens": 8331
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1.1",
            "reasoning": "The text emotion annotations are obtained via Amazon Mechanical Turk (AMT), a crowdsourcing platform, implying multiple non-expert annotators performed the text emotion annotation on speech transcripts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1.1",
            "reasoning": "The paper indicates the use of AMT for crowdsourcing text emotion annotation, which typically requires task instructions; although specific instructions are not detailed, the mention of identifying unreliable AMT workers suggests some annotation protocol was followed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not mention any scoring rubrics or detailed scoring criteria provided to annotators for emotion annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "There is no mention of example annotations or example tasks shown to annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "niu23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9310,
      "completion_tokens": 434,
      "total_tokens": 9744
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.1 and Section 3.2 (paragraphs referencing Amazon Mechanical Turk annotations)",
          "reasoning": "The paper states that transcripts and text emotion annotations were obtained via the Amazon Mechanical Turk crowdsourcing platform, implying multiple non-expert human annotators performed the annotation. Additionally, they identified unreliable AMT workers by calculating mismatch distances and covariance between ratings. This describes quality assurance involving multiple non-expert annotators and a process for screening unreliable annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for automatic emotion recognition, the paper does not describe AI models performing quality assurance of annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 (identification of unreliable AMT workers via calculation of mismatch distances and covariance)",
          "reasoning": "The paper describes an automatic process to identify unreliable annotators by calculating mismatch distances and covariance between text and acoustic emotion ratings, which acts as an automated quality assurance step."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance procedures are described in the paper."
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8920,
      "completion_tokens": 493,
      "total_tokens": 9413
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.1 and 2",
          "reasoning": "The PRIORI-Emotion Dataset is a subset of the PRIORI dataset collected from 19 consenting subjects over a period of six to twelve months. Speech recordings were made from participants' smartphones capturing their side of the conversation. Transcripts of these recordings were obtained through the Amazon Mechanical Turk (AMT) crowdsourcing platform, involving human annotators. This represents original data collection involving human contributors creating new content (speech and annotations) from scratch rather than adapting existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses machine learning models to generate features (e.g., emotion recognition using BERT and CNN models), but does not generate new data solely from models. The data remains grounded in human-recorded speech and human-annotated transcripts."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence in the paper of data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or evidence in the paper of data being produced through machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not presented as being collated from pre-existing sources without significant modification. Instead, it is newly collected human speech and annotations."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.2 and 3.2",
          "reasoning": "The emotion recognition features and Emotional MisMatch (EMM) features are derived from the raw speech and text transcripts through transformations and computational methods (automatic speech recognition, emotion recognition models, statistical calculations). These represent derived data based on the original recorded and annotated data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins of the data are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9438,
      "completion_tokens": 241,
      "total_tokens": 9679
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Mood prediction",
          "reasoning": "The PRIORI-Emotion Dataset is used for mood prediction tasks where various features are evaluated and compared. The paper details experiments measuring predictive performance on mood severity and classification using this dataset, indicating its use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1 EMM feature analysis",
          "reasoning": "The dataset is analyzed to characterize Emotional MisMatch (EMM) features and their statistical differences among mood classes. The analysis captures patterns and trends relevant to mood symptoms."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10161,
      "completion_tokens": 433,
      "total_tokens": 10594
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the dataset contains exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1, Section 2 PRIORI dataset",
          "reasoning": "The dataset consists of speech and transcripts from individuals with Bipolar Disorder, and the transcripts were obtained via Amazon Mechanical Turk in English. The paper references the use of the bert-base-uncased pre-trained language model, which is designed for English, and no other languages are mentioned. Thus, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language is mentioned or indicated in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech and text related to human language and emotion; there is no mention of programming or structured code-related content as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe features and analyses, the dataset itself does not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human speech and text; there is no mention of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages such as Klingon or Esperanto."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the language of transcripts as English; thus, the language is known and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language content (speech and text transcripts), so it is not applicable to label as having no language."
        }
      }
    }
  },
  {
    "id": "niu23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7379,
      "completion_tokens": 174,
      "total_tokens": 7553
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link provided in the paper",
          "reasoning": "The paper does not provide any explicit link or mention of publicly available code related to the dataset collection, preprocessing, or generation. The only related links are to pretrained models and feature extraction code used as baselines, not code to reproduce the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (PRIORI dataset) and Section 3.1.1",
          "reasoning": "The paper documents the dataset creation process describing the longitudinal data collection from individuals with Bipolar Disorder using a smartphone app, mentions speech recording procedures, IRB approval, transcription via Amazon Mechanical Turk with details on annotator reliability, and the labeling procedure for emotion and mood states. Thus, the dataset creation process is transparently described."
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5914,
      "completion_tokens": 192,
      "total_tokens": 6106
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.2",
          "Reasoning": "The new dataset consists of audio recordings collected by the authors from 2012 to 2016 of participants naming objects either shown in pictures or described by the experimenter. The recordings were made with a Sony microphone and recorder, involving human participants speaking naturally. This indicates human-generated audio data collected specifically for this study."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The dataset includes transcriptions of the recorded words, as well as annotation files generated semi-automatically and then manually corrected by trained phoneticians. The transcriptions and annotations were produced through human involvement and careful manual correction, indicating human-generated text data linked to the audio."
        }
      ]
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6714,
      "completion_tokens": 190,
      "total_tokens": 6904
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The annotation involved checking and correcting semi-automatic segmentation output manually by a trained phonetician, with another phonetician confirming the changes, indicating expert human annotators were involved."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated in the paper",
            "reasoning": "The paper does not mention specific detailed annotation instructions provided to annotators; it only describes the manual checking and correction process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not stated in the paper",
            "reasoning": "There is no mention of any scoring rubrics or grading criteria provided for the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not stated in the paper",
            "reasoning": "The paper does not provide annotation examples or guidelines containing examples for annotation."
          }
        }
      ]
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7904,
      "completion_tokens": 359,
      "total_tokens": 8263
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset annotations (segmentation and corrections) were checked and fixed by a trained phonetician and confirmed by another phonetician, indicating multiple experts were involved in quality checking; however, each annotation step involved individual expert annotators, implying a single human expert performed QA at each stage."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The segmentation and corrections were done by a trained phonetician and then confirmed by another phonetician, indicating multiple human experts reviewed the dataset annotations and thus performed QA in multiple stages."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies that trained phoneticians performed annotation checking and confirmation, indicating QA was not done by non-experts."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting any QA was conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models performing quality assurance as judges is provided in the paper."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "Semi-automatic segmentation was done using Julius speech recognition software, and automatic annotation files were generated and converted; thus, parts of the annotation process were automated and included algorithmic verification steps."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes several QA steps including manual checking by phoneticians and semi-automatic segmentation, therefore some form of QA is present and documented."
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7514,
      "completion_tokens": 449,
      "total_tokens": 7963
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The study utilized speech data collected by one of the authors from 2012 to 2016 specifically for their research purposes involving 23 participants across eight cities in the Tohoku region of Japan. The data collection involved participants naming objects in picture and riddle tasks in their usual spoken language. This dataset was newly collected original human-produced speech data, not translated or adapted from existing datasets, as stated in Sections 2.1 and 2.2."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the study used some previously collected data from earlier studies (e.g., data from [1] and [5]), the focus in this paper is on newly collected original speech data and the reanalysis including fully pre-voiced tokens measured anew, rather than aggregation or collection without modification. Therefore, the data is not merely collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper mentions using data originally collected for another study [5] and previously analyzed in [1], but in the current study, the authors re-measured and included fully pre-voiced tokens using a method proposed in a prior study [6]. This involves deriving new measurements and extending analysis on existing data with modifications and adaptations, thus the data is partly derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8032,
      "completion_tokens": 255,
      "total_tokens": 8287
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Method) and Section 3 (Results and Analysis)",
          "reasoning": "The paper introduces a dataset of spoken words from Tohoku Japanese dialect speakers, collected across multiple cities, with detailed acoustic measurements such as voice onset time (VOT), fundamental frequency (F0), and preceding vowel duration. This dataset is employed primarily for analyzing phonetic and phonological characteristics of intervocalic voicing neutralization, covariation of VOT and F0, and other linguistic patterns, rather than for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8755,
      "completion_tokens": 368,
      "total_tokens": 9123
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset is composed of speech data in the Tohoku dialects of Japanese, a non-English human language. The participants produced words in their usual spoken language, which is Tohoku Japanese dialect, and the corpus consists entirely of this single language variety. No other languages are mentioned as part of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "noguchi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5973,
      "completion_tokens": 138,
      "total_tokens": 6111
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not mention or provide any link or reference to publicly available code repositories for the dataset construction, data collection, preprocessing, or analysis. There is no indication that the code is accessible for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including descriptions of recordings, participants, materials and procedures, annotation and measurement methods, and data analysis. The methodology is explained thoroughly in Section 2, which supports reproducibility and transparency despite no code availability."
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8262,
      "completion_tokens": 204,
      "total_tokens": 8466
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2 and Figure 1",
          "Reasoning": "The dataset consists of audio recordings of speech and respiration captured using a Shure MX150 microphone placed inside the mask of the AltiTrainer device worn by human participants during experimental conditions involving normoxia and hypoxia. This clearly indicates human-generated audio data collected via human-operated equipment."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2 and Figure 1",
          "Reasoning": "Physiological signals such as Near-infrared spectroscopy (NIRS) data measuring cerebral oxygenation and ECG data measuring heart rate were collected from participants using dedicated sensor hardware (Portalite NIRS sensors and Polar H10 ECG chest belt). These signals were recorded from human subjects using sensor devices, indicating human-generated signal sensor data."
        }
      ]
    }
  },
  {
    "id": "obrien23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9062,
      "completion_tokens": 286,
      "total_tokens": 9348
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1, 2.2",
            "reasoning": "The dataset was collected from 20 student participants with data manually curated and processed by the authors, who are experts conducting the hypoxia detection study. The experimental protocol and feature extraction steps suggest expert involvement in annotation and segmentation (e.g., voice activity detection, segmentation of speech/non-speech, ECG and NIRS data segmentation). No mention of crowdsourced annotations or multiple annotators was made."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section or mention of annotation instructions for human annotators.",
            "reasoning": "The paper does not describe any annotation guidelines, instructions, or protocols directed to annotators beyond the experimental protocol. The segmentation uses an automatic voice activity detection method, and physiological signal processing is performed programmatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention of rubrics or scoring rules for annotation.",
            "reasoning": "No scoring criteria or rubric-based assessments are described for annotations. Annotation comprises segmentation and feature extraction."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotations or annotated data samples are provided anywhere in the paper.",
            "reasoning": "The paper does not present any sample labeled data or annotation examples, nor does it provide sample audio segments or physiological data samples with labels explicitly as examples."
          }
        }
      ]
    }
  },
  {
    "id": "obrien23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10252,
      "completion_tokens": 347,
      "total_tokens": 10599
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert annotator on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset or annotations in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information indicating that a single non-expert human performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or description of multiple non-expert humans performing quality assurance for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models (SVM, Nystr\u00f6m mapping) were used for classification, there is no indication that an AI model was used specifically for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 (Feature extractions) and Section 2.3 (Data processing)",
          "reasoning": "The dataset quality was implicitly ensured through automatic methods such as automated voice activity detection (SpeechBrain VAD) to segment speech and non-speech, signal processing steps (filtering, artifact correction), feature extraction algorithms, and synchronization procedures. These automated, algorithmic techniques serve as an automatic quality assurance process to validate and prepare the dataset for modeling."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes several automated processing steps contributing to QA, so it is not accurate to say no quality assurance process was applied or documented."
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9862,
      "completion_tokens": 444,
      "total_tokens": 10306
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data; Section 2.1.1 Participants; Section 2.1.2 Experimental protocol",
          "reasoning": "The study involved recording acoustic, cerebral blood oxygenation, and cardiac signals from 20 human participants who underwent control and normobaric hypoxia experimental conditions. The data were collected directly from these human subjects during controlled experiments specifically conducted by the authors. This constitutes original data created entirely from scratch by human contributors, not adapted or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of data being human-translated from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or mention of data being machine-translated."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe collecting or aggregating existing data from other sources; the dataset was newly collected."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Feature extractions",
          "reasoning": "While the raw data were newly collected from humans, the acoustic and physiological features were extracted and transformed from these raw recordings. For example, acoustic features (MFCCs), NIRS-based features, and ECG-based features were computed via established processing methods and feature extraction algorithms. Thus, the feature datasets can be considered derived from the raw recorded data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are explicitly described."
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10380,
      "completion_tokens": 322,
      "total_tokens": 10702
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.4.2 Model training",
          "reasoning": "The newly collected dataset, comprising acoustic, cerebral oxygenation, and cardiac signals from participants under normoxia and hypoxia conditions, was used to train Support Vector Machine models with a 4-fold cross-validation design to detect hypoxia. This indicates training models from scratch with the dataset features."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.5 Evaluation and Section 3.2 Evaluation results",
          "reasoning": "The dataset was partitioned into training and independent testing sets; models trained on the training dataset were evaluated on the testing dataset to measure hypoxia detection performance, indicating its use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3. Results and Section 4. Discussion",
          "reasoning": "The dataset was used to analyze the impact of speech and non-speech conditions on acoustic and physiological features related to hypoxia and to understand the characteristics and patterns associated with hypoxia-induced changes in speech production and physiological signals."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11103,
      "completion_tokens": 391,
      "total_tokens": 11494
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1.1 Participants and Section 2.1.2 Experimental protocol",
          "reasoning": "The dataset contains acoustic recordings of participants performing tasks and speech in a controlled experimental setting at Universit\u00e9 de Montpellier, France. Although the paper does not explicitly state the language spoken in the recordings, all participants are French university students and the only explicitly written commands or utterances mentioned (e.g., the word 'Top' for synchronization) are in English. The paper is written in English and describes the speech recordings without indication of other languages. Thus, the dataset is considered monolingual English in content, especially since the analysis focuses on acoustic features rather than on linguistic content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2.2.2 NIRS-based features and Section 2.2.3 ECG-based features",
          "reasoning": "The dataset includes physiological signals such as cerebral oxygenation data from near-infrared spectroscopy (NIRS) sensors and heart rate data from ECG chest belts. These data represent biological signals and non-human communication systems, as they are biological measurements rather than human spoken languages."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "obrien23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8321,
      "completion_tokens": 153,
      "total_tokens": 8474
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not include any links or references to publicly available code repositories or supplementary materials containing code for data collection, preprocessing, or generation related to the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods), subsections 2.1 (Data) and 2.2 (Feature extractions)",
          "reasoning": "The paper provides detailed descriptions of the dataset creation including participant recruitment, experimental protocol, data collection hardware and software setup, data synchronization, feature extraction methods for acoustic, NIRS, and ECG data; thus, the dataset creation process is well documented in the Methods section."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8904,
      "completion_tokens": 106,
      "total_tokens": 9010
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset: Afro-TTS",
          "Reasoning": "The Afro-TTS dataset consists of 136 hours of 16-bit, 48 kHz audio files recorded by 747 volunteers from 9 African countries speaking 86 different African English accents. The recordings were made by human contributors who read English texts, thus the data modality is audio and the origin is human generated."
        }
      ]
    }
  },
  {
    "id": "ogun24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9704,
      "completion_tokens": 295,
      "total_tokens": 9999
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4 Experiments and Section 5 Results and Discussion",
            "reasoning": "In Section 4, the paper mentions that 427 unique participants provided 26,974 human ratings representing 102 unique speaker combinations in subjective listening tests. These participants appear to be crowd-sourced raters evaluating synthetic speech naturalness, accentedness, and preference, implying multiple human non-expert annotators were used."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4 Evaluation protocol and Section 5 Results and Discussion",
            "reasoning": "The participants performed MOS and preference evaluations following a subjective evaluation protocol described in Sections 4 and 5. The paper states participants were asked specific questions about naturalness, accentedness, and preference, implying detailed instructions were provided for consistent rating."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4 Evaluation protocol",
            "reasoning": "The subjective evaluation involved specific quantitative scales such as MOS (Mean Opinion Score), Nat-MOS, A-MOS, Country-M, Accent-M, and structured preference tests. These represent scoring rubrics guiding the annotators' ratings."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not explicitly mention or show examples of annotation or rating guidelines provided to annotators for the subjective evaluation tasks. Therefore, we cannot assert the presence of such examples."
          }
        }
      ]
    }
  },
  {
    "id": "ogun24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10894,
      "completion_tokens": 500,
      "total_tokens": 11394
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert or subject matter expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple human experts or annotators with subject matter expertise in the dataset creation or validation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention QA by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset: Afro-TTS",
          "reasoning": "The dataset was crowd-sourced from 747 paid contributors who were volunteers asked to read and record English texts. Although their expertise level is not explicitly stated, they are considered non-expert annotators. Since annotation involved multiple such contributors, this corresponds to quality assurance conducted by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset pre-processing",
          "reasoning": "An automated quality estimator WVMOS was used to evaluate different processed versions of each utterance, selecting the sample with the highest predicted MOS score. Additionally, speech enhancement and bandwidth-extension models (VoiceFixer) were used for denoising and quality improvement, which is an AI/ML-based automatic process for data quality enhancement."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset pre-processing",
          "reasoning": "The dataset underwent systematic automated processing for quality assurance, including denoising by a speech enhancement model, bandwidth extension with VoiceFixer, and volume normalization and voice activity detection using algorithmic tools. This automated verification and processing ensured data consistency and improved quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents several quality assurance processes; therefore, it is not the case that no QA was applied or documented."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10504,
      "completion_tokens": 406,
      "total_tokens": 10910
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The Afro-TTS dataset was created by online crowd-sourcing recordings from 747 paid human contributors across 9 African countries who read specific English texts. The dataset is original and created from scratch by human speakers producing audio data, with no indication of translated, adapted, or derived content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any portion of the Afro-TTS dataset was generated by AI or machine learning models; all speech data is human recorded."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the dataset involving translation of content from other languages into English by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include data translated from other languages using machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Afro-TTS dataset was not collected or aggregated from existing sources; it was newly recorded and curated specifically for this work."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "The raw recordings were processed by removing noise, enhancing quality with models like VoiceFixer, normalizing volume, and trimming long pauses, representing transformations applied to the original data before use."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11022,
      "completion_tokens": 566,
      "total_tokens": 11588
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Afro-TTS dataset is not described as used for pre-training large models on general patterns or in an unsupervised/self-supervised manner. Instead, it is used for supervised training and fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4 Experiments",
          "reasoning": "The paper states in Section 3.2 that a VITS model was trained from scratch (termed VITS-O) on the Afro-TTS dataset to validate the quality of the data for TTS experiments, indicating training from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4 Experiments",
          "reasoning": "The Afro-TTS dataset is used to fine-tune pre-trained models (VITS-FT and XTTS-FT) as described in Section 3.2 and the experiments section, showing supervised fine-tuning of TTS models on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training techniques such as RLHF used with the Afro-TTS dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The Afro-TTS dataset includes development and test splits used for evaluation of TTS model performance, including subjective and objective metrics as described in Sections 4 and 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 and throughout the paper",
          "reasoning": "The paper provides detailed analysis of accent representation, naturalness, accentedness, preference scores, and regional diversity derived from the Afro-TTS dataset, used to understand trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the Afro-TTS dataset is used as a knowledge base for augmenting models through retrieval or similar."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The Afro-TTS dataset is actively used throughout the pipeline for training, fine-tuning, evaluation, and analysis. Hence, 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11745,
      "completion_tokens": 495,
      "total_tokens": 12240
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Afro-TTS dataset introduced focuses on English content with multiple African accents but does not contain entries in multiple different human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies the dataset consists of spoken English with African accents; it does not mention bilingual content involving exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset: Afro-TTS",
          "reasoning": "The Afro-TTS dataset contains English speech recordings only, as volunteers read sequences of English text. The paper repeatedly emphasizes that the dataset is pan-African accented English. No other spoken languages are included in the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries contain English speech, not a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists only of human English speech and does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are not in constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content in the dataset is explicitly described as English; it is not unspecified or undocumented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains natural language speech data."
        }
      }
    }
  },
  {
    "id": "ogun24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8963,
      "completion_tokens": 217,
      "total_tokens": 9180
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Dataset: Afro-TTS",
          "reasoning": "The paper states that the Afro-TTS dataset and models can be accessed via a link to Hugging Face (https://huggingface.co/intronhealth/afro-tts), but there is no explicit mention or link to the code used for data collection, preprocessing, or dataset construction. Hence, code for dataset creation is not indicated as publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset: Afro-TTS and Section 3.1.1 Dataset pre-processing",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including data collection (crowd-sourcing protocol), demographics of contributors, recording settings, consent, data preprocessing steps such as denoising, bandwidth extension, quality evaluation, normalization, splitting, and text normalization methods. This information is provided mainly in Sections 3.1 and 3.1.1, offering transparency about dataset creation."
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8323,
      "completion_tokens": 159,
      "total_tokens": 8482
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Introduction, Discussion",
          "Reasoning": "The new dataset introduced in the paper consists of real-time Magnetic Resonance Imaging (rtMRI) data and accompanying measurements of vertical larynx movements (VLMs) from five native speakers of Seoul Korean. This modality involves sensor data captured using rtMRI equipment during human speech production, thus it is human generated data from human subjects recorded with specialized sensors. The paper explicitly states that 'Target stimuli were produced by five native speakers of Seoul Korean using real-time Magnetic Resonance Imaging (rtMRI) of the larynx and tongue.' This confirms the data modality as sensor data and human generated origin."
        }
      ]
    }
  },
  {
    "id": "oh22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9123,
      "completion_tokens": 230,
      "total_tokens": 9353
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Abstract; Section 2.2; Discussion",
            "reasoning": "The paper states that target stimuli were produced by five native speakers of Seoul Korean, implying data collection was performed by human experts native in the language. The detailed imaging and analysis also indicate expert involvement in annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Collection and Annotation",
            "reasoning": "The paper describes designed target stimuli to elicit prosodic focus, indicating specific task instructions were present to guide speakers and annotators in eliciting and identifying focus phenomena."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No explicit mention of scoring rubrics or formal rubric guidelines in annotation procedures is found in the paper."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Abstract; Examples in Introduction and Discussion sections",
            "reasoning": "The paper includes example sentences (e.g., 'sold six yards of shabby garden field') illustrating the annotations and focus conditions, which serve as examples for the annotation task."
          }
        }
      ]
    }
  },
  {
    "id": "oh22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10313,
      "completion_tokens": 313,
      "total_tokens": 10626
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert for the datasets; no such description is provided."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the datasets. The paper lacks details on human expert validation of annotations or data."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance carried out by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert annotators validating or assuring quality of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of AI models for quality assurance of the dataset or annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 (Methods and Results sections)",
          "reasoning": "The paper describes the use of automated tools such as an automated centroid tracking tool (ACT) and MVIEW find_gest software to extract features and track vertical larynx movements. This indicates an automatic process was used to verify and analyze the articulatory data, suggesting automated quality assurance through algorithmic techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some form of automated quality assurance is documented; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9923,
      "completion_tokens": 180,
      "total_tokens": 10103
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Abstract, Methods, Discussion",
          "reasoning": "The paper reports data collected through real-time Magnetic Resonance Imaging (rtMRI) from five native speakers of Seoul Korean producing designed target stimuli sentences. This data consists of original speech production recordings uniquely created by human contributors specifically for this study and not adapted or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10441,
      "completion_tokens": 297,
      "total_tokens": 10738
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the novel dataset for pre-training any machine learning models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset was used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for fine-tuning pre-trained models in a supervised manner."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning post-training methods using the dataset are described."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not indicated to be used purely for evaluation or benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Results and Discussion sections",
          "reasoning": "The dataset, consisting of real-time MRI data of vertical larynx movements under prosodic focus, is used primarily to analyze spatio-temporal articulatory patterns, correlations with prosodic focus, and phonetic characteristics rather than for any model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for augmenting models with external knowledge."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11164,
      "completion_tokens": 558,
      "total_tokens": 11722
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced involves only Seoul Korean utterances for prosodic focus analysis. No other languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication the dataset contains exactly two languages. Only one language, Korean, is used."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some example sentences are given in English for illustrative purposes, the actual dataset consists of Korean sentences produced by native speakers. The English is not part of the data but only used for explanation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Introduction; Discussion",
          "reasoning": "The dataset consists of Seoul Korean sentences produced by native Korean speakers (five speakers). The paper focuses on prosodic focus in Korean and measurements of vertical larynx movements for Korean utterances. Hence, the dataset is monolingual Korean (non-English)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any code or programming language data. Tools and methods (software) are mentioned, but the dataset entries are not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or logical symbolic expressions; it is composed of speech utterances and larynx movement measurements."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech articulations and vocal tract imaging, but it is linguistic data from human speakers, not biological sequences or non-human communications."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain fictional or constructed languages. The language used is Seoul Korean."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as Seoul Korean."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken Korean utterances and thus involves linguistic content."
        }
      }
    }
  },
  {
    "id": "oh22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8382,
      "completion_tokens": 140,
      "total_tokens": 8522
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not include any links, appendices, or references to publicly available code repositories related to the dataset collection, preprocessing, or analysis."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data collection and IRB protocol) and Discussion sections",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process, including the use of real-time MRI data acquisition from five native Seoul Korean speakers, the stimuli design, and the analysis methods used to extract vertical larynx movement measures, indicating comprehensive documentation of the dataset's construction."
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7727,
      "completion_tokens": 232,
      "total_tokens": 7959
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 \"Design of CAPTDURE\", Table 3, Section 2.2 \"Recording environment and setup\"",
          "Reasoning": "The dataset includes 1,044 single-source sounds of various daily sound events that were recorded by the authors using multiple types of recording equipment and environments as detailed in Section 2.2 and Table 3. These sounds are captured from real-world sound events via human-operated recording setups, indicating human generation of the audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 \"Design of CAPTDURE\", Section 2.3 \"Captions collection\"",
          "Reasoning": "The dataset contains a total of 4,902 captions for the single-source sounds collected from Japanese crowdworkers using the Lancers crowdsourcing service as described in Section 2.3. The captions were manually written by human workers describing the single-source sounds, thus the text data is human generated."
        }
      ]
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8527,
      "completion_tokens": 220,
      "total_tokens": 8747
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3",
            "reasoning": "The captions were collected from Japanese crowdworkers using the Lancers crowdsourcing service as stated in Section 2.3. Lancers is a platform for non-expert crowdworkers rather than expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "Crowdworkers were given instructions to write different sentences for each sound and to avoid only naming the sound type or only using onomatopoeic words, which indicates the presence of instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "The paper does not mention any scoring rubrics or detailed criteria used to guide or evaluate the caption collection process by annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "No examples of captions or annotation samples provided to crowdworkers during caption collection are described in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9717,
      "completion_tokens": 314,
      "total_tokens": 10031
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any QA conducted by a single human expert or annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple experts or subject matter experts performed quality assurance or annotation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Captions collection; Section 2.4 Captions evaluation",
          "reasoning": "Captions and appropriateness scores were collected from multiple crowdworkers via crowdsourcing on the Lancers platform. The crowdworkers are non-experts as no specialized expertise is mentioned. Multiple non-expert annotators transcribed captions and rated appropriateness scores for captions produced by others, indicating a QA process involving multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of AI models being used for quality assurance of annotations or captions."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic verification process for quality assurance is described in the dataset creation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a quality assurance step involving appropriateness scoring by multiple crowdworkers, so no QA process is not applicable."
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9327,
      "completion_tokens": 488,
      "total_tokens": 9815
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.3",
          "reasoning": "The dataset CAPTDURE was constructed by the authors who recorded 1,044 single-source environmental sounds themselves using various recording equipment and conditions (Section 2.1, 2.2). Captions for these recorded sounds were collected from Japanese crowdworkers via the crowdsourcing service Lancers, where human contributors wrote original captions describing the sounds (Section 2.3). Thus, both the audio data and the captions were newly created from scratch by human effort."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any dataset content entirely by AI or machine learning models; all data collection and captioning were performed by human recording and crowdworkers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Captions were originally collected in Japanese and then translated into English using the DeepL API, which is a machine translation system. No mention of human translation is made."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The captions collected in Japanese were translated into English using the DeepL API, a machine translation service, as explicitly stated in Section 2.3."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources; it was created by recording new sounds and collecting new captions."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "Multiple-source sounds were created by mixing two different single-source sounds recorded by the authors to simulate mixture sounds, representing data derived from the original single-source sounds with modifications (mixing and SNR adjustment)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are described clearly in the paper."
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9845,
      "completion_tokens": 281,
      "total_tokens": 10126
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The dataset CAPTDURE is used to train a neural network model from scratch using supervised learning to extract single-source sounds from mixture sounds based on captions, as described in the network architecture and training details in Section 3.3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The dataset is used for evaluation of the sound extraction performance, including test sets with mixtures created from the test single-source sounds, as described in Section 3.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.4, Fig. 1",
          "reasoning": "The dataset includes appropriateness scores collected from crowdworkers to analyze caption quality and appropriateness, as detailed in Section 2.4 and shown in Figure 1."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10568,
      "completion_tokens": 653,
      "total_tokens": 11221
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.3 Captions collection; Table 4 Dataset statistics",
          "reasoning": "The dataset CAPTDURE contains captions collected from Japanese crowdworkers in Japanese. These captions were also translated into English using the DeepL API (Section 2.3). Thus, the dataset contains captions in more than two human languages: Japanese (original captions) and English (machine-translated captions). Since two languages are explicitly presented, and the translation process suggests both languages are included, and no other languages are mentioned, the dataset can be considered bilingual rather than multilingual. However, the presence of original captions in Japanese and their translated versions in English indicates it includes exactly two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.3 Captions collection; Table 4 Dataset statistics",
          "reasoning": "CAPTDURE contains captions in Japanese collected from crowdworkers and English translations obtained via the DeepL API, explicitly mentioned in Section 2.3. No other human languages are mentioned. Thus, the dataset contains exactly two human languages: Japanese and English."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains Japanese captions as original data and English as translated captions. It is not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains Japanese captions but also English translations, so it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains sound recordings and natural language captions; there is no indication of programming code or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio sounds and natural language captions only; no mathematical or logical symbolic notation is included in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human environmental sounds with captions and does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of the dataset containing fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly stated as Japanese and English; therefore, language coverage is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language captions (Japanese and English), so it is not language-free."
        }
      }
    }
  },
  {
    "id": "okamoto23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7786,
      "completion_tokens": 212,
      "total_tokens": 7998
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not mention any link, repository, or location where the code for dataset construction or related processing is made publicly available. There is only a mention that the dataset is freely available online, but no indication the code to create the dataset has been released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Creation of CAPTDURE) and its subsections",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 2, including dataset design, recording environment and setup (Sec. 2.2), captions collection (Sec. 2.3), captions evaluation (Sec. 2.4), and data splitting (Sec. 2.5). It includes information on the number and types of recordings, recording equipment, microphone distance, duration, caption collection procedure, appropriateness scoring, and dataset split statistics. This constitutes thorough documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7127,
      "completion_tokens": 142,
      "total_tokens": 7269
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.1.1",
          "Reasoning": "The authors introduced a new dataset composed of question-answer pairs extracted from a subset of the Spotify 100,000 Podcast dataset, which consists of real spontaneous two-speaker conversations with audio recordings. They applied filtering steps for speaker diarisation, overlapping speech, laughter, and speaker verification, resulting in approximately 26,876 question-answer pairs (~38 hours of audio). This dataset contains audio modality recorded from human speakers in natural podcast conversations, hence is human generated and not model generated or unknown origin."
        }
      ]
    }
  },
  {
    "id": "omahony22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7927,
      "completion_tokens": 252,
      "total_tokens": 8179
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1.1 and 2.1.2",
            "reasoning": "The question-answer pairs were extracted automatically from the Spotify 100k Podcasts dataset using automated filters based on punctuation and diarisation, and further audio filtering including speaker verification using SpeechBrain ECAPA-TDNN, overlap detection with Pyannote audio, and laughter detection. No mention of human manual annotation or correction was made for the new question-answer data creation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No annotation guideline section present",
            "reasoning": "The paper does not describe any instructions provided to annotators for creating or filtering the question-answer dataset. The process is fully automated with no mention of human annotators or instruction materials."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No annotation rubric section present",
            "reasoning": "No scoring or rubric is described for annotation of the data; data filtering is automatic and based on predefined criteria, thus no rubric is applicable."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples given",
            "reasoning": "There are no annotation examples or illustrative examples provided for annotation guidelines since the dataset creation is automated."
          }
        }
      ]
    }
  },
  {
    "id": "omahony22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9117,
      "completion_tokens": 435,
      "total_tokens": 9552
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance performed by multiple human experts in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that multiple non-expert humans performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models were used during training and evaluation, there is no indication that an AI model was used specifically to judge or validate the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.1.1",
          "reasoning": "The dataset construction involved multiple automated filtering steps: automatic speaker diarisation and automatic transcription from Google Cloud Services, automatic punctuation for sentence splitting, detection and removal of overlapping speech and laughter using Pyannote audio and a laughter detector, speaker verification with embeddings using SpeechBrain ECAPA-TDNN, and filtering based on length and presence of symbols or numbers. These automated, rule-based, and machine learning-based processes systematically filtered the data for quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes an extensive series of automated quality assurance steps for the spontaneous speech dataset, so it is not the case that no QA was performed."
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8727,
      "completion_tokens": 435,
      "total_tokens": 9162
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are not newly recorded or created from scratch by human contributors specifically for this paper. The authors did not create any original content by recording new speech data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any datasets are produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation to generate or adapt any dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.1.1",
          "reasoning": "The authors created a new dataset of question-answer pairs by extracting and filtering data from the existing Spotify 100,000 Podcast dataset. This involved selecting podcasts with two speakers, segmenting transcripts, filtering utterances, and applying speaker verification and audio filters. This process aggregates and selects data from an existing large corpus without generating original recordings, which fits the 'collated' category."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors applied filtering and processing to data from the Spotify podcast corpus, these steps are consistent with collating and selecting data rather than significant modifications or transformations to constitute derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the new datasets are clearly documented and described in detail in the paper."
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9245,
      "completion_tokens": 224,
      "total_tokens": 9469
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The new dataset of spontaneous question-answer pairs extracted from podcasts (Section 2.1) is combined with the read speech dataset to train two FastPitch models from scratch, one on read speech only and another combining read and spontaneous speech data. This demonstrates use of the new spontaneous speech dataset for training a model from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9968,
      "completion_tokens": 617,
      "total_tokens": 10585
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 and 3.1",
          "reasoning": "The new dataset introduced is derived from podcasts labeled with countries including UK, US, Canada or general English, but there is no indication of multiple languages being present. The dataset is filtered for English content only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1.1",
          "reasoning": "The dataset consists of question-answer pairs in English extracted from two-speaker podcasts, both speakers using English. There is no evidence of exactly two different human languages included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.1.1, and 3.1",
          "reasoning": "The new dataset extracted from the Spotify 100k Podcasts is in English, as indicated by country labels (UK, US, Canada) for filtering and by the text being punctuated and transcribed in English. The read speech data is from LJ Speech, an American English audiobook corpus. Therefore, both components of the new dataset contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Overall data description",
          "reasoning": "No non-English single language datasets are introduced in this work; all data is English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No datasets with programming or structured code content are introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The datasets contain speech and text transcripts but no mathematical or logical notations are part of the data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No fictional or constructed languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Sections 2.1 and 2.1.1",
          "reasoning": "The language content is clearly described as English and is documented through automatic transcriptions and filtering, so language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of speech with corresponding transcriptions in English; therefore, they do contain language."
        }
      }
    }
  },
  {
    "id": "omahony22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7186,
      "completion_tokens": 185,
      "total_tokens": 7371
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not mentioned",
          "reasoning": "The paper does not include any link or mention of publicly available code repositories for the dataset construction, filtering, or processing steps. No URLs or references to codebases are provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Data), Sections 2.1.1 (Data Filtering) and 2.1.2 (Audio Filtering)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process from the Spotify 100,000 Podcast dataset, including criteria for filtering the data by speaker count, utterance completeness, punctuation, speaker diarisation, extraction of question-answer pairs, removal of low-quality data via speaker verification, overlap and laughter detection, and final dataset statistics. This thorough description supports reproducibility of the dataset creation steps even if code is not provided."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8362,
      "completion_tokens": 167,
      "total_tokens": 8529
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4 (Data and Evaluation Setup)",
          "Reasoning": "The new dataset introduced in the paper is derived from TED-LIUM 3 corpus, consisting of 450 hours of English speech audio. This is human-generated data as the TED talks are real human recordings."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4 (Data and Evaluation Setup)",
          "Reasoning": "The authors use GPT-3.5 (a model) to generate the Speech Comprehension Test Question-Answer (SQA) pairs from transcripts of the TED-LIUM 3 corpus, thus the QA pairs text data are model-generated."
        }
      ]
    }
  },
  {
    "id": "pan24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9162,
      "completion_tokens": 284,
      "total_tokens": 9446
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 4: Data and Evaluation Setup",
            "reasoning": "The paper states that GPT-3.5 (an AI language model) is used to generate Speech Comprehension Test Question-Answer (SQA) pairs from speech transcriptions to prepare the instruction tuning data based on TED-LIUM 3 corpus. There is no indication that humans performed manual annotation for the new dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not provide or mention any detailed human annotation instructions for the generation or verification of the SQA pairs. The generation is fully automated by GPT-3.5 prompts without described human annotator instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "There is no mention or description of any scoring rubrics or evaluation criteria provided as part of annotation guidelines for the newly generated SQA dataset. The evaluation metrics are applied later for model outputs but are not part of annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4",
            "reasoning": "The paper does not mention providing annotation examples or exemplary QAs to annotators (human or AI) during data generation. The generation is done by GPT-3.5 prompts without described annotated examples for training annotators."
          }
        }
      ]
    }
  },
  {
    "id": "pan24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10352,
      "completion_tokens": 395,
      "total_tokens": 10747
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert for the new dataset. There is no description of expert annotation or verification."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by multiple human experts or annotators for the dataset. No human expert involvement is documented."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single human non-expert performed quality assurance on the new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any QA performed by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4, Data and Evaluation Setup",
          "reasoning": "The authors use GPT-3.5 to automatically generate speech comprehension test question-answer (SQA) pairs from transcripts, thus employing an AI model for dataset generation and implicitly for quality control of the QA pairs."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention an automated algorithmic or rule-based verification as part of quality assurance for the generated dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some quality assurance is performed through the use of GPT-3.5 for SQA generation, so QA is present and documented."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9962,
      "completion_tokens": 479,
      "total_tokens": 10441
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any entirely new speech or text data was created from scratch by human contributors. The speech data used (TED-LIUM 3) is pre-existing, and the transcripts are original transcripts, but the paper does not claim the authors themselves created new human-annotated data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4 Data and Evaluation Setup",
          "reasoning": "The authors use GPT-3.5 (an AI model) to generate Speech Comprehension Test Question-Answer (SQA) pairs based on the transcripts of their training corpus. This is model-generated new data in the form of QA pairs that do not exist originally."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that any human translators produced translations in the paper."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 4 Data and Evaluation Setup",
          "reasoning": "The authors use GPT-3.5 to translate English transcripts into Spanish, French, and German for unseen speech-to-text translation tasks. This counts as machine-translated data generated for evaluation purposes."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4 Data and Evaluation Setup",
          "reasoning": "The main speech data source is TED-LIUM 3, which is an existing corpus. The authors utilize this existing dataset without fundamental changes, thus 'collated' applies."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that they derived a new dataset by modifying or adapting existing data in a significant way other than generating QA pairs or translations as noted."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the origins of all data used, including the source of speech data and generated QA pairs and translations."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10480,
      "completion_tokens": 490,
      "total_tokens": 10970
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly created SQA dataset or other generated data for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper initializes model components from pre-trained models (e.g., Whisper encoder and LLaMA-2 checkpoints) and does not report training any model from randomly initialized parameters with the new datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Speech Instruction-Tuning; Section 4 Data and Evaluation Setup; Section 5 Experiments and Results",
          "reasoning": "The new dataset, consisting of GPT-3.5 generated speech comprehension test question-answer (SQA) pairs from TED-LIUM 3 transcripts (about 450 hours English speech), along with ASR labels, is used for supervised instruction tuning of the COSMIC speech-Large Language Model system to develop instruction-following and in-context learning capabilities (Section 3.2 and Section 4). This supervised fine-tuning is explicitly described and experimentally validated in the paper (Section 5)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using the new datasets for reinforcement learning based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not used exclusively for evaluation or benchmarking; rather it is employed in training (instruction tuning). Existing standard datasets like TED-LIUM 3 test sets and FLEURS are used for evaluation, but these are pre-existing and not created by the authors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the new dataset primarily for data analysis or to investigate trends or patterns; the focus is on instruction tuning and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the new dataset as a knowledge base to augment models via retrieval or other methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the new dataset in supervised fine-tuning (instruction tuning); thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11203,
      "completion_tokens": 635,
      "total_tokens": 11838
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced by the authors is based on TED-LIUM 3 English speech data only. Although the evaluation involves speech translation into multiple languages, the instruction-tuning dataset itself contains English speech and corresponding English text. Hence, the dataset does not include entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced contains only English speech and text in English. There is no mention of entries containing exactly two human languages in the dataset created by the authors."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4 'Data and Evaluation Setup', Section 5 'Experiments and Results'",
          "reasoning": "The dataset constructed for instruction-tuning is derived from TED-LIUM 3 corpus which contains only English speech and transcripts. GPT-3.5 is prompted to generate question-answer pairs based on these English transcripts. The text responses and speech inputs used for training are exclusively English, establishing the dataset as monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset including non-English language data was introduced by the authors. The data used for instruction tuning is English speech and transcripts only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not mention inclusion of programming or code-related content in their dataset. The data consists of speech and textual natural language question-answer pairs."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly introduced dataset contains mathematical or formal logic notations or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists solely of human English speech and corresponding text; no biological or non-human communication data is introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificial languages in the new dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is explicitly stated to be English from TED-LIUM 3 corpus and GPT-3.5 generated English text; thus, the dataset language is fully known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries with English language (speech and text), so it does contain language."
        }
      }
    }
  },
  {
    "id": "pan24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8421,
      "completion_tokens": 196,
      "total_tokens": 8617
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention any public availability of code related to the dataset construction, data generation from speech transcripts, or instruction tuning data generation such as GPT-3.5 prompted question-answer pairs. There is no link or pointer to repositories or supplemental materials providing such code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 - Data and Evaluation Setup",
          "reasoning": "The paper provides detailed documentation on how the instruction tuning dataset is created: using TED-LIUM 3 speech corpus with 450 hours of English speech data; GPT-3.5 is prompted with speech transcriptions to generate question-answer pairs; data sizes and segmentations are reported (e.g., 50K audio segments, 856K QA pairs, average 17 QA pairs per utterance). The methodology for data preprocessing and generation is described clearly in Section 4."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8802,
      "completion_tokens": 116,
      "total_tokens": 8918
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Description of dataset",
          "Reasoning": "The paper explicitly states that they extended the Blizzard Challenge 2013 dataset with two new synthetic voices generated using WaveNET vocoders: 'Fastpitch Wavenet (Y)' and 'Tacotron Wavenet (Z)'. These are synthesized audio data generated by neural TTS models, thus the modality is 'audio', generated by models, not human recorded or natural recordings."
        }
      ]
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9602,
      "completion_tokens": 206,
      "total_tokens": 9808
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2",
            "reasoning": "Feature extraction and acoustic-phonetic measurements were performed using automatic forced alignment and computational feature extraction methods such as Montreal Forced Aligner and Praat software, indicating an automatic, rule-based procedure rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No annotation instructions for manual annotators are described; the process uses automated forced alignment and feature extraction without explicit human annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No scoring rubrics or judgment criteria for annotators are mentioned since the extraction is automatic and rule-based, not based on human rating or scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No examples of annotation or labeling are given, as the process is automated without manual annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10792,
      "completion_tokens": 414,
      "total_tokens": 11206
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset or annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by multiple human experts or annotators."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA involving multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge for quality assurance of the dataset or its annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper describes the use of automated tools for feature extraction and annotation validation, including the Montreal Forced Aligner (MFA) for phoneme-level boundaries and rule-based temporal boundary identification for sub-phonemic segmentation (Section 3.2). The features extracted (e.g., vowel durations, formants, amplitudes) rely on automated algorithms such as Praat formant tracking and automated spectral analysis, indicating an automated verification and annotation process without manual correction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documented quality assurance in the form of automated annotation and verification, so QA cannot be considered absent."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10402,
      "completion_tokens": 506,
      "total_tokens": 10908
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states that they extended the existing 2013 Blizzard Challenge dataset by including two new voices generated using a WaveNET vocoder. The natural voice recordings used as a reference are original human speech recordings, created from scratch by human speakers as part of the Blizzard Challenge dataset. Thus, the natural voice data in the dataset is original human-generated data, not derived or adapted from pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The two new TTS voices (named Y and Z) in the extended dataset were generated entirely by neural text-to-speech models using the WaveNET vocoder, conditioned on FastPitch and Tacotron acoustic models respectively. These synthetic voices and their speech utterances are newly generated data created by AI models, not transformations or translations of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data created via machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset analyzed is an extension of the existing Blizzard Challenge 2013 corpus, which was aggregated from previous TTS systems and the natural voice recordings. The authors aggregated these pre-existing data sources along with their two new neural TTS voices to create a combined comparative dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although features are derived from the speech signals, the paper does not describe the dataset itself as derived or adapted from existing data with transformations. The dataset is primarily an extension aggregating natural and synthetic speech data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are explicitly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10920,
      "completion_tokens": 448,
      "total_tokens": 11368
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or indicate that the new dataset is used for pre-training large models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the new dataset is used for training a model from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not reported to be used for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of reinforcement learning or RLHF post-training methods using this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4 Results",
          "reasoning": "The extended BC-2013 corpus with two new WaveNET vocoder voices is used for comparative analysis and benchmarking against natural and other TTS systems, as described in Section 3.1 and analyzed in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The dataset is primarily used to analyze acoustic-phonetic features of obstruent consonants and surrounding vowels to characterize differences and trends in segmental properties across TTS systems (Section 4 Results, Section 5 Discussion)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models; no such utility is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clear uses of the dataset for evaluation and analysis."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11643,
      "completion_tokens": 367,
      "total_tokens": 12010
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described is exclusively English, with no indication of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries in two human languages; only English is used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The extended Blizzard Challenge 2013 dataset used in the study consists of English utterances only, as stated in the description of the dataset containing 100 identical English utterances synthesized by multiple TTS systems."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly English; no mention of non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of programming or code-related content as dataset entries; the dataset consists of audio utterances."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical symbolic content is included in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech utterances and does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or artificial languages are mentioned; the dataset is composed of natural English speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly specified as English in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language (English) audio utterances, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "pandey22b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8861,
      "completion_tokens": 199,
      "total_tokens": 9060
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention any availability of code or provide links to code repositories related to the data collection, preprocessing, or dataset generation. The description only mentions using the Blizzard Challenge 2013 corpus extended with two new WaveNET vocoder voices, but no code release or reference is provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Experimental setup - Description of dataset",
          "reasoning": "The paper documents the dataset construction process in Section 3.1, describing how the Blizzard Challenge 2013 data was extended with two new WaveNET vocoder voices (Fastpitch Wavenet and Tacotron Wavenet). It details the included systems, the number of utterances (100 identical utterances synthesized by 8 systems plus original natural voice), and the rationale for inclusion. This documentation is sufficient to understand the data used for the comparisons."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7309,
      "completion_tokens": 136,
      "total_tokens": 7445
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.3",
          "Reasoning": "The Greek Podcast Corpus (GPC) is a newly introduced dataset composed of audio recordings collected from publicly available podcasts in Greek. These podcasts were recorded by humans and downloaded via web crawling and open-source tools. The audio data is single-channel, 16 kHz sampled speech, representing real human speech from different speakers and domains. This clearly establishes the data modality as audio and the origin as human-generated since it consists of original human speech recordings collected from human-operated podcast production."
        }
      ]
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8109,
      "completion_tokens": 207,
      "total_tokens": 8316
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.3 Automatic Transcription",
            "reasoning": "The GPC-50 corpus is transcribed automatically using the WhisperX pipeline leveraging the large-v3 Whisper model for transcription, indicating annotations were generated by an AI Model."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section describes provided manual annotation instructions.",
            "reasoning": "The paper does not mention any human annotator instructions; transcription is automatic, so no instructions provided for manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No rubric information present for annotations.",
            "reasoning": "There is no mention in the paper of any scoring rubrics or criteria applied during annotation, consistent with an automatic AI transcription process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No examples of annotation guidelines or instructions provided.",
            "reasoning": "The paper does not provide annotation examples or guidelines since transcriptions are from an automatic AI pipeline, not manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9299,
      "completion_tokens": 325,
      "total_tokens": 9624
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the annotations or dataset validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3 Automatic Transcription",
          "reasoning": "The transcription of the dataset was performed using the Whisper large-v3 ASR model, an AI model, to generate silver-standard transcriptions, effectively serving as quality assurance by an AI model."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Automatic Transcription",
          "reasoning": "The dataset processing included an automated pipeline (WhisperX) involving Voice Activity Detection (VAD) with pyannote, segmentation, transcription, and forced alignment; post-processing steps removed segments with hallucinations and inaccurate timestamps automatically, indicating quality assurance through automated verification and filtering."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described via AI models and automatic procedures; thus, it is not 'N/A'."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8909,
      "completion_tokens": 397,
      "total_tokens": 9306
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is collected from existing podcasts and not created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the speech transcriptions are generated by the Whisper large-v3 model, the audio data itself is not generated by a model but collected from podcast recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data done by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection and preprocessing",
          "reasoning": "The Greek Podcast Corpus is compiled by collecting existing podcast audio through a web crawler and podcatching tools, aggregating open-source audio data from multiple podcasts and episodes without significant modification of the audio content."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.3 Automatic Transcription",
          "reasoning": "The transcripts are obtained by applying the WhisperX pipeline with the Whisper large-v3 model to the collected podcast audio, thus the transcriptions are derived from existing audio data with modifications via automatic speech recognition processing and post-processing filtering."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data collection and transcription process; the origin of the data is clearly stated."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9427,
      "completion_tokens": 529,
      "total_tokens": 9956
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The GPC includes a large 3124-hour untranscribed pre-training corpus of podcast audio collected for Modern Greek, indicating it is used for pre-training large models on general speech patterns in an unsupervised or self-supervised fashion."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes fine-tuning pre-trained Whisper models rather than training models from scratch, so there is no indication that the dataset is used to train a model from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2, 3.3 and Section 4.1",
          "reasoning": "The authors use the transcribed subset of GPC (800 hours with pseudo-labels) to fine-tune pre-trained Whisper-small and Whisper-medium models in a supervised manner, demonstrating improvements in ASR performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning post-training techniques such as RLHF being applied using the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new GPC dataset is not used exclusively for evaluation; instead, it is used for fine-tuning and pre-training. The paper uses other established datasets exclusively for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The paper conducts thorough multi-domain and out-of-training analysis using the GPC test set to evaluate ASR performance across domains and data scales, indicating analysis use of the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the GPC corpus serves as a knowledge base for retrieval-augmented generation or similar augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There are documented practical usages of the GPC dataset in pre-training, fine-tuning, and analysis."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10150,
      "completion_tokens": 549,
      "total_tokens": 10699
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper introduces the Greek Podcast Corpus (GPC) which is exclusively for Modern Greek. Although the Whisper model used is multilingual, the dataset itself contains only Greek speech."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.3",
          "reasoning": "The dataset is primarily Modern Greek. Some segments with English code-switching were removed during post-processing, so the dataset does not include bilingual entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include any English-only content; it is focused on Modern Greek podcasts."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Sections 3.1 to 3.3",
          "reasoning": "The Greek Podcast Corpus is composed exclusively of Modern Greek speech collected from podcasts, covering 16 distinct domains. The authors explicitly state it is a corpus for Modern Greek and remove segments containing English code-switching."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of code or programming language content within the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is speech recordings with transcriptions; it does not contain mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech and does not include biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are included; only Modern Greek is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is well specified as Modern Greek."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly consists of human language content (Modern Greek speech), so it is not language free."
        }
      }
    }
  },
  {
    "id": "paraskevopoulos24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7368,
      "completion_tokens": 234,
      "total_tokens": 7602
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3",
          "reasoning": "The paper states in the abstract and in the contributions section (Section 3) that the trained model checkpoints and recipes to recreate the corpora and models are publicly available at the provided GitHub link (https://github.com/georgepar/greek_podcasts_asr). They also specify that they do not redistribute the original podcast audio data, but provide scripts for corpus reproduction, indicating that the code for data collection and preprocessing is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.1, 3.2, 3.3",
          "reasoning": "The paper provides a detailed description of the data collection process (Section 3.1), including the use of RSS feed crawling, audio downloading, format conversion, and VAD segmentation. They further document the process for building training, validation, and test splits stratified by domain (Section 3.2), and explain the automatic transcription and post-processing steps (Section 3.3). This detailed description constitutes thorough documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9189,
      "completion_tokens": 196,
      "total_tokens": 9385
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data-set and Table 1",
          "Reasoning": "The Gram Vaani ASR Challenge 2022 introduced a new dataset consisting of spontaneous telephone speech audio recordings collected from users across India using the Mobile Vaani platform. The dataset is explicitly described as audio (.mp3 files) with sampling rates ranging from 8kHz to 48kHz, and recordings were done in real conditions, implying human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data-set and Table 1",
          "Reasoning": "The labeled portion of the dataset contains human-generated transcriptions created by crowd workers recruited via the Uliza platform. These transcriptions are manually created text corresponding to the audio recordings, hence human-generated text data."
        }
      ]
    }
  },
  {
    "id": "patel22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9989,
      "completion_tokens": 176,
      "total_tokens": 10165
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.1",
            "reasoning": "The transcriptions were done by crowd workers recruited via the Uliza platform (Section 2.1), indicating multiple non-expert annotators were involved in annotating the data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not explicitly mention detailed annotation instructions provided to the crowd workers."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "There is no description or mention of scoring rubrics for annotators in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1",
            "reasoning": "The paper does not report any annotation examples provided to the annotators."
          }
        }
      ]
    }
  },
  {
    "id": "patel22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11179,
      "completion_tokens": 514,
      "total_tokens": 11693
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by any single human expert annotator. The transcriptions were done by crowd workers, and no mention is made of expert validation by a single expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by multiple human experts. The transcriptions were performed by crowd workers via the Uliza platform, and no information about expert annotators or expert validation is provided."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was performed by a single non-expert human annotator. The paper only mentions transcription by crowd workers but no QA by a single individual."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The transcriptions were done by crowd workers recruited via the Uliza platform (Section 2.1). Crowd workers are presumably non-experts performing manual transcription. This implies multiple human non-expert annotators contributed to the data labeling. No further QA processes by experts are described."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit description of AI models being used as judges for quality assurance of the dataset annotations. AI is used for ASR model training and pseudo-label generation but not for QA of the labeled dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1.1",
          "reasoning": "An automated process is used to estimate transcription quality by computing edit distances between manual transcripts and decoding output from an in-domain acoustic and biased language model (Section 3.1.1). This automated verification estimates the accuracy of the ground truth in the training data and identifies possibly erroneous transcriptions."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some QA processes are documented, including the use of crowd workers for transcription and automated verification of transcription accuracy. Thus, QA is present."
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10789,
      "completion_tokens": 406,
      "total_tokens": 11195
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 3.1.1",
          "reasoning": "The Gram Vaani ASR Challenge 2022 dataset consists of original spontaneous telephonic speech collected through the Mobile Vaani platform across India, including regional/dialectal variations of Hindi. The transcriptions were done by crowd workers via the Uliza platform, indicating that both audio and transcripts were created from scratch by human participants and annotators, not adapted or translated from existing sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any part of the dataset itself was generated purely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data generated by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any usage of machine translation to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was collected directly from human participants via the Mobile Vaani platform and not aggregated or collected from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is presented as original spontaneous speech data, not derived from existing datasets or modified versions thereof."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data collection and annotation process, so the origin is not unknown."
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11307,
      "completion_tokens": 316,
      "total_tokens": 11623
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, Section 3.1.2",
          "reasoning": "The Gram Vaani dataset labeled portion (100 hours train-set and 5 hours dev-set) is used to train End-to-End (E2E) ASR models from scratch without use of pre-trained models, as described in the analysis and experiments sections."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 3.3, Table 5",
          "reasoning": "The provided labeled and blind test sets are used for evaluation and benchmarking of ASR models, including model performance measurements on dev and blind sets in all challenge tracks."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1.1, Section 3.2.1, Figure 2, Figure 3",
          "reasoning": "The dataset is used to analyze the transcription quality, speaker variations, sampling frequencies, confidence of decoded utterances, and data characteristics to guide model training and data selection."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12030,
      "completion_tokens": 591,
      "total_tokens": 12621
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The dataset is described as consisting of spontaneous telephone speech in Hindi, including regional dialectal variations of Hindi, but does not mention inclusion of more than two distinct human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1",
          "reasoning": "The dataset is only described as containing Hindi speech; while it mentions removal of English text from transcripts for model training, there is no indication that the dataset entries themselves contain exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.1 and 3.1.2",
          "reasoning": "The dataset is explicitly Hindi speech data and not described as containing only English. English text is removed from transcripts, indicating the data is not English monolingual."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 and 3.1.2",
          "reasoning": "The dataset exclusively contains Hindi speech, including regional and dialectal variations of Hindi, without mention of other languages included in the entries. English text in transcripts is removed, indicating the dataset is monolingual non-English in Hindi."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset comprises speech and transcriptions only; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of spontaneous Hindi speech and corresponding text transcriptions; it does not include mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Dataset contains human speech only; no biological sequences or non-human communication data is mentioned."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset entries is clearly specified as Hindi."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain Hindi language speech and are therefore not language-free."
        }
      }
    }
  },
  {
    "id": "patel22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9248,
      "completion_tokens": 215,
      "total_tokens": 9463
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention in the paper",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories or mention that code used for data collection, preprocessing, or dataset construction is released. The dataset is referenced as provided by the Gram Vaani ASR Challenge 2022, but no code release related to dataset construction is indicated."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 'Details of the Data-set' and relevant discussions in Section 2 and 3",
          "reasoning": "The paper provides detailed descriptions of the dataset characteristics, including the origin, recording conditions, language characteristics, and segmentation information. Specifics such as duration, format, sampling rates, annotation method (crowd-sourced transcription via the Uliza platform), and presence of regional and speaker variations are documented. The paper also documents preprocessing steps such as voice activity detection segmentation for unlabeled data. These details constitute a reasonably transparent documentation of the dataset creation and characteristics."
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8057,
      "completion_tokens": 138,
      "total_tokens": 8195
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Sections 3.2 and 4.1",
          "Reasoning": "The new dataset consists of synthetic reverberant mixtures of speech created by convolving human-recorded clean speech (from the Libri-light dataset, which is human-generated audio) with simulated room impulse responses generated via an acoustic room simulator (model-generated). Thus, the audio data modality includes both human-generated (clean speech recordings) and model-generated (simulated reverberation and mixtures) components explicitly described in Sections 3.2 Acoustic Simulation and 4.1 Data Preparation."
        }
      ]
    }
  },
  {
    "id": "patterson22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8857,
      "completion_tokens": 222,
      "total_tokens": 9079
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2 Acoustic Simulation",
            "reasoning": "The paper states that the training data for distance-based sound separation is created using a synthetic acoustic room simulator (image-method with frequency-dependent wall filters), simulating reverberation impulse responses and generating mixtures synthetically rather than from human-annotated datasets. This simulation process is a deterministic and algorithmic method, not relying on human labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation instructions for human annotators are described; data is synthetically generated via simulation rather than manually annotated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics are discussed related to annotation because the data labels are generated automatically from simulation parameters and source positions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No annotation examples are provided given no human annotation process; dataset creation involves synthetic mixture generation with known source positions and distance-based grouping."
          }
        }
      ]
    }
  },
  {
    "id": "patterson22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10047,
      "completion_tokens": 411,
      "total_tokens": 10458
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple non-expert humans performed any quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that any AI models were used as judges to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic Simulation; Section 4.1 Data Preparation",
          "reasoning": "The dataset is synthetically generated using an automated acoustic room simulator with parameterized room properties and source locations, combined with a large speech dataset (Libri-light). This automated data generation process implicitly ensures consistent and controllable annotations regarding source distances. There is no mention of manual verification; hence, quality assurance relies on the correctness and reliability of the simulation and data preparation code."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the data generation process clearly with synthetic simulation tools, implying an automatic process for data generation and quality control."
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9657,
      "completion_tokens": 434,
      "total_tokens": 10091
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data created entirely from scratch by human contributors. Instead, it uses existing speech datasets (Libri-light and LibriSpeech) combined with simulated room impulse responses."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated by AI or machine learning models. Instead, data is constructed by convolving clean speech recordings with acoustically simulated impulse responses."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The work does not involve translation or any data produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated through machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Preparation",
          "reasoning": "The authors use existing speech recordings from the Libri-light dataset and LibriSpeech corpus, aggregating these sources without significant modification as audio content to be used in their experiments."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic Simulation and Section 4.1 Data Preparation",
          "reasoning": "The dataset used for training is derived by convolving the existing clean speech recordings with simulated room impulse responses generated by an acoustic room simulator. This processing modifies the original data to simulate reverberation and various distances, effectively transforming existing speech data into synthetic reverberant mixtures reflecting near and far sound sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and method of generation of the training data, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10175,
      "completion_tokens": 361,
      "total_tokens": 10536
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic Simulation, Section 4.1 Data Preparation, Section 5 Results",
          "reasoning": "The authors create a synthetic dataset using an acoustic room simulator with randomized microphone and source locations and reverberation effects to generate mixtures of near and far sounds. They use this synthetic data to train their neural network model from randomly initialized weights (Adam optimizer with training for one million steps), demonstrating usage in training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 Metrics, Section 5 Results",
          "reasoning": "The synthetic datasets generated are used to evaluate separation performance using SI-SDR improvement and noise reduction metrics on test sets, demonstrating the use of the dataset for performance benchmarking and evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1 Effect of distance threshold, Section 5.2 Effect of model size, Section 5.3 Effect of varied speaker count in evaluation set, Section 5.4 Effect of silent targets",
          "reasoning": "The authors analyze model performance trends across variations in distance thresholds, model size, speaker count, and presence of silent targets using their synthetic datasets, indicating use primarily for analyzing patterns and effects related to the task."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10898,
      "completion_tokens": 569,
      "total_tokens": 11467
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses speech recordings exclusively from the Libri-light dataset and LibriSpeech for validation and testing. There is no mention of multiple human languages; the speech data is English as implied by the use of LibriSpeech."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages. Only English speech is used, so bilingual is not applicable."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Data Preparation",
          "reasoning": "The training data is constructed using speech recordings from the Libri-light dataset with validation and test partitions from LibriSpeech. Both are well-known English speech corpora, indicating that the dataset contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses English speech recordings (Libri-light, LibriSpeech), so it is not a non-English monolingual dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are audio speech signals and simulated reverberant mixtures; there is no mention of programming or structured code-related content within the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Mathematical formulas appear in the paper to describe loss functions and signal processing methods but these are not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only human speech audio; there is no biological sequence or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No artificial or constructed languages are mentioned in the dataset; only English human speech is included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are explicitly documented as English, so the language is known and specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language human speech content; therefore it does contain language and is not N/A."
        }
      }
    }
  },
  {
    "id": "patterson22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8116,
      "completion_tokens": 174,
      "total_tokens": 8290
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section specifies code repository or access",
          "reasoning": "The paper does not provide any explicit links or references to publicly available code repositories for the dataset construction or data generation process. Although demos are available online, these pertain to model outputs rather than dataset or data construction code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.2 Acoustic Simulation and 4.1 Data Preparation",
          "reasoning": "The paper contains detailed documentation of the dataset construction and simulation process including the use of the acoustic room simulator, randomization of microphone and source locations, and mixing procedures. It describes the creation of near and far targets based on distance thresholds and the distributions of distances and source counts within the synthetic rooms, providing transparency into how the data was generated and processed for training and evaluation."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 2508,
      "completion_tokens": 120,
      "total_tokens": 2628
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Augmentation and Section 2.2 Augmentation with Machine Translation",
          "Reasoning": "The new dataset is generated by a data augmentation system that algorithmically converts written text into multiple variant spoken forms using rewrite rules. Additionally, neural machine translation (NMT) models translate the English spoken-written pairs into target languages. Both steps are automated, with no indication of manual human creation of these pairs, thus the data is model-generated text."
        }
      ]
    }
  },
  {
    "id": "paul22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 3308,
      "completion_tokens": 169,
      "total_tokens": 3477
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The new dataset is generated via a data augmentation system and neural machine translation pipeline, which are automated processes not involving human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "There is no mention of detailed annotation instructions provided, as the dataset creation is automated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit reference",
            "reasoning": "The paper does not mention any scoring rubrics used in the data augmentation or translation process."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 2 and Table 3",
            "reasoning": "Examples of generated spoken-written pairs via augmentation and machine translation are provided."
          }
        }
      ]
    }
  },
  {
    "id": "paul22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 4498,
      "completion_tokens": 315,
      "total_tokens": 4813
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the newly introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts were involved in quality assurance of the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of a single human non-expert performing quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although neural machine translation models are used for data generation, the paper does not describe using AI models for quality assurance of the dataset itself."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2. Data Driven ITN, specifically subsections 2.1 Data Augmentation and 2.2 Augmentation with Machine Translation",
          "reasoning": "The quality assurance process relies on automated filtering of generated spoken-written pairs where text normalization forms are not intact, and selection of NMT models with reasonable BLEU scores for translation quality. This is an automatic, algorithmic verification process rather than manual QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is evidence of an automatic verification/filtering process as quality assurance, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 4108,
      "completion_tokens": 444,
      "total_tokens": 4552
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of spoken-written pairs entirely from scratch by human contributors. The data augmentation and text normalization system is automated and no explicit human-generated novel data is described."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "Section 2.1 describes a data augmentation method that generates diversified spoken forms for written text using an automated process involving rewrite rules and a data augmentation system. This produces new spoken-written pairs synthetically and is generated by a model or automated system rather than humans. Hence, this is new data generated by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of human translations being used or the target data being acquired via human translators."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "Section 2.2 details the use of neural machine translation models to translate English spoken-written pairs generated by the augmentation system into other target languages. This process generates new spoken-written pairs in other languages via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as directly collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The data augmentation system derives new spoken forms by applying rewrite rules to existing written text and converting them into multiple spoken variations. This represents data derived from existing written texts with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation methodology is explicitly described, thus not unknown or undocumented."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 4626,
      "completion_tokens": 410,
      "total_tokens": 5036
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new dataset for pre-training large models typically in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state that the models are trained from scratch on the new dataset without pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The paper describes training ITN models with synthetic spoken-written pairs generated by their data augmentation and machine translation pipeline to improve inverse text normalization performance, which involves supervised fine-tuning of models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of reinforcement learning post-training methods using the new dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is used primarily for training, not exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the new dataset primarily for analysis of trends or patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not serve as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The usage of the new dataset is clearly described in the paper for supervised fine-tuning; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 5349,
      "completion_tokens": 487,
      "total_tokens": 5836
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.2 Augmentation with Machine Translation",
          "reasoning": "The new dataset introduced is synthesized via neural machine translation from English spoken-written text pairs to multiple target languages, explicitly including French, Italian, and Spanish as shown in experimental results. Hence, it contains entries with more than two human languages, qualifying it as multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Augmentation",
          "reasoning": "The initial synthetic spoken-written text pair dataset generated from English text normalization augmentation contains only English language content before the machine translation step."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains spoken-written text pairs, not programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset includes numerical and symbolic expressions in text form (e.g., '20:20'), it does not explicitly contain formal mathematical notation or symbolic logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly documented and identified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain human natural language text."
        }
      }
    }
  },
  {
    "id": "paul22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 2567,
      "completion_tokens": 146,
      "total_tokens": 2713
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any link to publicly available code repositories or mention that the code used for the data augmentation system or the machine translation data generation is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 and Section 2.2",
          "reasoning": "The paper documents the dataset creation process in detail, including the data augmentation pipeline that generates diverse spoken forms from written text, and the machine translation approach to produce spoken-written pairs in target languages. Steps such as extraction, reformatting, augmentation with rewrite rules, and filtering of translated pairs are described, providing transparency about the dataset creation method."
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7681,
      "completion_tokens": 131,
      "total_tokens": 7812
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3",
          "Reasoning": "The SIAEW Corpus consists of Spanish-accented English word tokens created by recording bilingual speakers producing words, from which accented-to-native continua were generated by splicing segments and applying acoustic weighting as per a computational procedure. Hence, the audio data was originally human speech recordings (human generated) subsequently manipulated algorithmically to generate continuums with graded accentedness (model generated). The paper describes selection of speakers, recordings, and the method to generate accented tokens, confirming the mixed origin."
        }
      ]
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8481,
      "completion_tokens": 208,
      "total_tokens": 8689
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3",
            "reasoning": "The nativeness categorisation task to obtain accentedness judgements for the SIAEW Corpus was performed by a cohort of 17 native English listeners recruited from the student population, who were na\u00efve non-expert participants regarding phonetic annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "Participants performed a two-alternative forced choice task categorizing tokens as native or foreign-accented, indicating task instructions were given to guide this annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "The judgement task was binary (native vs foreign-accented), constituting a clear rubric for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not mention providing annotation examples or practice trials to the listener cohort for the nativeness categorisation task."
          }
        }
      ]
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9671,
      "completion_tokens": 441,
      "total_tokens": 10112
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human annotator who is a subject matter expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance performed by multiple human annotators with subject matter expertise or belonging to the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that a single non-expert human annotator performed quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "Quality assurance of the SIAEW Corpus involved 17 native English listeners, recruited from the student population at Anglia Ruskin University, who were non-expert annotators without knowledge of Spanish beyond A1 level. They performed a large-scale native/non-native categorisation task to validate the accentedness judgements for the continua. Since multiple non-expert human listeners carried out these annotations, this label applies."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe use of an AI model to perform quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automated methods were used to generate the continua and fit logistic functions, these are part of the dataset creation method rather than quality assurance of annotations or content. The QA process per se involved human listeners."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented involving multiple human non-expert listeners performing perceptual categorisation judgements to validate the dataset."
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9281,
      "completion_tokens": 449,
      "total_tokens": 9730
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The SIAEW Corpus was created by recording bilingual speakers producing English words and Spanish non-words, specifically recruited and evaluated for minimal accent. The recordings are original speech data produced entirely by human speakers from scratch. The paper details the recording process and speaker evaluation, indicating these are original human-generated speech stimuli, not translations or adaptations of pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated entirely by AI or machine learning models. Although the paper uses computational techniques such as acoustic manipulation and blending derived from existing recordings, the original data are human speech recordings, not model-generated data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was created via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of machine translation being used to create the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources; it was newly recorded from human speakers."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 and Section 3",
          "reasoning": "The original human recordings are manipulated via acoustic splicing and weighting methods to create continua of accented speech tokens with graded levels of accentedness. This involves transforming and generating new stimuli that are adaptations based on existing human recordings, thus making the final dataset 'derived' due to these modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data generation methodology; thus, this label does not apply."
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9799,
      "completion_tokens": 316,
      "total_tokens": 10115
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The SIAEW Corpus is used to evaluate the perception of segmental foreign accent by listener cohorts with different L1 backgrounds, as shown in the native listener categorization task detailed in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The dataset enables analysis of trends and differences in accent perception across listener groups differing in phonological inventory and familiarity with the accented language, as discussed in Section 4."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10522,
      "completion_tokens": 491,
      "total_tokens": 11013
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset, called SIAEW Corpus, contains English words produced with Spanish-accented English sounds, involving only two human languages: English and Spanish; thus it is bilingual but not multilingual (more than two languages)."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The SIAEW Corpus contains lexical tokens of English words produced with graded Spanish-accented English segments. The data involves precisely two human languages: English (native language of the words) and Spanish (accent language) as evidenced throughout Section 3."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "Although the words are English, the collected speech tokens contain Spanish-accented English segments, representing accentedness from a second language (Spanish). Therefore, the dataset is not monolingual English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The dataset does not contain only non-English language entries; it contains English words pronounced with Spanish-accented features, maintaining the English lexical content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists entirely of speech recordings and no programming code or structured code data is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of speech tokens and contains no mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech data only and does not include any biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset involves fictional or constructed languages; it involves natural human languages English and Spanish."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages involved in the dataset are clearly documented as English and Spanish accented English; thus language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of speech recordings of human languages (English and Spanish-accented English); therefore, it includes language."
        }
      }
    }
  },
  {
    "id": "perezramon22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7740,
      "completion_tokens": 195,
      "total_tokens": 7935
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3",
          "reasoning": "The paper provides a download link for the SIAEW Corpus dataset itself (https://doi.org/10.5281/zenodo.6371655) but does not mention or provide any code repository or link for the code used to generate the dataset or any data processing scripts."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The dataset creation process is documented in detail across sections 2 and 3. Section 2 describes the methodology for generating iso-perceptual steps from acoustic continua using logistic fitting and inversion. Section 3 details the selection of target consonants and vowels, the recording procedure with bilingual speakers, the generation of continua, native listener experiments for perceptual testing, and selection of speakers for the final corpus. The paper thoroughly explains the procedures used, enabling understanding of dataset construction."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8319,
      "completion_tokens": 183,
      "total_tokens": 8502
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2",
          "Reasoning": "The new dataset introduced in this paper is the Chilean Spanish AD corpus recorded by Universidad de Chile and Hospital del Salvador (Chile), consisting of semi-spontaneous speech recordings from Chilean Spanish speakers describing the \"cookie theft picture\". These audio recordings were manually segmented and transcribed, indicating human involvement in recording and creation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2",
          "Reasoning": "The Chilean Spanish AD dataset includes manual transcriptions (text) of the semi-spontaneous speech recordings. These transcriptions were created by humans as they were manually segmented and transcribed, demonstrating human generation of text data."
        }
      ]
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9119,
      "completion_tokens": 245,
      "total_tokens": 9364
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1.2 Chilean Spanish AD",
            "reasoning": "The Chilean Spanish AD dataset was recorded by the Universidad de Chile and Hospital del Salvador in Chile and the participants were evaluated by an expert neurologist following established clinical criteria for Alzheimer's Disease. The recordings were manually segmented and transcribed, implying multiple human experts were involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1.2 Chilean Spanish AD",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions provided for the manual segmentation or transcription tasks in the Chilean Spanish AD dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1.2 Chilean Spanish AD",
            "reasoning": "No mention of scoring rubrics or formal scoring procedures was made regarding the annotation or evaluation of the Chilean Spanish AD dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1.2 Chilean Spanish AD",
            "reasoning": "The paper does not provide or mention any annotation examples for manual segmentation or transcription for the Chilean Spanish AD dataset."
          }
        }
      ]
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10309,
      "completion_tokens": 194,
      "total_tokens": 10503
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1.2 Chilean Spanish AD",
          "reasoning": "The Spanish AD dataset was evaluated by an expert neurologist following clinical criteria for Alzheimer's Disease, indicating quality assurance by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or algorithmic QA methods for dataset validation are described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly notes expert clinical evaluation for Spanish data; thus, QA is documented."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9919,
      "completion_tokens": 412,
      "total_tokens": 10331
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1.2 Chilean Spanish AD",
          "reasoning": "The paper describes a new Spanish Alzheimer's Disease dataset composed of speech recordings and manual transcriptions from 39 Chilean Spanish speakers. This dataset was recorded by the Universidad de Chile and Hospital del Salvador (Chile) and is presented as a newly created data collection by human participants, not derived from or translated from other datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data in the paper is described as generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description in the paper that any dataset content was produced by translating material from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention usage of machine translation to create data in any dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are either original collections or publicly available corpora; the paper does not describe assembling data from multiple existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset is described as being transformed or adapted from existing datasets; the Spanish dataset was newly recorded."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origins of the datasets used, especially for the Spanish dataset; therefore, the data source is specified."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10437,
      "completion_tokens": 376,
      "total_tokens": 10813
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper uses the new Chilean Spanish AD dataset to fine-tune pre-trained embeddings for Alzheimer's Disease classification through supervised learning methods. Section 3.1 describes classification experiments with the Spanish dataset alone, and Section 3.2 reports transfer learning approaches where English pre-trained embeddings are fine-tuned for Spanish data, demonstrating use for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The Chilean Spanish dataset is also used as an evaluation benchmark to assess model performance for Alzheimer's detection. Classification performance (F-score) is reported for this dataset alone and in combination with English data, evaluating the impact of transfer learning."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4",
          "reasoning": "The dataset is used for analyzing language dependencies and differences between linguistic and acoustic embeddings for AD detection in Spanish speech, as shown in Sections 3.1 and 4. The paper discusses patterns, such as stronger acoustic importance in Spanish and language differences influencing embeddings."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The Chilean Spanish AD dataset introduced by the authors is directly used in supervised fine-tuning, evaluation, and analysis as detailed in multiple sections of the paper."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11160,
      "completion_tokens": 611,
      "total_tokens": 11771
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 (Data)",
          "reasoning": "The two datasets introduced are separate: one comprises English speakers (Pitt Corpus) and the other comprises Spanish speakers (Chilean Spanish AD corpus). There is no indication that any single dataset entry contains multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.1 (Data)",
          "reasoning": "Neither dataset contains entries with exactly two languages. The English dataset contains English only, and the Spanish dataset contains Spanish only. The study uses both languages but not combined within same data entries."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (The Pitt Corpus)",
          "reasoning": "The Pitt Corpus subset used in this study consists exclusively of native American English speakers describing a picture. Thus, the dataset entries contain English speech and transcripts only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1.2 (Chilean Spanish AD)",
          "reasoning": "The Chilean Spanish AD corpus consists exclusively of native Spanish speakers from Chile describing the same picture. The speech and transcripts are in Spanish only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets introduced are composed of human speech and transcripts, not containing any code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets consist of audio speech recordings and text transcripts describing a picture; no entries contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not include biological sequences or non-human communication; the content is human natural language speech."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication or mention of any fictional or constructed language data in the introduced datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The languages of the datasets are clearly documented and explicitly described as English (Pitt Corpus) and Spanish (Chilean Spanish AD)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human speech and corresponding transcripts, so they contain language data."
        }
      }
    }
  },
  {
    "id": "pereztoro22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8378,
      "completion_tokens": 190,
      "total_tokens": 8568
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper suggests the availability of code for dataset construction.",
          "reasoning": "The paper does not provide any direct link, repository, or indication that the code related to the construction, collection, or preprocessing of the new datasets (specifically the Chilean Spanish AD corpus) is publicly available. The datasets are described, but no code is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1.2 (Chilean Spanish AD)",
          "reasoning": "The paper provides detailed information on the Chilean Spanish AD dataset: number of subjects, demographic details, speech task description, clinical evaluation criteria, recording and transcription process, and average duration of recordings. Although the dataset is previously reported in [18], the key aspects relevant to dataset creation and characteristics are documented within the paper to enable understanding and reproducibility to some extent."
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6906,
      "completion_tokens": 353,
      "total_tokens": 7259
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 4",
          "Reasoning": "The dataset includes audio recordings of speech from 79 participants captured during stress induction tasks, recorded using human participants speaking and recorded by audio devices."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 4",
          "Reasoning": "The dataset includes video recordings (face and multiple postures) of 79 participants performing tasks, captured by cameras operated and recording the humans during the experiment."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 4 and Section 2.5",
          "Reasoning": "Physiological data streams were captured from sensors worn by participants, including electrodermal activity, skin temperature, acceleration, ECG and heart activity from Empatica E4 and Faros 180 devices, reflecting human physiological signals during tasks."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 4",
          "Reasoning": "Psychological questionnaires self-reported by participants, and performance results (Rebus performance) are provided as tabular data reflecting manually collected human-generated responses and measurements."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.5 and overall protocol sections",
          "Reasoning": "Semantic segmentation labels were manually annotated using the BESS-TiANNO tool by humans, creating structured text labels contextualizing portions of the session."
        }
      ]
    }
  },
  {
    "id": "pesan24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7706,
      "completion_tokens": 297,
      "total_tokens": 8003
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.5",
            "reasoning": "Section 4.5 states that manual semantic labeling of segments was carried out using the BESS-TiANNO tool, indicating that humans performed the annotation. The term 'manual' and 'expert ECG verification' in Section 4.1 suggest expert-level involvement for accurate annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 and 2.5",
            "reasoning": "Section 2.2 describes detailed protocols and procedures for the stress induction tasks, implying detailed instructions were provided to annotators for task labeling and data collection. Section 2.5 details the data modalities recorded, indicating structured guidelines for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1.2 and 4.5",
            "reasoning": "Section 3.1.2 explains that the Physical Stress subset adopted labeling methodology from the Munich Biovoice Corpus, which implies use of standardized labeling schemes (rubrics). Section 4.5's semantic labeling suggests categorization was based on defined segment types (e.g., Table 1 and 2), consistent with rubric usage."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not explicitly mention providing annotation examples or example annotations in the guidelines or appendices for semantic labels or stress load levels."
          }
        }
      ]
    }
  },
  {
    "id": "pesan24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8896,
      "completion_tokens": 450,
      "total_tokens": 9346
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4.1 Validation Process for Each Modality",
          "reasoning": "For the ECG signal preprocessing, a robust QRS complex detection algorithm was used followed by manual verification by an ECG expert to ensure precise QRS complex identification and accurate correction of detected positions. This demonstrates quality assurance by a single human expert in the ECG modality."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description indicating quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description indicating multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While an AI model was used for voice activity detection (VAD), the paper does not describe its use as a judge for quality assurance of annotations or dataset content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 Validation Process for Each Modality; Section 4.4 Speech recognition system",
          "reasoning": "Automated validation and preprocessing steps were employed across multiple modalities, including audio volume normalization, noise and artifact removal in physiological signals, and cross-correlation techniques for synchronization. Additionally, VAD and ASR systems based on neural network and hybrid DNN-HMM models were used, indicating automated processes in quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides detailed descriptions of quality assurance steps including expert manual verification and automated processing, so QA is documented and applied."
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8506,
      "completion_tokens": 358,
      "total_tokens": 8864
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.2, and 3",
          "reasoning": "The BESST dataset was created by collecting new experimental data from human participants performing stress-inducing tasks, such as Hand Immersion Task and Reading Span Task. The participants were native Czech speakers and the data, including audio, video, physiological signals, and psychological assessments, were recorded specifically for this dataset as described in the protocol sections and dataset description. This indicates original content generated entirely from scratch by human subjects."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "Derived data streams such as Voice Activity Detection segmentations, semantic segmentations, and RR interval calculations were produced by applying post-processing and transformations to native recordings (audio, physiological data). These processed data representations are based on the original recorded data but transformed non-trivially to enhance usability."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9024,
      "completion_tokens": 248,
      "total_tokens": 9272
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 and Section 6",
          "reasoning": "The BESST dataset is introduced as a resource to facilitate research and analysis on the interplay between stress, cognitive load, and speech. The paper discusses the dataset's design, collection protocols, and its role in enabling investigations into stress-induced speech variations, cognitive load effects on speech, and multimodal interaction rather than describing direct use for training or evaluation of machine learning models. Thus, the dataset is primarily intended for analyzing patterns and relationships in multimodal stress and speech data."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9747,
      "completion_tokens": 427,
      "total_tokens": 10174
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "The dataset exclusively contains speech data recorded from native Czech speakers, with all content in the Czech language only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "No indication of data collected in exactly two languages; the dataset is solely in Czech."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "The dataset content is not in English; it is exclusively Czech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "The dataset consists solely of Czech speech data, confirmed as native Czech speakers only. This indicates the dataset is monolingual and non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No programming or structured code-related content is part of the dataset; all data pertains to speech, physiological, and video signals."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No mathematical or logical notation is present as dataset entries; only physiological signals and speech data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "Though physiological signals like ECG and skin conductance are included, the data does not include biological sequences or non-human communication systems as dataset entries."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Section 2.6 and overall dataset description",
          "reasoning": "No mention or indication of artificial or constructed languages present in the dataset; only natural Czech language is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 2.6 Language specifics",
          "reasoning": "The language of the dataset is explicitly specified as Czech."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech data, thus it does contain language."
        }
      }
    }
  },
  {
    "id": "pesan24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6965,
      "completion_tokens": 203,
      "total_tokens": 7168
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 5",
          "reasoning": "The paper explicitly states in Section 5 that the BESST dataset, along with relevant code and tools, is accessible for research and development at https://speech.fit.vutbr.cz/besst. This indicates that code related to the dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2, 3, and 4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including participant recruitment and criteria (Section 2.1), stress induction protocols (Section 2.2), session procedures (Section 2.3), psychological measurements (Section 2.4), data streams and devices used (Section 2.5, Table 3), dataset structure and subsets (Section 3 and 3.1), and validation and preprocessing steps of all modalities (Section 4). This extensive documentation covers the dataset creation comprehensively."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6959,
      "completion_tokens": 419,
      "total_tokens": 7378
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 and 3",
          "Reasoning": "The dataset comprises over 87,000 utterances spoken by 1,000 Vietnamese celebrities, collected from real-world video sources on YouTube and TikTok (Section 2.4 and 3). These audio utterances capture spontaneous speech under noisy conditions, and are thus human generated speech recordings from natural conditions, not synthesized or generated by any model."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 and 2.5",
          "Reasoning": "The dataset construction pipeline involves crawling videos from YouTube and TikTok channels of Vietnamese celebrities as the raw data source. These videos are human-generated content (interviews, podcasts, shows) recorded by humans and posted online. The authors applied visual-aided processing techniques such as face tracking, face verification, and active speaker verification to process the videos, confirming that the videos themselves originate from human-generated content online."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "During the dataset construction, images were crawled from Google Images and TikTok profile pictures that depict the faces of persons of interest (POIs). These images are photographs manually captured by humans and publicly available online, hence their origin is human generated. They are used for face verification steps in the pipeline."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.6; Table 3 and 4",
          "Reasoning": "The dataset includes annotated labels for each speaker, specifically gender and dialect information, which were manually assigned by human annotators through majority voting on sampled utterances. Therefore, the tabular data representing speaker metadata (gender, dialect, speaker ID) is human-generated annotation data included in the dataset."
        }
      ]
    }
  },
  {
    "id": "pham23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7759,
      "completion_tokens": 221,
      "total_tokens": 7980
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.6",
            "reasoning": "Section 2.6 states that three annotators, each from different Vietnamese regions (Northern, Central, Southern), manually annotated speaker genders and dialects for each speaker by majority voting on 5 utterances, indicating multiple human non-expert annotators for labeling."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.6",
            "reasoning": "The paper mentions annotators from different regions to correctly classify dialects, implying protocol or instructions were given to ensure accurate annotation from regional perspectives."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.6",
            "reasoning": "There is no mention of explicit scoring rubrics or detailed criteria for annotation provided in the paper for the gender or dialect annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.6",
            "reasoning": "The paper does not provide or mention any examples used in the annotation guidelines for gender or dialect labeling."
          }
        }
      ]
    }
  },
  {
    "id": "pham23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8949,
      "completion_tokens": 486,
      "total_tokens": 9435
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert or annotator for the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.6",
          "reasoning": "The annotation of speaker genders and Vietnamese dialects was conducted by three annotators who come from the three different Vietnamese regions (Northern, Central, Southern), implying they are knowledgeable about the dialects. The annotation employed majority voting, indicating multiple expert annotators performed QA for gender and dialect labels."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies that annotators come from relevant regions to ensure correct dialect classification, indicating expertise; thus, multiple non-expert annotators were not involved."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.5",
          "reasoning": "The data cleaning involved using an automatic speech representation model (Wav2Vec2) pretrained on Vietnamese data to detect duplicated utterances and to compute similarity scores for removing invalid speakers and noisy utterances, indicating the use of AI models for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.5",
          "reasoning": "Several automatic, rule-based or algorithmic filtering processes were applied: facial detection using RetinaFace, face embedding extraction with ArcFace, k-means clustering to filter images, outlier detection using interquartile range (IQR) on cosine similarity scores, and merging speakers based on audio and visual similarity thresholds, reflecting rigorous automated QA methodologies."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes multiple quality assurance processes, both human and automated, so QA is present."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8559,
      "completion_tokens": 458,
      "total_tokens": 9017
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by human contributors; instead, it is collected from existing media sources such as YouTube and TikTok."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated or synthesized by AI or machine learning models; it is collected from real-world videos of speakers."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involves translating content from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset involves translating content using machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset collection & construction), specifically 2.4 Video crawling and 2.3 Image crawling and processing",
          "reasoning": "The dataset is collected from existing online media sources (YouTube, TikTok, Google Images) without creation of new content. It involves crawling and downloading pre-existing videos, images, and audio of Vietnamese speakers."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.5 Audio-visual processing and Section 2.6 Annotating speaker genders and Vietnamese dialects",
          "reasoning": "The dataset is based on existing audio-visual data but includes significant processing and transformations such as removing duplicates, face detection, clustering, filtering, active speaker verification, merging duplicated speakers, and manual annotation of gender and dialect labels. These processing steps transform the raw collected data into a refined, annotated dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset's data sources and construction methods are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9077,
      "completion_tokens": 496,
      "total_tokens": 9573
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using Vietnam-Celeb for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Experimental setup and Section 4.3 Experimental results",
          "reasoning": "The dataset (Vietnam-Celeb-T) is used to train ECAPA-TDNN models from scratch. The paper details training procedures and reports results comparing models trained from scratch on Vietnam-Celeb-T versus other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 Experimental setup and Section 4.3 Experimental results",
          "reasoning": "The dataset is used for supervised fine-tuning of pre-trained VoxCeleb models, showing that fine-tuning with Vietnam-Celeb-T improves performance on Vietnamese speaker verification tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning based post-training methods using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.4 Final dataset and Section 4.3 Experimental results",
          "reasoning": "The dataset is split into training and test subsets (Vietnam-Celeb-E and Vietnam-Celeb-H) used for benchmarking and performance evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is used primarily for analysis of trends or patterns separate from training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval-augmented generation or similar tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has clear documented uses in training (from scratch), supervised fine-tuning, and evaluation."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9800,
      "completion_tokens": 584,
      "total_tokens": 10384
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 and Table 1",
          "reasoning": "The Vietnam-Celeb dataset contains only Vietnamese language utterances, as explicitly stated in Section 3 and Table 1; there is no mention of other languages included. The paper mentions the dataset is specifically for Vietnamese speaker recognition."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 and Table 1",
          "reasoning": "The dataset contains only Vietnamese language speaker utterances. There is no indication or mention of it containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3 and Table 1",
          "reasoning": "The dataset contains Vietnamese speech only, not English, so it is not monolingual (English)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 3; Table 1",
          "reasoning": "The dataset exclusively contains spoken Vietnamese language data. The entire dataset is described as a Vietnamese speaker recognition dataset, with no mention of any other languages. Hence, it is monolingual (non-English) with the language being Vietnamese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper includes mathematical formulas for processing steps, the dataset itself does not contain mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech data only; there is no biological or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes only natural Vietnamese language; there is no indication of any constructed or fictional language."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly documented as Vietnamese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data (Vietnamese), so it is not applicable."
        }
      }
    }
  },
  {
    "id": "pham23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7018,
      "completion_tokens": 204,
      "total_tokens": 7222
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 and Conclusion",
          "reasoning": "The paper mentions that Vietnam-Celeb is publicly available under a GitHub repository; however, it does not provide any link or explicit mention of the code repository for the dataset construction pipeline or related scripts. The paper describes the data collection and preprocessing pipeline in detail, but does not indicate that the code for these processes is publicly released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 2 (Dataset collection & construction) and Section 3 (The Vietnam-Celeb dataset). It explains each stage of the data collection, preprocessing, filtering, and annotation pipeline, including the visual-aided techniques used (face detection, verification, active speaker verification), the handling of duplicates, and gender and dialect annotation. This detailed documentation supports reproducibility and transparency of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9262,
      "completion_tokens": 299,
      "total_tokens": 9561
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, paragraph 1; Table 1",
          "Reasoning": "The datasets m-MUSIC-AVQA and m-AVQA are built upon the existing benchmark AVQA datasets MUSIC-AVQA and AVQA which contain videos as one modality. These videos were originally recorded and provided as part of these datasets, indicating human involvement in the acquisition of the video data."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, paragraph 1; Table 1",
          "Reasoning": "The audio modality in the datasets originates from the original benchmark datasets MUSIC-AVQA and AVQA, which contain natural audio recordings corresponding to the videos. The audio thus comes from human-recorded real-world events captured in the videos, not generated by models."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3, paragraphs 2 and 3",
          "Reasoning": "The question-answer pairs in multiple languages were created by machine-translating the original question-answer pairs from the English datasets MUSIC-AVQA and AVQA using Google's machine translation API, an algorithmic system, followed by human verification and refinement. Hence, the textual QA data in various languages is model-generated (machine translation) with subsequent human curation."
        }
      ]
    }
  },
  {
    "id": "phukan24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10062,
      "completion_tokens": 198,
      "total_tokens": 10260
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3, paragraph describing the creation of multilingual datasets",
            "reasoning": "The paper states that after machine translation of question-answer pairs into seven languages using Google's API, human verification and refinement were conducted on these translated question-answer pairs for accuracy. This implies multiple human experts were involved in the verification and refinement annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention providing detailed annotation instructions or guidelines for the human annotators performing verification and refinement."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No mention of scoring rubrics or formal evaluation criteria for the annotation/verification process is present in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe providing examples or sample annotations for the annotators in their guideline."
          }
        }
      ]
    }
  },
  {
    "id": "phukan24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11252,
      "completion_tokens": 387,
      "total_tokens": 11639
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert or member of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple human experts performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3, paragraph 3",
          "reasoning": "The paper states that after machine translation of questions and answers, human verification and refinement were conducted to ensure accuracy. Although the expertise level of these humans is not explicitly stated, they appear to be non-expert annotators performing verification to ensure quality."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using an AI model as a judge for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based quality assurance methods applied to dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly mentions a human verification and refinement step after machine translation for dataset quality, so it is not the case that no QA is performed or documented."
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10862,
      "completion_tokens": 471,
      "total_tokens": 11333
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any new data was created entirely from scratch by human contributors. The new datasets are derived from existing datasets and involve translation rather than original human annotation."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets are not generated by AI or machine learning models creating new content from scratch. Instead, they leverage machine translation applied to existing datasets."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "Section 3, paragraphs 2 and 3",
          "reasoning": "Although the paper mentions human verification and refinement of machine-translated question-answer pairs, the primary translation to create the datasets was performed by machine translation (Google Translate). There is no indication that the translations were entirely produced by human translators originally."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 3, paragraphs 2 and 3",
          "reasoning": "The multilingual datasets m-MUSIC-AVQA and m-AVQA were created by translating existing English question-answer pairs from benchmark AVQA datasets into seven additional languages using Google's machine translation API, as explicitly stated in Section 3."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the datasets are based on existing sources (MUSIC-AVQA and AVQA), the paper specifies translation to produce new multilingual datasets rather than simple aggregation without modifications."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3, paragraphs 2 and 3",
          "reasoning": "The multilingual datasets are created by transforming existing English datasets via translation into new languages, which constitutes deriving new data from existing sources with adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly described as machine translation from existing datasets; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11380,
      "completion_tokens": 311,
      "total_tokens": 11691
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Methodology) and Section 5 (Experimental Results)",
          "reasoning": "The multilingual AVQA datasets (m-MUSIC-AVQA and m-AVQA) are used to fine-tune and train downstream models (MERA suite models) for supervised learning to answer questions in multiple languages, as described in Sections 4 and 5."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Experimental Results)",
          "reasoning": "The datasets are used for evaluation to benchmark model performance across languages and question types, as presented in Table 3 and related discussions."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Multilingual AVQA Datasets), Table 2 and Section 5 (Experimental Results)",
          "reasoning": "The authors analyze question types and answer distributions (Table 2) and investigate model performance trends across languages and question types (Section 5). Hence, the datasets support analysis of patterns and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12103,
      "completion_tokens": 520,
      "total_tokens": 12623
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3, Introduction, and Abstract",
          "reasoning": "The paper introduces two new multilingual AVQA datasets: m-MUSIC-AVQA and m-AVQA. These datasets are created by translating existing English AVQA datasets into seven additional languages: French, Hindi, German, Spanish, Italian, Dutch, and Portuguese, along with the original English. Hence, the datasets contain entries in eight human languages, making them multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain entries for more than two languages (eight languages), so the bilingual label does not apply."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors explicitly extend the dataset to multiple languages and do not present a new monolingual English-only dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are available in multiple languages, not restricted to a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of natural language questions and answers but no mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are included in the datasets."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain entries in any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included are explicitly stated and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language entries in multiple human languages."
        }
      }
    }
  },
  {
    "id": "phukan24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9321,
      "completion_tokens": 168,
      "total_tokens": 9489
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 3",
          "reasoning": "The paper explicitly provides a GitHub link (https://github.com/swarupbehera/mAVQA) for both the dataset and code. This indicates the code related to creating the multilingual AVQA datasets from existing datasets (including translation steps) is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Multilingual AVQA datasets)",
          "reasoning": "The paper clearly documents the dataset creation process, including the source datasets (MUSIC-AVQA and AVQA), the use of Google's machine translation API to translate questions and answers into seven additional languages, and mentions human verification and refinement of translations. Statistical analyses and detailed descriptions are also included, providing transparency about the dataset creation."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7431,
      "completion_tokens": 160,
      "total_tokens": 7591
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3",
          "Reasoning": "The authors introduced newly extracted additional audio data from the original MALACH speech corpus, increasing training and test data by 18 hours and 3 hours respectively, along with 150 hours of unlabeled audio. This is human generated data recorded from Holocaust survivor interviews using microphones."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3",
          "Reasoning": "Manual transcriptions (text) of the additional data were processed and segmented, including annotations such as speaker turns and overlaps. These transcriptions were manually created by human transcribers associated with the original dataset."
        }
      ]
    }
  },
  {
    "id": "picheny23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8231,
      "completion_tokens": 217,
      "total_tokens": 8448
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3",
            "reasoning": "Section 3 describes that the original MALACH data included manual transcriptions with manually annotated speaker turns and overlaps. The transcriptions and manual annotations imply the involvement of multiple human experts to produce the data and annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators for the transcription or segmentation work; only the process and challenges are described without explicit instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "No mention or description of scoring rubrics or formal criteria for evaluating annotation quality is provided in the paper for the new data annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not discuss providing annotation examples for the annotators in the transcription or segmentation process of the new data; only a general description of the annotation approach is given."
          }
        }
      ]
    }
  },
  {
    "id": "picheny23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9421,
      "completion_tokens": 454,
      "total_tokens": 9875
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was done by a single human expert. No details about individual expert annotators are given."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple expert annotators performing quality assurance. There is no description of experts performing validation or corrections on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided of quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3: New Data Extraction",
          "reasoning": "The paper describes that speaker turns and transcription segments were manually annotated, and that transcription involved handling numerous named entities which presented challenges. The large volume of manual annotation suggests multiple annotators conducted the work, likely non-experts as the paper does not mention specific expert transcribers or subject matter experts performing quality assurance. Therefore, this corresponds to multiple human non-experts performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses AI models for recognition and pseudo-label generation, it does not describe AI models performing quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes use of an automated voice activity detection (VAD) to segment unlabeled audio data, but this is a data preprocessing step rather than quality assurance on annotations. No algorithmic or rule-based verification of the manual transcriptions is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents manual transcription and annotation processes, indicating some quality assurance was performed."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9031,
      "completion_tokens": 529,
      "total_tokens": 9560
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3: New Data Extraction",
          "reasoning": "The paper describes extracting and processing approximately 18 hours of additional transcribed data and 3 hours of test data from the original MALACH audio tapes. These manual transcriptions were created by human annotators working with the recordings and XML files, including manual annotation of speaker turns and transcriptions with special prefix notation for named entities. This constitutes new human-created data beyond the previous release."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 5: Use of Pseudo-labeled Data",
          "reasoning": "The authors generate pseudo-labels for previously untranscribed audio data by using a fine-tuned speech recognition model to automatically transcribe unlabeled data. These pseudo-labels are thus newly generated data by a model and then used to augment training data. This process creates new machine-generated transcriptions from unlabeled audio."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3: New Data Extraction",
          "reasoning": "The newly extracted data comes from the existing large MALACH archive of recordings and transcripts. The authors processed and aggregated this existing data (including unlabelled speech and partially transcribed data) to produce new training and test sets for speech recognition experiments. This reuse and aggregation of existing archival data is characteristic of collated data."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the data was processed and segmented, the paper does not describe the new datasets as transformations or adaptations derived from existing data beyond aggregation and transcription. The pseudo-labels are new model-generated data rather than derived in the sense of modification of existing text data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins and generation methods of the new datasets introduced in the paper are clearly described."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9549,
      "completion_tokens": 585,
      "total_tokens": 10134
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe usage of the MALACH dataset for pre-training large models in an unsupervised or self-supervised manner. Instead, pre-training is done on other corpora with MALACH used for fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "End-to-end ASR models trained from scratch solely on MALACH data were attempted (Section 4.2), but results were worse than fine-tuned models, and the main focus is on fine-tuning pretrained models rather than training from scratch on MALACH."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4.4 and 5",
          "reasoning": "The MALACH dataset is used for supervised fine-tuning of large pretrained wav2vec2 models (Section 4.4). Both the original labeled MALACH data and pseudo-labeled data generated from unlabeled MALACH data are used in fine-tuning to improve recognition performance (Section 5)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of using the MALACH dataset for reinforcement learning post-training techniques such as RLHF is made in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.1, 4.3, 4.5, and 5",
          "reasoning": "The MALACH data is extensively used for evaluation and benchmarking word error rates (WER) of various speech recognition systems (ESPNET architectures, Whisper, wav2vec2 models) and pseudo-labeling methods throughout Sections 4 and 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6",
          "reasoning": "Section 6 provides error analysis and discusses sources of remaining errors, effects of segmentation, challenges with named entities, and accents in MALACH, demonstrating use of the dataset for detailed analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the MALACH dataset as a knowledge base for retrieval-augmented generation or similar purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents practical and multiple uses of the MALACH dataset; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10272,
      "completion_tokens": 600,
      "total_tokens": 10872
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper describes the MALACH Interviews and Transcripts English corpus, which contains English language interviews. It mentions foreign named entities but does not state that multiple human languages appear in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is described as English language survivor testimonies. While there are many foreign named entities (names and places), the content is in English and does not include a second fully supported human language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 1 (Introduction), Section 3 (New Data Extraction), Section 4 (Recognition Systems)",
          "reasoning": "The paper consistently refers to the MALACH English corpus containing English interviews of Holocaust survivors. The data used for speech recognition experiments is English speech with transcripts in English. Though it contains many foreign named entities, the predominant linguistic content is English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset focuses on English language interviews; non-English only datasets are not described as a new contribution."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of audio interviews and transcripts; there is no indication that code or programming language entries are present in the dataset itself."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention is made of mathematical or formal logical expressions as part of the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is human speech testimony; no biological sequences or non-human communication systems are present."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention of any fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly documented as English with many foreign named entities. The language content is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains spoken human language data and corresponding transcripts, so it is not devoid of any language."
        }
      }
    }
  },
  {
    "id": "picheny23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7490,
      "completion_tokens": 144,
      "total_tokens": 7634
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3",
          "reasoning": "The paper describes the process of extracting and processing additional data from the original MALACH dataset, including segmentation and handling cross-channel audio, but no mention is made of any publicly available code repository or software accompanying these data extraction or preprocessing steps."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3",
          "reasoning": "Section 3 provides detailed documentation of the data extraction and processing methods used to prepare additional MALACH corpus data, including descriptions of the original data format, channel summation, segmentation criteria, handling of speaker overlap, and transcription conventions. This constitutes transparent documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 4695,
      "completion_tokens": 98,
      "total_tokens": 4793
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Procedure",
          "Reasoning": "The new dataset consists of 48 minimal pair items contrasting Bern and Zurich German variants, recorded as single-word utterances by two 24-year-old female native speakers and a trained professional actress. These audio recordings were made specifically for this study, with human speakers and human involvement in data collection clearly stated."
        }
      ]
    }
  },
  {
    "id": "pistor24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 5495,
      "completion_tokens": 233,
      "total_tokens": 5728
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Participants and Section 2.3 Procedure",
            "reasoning": "The study involved 46 human participants as raters evaluating dialect features in an online perception study. These raters were normal participants from Bern, Zurich, and Hessen, Germany, with no indication of expert annotator status; hence, multiple human non-experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Procedure",
            "reasoning": "The procedure details that participants were instructed to allocate stimuli to categories 'as fast as they can' using a keyboard, describing the evaluation categories and task, which constitutes annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention found in paper",
            "reasoning": "The paper does not describe any scoring rubrics or detailed scoring guidelines given to raters beyond task instructions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No mention found in paper",
            "reasoning": "No annotation examples or annotated exemplars are provided or described in the paper for the raters."
          }
        }
      ]
    }
  },
  {
    "id": "pistor24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 6685,
      "completion_tokens": 258,
      "total_tokens": 6943
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Procedure",
          "reasoning": "The study indicates that recordings were made by two native L1 speakers of Bern and Zurich German, and then a professional actress was trained to imitate these native speakers. Furthermore, quality control measures were implemented by native speakers to ensure accuracy and naturalness. This implies that multiple human experts, native speakers familiar with the dialects, were involved in validating the quality of recorded stimuli, representing a form of quality assurance by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While statistical analyses were conducted (e.g., mixed effects models using R and Jamovi), these relate to data analysis and not QA of dataset annotations or content directly. No mention of automated verification of dataset annotations is present."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 6295,
      "completion_tokens": 420,
      "total_tokens": 6715
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.3",
          "reasoning": "The authors created new original speech stimuli by recording 66 individual words from two native female speakers of Bern and Zurich German dialects. Additionally, a professional actress trained in these dialects recorded imitations as part of a matched-guise design. The words and phonetic features were carefully selected and controlled for semantic affectivity, resulting in new recorded data not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of machine translation to generate data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the linguistic features were selected informed by scholarly literature and a Google questionnaire, the main data\u2014the speech stimuli\u2014are newly recorded. The paper does not describe aggregating pre-existing audio data without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the selection of phonetic variables was influenced by existing studies and a questionnaire, the data itself consists of newly recorded audio, not adapted from prior audio recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented in Section 2.1 and 2.3."
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 6813,
      "completion_tokens": 229,
      "total_tokens": 7042
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Methods and Materials), Section 3 (Results)",
          "reasoning": "The new dataset of 48 minimal pair stimuli contrasting 24 Bern and Zurich German variants constitutes the core data used for analyzing perceptual evaluations by different rater groups. The dataset is used primarily for analyzing trends and patterns in linguistic feature evaluations, such as aesthetic perceptions and stereotype associations, rather than for training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 7536,
      "completion_tokens": 350,
      "total_tokens": 7886
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 Materials",
          "reasoning": "The dataset consists of spoken single-word utterances representing 24 linguistic features that contrast Bern German and Zurich German dialect variants, which are two dialects of the German language. The dataset entries therefore contain two human languages: Bern German and Zurich German."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "pistor24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 4754,
      "completion_tokens": 170,
      "total_tokens": 4924
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section mentioning code availability",
          "reasoning": "The paper does not provide any links to code repositories or mention that the code for data collection, preprocessing, or dataset generation is publicly available. There is no indication that code used in the study has been shared for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods and materials), subsections 2.1 (Materials) and 2.3 (Procedure)",
          "reasoning": "The paper thoroughly documents the dataset creation process by describing the selection of linguistic features, minimal pair stimuli design, recording procedures including actors and native speakers, validation steps for naturalness, and the rating procedure involving participants and tasks. These sections provide transparency and detail about how the dataset was constructed and processed."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8620,
      "completion_tokens": 131,
      "total_tokens": 8751
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 5 - Results for streamed ASR transcripts; footnote 3",
          "Reasoning": "The new dataset consists of manually corrected transcripts of automatic speech recognition (ASR) outputs from radio talks and TV political debates in Czech. The authors have created and are publishing this dataset, which includes transcripts and corresponding recordings annotated with punctuation. The data are in text format because they represent transcripts, and are human generated as they come from manual correction and human annotation of ASR outputs, not model-generated or unclear provenance data."
        }
      ]
    }
  },
  {
    "id": "polacek23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9420,
      "completion_tokens": 204,
      "total_tokens": 9624
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 and Section 5",
            "reasoning": "The training data includes 1.5 GB of manually corrected transcripts (Section 4.2), and test sets contain automatic transcripts with manually added punctuation (Section 5). The manual corrections imply multiple human experts performed the annotations for both training fine-tuning data and new test data introduced by the authors."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide explicit details or references to annotation instructions in any section."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No mention of any scoring rubrics or criteria used for annotation quality or evaluation guidelines is found in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not include or reference annotation examples or samples in annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "polacek23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10610,
      "completion_tokens": 414,
      "total_tokens": 11024
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance of the datasets was performed by a single human expert. There is no information about the role or qualifications of individual annotators."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information that multiple human experts or domain specialists annotated or verified the datasets. No explicit mention of expert annotators or their QA involvement is given."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance by a single non-expert human annotator in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert humans performing quality assurance or annotation on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being employed as a quality assurance judge or to validate dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper of using automated verification techniques or algorithmic rules as part of quality assurance for dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new test set of automatic transcripts with manually added punctuation, but does not describe any specific quality assurance process for the annotations. There is no description of how the manual punctuation annotations were validated or verified, nor any process ensuring consistency or reliability. Therefore, no documented QA process is present."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10220,
      "completion_tokens": 425,
      "total_tokens": 10645
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The authors mention that they are publishing new data consisting of manually added punctuation to automatic transcripts of radio talks and TV debates, created using an ASR system. These transcripts are manually annotated with punctuation marks, indicating original content created by human contributors from scratch (i.e., not translations, collated, or derived)."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was created by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that any dataset was generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the training data includes multiple sources such as newspaper articles, diploma theses, and legal texts, these are used only for training and not introduced as new datasets by the authors. The only new dataset mentioned consists of manually annotated transcripts."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the new dataset is derived from existing sources via transformation or adaptation. The new transcripts appear to be newly created manual annotations on ASR outputs."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and creation of the new test set with manually added punctuation is specified in the paper (Section 5)."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10738,
      "completion_tokens": 514,
      "total_tokens": 11252
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the new dataset exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The authors mention training the ELECTRA-Small model from scratch using the training corpus consisting of 23 GB of Czech texts including manually corrected automatic transcripts of Czech TV/R broadcasts (Section 4.2). The model trained from scratch yields good performance, as shown in Table 1 in Section 4.3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.3 and 4.2",
          "reasoning": "The training data includes 1.5 GB of manually corrected transcripts used for fine-tuning the model. Fine-tuning is performed on this dataset with supervised learning methods to optimize the classification head and improve punctuation restoration performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for reinforcement learning or RL-based post-training methods."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The authors publish the test dataset consisting of automatic transcripts of radio talks and TV debates with manual punctuation annotation. This dataset is explicitly used for evaluation and benchmarking of the proposed APR models."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses analysis of performance and accuracy, it does not present the dataset primarily for analyzing characteristics or trends outside of training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used or described as serving as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has documented use cases within training from scratch, fine-tuning, and evaluation as described above."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11461,
      "completion_tokens": 416,
      "total_tokens": 11877
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or provide any dataset containing multiple human languages beyond one or two; only Czech language datasets are discussed."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced and used are exclusively related to Czech language transcripts; there is no mention of datasets containing exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new datasets introduced are not English language datasets; the focus is on Czech language transcripts."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Sections 4.1, 4.2, 5",
          "reasoning": "The new datasets introduced consist of manually corrected transcripts and ASR automatic transcripts of Czech TV and radio broadcasts. The text and evaluation data are exclusively in Czech, a non-English language, thus the dataset is monolingual (Non-English)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although models and code are discussed, the dataset itself contains only natural language transcripts, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No parts of the introduced datasets contain mathematical or logical symbolic expressions; all data are textual transcripts."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain human spoken language transcripts and do not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention that the dataset involves any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages are clearly specified as Czech with manual and ASR transcripts; language identity is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains natural language transcripts, so it is not without any language."
        }
      }
    }
  },
  {
    "id": "polacek23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8679,
      "completion_tokens": 272,
      "total_tokens": 8951
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5",
          "reasoning": "The paper mentions that the authors are publishing the test data (transcripts and recordings) at a specified URL (https://owncloud.cesnet.cz/index.php/s/fHqtWwZ5G9V5pPN) but there is no mention or link provided regarding the availability of code for data collection, preprocessing, or dataset construction. The paper does not provide explicit information about the release of code related to the dataset construction process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5 and throughout Section 4.2",
          "reasoning": "The paper provides documentation on the dataset creation process, describing that the training data includes 23 GB of Czech texts from various sources such as newspaper articles, manually corrected transcripts of Czech TV/radio broadcasts, diploma theses, and legal texts. Also, the fine-tuning data includes an additional 1.5 GB of manually corrected transcripts. Further details on pre-processing and tokenization steps are described in Section 3.2, and the dataset composition and sizes are elaborated in Section 4.2. Moreover, Section 5 describes the test datasets (radio talks and TV debates) and their manual annotation. These descriptions indicate transparent and detailed documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10809,
      "completion_tokens": 117,
      "total_tokens": 10926
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction; Section 2 Methodology; Abstract",
          "Reasoning": "The paper explicitly states that the authors have collected and annotated a new balanced dataset containing voice recordings from both healthy participants and participants diagnosed with CHF. These voice recordings originate from patients and healthy subjects, indicating that the data was recorded from humans (i.e., human generated audio). There is no indication the audio data is synthetic or model generated, nor is the origin unknown."
        }
      ]
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11609,
      "completion_tokens": 189,
      "total_tokens": 11798
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5 Conclusion",
            "reasoning": "The paper states that the dataset contains speech recordings from patients diagnosed with CHF by medical professionals, indicating expert involvement in labeling. There is no indication of multiple annotators or crowdsourcing. Thus, a single human expert or a small group of experts provided annotations or labels."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not mention any annotation instructions or guidelines provided to annotators regarding labeling the data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No scoring rubrics or criteria for annotation labeling are described in the paper for the dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not provide or mention any annotation examples for the dataset."
          }
        }
      ]
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12799,
      "completion_tokens": 287,
      "total_tokens": 13086
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Abstract and Section 5 (Conclusion)",
          "reasoning": "The dataset contains voice recordings from participants diagnosed with CHF by medical professionals, indicating expert annotation of the CHF status labels. Each recording is labeled according to confirmed diagnosis, implying quality assurance by single human medical experts annotating each participant's CHF diagnosis."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that multiple human experts performed annotation or quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single non-expert annotated or quality controlled the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple non-expert annotators involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used to perform quality assurance or annotation validation on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic quality assurance of the dataset annotations is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly states that the dataset samples are labeled based on medical professional diagnosis of CHF, therefore quality assurance is documented and present."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12409,
      "completion_tokens": 404,
      "total_tokens": 12813
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Methodology, Section 5 Conclusion",
          "reasoning": "The authors explicitly state that they have collected and annotated a novel speech dataset consisting of voice recordings from patients diagnosed with CHF and healthy subjects. This dataset was created from scratch using recordings from human participants, as described in Section 2 and confirmed in the Conclusion where they mention 'We have collected and annotated a speech dataset...'. There is no indication that the data was translated, derived, or adapted from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention data generated entirely by AI or machine learning models. The dataset consists of human speech recordings collected from participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any machine translation being involved in data creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources, but rather as newly created from fresh recordings."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as derived from existing sources with modifications or transformations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the new dataset is explicitly documented as newly collected human speech recordings."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12927,
      "completion_tokens": 473,
      "total_tokens": 13400
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Methodology) and Section 4 (Experiments)",
          "reasoning": "The newly introduced CHF speech dataset is used to train models, including transfer learning on pretrained SoundNet and VGG-16 models, fine-tuning them on this dataset to learn CHF-specific representations. The paper benchmarks conventional machine learning and deep learning methods trained on this dataset, indicating it is actively used for supervised model training from scratch or via fine-tuning."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Methodology) and Section 4 (Experiments)",
          "reasoning": "The paper applies transfer learning to pre-trained SoundNet and VGG-16 models and fine-tunes them on the new CHF speech dataset. This supervised fine-tuning helps the models adapt to the CHF detection task, as explicitly described in the methodology and evaluation sections."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate any use of reinforcement learning or RL-based post-training techniques with the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The dataset is also used for evaluation and benchmarking various models' performance under subject-independent evaluation protocols such as LOSO and LOSGO cross-validation. This shows explicit usage of the dataset for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Tables 1 and 2, Figure 3",
          "reasoning": "The paper analyzes model performances, class-wise accuracy, and subject-wise results using the dataset, discussing patterns such as higher accuracy for CHF diagnosed subjects. This constitutes analysis of trends and characteristics in the dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation; no such usage is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has multiple practical utilities described and demonstrated in the paper, including training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13650,
      "completion_tokens": 438,
      "total_tokens": 14088
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper describes a dataset of voice recordings from patients diagnosed with CHF and healthy participants, but does not mention multiple languages. No evidence that more than two languages are present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper does not indicate presence of exactly two languages in the dataset. No information on bilingual content provided."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 3 (Dataset Description)",
          "reasoning": "The dataset consists of voice recordings collected from patients and healthy participants speaking English. The paper does not mention any other language being used. Hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication in the paper that the dataset contains recordings in any other single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of voice recordings only; no programming or code-related content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Though the paper contains some formulas and notation in the methodology, the dataset itself contains voice recordings, not mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech recordings, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves natural human language (English) only; no constructed or artificial languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper clearly describes the dataset as human voice recordings used for CHF detection, with English as the language; language is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains voice recordings which are human language; therefore, it does contain language."
        }
      }
    }
  },
  {
    "id": "priyasad22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10868,
      "completion_tokens": 202,
      "total_tokens": 11070
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 5 Conclusion",
          "reasoning": "The paper mentions in the conclusion that the dataset will be made publicly available to researchers after acquiring the required clearances, but does not provide any link or mention availability of code associated with dataset collection, preprocessing, or generation. There is no indication that any code has been released or is accessible to the public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Methodology and Section 3 Dataset description (implied)",
          "reasoning": "The paper provides a description of the dataset's collection and annotation process, including that speech recordings were collected from patients diagnosed with CHF and healthy participants. Although the dataset details like number of samples, segmentation into 5-second windows, and class labels are described, the paper does not provide exhaustive detail such as full acquisition protocols or exact recording conditions. Still, a reasonable level of documentation on dataset creation is presented within the methodology and experimental sections."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7234,
      "completion_tokens": 88,
      "total_tokens": 7322
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "The MOCKS testset uses audio data extracted from existing human-recorded speech datasets LibriSpeech and Mozilla Common Voice, which contain human speech recordings. The paper specifies that these source datasets contain recordings of human speech, thus the audio data is human generated."
        }
      ]
    }
  },
  {
    "id": "pudo23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8034,
      "completion_tokens": 295,
      "total_tokens": 8329
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.2",
            "reasoning": "The paper describes the creation of the MOCKS testset mostly using automated methods such as rule-based grapheme-to-phoneme algorithm, Montreal Forced Aligner (MFA) for word-level alignments, and random selections for similar and different phrases. Manual intervention was limited to a final step of manual checking to exclude obviously incorrect samples, indicating the primary annotation process was automatic."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 and 4.2",
            "reasoning": "Section 3 details explicit annotation requirements and criteria for selecting keywords, positive and negative samples and their phonetic distance thresholds. Section 4.2 elaborates on these criteria and how they were operationalized, indicating clear instructions and guidelines for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3",
            "reasoning": "Section 3 specifies detailed requirements including phoneme length constraints for keywords, definitions of positive and negative sample sets based on phonetic distance ranges, and selection rules such as maximum number of similar phrases, implying the existence of rubrics for data annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not provide examples of annotation or sample annotated data demonstrating the instructions or rubrics. There are no explicit example annotations or data excerpts given as annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "pudo23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9224,
      "completion_tokens": 516,
      "total_tokens": 9740
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being performed by a single human expert on the dataset. The only human involvement mentioned relates to selection of phonetic transcription variants by language experts, but no indication of single-expert QA was reported."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though language experts selected the most popular phonetic transcriptions, the paper does not describe any comprehensive QA process involving multiple human experts for dataset validation. Therefore, this does not apply."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset development used crowdsourced data from Mozilla Common Voice, the paper does not specify any human non-expert group performing quality assurance of the created testset. No mention of multiple non-expert annotators conducting QA is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.2 Creation of our testset",
          "reasoning": "The dataset was created using word-level alignments generated by Montreal Forced Aligner (MFA), a rule-based grapheme-to-phoneme (G2P) algorithm, and automated phonetic distance calculations to select 'similar' and 'different' phrases. These automated rule-based and algorithmic techniques constitute an automatic quality assurance process for data extraction and selection. Additionally, the paper states that the final step included manual checking to exclude obviously incorrect samples, indicating that automatic methods were primarily used for verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are documented in the paper with descriptions of use of automated alignment tools and manual checking, so it is not the case that no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8834,
      "completion_tokens": 468,
      "total_tokens": 9302
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any audio data was recorded or created entirely from scratch by human contributors specifically for this dataset. It uses pre-existing datasets as the base."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain content generated entirely by AI or machine learning models. The only model-related data generation mentioned is the use of forced alignments and G2P transcriptions, but these are processes applied to existing data, not original data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation from another language by humans in the creation of this dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any use of machine translation to produce or modify the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 4.1",
          "reasoning": "The dataset is not merely an aggregation or collection of existing data from multiple sources without significant modification; the paper describes deriving new data from existing datasets via segmentation and processing."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.2",
          "reasoning": "The MOCKS testset is created by deriving new data from existing large public datasets (LibriSpeech and Mozilla Common Voice). This includes using forced alignments to segment audio into keywords, applying phonetic transcription with an internal G2P algorithm, selecting positive and negative samples, and creating offline and online audio variants with additional audio padding. The process involves modifications, transformations and adaptations of existing recordings rather than direct reuse."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9352,
      "completion_tokens": 282,
      "total_tokens": 9634
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5: Initial experiments with MOCKS",
          "reasoning": "The MOCKS dataset is explicitly introduced and used as a benchmark testset for evaluating custom keyword spotting (KWS) models, as detailed in Section 5 where baseline evaluation results are presented comparing performance on MOCKS and other testsets. This demonstrates its utility for evaluation and benchmarking purposes."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3: MOCKS description and analysis",
          "reasoning": "The authors conducted detailed analysis of MOCKS dataset properties such as keyword lengths distribution and gender distribution of speakers, characterizing the dataset beyond just its use for evaluation, indicating usage for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes practical usage of the MOCKS dataset for evaluation and analysis of KWS models."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10075,
      "completion_tokens": 553,
      "total_tokens": 10628
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 4.1.2; Table 1",
          "reasoning": "The MOCKS dataset contains audio data in multiple human languages, specifically English, French, German, Italian, and Spanish as stated in the Abstract and detailed in Section 4.1.2. Table 1 lists keywords and other statistics for subsets corresponding to these five languages, confirming inclusion of more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than two languages, so the bilingual label does not apply."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although English subsets exist within MOCKS, the dataset is not restricted to only English and includes other languages as well."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While there are subsets for single non-English languages such as German, French, Italian, and Spanish, the overall proposed dataset includes multiple languages, so the monolingual (non-English) label does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises audio recordings of spoken phrases, with no indication of programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of mathematical or logical expressions in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human speech audio and does not contain biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset are explicitly specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural human language audio data; thus, this label does not apply."
        }
      }
    }
  },
  {
    "id": "pudo23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7293,
      "completion_tokens": 230,
      "total_tokens": 7523
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or link providing code",
          "reasoning": "The paper mentions that the MOCKS dataset has been made publicly available at a Hugging Face URL, but it does not provide any information about publicly releasing the code or scripts used for data collection, preprocessing, keyword extraction, or dataset generation. There is no link or description about code repositories or implementations to reproduce the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (especially 4.2 Creation of our testset and 4.3 MOCKS description and analysis)",
          "reasoning": "The paper gives detailed documentation about the dataset creation process, including data sources (LibriSpeech and MCV), the requirements guiding keyword selection, the use of grapheme-to-phoneme algorithms, alignment via Montreal Forced Aligner (MFA), selection criteria for positive and negative samples, and details about offline and online audio segments. They also describe some manual checking steps and provide extensive statistics and analysis in Section 4. This information provides a transparent and thorough overview of the dataset construction methodology."
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 1239,
      "completion_tokens": 84,
      "total_tokens": 1323
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2",
          "Reasoning": "The paper explicitly introduces Vox-CA as a new dataset for cross-age speaker verification, consisting of audio pairs of speech recordings. These audio samples are recorded speech segments from human speakers, implying the data is human generated rather than simulated or model generated."
        }
      ]
    }
  },
  {
    "id": "qin22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 2039,
      "completion_tokens": 188,
      "total_tokens": 2227
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2",
            "reasoning": "The paper discusses the Vox-CA dataset created by removing certain audio pairs to better reflect real application scenarios, indicating an automatic or algorithmic process for constructing the dataset rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "There is no mention of detailed annotation instructions provided for Vox-CA since the dataset construction is described in terms of removing pairs automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not describe any scoring rubrics used in the creation or annotation of the Vox-CA dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "No examples of annotations or sample annotation decisions are provided for the Vox-CA dataset in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "qin22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 3229,
      "completion_tokens": 276,
      "total_tokens": 3505
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper mentions the introduction of a new dataset Vox-CA but does not provide any detailed description of a quality assurance process applied to the dataset, nor any information about manual or automatic validation of the data. There is no indication that experts, non-experts, AI models, or automatic procedures were used for quality assurance. The paper does not document any QA process."
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 2839,
      "completion_tokens": 241,
      "total_tokens": 3080
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The dataset Vox-CA is described as constructed by selecting and pairing audio segments from existing VoxCeleb datasets to create cross-age positive pairs, indicating data aggregation from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 3357,
      "completion_tokens": 466,
      "total_tokens": 3823
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the newly introduced Vox-CA dataset is used for pre-training purposes."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit indication in the paper that Vox-CA is used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results)",
          "reasoning": "The dataset Vox-CA is used to fine-tune or improve speaker verification systems as shown in experiments where the model\u2019s performance improves on Vox-CA, indicating supervised use of the dataset for fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning or RL-based post-training methods using the Vox-CA dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experimental Results)",
          "reasoning": "Vox-CA dataset is employed as a benchmark to evaluate speaker verification system performance, such as reporting EER in tables, indicating its usage for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2 (Dataset Description and Comparison), Section 4",
          "reasoning": "The authors analyze characteristics of Vox-CA compared to other datasets and discuss patterns related to cross-age speaker verification, indicating analytical use."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that Vox-CA is used as an external knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset Vox-CA has documented applications in supervised fine-tuning, evaluation, and analysis as described in multiple sections."
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 4080,
      "completion_tokens": 236,
      "total_tokens": 4316
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 and Table 1",
          "reasoning": "The proposed Vox-CA dataset is introduced and appears to be derived from VoxCeleb, which consists primarily of English speech. The paper does not mention any other languages being included; hence the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "qin22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 1298,
      "completion_tokens": 115,
      "total_tokens": 1413
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any information or link regarding the public availability of code related to the dataset construction, collection, or preprocessing for the new dataset Vox-CA."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "Section 2 of the paper describes the details of the new dataset Vox-CA and compares it with another dataset (Interspeech 2022). This section documents the dataset creation process to some extent through explanation and analysis."
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6440,
      "completion_tokens": 120,
      "total_tokens": 6560
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. Performance Evaluation",
          "Reasoning": "The paper states that a new database was created in a studio environment consisting of 384 audio tracks recorded by 15 professional singers familiar with the oktoechos singing modes. The recordings are of Malayalam hymns collected from the liturgical book of the Indian Orthodox church. The recordings were done at 44.1 kHz, indicating audio modality data that was produced through human performance and captured via studio recording."
        }
      ]
    }
  },
  {
    "id": "rajan22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7240,
      "completion_tokens": 202,
      "total_tokens": 7442
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3: Performance Evaluation",
            "reasoning": "The dataset consists of 384 audio tracks recorded in a studio environment with participation of 15 professional singers familiar with the singing modes in oktoechos, indicating expert human involvement in creating and annotating the dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not mention any annotation instructions or guidelines provided to annotators or singers."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "There is no mention of scoring rubrics or criteria guiding the annotation or classification of the oktoechos modes in the newly created dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not provide or mention any annotation examples or sample annotations as part of dataset creation or labeling."
          }
        }
      ]
    }
  },
  {
    "id": "rajan22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8430,
      "completion_tokens": 271,
      "total_tokens": 8701
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert for the created liturgical music corpus."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance or annotation validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple non-expert annotators were involved in quality assurance processes."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the paper mentions the use of AI models to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any automated or algorithmic quality assurance processes for dataset validation or annotation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the creation of a new liturgical music corpus recorded with professional singers in a studio environment but does not report or document any quality assurance or validation process to verify the dataset annotations or contents."
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8040,
      "completion_tokens": 487,
      "total_tokens": 8527
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Performance Evaluation",
          "reasoning": "The paper states that a database was created in a studio environment consisting of eight nirams (colours) with 384 audio tracks recorded by 15 professional human singers familiar with the 'oktoechos' singing modes. The hymns used were collected from the liturgical book of the Indian Orthodox church and recorded specifically for this work. This indicates that the audio data is original content created from scratch by human contributors for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated or synthesized entirely by AI or machine learning models. All data pertains to recordings or features extracted from human-created music."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that data was produced by translating content from another language through human translators. The hymns are in Malayalam and collected from liturgical books; no translation process is described."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any machine translation process for the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While hymns were collected from existing liturgical books, the dataset itself consists of newly recorded audio tracks, not merely aggregated or collected recordings from existing datasets. Therefore, it is not merely collated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though musical texture features and Mel-spectrograms are derived from the raw audio data through signal processing, the rubric targets data novelty at the dataset level, not feature extraction. The dataset itself is original recordings, not derived from existing data with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of the dataset, which was newly created through studio recordings by human singers."
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8558,
      "completion_tokens": 354,
      "total_tokens": 8912
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 - Performance Evaluation, Section 4 - Results and Analysis",
          "reasoning": "The newly created liturgical music corpus of 384 audio tracks was used to train the proposed stacked bidirectional and unidirectional LSTM and GRU architectures from scratch, as described in the performance evaluation and results sections. The corpus was used for supervised training with 60% training split, 5% validation, and the rest for testing, indicating training models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 - Performance Evaluation, Section 4 - Results and Analysis",
          "reasoning": "The dataset serves as both training and test data to evaluate and benchmark the performance of different models including DNN, CNN, vanilla LSTM/GRU, and the proposed stacked architectures in terms of classification accuracy and other metrics."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 - Results and Analysis",
          "reasoning": "The dataset is used to analyze classification results, including per-class accuracy and F1-score analyses, confusion matrices, and detailed discussion on performance per mode (niram), helping to understand the characteristics and challenges of the oktoechos classification task."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9281,
      "completion_tokens": 380,
      "total_tokens": 9661
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3. Performance Evaluation",
          "reasoning": "The newly created corpus consists of 384 audio tracks of liturgical music in Malayalam language, as explicitly stated in Section 3. The hymns used are collected from the Indian Orthodox church liturgical book, and all are in Malayalam, a non-English language. No other languages are mentioned as part of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of liturgical music in Malayalam language, hence it contains language."
        }
      }
    }
  },
  {
    "id": "rajan22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6499,
      "completion_tokens": 191,
      "total_tokens": 6690
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention any link, repository, or reference to code related to the dataset collection, preprocessing, or generation. No public availability of such code is indicated in any section of the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Performance Evaluation)",
          "reasoning": "The paper provides explicit details on dataset creation, including the studio recording environment, number of audio tracks (384), duration of each track, number and demographics of singers (15 professional singers aged 12 to 50), the language (Malayalam hymns), and references the source of hymns (liturgical book of the Indian Orthodox church). Additionally, specifics about recording sampling rate (44.1kHz) and data splits (60% training, 5% validation, rest testing) are described, constituting documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 4555,
      "completion_tokens": 168,
      "total_tokens": 4723
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Protocol description",
          "Reasoning": "The new dataset introduced involves collection of voice, speech, and respiratory sound recordings through acoustic tasks performed by participants. These audio data are captured via human participants speaking or performing tasks, therefore originating from human-generated recordings."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 and Protocol description",
          "Reasoning": "The protocols include demographic questions, patient-reported outcome measures (PROMs), validated patient questionnaires (e.g., GAD-7, VHI-10), and clinical validation questions. This metadata and questionnaire data are text-based and recorded from human participants, thus human-generated."
        }
      ]
    }
  },
  {
    "id": "rameau24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 5355,
      "completion_tokens": 210,
      "total_tokens": 5565
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.4",
            "reasoning": "The protocol development and validation involved multiple experts including clinical experts, bioethicists, standards experts, diversity experts, and software developers who reviewed and amended each question of the protocol during three review sessions."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "Team science followed a structured process including literature review and cohort leads developing protocols according to defined standards, ethics, clinical expertise, and other domains, implying presence of detailed instructions for protocol and annotation development."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "Clinical experts assessed tasks and rated them in order of clinical importance, indicating use of scoring rubrics for evaluation during annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not explicitly mention inclusion or provision of annotation examples in the annotation guidelines for the developed datasets."
          }
        }
      ]
    }
  },
  {
    "id": "rameau24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 6545,
      "completion_tokens": 381,
      "total_tokens": 6926
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by a single human expert. Instead, it describes a collaborative and multi-expert review process."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.4 Validation of the protocols for Standards, Ethics, Clinical expertise, Tools and PEDP",
          "reasoning": "Quality assurance was performed by multiple human experts including clinical experts, bioethicists, standards experts, diversity experts, and software developers who reviewed each section of the protocol multiple times to ensure standards, ethics, diversity, technological feasibility, and clinical validity."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single non-expert human."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate quality assurance by any AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated or algorithmic quality assurance process is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly outlines a comprehensive and multidisciplinary expert review process for quality assurance, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 6155,
      "completion_tokens": 403,
      "total_tokens": 6558
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 and 3.1",
          "reasoning": "The paper describes protocols developed by interdisciplinary human experts over a 6-week period to collect new voice, speech, and related clinical data from human participants across multiple disease cohorts and a control cohort. These datasets are generated through direct data acquisition from human subjects following these newly developed protocols, indicating original data created from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets generated entirely by AI or machine learning models without reference to pre-existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention in the paper that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate any dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on the development of new data collection protocols for prospective data acquisition, not aggregation of existing datasets without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset is based on existing sources with modifications or adaptations; rather, the protocols were designed for new data collection."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The authors explicitly describe the protocols for new human data collection and no ambiguous or undocumented data origin is indicated."
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 6673,
      "completion_tokens": 248,
      "total_tokens": 6921
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 and Sections 2.3-2.4",
          "reasoning": "The paper focuses on developing standardized multi-disorder voice data collection protocols, harmonizing data acquisition methods, and establishing a foundation for future clinical and voice AI biomarker research. There is no description of actual data usage for training or evaluation of models, but rather a detailed development and analysis of the protocols and data characteristics, including clinical, ethical, and DEI considerations, indicating the dataset is primarily used for analysis and future research facilitation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 7396,
      "completion_tokens": 540,
      "total_tokens": 7936
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Entire paper, implicit in all sections describing the protocols and tasks",
          "reasoning": "The paper describes the development of voice, speech, and respiratory sounds collection protocols intended for clinical research use. All protocols, questionnaires, acoustic tasks, and screening tools mentioned (e.g., GAD-7, VHI-10) and descriptions are presented solely in English. There is no mention of inclusion of other human languages or translations in the dataset. Thus, the dataset entries are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions software developers and data scientists involved in the protocol development and references to a GitHub repository, there is no indication that the dataset entries themselves contain programming or structured code-related content. The dataset consists of clinical, acoustic, demographic, and questionnaire data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention inclusion of mathematical or formal logical expressions within the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes human voice, speech, and respiratory sounds but does not include biological sequences (e.g., DNA) or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any fictional or constructed languages in the dataset or protocols."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's linguistic content is clearly specified as English; language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries with human language (English) in the form of questionnaires, acoustic tasks, and clinical metadata."
        }
      }
    }
  },
  {
    "id": "rameau24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 4614,
      "completion_tokens": 137,
      "total_tokens": 4751
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3 (Results)",
          "reasoning": "The paper states that the full protocols are available for download on their GitHub repository (https://github.com/eipm/bridge2ai-redcap), indicating the associated code and protocols for data acquisition are publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper extensively documents the development process of the protocols, including literature review, expert reviews, iterative development, alignment of protocols across cohorts, ethical and DEI considerations, and finalization. This provides transparency and completeness about dataset creation."
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7749,
      "completion_tokens": 274,
      "total_tokens": 8023
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.3",
          "Reasoning": "The new dataset consists of audio recordings of six monolingual Australian English speakers pronouncing target monosyllables in carrier phrases. The recordings were made using an electret condenser microphone during the experimental sessions, indicating human-generated data through recorded natural speech."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "Laryngeal activity was captured using electroglottography (EGG) via a Laryngograph microProcessor EGG-D200. This sensor data was collected synchronously with audio during human speech production, thus the sensor data originates from human involvement in the recording process."
        },
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.4 and 3",
          "Reasoning": "Statistical analysis was performed on measured open quotient (OQ) values, including data processing and modeling with generalized additive mixed models (GAMMs) implemented in R. The resulting processed data tables and statistical outputs are model generated, derived from human-generated original data but produced by computational modeling."
        }
      ]
    }
  },
  {
    "id": "ratko23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8549,
      "completion_tokens": 198,
      "total_tokens": 8747
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.3 Data collection and analysis",
            "reasoning": "The paper describes data collection and annotation procedures conducted by human experimenters (e.g., exclusion of mispronunciations and tokens with multiply-pulsed voice was determined by the first author), indicating annotation was performed by a single human expert."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not explicitly mention the provision of annotation guidelines or detailed instructions for the annotation process; thus, no evidence supports the presence of such instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "No information about scoring rubrics or criteria for annotation is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly stated",
            "reasoning": "The paper does not provide any annotation examples or guidelines that include examples."
          }
        }
      ]
    }
  },
  {
    "id": "ratko23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9739,
      "completion_tokens": 353,
      "total_tokens": 10092
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process conducted by a single human expert for dataset annotation or validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper describing quality assurance conducted by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance process performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert annotators performing quality assurance on the data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report usage of AI models for quality assurance purposes."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Data collection and analysis",
          "reasoning": "The paper describes automated processing steps for data extraction and analysis, including the use of the Praatdet software for computing open quotient (OQ) values from EGG data. This automated measurement serves as an objective procedure to quantify vocal fold contact, providing a reliable form of quality assurance for the dataset values. Although manual exclusion of problematic tokens (due to mispronunciations or multiply-pulsed voice) was performed by the first author, the core analysis relied on automated, algorithmic calculation. Hence, quality assurance can be considered to include automated verification steps."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some forms of quality assurance are documented, specifically automated processing and manual exclusion of unusable tokens, thus N/A does not apply."
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9349,
      "completion_tokens": 407,
      "total_tokens": 9756
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 (Participants), Section 2.2 (Experimental materials), Section 2.3 (Data collection and analysis)",
          "reasoning": "The paper reports original electroglottographic recordings collected from six monolingual Australian English speakers producing designed CVC stimuli. The data were elicited specifically for this study via controlled experimental methods, including new recordings and subsequent analysis of vocal fold activity. This dataset represents original human data created from scratch by the authors' experimental procedure."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any dataset generated entirely by AI or machine learning models. All data analyzed are empirical recordings captured from human participants."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any data being produced by translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Data were not collated or aggregated from existing sources; rather, they were newly collected through experiments."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although statistical and signal processing analyses were performed, the data themselves are original recordings rather than derived or adapted from pre-existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data collection method and origin are clearly specified in the paper."
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9867,
      "completion_tokens": 242,
      "total_tokens": 10109
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 and 4 (Results and Discussion)",
          "reasoning": "The newly collected dataset of electroglottographic (EGG) recordings of Australian English speakers' productions is used primarily for analyzing trends and characteristics of glottal activity during voiceless coda consonants, specifically investigating open quotient (OQ) changes relating to different coda consonants. The paper describes the analysis of these data to reveal phonetic patterns rather than any training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10590,
      "completion_tokens": 494,
      "total_tokens": 11084
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced consists solely of Australian English speech tokens; no other human languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is exclusively Australian English; no second human language is present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 Experimental materials; Section 2.1 Participants",
          "reasoning": "The dataset consists of recordings of Australian English speakers producing Australian English words; all linguistic content is in English as confirmed by participant description and stimuli."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is in English, specifically Australian English, so non-English monolingual does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming language or code entries are included in the dataset; it consists of speech recordings of English words only."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although statistical modelling methods (GAMMs) are used in analysis, the dataset itself does not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech recordings, not biological sequences or non-human communication signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content is specified as Australian English; there is no ambiguity or undocumented language content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English speech recordings, so it does contain language."
        }
      }
    }
  },
  {
    "id": "ratko23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7808,
      "completion_tokens": 194,
      "total_tokens": 8002
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention",
          "reasoning": "The paper does not provide any links, URLs, or references to repositories hosting code related to data collection, preprocessing, analysis scripts, or dataset generation. There is no mention of publicly available code or software."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods), including 2.1 (Participants), 2.2 (Experimental materials), 2.3 (Data collection and analysis), and 2.4 (Statistical analysis)",
          "reasoning": "The paper provides detailed and transparent documentation of the dataset creation process including recruitment of participants, the design of the experimental materials with target words and phonetic contexts, equipment used for data collection, analysis methodology including signal processing for EGG data, and statistical modeling approaches. This sufficiently documents the construction process and characteristics of the collected data for reproducibility and downstream use."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7657,
      "completion_tokens": 137,
      "total_tokens": 7794
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, 'Compositional Benchmark Datasets'",
          "Reasoning": "The paper introduces new compositional splits of existing benchmark SLU datasets (ATIS and SNIPS) to evaluate compositional generalization. These splits are created by selecting and manipulating subsets of human-generated utterances originally from these datasets, which are textual data representing spoken language understanding utterances. The process involves removing or selecting utterances based on slot combinations and lengths, and substituting slot values with values from the training set, but all original data is human-generated text."
        }
      ]
    }
  },
  {
    "id": "ray23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8457,
      "completion_tokens": 255,
      "total_tokens": 8712
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2, 'Compositional Benchmark Datasets'",
            "reasoning": "The compositional splits of benchmark SLU datasets ATIS and SNIPS are created by systematic selection and processing of utterances from existing standard datasets using defined procedures (removing certain utterances and replacing OOV slot values). The paper describes explicit algorithmic steps to generate compositional splits rather than human annotators labeling new data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe any annotation guidelines, instructions, or detailed procedures given to annotators for data annotation of the compositional splits because the splits are generated via an automatic data selection and processing procedure."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No scoring rubrics or criteria for manual annotation are discussed since no human annotators were involved."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2, Figure 1",
            "reasoning": "The paper provides example utterances from the compositional splits, illustrating how the novel slot combination and length generalization splits look, serving as illustrative annotation examples despite the dataset being generated automatically."
          }
        }
      ]
    }
  },
  {
    "id": "ray23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9647,
      "completion_tokens": 321,
      "total_tokens": 9968
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset splits they created."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or documentation in the paper of quality assurance conducted by multiple human experts on the dataset annotations or splits."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process involving a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper of QA performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No use of AI models for quality assurance of the datasets is mentioned in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors created new compositional splits algorithmically by selecting subsets of the existing benchmark datasets, the paper does not discuss any automated quality assurance or verification process applied to validate the correctness or quality of the splits or their annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces new compositional splits of benchmark SLU datasets by selecting subsets of existing data and replacing OOV slot values, but does not describe any quality assurance process such as human annotation review, expert validation, or automated verification methods. Therefore, no quality assurance process is documented or applied for these new datasets."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9257,
      "completion_tokens": 470,
      "total_tokens": 9727
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state or imply that any new dataset was created entirely from scratch by human contributors. Instead, it uses existing benchmark datasets (ATIS and SNIPS) and creates new compositional splits from them."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used for training and evaluation are derived from existing benchmark datasets. The paper does not mention any dataset generated entirely by AI or model outputs."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation or multilingual datasets in the paper. All data is in English and no human translation process is described."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that any dataset was produced or modified through machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 'Compositional Benchmark Datasets'",
          "reasoning": "The paper creates new compositional splits from existing benchmark datasets ATIS and SNIPS by selecting subsets of existing utterances with certain slot type combinations removed or selected to test compositional generalization. Thus, the data is collected and aggregated from existing sources without significant modification of the individual utterances."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 'Compositional Benchmark Datasets'",
          "reasoning": "The compositional splits are derived from the original datasets by filtering training and test utterances according to slot type combination criteria as well as replacing out-of-vocabulary slot values with randomly selected in-vocabulary slot values of the same slot label. This involves transformations and adaptations applied to the existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of generation of the compositional splits are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9775,
      "completion_tokens": 446,
      "total_tokens": 10221
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The authors train their compositional SLU models from pre-trained BERT checkpoints but further train (fine-tune) on their compositional splits; the datasets are used directly to train models on intent classification and slot tagging tasks (jointly trained). This corresponds to supervised training on these datasets, not pre-training from scratch, but there is no mention of random initialization training, so does not apply."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2",
          "reasoning": "The compositional splits of ATIS and SNIPS datasets created by the authors are used for supervised fine-tuning of pre-trained BERT SLU models, as described in the experiments training setup. The datasets serve as supervised fine-tuning data to improve compositional generalization."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning or RLHF methods involving the new datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2, 4.2",
          "reasoning": "The compositional splits (novel slot combination and length generalization) made from ATIS and SNIPS are explicitly used as new benchmarks to evaluate compositional generalization of SLU models. The splits provide evaluation sets to measure performance on compositionality challenges."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The compositional splits enable analysis of model behavior regarding novel slot combination and length generalization, facilitating in-depth analysis on why baseline SLU models fail in these scenarios."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the introduced datasets serve as knowledge bases for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are actively used for supervised fine-tuning, evaluation, and analysis as described in multiple sections."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10498,
      "completion_tokens": 462,
      "total_tokens": 10960
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces compositional splits of benchmark SLU datasets (ATIS and SNIPS), which are English language datasets. There is no mention of multiple languages or multilingual content in the datasets introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced (compositional splits of ATIS and SNIPS) contain English utterances only, with no indication of any second human language being included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2: Compositional Benchmark Datasets",
          "reasoning": "The compositional splits are created from the ATIS and SNIPS datasets, which contain English utterances. The paper provides example utterances such as 'show flights to Boston' and 'play rock from the eighties', clearly indicating English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the composed splits contain non-English language data."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of spoken language utterances related to slot filling and intent detection; no programming or code snippets are present in these datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical notation to describe model losses, the datasets themselves do not contain mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are human spoken language utterances; there is no biological or non-human communication data reported."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no presence of constructed or fictional languages such as Klingon or Esperanto in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets' linguistic content is clearly specified as English from known benchmark datasets; the language is documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language utterances in English; thus they include language and are not inapplicable to the language metric."
        }
      }
    }
  },
  {
    "id": "ray23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7716,
      "completion_tokens": 163,
      "total_tokens": 7879
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2, Compositional Benchmark Datasets",
          "reasoning": "The paper describes the method to create compositional splits of benchmark SLU datasets ATIS and SNIPS in detail within Section 2 but does not mention any link, URL, or location for publicly available code or scripts for constructing these dataset splits."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, Compositional Benchmark Datasets",
          "reasoning": "The paper provides a clear and detailed description of how the compositional train/test splits are created, including specific steps for novel slot combination split and length generalization split, with explanation of data selection and substitution of OOV slot values, making the dataset creation process explicit and transparent within Section 2."
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8400,
      "completion_tokens": 91,
      "total_tokens": 8491
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 (SMC data description)",
          "Reasoning": "The SSPNet Mobile Corpus (SMC) is composed of 60 phone conversations recorded from 120 participants engaged in spontaneous speech tasks. The audio data was created through human conversations during an experimental task setting, making it human-generated audio data."
        }
      ]
    }
  },
  {
    "id": "rennie22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9200,
      "completion_tokens": 258,
      "total_tokens": 9458
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 2.2 Data",
            "reasoning": "The paper states that the conversational data (SMC) was collected from 120 participants engaged in spontaneous conversations about 'The winter survival task'. There is no mention of expert annotation for laughter detection; the focus is on method evaluation rather than annotation details. The annotation process and annotators are not described, so it is assumed that non-expert humans (e.g., researchers or assistants) performed labeling, but this is not explicitly stated. No evidence indicates expert involvement or AI/automatic annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2 Data",
            "reasoning": "The paper does not describe any annotation instructions or guidelines provided to annotators for the SMC dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2 Data",
            "reasoning": "No mention or description of scoring rubrics for annotation of laughter events in the SMC dataset is present in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2 Data",
            "reasoning": "The paper does not provide or reference any annotation examples or guidelines for the SMC dataset."
          }
        }
      ]
    }
  },
  {
    "id": "rennie22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10390,
      "completion_tokens": 422,
      "total_tokens": 10812
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no information that quality assurance was conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple human annotators with subject matter expertise or belonging to the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of QA conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by multiple human annotators without subject matter expertise."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the paper indicates that an AI model was used as judge or for quality assurance in dataset annotation or validation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any algorithmic or rule-based automatic verification process for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces two datasets, the SSPNet Vocalisation Corpus (SVC) and the SSPNet Mobile Corpus (SMC). However, the paper does not provide any details about quality assurance procedures for annotation or validation of these datasets. There is no mention of who annotated the datasets, how annotations were verified, or any processes to ensure annotation reliability or consistency. Therefore, it appears that no quality assurance process is documented or applied in this paper regarding these datasets."
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10000,
      "completion_tokens": 437,
      "total_tokens": 10437
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The SSPNet Mobile corpus (SMC), which is used as the primary dataset in this paper, was generated through 60 spontaneous telephone conversations between 120 human participants engaged in 'The winter survival task'. This data was created from original human conversational speech recorded specifically for this corpus, representing naturalistic interaction without experimenter influence during the conversations, thus qualifying as original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generation by AI or machine learning models as part of the dataset creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was involved in the data preparation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SMC dataset was not assembled from existing sources without modification, but rather recorded specifically for this study, so it is not considered collated."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The SSPNet Vocalisation Corpus (SVC) is derived from the SMC by selecting clips of 11 seconds containing at least one laughter or filler event, effectively a subset of the SMC, representing a modified and transformed version of an existing dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are clearly specified for both datasets."
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10518,
      "completion_tokens": 320,
      "total_tokens": 10838
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.5 Training and Testing",
          "reasoning": "The paper describes using the SSPNet Mobile corpus (SMC), a new and more naturalistic conversational dataset introduced by the authors, to train neural networks from randomly initialized parameters to detect laughter. The k-fold training setup and multiple model initializations show that the dataset was actively used for training models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 Results and Discussion",
          "reasoning": "The SMC dataset is used as a benchmark for evaluating the performance of various laughter detection methods, with performance metrics such as AUC, precision, recall, and F1 scores reported to compare model effectiveness."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 Results and Discussion and Section 5 Conclusions",
          "reasoning": "The dataset is analyzed to understand characteristics of laughter detection methods, model ability to detect laughter event durations, and to critique evaluation metrics used in the field. The paper discusses trends and challenges in laughter detection using the SMC dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11241,
      "completion_tokens": 585,
      "total_tokens": 11826
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper describes the new dataset (SMC) as consisting of spontaneous conversations with participants who are all native English speakers (Section 2.2). There is no indication that multiple languages are present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of two human languages in the proposed dataset. The paper explicitly states all participants were native English speakers (Section 2.2)."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The authors state that all participants in the SSPNet Mobile Corpus (SMC), the new and largest dataset introduced in the paper, were native English speakers (Section 2.2). The conversations are spontaneous dialogues in English, therefore the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of English conversations only, so this label does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of audio recordings of human speech; no programming or code snippets are part of the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No evidence that the dataset contains mathematical or logical expressions; it is composed of speech audio."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human vocalizations (speech and laughter), but not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of constructed or fictional languages in the dataset; the language used is English."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language(s) of the dataset are explicitly stated as English; therefore, the dataset is not of unknown language origin."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain human language (English speech), so the label N/A does not apply."
        }
      }
    }
  },
  {
    "id": "rennie22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8459,
      "completion_tokens": 196,
      "total_tokens": 8655
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention found regarding code availability",
          "reasoning": "The paper does not provide any link, repository, or mention of publicly available code related to the construction, preprocessing, or generation of the dataset. There is no indication that code has been shared to facilitate reproduction of the dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data), specifically Sections 2.1 (SVC) and 2.2 (SMC)",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process for both the SVC and SMC datasets, including participant demographics, experimental design (e.g., 'The winter survival task' for the SMC), recording conditions, and statistics regarding laughter events. While the SVC is an extracted subset of the SMC, the parent corpus (SMC) is thoroughly described, indicating clear documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6647,
      "completion_tokens": 191,
      "total_tokens": 6838
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Data",
          "Reasoning": "The dataset consists of audio recordings sampled at 44.1 kHz from 250 sessions where participants interacted with a virtual agent while performing speech tasks. The audio data is recorded from human participants during these tasks, thus it is human generated data captured via recording devices during the study."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Data",
          "Reasoning": "The dataset also consists of corresponding video recordings with resolution 320 x 240 pixels and 15 fps from the same 250 sessions. These videos capture the participants' facial movements during the speech tasks. Since these videos were recorded from actual human participants in the study, the data is human generated, captured using cameras during the experiment."
        }
      ]
    }
  },
  {
    "id": "richter23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7447,
      "completion_tokens": 248,
      "total_tokens": 7695
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3, especially 3. Methods and 3.1 Audio quality rating",
            "reasoning": "The paper states that all recordings were rated by one speech researcher, and a second speech researcher rated a subset for inter-rater agreement. Given their designation as 'speech researcher' and involvement in defining rubrics, these annotators are domain experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 and subsections (3.1, 3.2, 3.3)",
            "reasoning": "The paper details the annotation rating process with explicit definitions of distortion categories and compliance criteria, constituting detailed instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Sections 3.1 and 3.2",
            "reasoning": "The paper describes detailed Likert scale rubrics tailored for audio and video quality ratings with explicit criteria for each score level, indicating presence of scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No explicit examples of annotation guidelines or rating examples were provided or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "richter23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8637,
      "completion_tokens": 363,
      "total_tokens": 9000
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3, Methods",
          "reasoning": "The paper states that all recordings were rated by one speech researcher, and the audio and video were rated separately. Speech researchers have relevant domain expertise making them subject matter experts for the dataset quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 4, Results",
          "reasoning": "The paper indicates that a second speech researcher rated 125 audio and video samples to assess inter-rater agreement. Both raters are speech researchers, implying multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any non-expert single annotator conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that non-expert multiple annotators conducted quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance performed by AI models."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of automated verification or algorithmic QA procedures for dataset annotations or quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance was clearly performed and described; hence, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8247,
      "completion_tokens": 178,
      "total_tokens": 8425
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The dataset consists of audiovisual recordings collected directly from human participants (people with ALS and healthy controls) interacting with a virtual agent to perform speech tasks. These recordings were newly recorded specifically for this study and are original data generated organically by human subjects rather than adapted from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8765,
      "completion_tokens": 215,
      "total_tokens": 8980
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 - Results",
          "reasoning": "The dataset introduced is primarily used to analyze data quality, feasibility, and task compliance of the multimodal dialog-based remote assessment system. The paper presents detailed analysis of audio and video quality ratings, compliance rates, and correlations with speech impairment severity, rather than using the dataset for training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9488,
      "completion_tokens": 494,
      "total_tokens": 9982
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing more than two human languages. The data consists solely of English speech tasks."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages; only English is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Data",
          "reasoning": "The participants interacted with the virtual agent in English performing speech tasks such as picture description and syllable repetition (/pAtAk/), with no mention of any other language. The instructions, tasks, and assessments all appear to be in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of a dataset containing a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio and video recordings of human speech; no programming or structured code content is present in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains no mathematical or formal logical expressions or symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though medical in nature, the dataset consists of human speech recordings and video, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial languages are mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human speech recordings and thus contains language."
        }
      }
    }
  },
  {
    "id": "richter23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6706,
      "completion_tokens": 146,
      "total_tokens": 6852
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not provide any links, references, or mentions of publicly available code related to the dataset construction, data collection, or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Data), Section 3 (Methods)",
          "reasoning": "The paper documents the dataset creation process including participant recruitment, the speech tasks used (picture description, DDK), the spoken and video recording parameters, and detailed rubrics for quality rating, audio and video assessment criteria, and compliance evaluation. The methods section provides clear, structured documentation of how data was collected, assessed, and the rationale behind these steps."
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10878,
      "completion_tokens": 100,
      "total_tokens": 10978
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 (EARS dataset description)",
          "Reasoning": "The EARS dataset is introduced as a high-quality speech dataset with 107 speakers recorded in an anechoic chamber. The speech data are captured via microphones in controlled environments, involving human speakers' recordings, confirmed in the Abstract and Section 2 where recording equipment and conditions are described."
        }
      ]
    }
  },
  {
    "id": "richter24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11678,
      "completion_tokens": 265,
      "total_tokens": 11943
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2, Section 3.1",
            "reasoning": "The EARS dataset was recorded with 107 speakers, described as participants from diverse demographic backgrounds, indicating multiple human experts or carefully controlled human participants performed the data collection. The paper mentions diverse demographic backgrounds and controlled lab anechoic chamber recordings, which implies expert involvement in gathering high-quality data."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2",
            "reasoning": "Section 2 describes multiple reading styles, emotional speech, and freeform speech tasks. This implies that detailed instructions were provided to speakers to perform seven different reading styles, 22 emotional styles, and freeform speech, as well as non-verbal sounds. The complexity and diversity of speech types indicate explicit recording instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "There is no explicit mention of scoring rubrics or specific evaluation criteria used as part of the annotation guidelines for the speakers during recording or annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not mentioned",
            "reasoning": "The paper does not provide or reference any annotation examples or prototypical examples given to the annotators or speakers as part of the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "richter24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12868,
      "completion_tokens": 286,
      "total_tokens": 13154
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance done by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance by multiple human non-expert annotators is provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for benchmarking speech enhancement methods, the paper does not describe any use of AI models for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process or algorithmic checks used as quality assurance for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance process for validating the dataset annotations or content. There is no mention of manual or automated QA, and no description of any human or AI review of the data."
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12478,
      "completion_tokens": 485,
      "total_tokens": 12963
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (EARS dataset)",
          "reasoning": "The paper describes the EARS dataset as containing 100 hours of anechoic speech recorded from 107 human speakers with diverse demographics and various speaking styles, recorded in an anechoic chamber. The data was collected directly from human participants performing speech tasks, indicating original content created entirely from scratch by humans."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described is based on human speech recordings; no indication exists that any part of the dataset was generated artificially by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced via machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The EARS dataset itself is newly recorded data, not collected insignificantly modified from pre-existing datasets. Although other RIR datasets and noise datasets are aggregated for creating reverberant or noisy versions (such as EARS-Reverb and EARS-WHAM), these are not independently released datasets introduced by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2 (EARS-WHAM and EARS-Reverb)",
          "reasoning": "The authors introduced EARS-WHAM and EARS-Reverb datasets derived from the newly recorded EARS clean speech data mixed with or convolved by existing noise and RIR datasets. These datasets are derived by applying transformations to the original clean recordings (adding noise or reverberation), thus based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset sources and methods are well documented in the paper."
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12996,
      "completion_tokens": 338,
      "total_tokens": 13334
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the EARS dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper that the EARS dataset is used to train models from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate using the EARS dataset for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of reinforcement learning (RL) based post-training methods involving the EARS dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The EARS dataset is used to construct benchmark datasets (EARS-WHAM and EARS-Reverb) for speech enhancement and dereverberation evaluation. The paper presents instrumental metric evaluations and listening tests on these benchmarks, and a blind test set with an evaluation server is introduced for performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on benchmarking and evaluation rather than analysis of dataset trends or characteristics independently."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the EARS dataset as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13719,
      "completion_tokens": 503,
      "total_tokens": 14222
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the EARS dataset as being composed of speech recordings from English speakers only. There is no mention of other languages included in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only recordings in English; no mention of two languages present."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 and Abstract",
          "reasoning": "The EARS dataset contains speech from 107 English speakers recorded in an anechoic chamber. The content includes various speaking styles and emotions, but all material is in English. Transcriptions and metadata confirm that the language of the dataset is English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is explicitly English; no non-English language is included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio speech data only; no programming language or structured code content is present."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper contains some technical and mathematical descriptions but the dataset itself comprises recorded speech audio only, not math or logic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech recordings, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages in the dataset; all speech is natural English."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language coverage is specified clearly as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language speech recordings."
        }
      }
    }
  },
  {
    "id": "richter24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10937,
      "completion_tokens": 227,
      "total_tokens": 11164
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper about publicly available code for dataset construction.",
          "reasoning": "The paper mentions download links for the dataset and an automatic evaluation server but does not mention any publicly available code repository or code for the data collection, preprocessing, or generation process for the EARS dataset or the derived datasets (EARS-WHAM, EARS-Reverb). Hence, the code is not made publicly available according to the paper content."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 and Section 3 (subsections 3.1 and 3.2) describe the dataset creation and processing.",
          "reasoning": "The paper provides detailed descriptions of dataset creation, including recording conditions in an anechoic chamber, microphone details, number and diversity of speakers, recording quality, speaking styles, and the creation of derived datasets EARS-WHAM and EARS-Reverb with details on mixing procedures, noise sources, impulse response usage, and test set details. This documentation is sufficient for transparency and reproducibility understanding from the paper content."
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6340,
      "completion_tokens": 157,
      "total_tokens": 6497
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Method, 2.2 Procedure, and Section 3. Results",
          "Reasoning": "The paper introduces a new dataset of acoustic recordings from seven male native Mehri speakers producing a set of items containing ejective and pulmonic alveolar and velar stops in various word positions. The data consists of manually recorded speech audio of these speakers during fieldwork in Oman, as described in Section 2.2, yielding 420 tokens which were analyzed acoustically (Section 3). This is explicitly newly recorded human speech data involving human participants; thus, the modality is audio and the data is human generated from direct field recordings."
        }
      ]
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7140,
      "completion_tokens": 210,
      "total_tokens": 7350
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2, Method and Data Collection",
            "reasoning": "The acoustic data were segmented and labeled using Praat by the researchers during fieldwork with seven native Mehri speakers. This suggests that the annotations (segmentation and labeling) were performed by the authors themselves or experts conducting the study, rather than multiple annotators or automatic processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide any explicit details on the provision of detailed annotation instructions for segmentation and labeling of the acoustic data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "There is no mention of scoring rubrics or formal annotation criteria used in the acoustic annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "No annotation examples or guideline excerpts are included or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8330,
      "completion_tokens": 333,
      "total_tokens": 8663
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of quality assurance performed by multiple human experts or annotators in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple non-expert annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 (Procedure) and Results (Section 3)",
          "reasoning": "The acoustic data were analyzed using automated software tools such as Praat, Parselmouth, and VoiceSauce for the extraction and measurement of acoustic parameters, and statistical analysis was performed using the statsmodels package. This indicates that some quality control and verification of data extraction and measurement relied on an automatic, algorithmic process, ensuring consistency and reliability of measurements."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The study describes an automatic process for acoustic measurement and statistical analysis, implying some level of quality assurance; thus, it is not the case that no quality assurance is applied or documented."
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7940,
      "completion_tokens": 413,
      "total_tokens": 8353
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 and throughout Results section",
          "reasoning": "The paper reports acoustic data recorded from seven native speakers of Omani Mehri during fieldwork. The stimuli consisted of 12 items designed for eliciting contrasts in ejective and pulmonic alveolar and velar stops. Each item was produced multiple times by each speaker, yielding a newly collected dataset of 420 tokens. The data were manually segmented and labeled before acoustic analysis, indicating entirely original content created from scratch by human contributors during fieldwork."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the data was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data from other languages by human translators as part of dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation being used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources without modification; it was explicitly recorded anew from speakers."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not appear to be based on existing sources with modifications; it is original data from human speakers."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and data collection methods for the dataset are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8458,
      "completion_tokens": 237,
      "total_tokens": 8695
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Results) and Section 4 (Discussion and Conclusion)",
          "reasoning": "The dataset collected from seven native Mehri speakers was used primarily for analyzing acoustic correlates of ejective stops, examining durational and non-durational parameters, and studying sound change and phonetic variability in the endangered Mehri language. No mention of model training or evaluation is made; the dataset serves as a basis for phonetic and linguistic analysis of patterns and trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9181,
      "completion_tokens": 638,
      "total_tokens": 9819
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of recordings and acoustic measurements from Omani Mehri speakers. While speakers are bilingual in Mehri and dialectal Arabic, the dataset entries themselves contain only Mehri language items, not entries in multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of recordings of Mehri language items; although speakers are bilingual and the context discusses Arabic influence, the dataset entries are exclusively in Mehri, not bilingually coded."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset recordings contain speech in Omani Mehri, a non-English language. English is only used as the language of the paper and documentation, not in dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 (Procedure) and throughout the paper",
          "reasoning": "The new dataset introduced consists of speech recordings and acoustic data of words from Omani Mehri only (a Modern South Arabian Semitic language) as produced by native Mehri speakers. The dataset items are exclusively in one non-English language, Mehri, as detailed in Section 2.2 and implied throughout the study."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are speech tokens in Mehri and associated acoustic measurements. Although computational tools like Parselmouth and Praat were used for analysis, the dataset itself does not contain code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses some mathematical notation (e.g., for D-prime calculation), but the dataset itself does not contain entries with mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech recordings in Mehri language; no biological sequences or non-human communication signals are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains data from a natural language (Mehri), not any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified and documented as Omani Mehri."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data in Omani Mehri, so it is not non-linguistic data."
        }
      }
    }
  },
  {
    "id": "ridouane22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6399,
      "completion_tokens": 166,
      "total_tokens": 6565
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention or include any link to the code used for data collection, preprocessing, acoustic measurement extraction, or analysis for constructing the dataset. There is no indication that any code was made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 - Method (2.2 Procedure)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including information about the speakers, recording procedure, stimuli used (Table 1), the phonetic items, annotation and measurement methods (tools like Praat, Parselmouth, and VoiceSauce), acoustic parameters measured, and the analytical methods used to analyze the dataset. This constitutes comprehensive documentation of the dataset creation within the paper."
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7782,
      "completion_tokens": 140,
      "total_tokens": 7922
    },
    "response": {
      "sources": [
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4 and 5.2",
          "Reasoning": "The paper introduces a new synthetic dataset generated by a small Generative Adversarial Network (GAN) model trained to distill discriminative speech emotion representations from the original human-generated speech data (IEMOCAP). The synthesized data consists of SSL feature representations in time series format for use in downstream speech emotion recognition tasks. Thus, the new dataset modality is time series, and the data is model generated by the proposed GAN method, not directly human recorded or raw audio, nor of unknown origin."
        }
      ]
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8582,
      "completion_tokens": 243,
      "total_tokens": 8825
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4 and Section 5.2",
            "reasoning": "The new dataset is a synthetic dataset generated by a GAN-based dataset distillation generative model trained on IEMOCAP data. The synthetic samples are automatically produced by the trained generative model rather than labeled or annotated by human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit annotation instructions are described in the paper",
            "reasoning": "The paper does not describe any annotation instructions provided for the synthetic dataset generation because the data is automatically generated by the GAN model rather than annotated by humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No mention in Sections 4 or 5 regarding rubrics",
            "reasoning": "Scoring rubrics or evaluation criteria for annotation are not applicable since this is not a human annotation process but an automated generation of synthetic data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples are provided as per the paper content",
            "reasoning": "There are no annotation examples for the synthetic data as it's produced automatically by a generative adversarial network rather than through manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9772,
      "completion_tokens": 353,
      "total_tokens": 10125
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human annotator who is a subject matter expert or member of the target demographic on the introduced synthetic dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple human experts on the synthetic dataset or its annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description that a single non-expert human annotator validated the synthetic dataset or its features."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert human annotators involved in the quality assurance of the synthetic dataset generated by the GAN."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the method involves neural networks and a GAN for dataset distillation, the process of quality assurance or validation of the synthetic dataset does not appear to be performed or judged by an AI model specifically as a QA step."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any automated verification or rule-based algorithmic process for quality assurance of dataset annotations. The evaluation focuses on downstream model training performance rather than explicit QA of dataset content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The introduced synthetic dataset is generated procedurally by a GAN from real data; the paper evaluates downstream task performance but does not describe any dedicated quality assurance process (such as human annotator review or automated verification), nor any explicit QA protocol for the synthetic data itself."
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9382,
      "completion_tokens": 414,
      "total_tokens": 9796
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention creation of new datasets by human contributors. It focuses on distilling existing data and generating synthetic data via models."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4, 5.2 and multiple mentions throughout the paper",
          "reasoning": "The paper proposes a generative model (GAN) trained on existing speech emotion data to generate synthetic datasets. These synthetic datasets are new data generated entirely by the AI model, distinct from original data points, intended to train downstream models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data is produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of machine translation to produce data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not merely collected or aggregated; rather, synthetic datasets are produced via GANs, which transform and generate data representations."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 and 4: Dataset Distillation and Dataset Distillation for Speech Emotion",
          "reasoning": "The synthetic datasets are derived from the original IEMOCAP dataset by learning a distribution and generating data adapted from the original, but significantly transformed into synthetic representations for downstream tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method and source of the data are clearly described; thus, this label is not applicable."
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9900,
      "completion_tokens": 198,
      "total_tokens": 10098
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2 and Table 2",
          "reasoning": "The synthetic dataset generated by the proposed GAN-based dataset distillation method is used to train downstream speech emotion recognition models in a supervised manner, with the evaluation performed on real data test sets to measure task performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10623,
      "completion_tokens": 595,
      "total_tokens": 11218
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper focuses on dataset distillation for speech emotion recognition using the IEMOCAP dataset which is an English dataset. There is no indication that multiple languages are included in the proposed synthetic dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset discussed is the IEMOCAP, which is English only. No mention of any second human language was made for the newly generated synthetic dataset."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 4 (Dataset Distillation for Speech Emotion); usage of IEMOCAP dataset",
          "reasoning": "The new dataset is distilled from the IEMOCAP dataset, which consists of English speech data. The paper explicitly uses IEMOCAP for speech emotion recognition and does not mention any other language, indicating the synthetic data is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states usage of the IEMOCAP dataset which is English; no non-English language is described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code data entries are included in the proposed synthetic dataset. The paper focuses on generative modeling of speech feature representations, not code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper includes mathematical formulae to describe loss functions and GAN training, the dataset itself does not contain mathematical or logical notation as content entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset concerns human speech emotion recognition, no biological sequences or non-human communication signals are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial languages are present in the dataset or discussed in the paper."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English from the use of IEMOCAP dataset; hence the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains English language speech data (or their synthetic representations), so it contains language entries."
        }
      }
    }
  },
  {
    "id": "rittergutierrez24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7841,
      "completion_tokens": 211,
      "total_tokens": 8052
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not mention any link, URL, or repository where the code for the dataset distillation generative model (GAN-based) or dataset generation is made publicly available. There is no indication of code release for reproducing the dataset generation or GAN training."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3, 4, and 5",
          "reasoning": "The paper documents the dataset distillation process in detail, including the methodology of training the GAN generator as a dataset distillator, the specific losses introduced (softmax matching loss, diversity penalty), architectures used (GAN-CNN and GAN-ATT), and experimental set-up. It describes the conditioning on IEMOCAP emotion labels, the dataset size budgets, and evaluation procedures. Although the original dataset (IEMOCAP) is publicly known, the method to distill it into a generative model dataset is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7728,
      "completion_tokens": 104,
      "total_tokens": 7832
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and Table 1",
          "Reasoning": "The paper describes synthetic audio produced by running target language text through trained TTS systems of higher-resource pivot languages, generating synthetic text-speech pairs used for ASR augmentation. This synthetic audio is generated by AI models (Microsoft TTS and Google Cloud TTS neural models) without direct human recording, hence model generated."
        }
      ]
    }
  },
  {
    "id": "robinson22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8528,
      "completion_tokens": 193,
      "total_tokens": 8721
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 Data setup; Section 4 Results",
            "reasoning": "The TTS synthetic audio generation is conducted by neural TTS systems from Microsoft and Google Cloud as described in Section 3.1. This is an automatic process without human annotation involved in generating synthetic text-speech pairs for training."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not mention any annotation instructions provided for synthetic data generation or human annotation related to the new datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No mention of scoring rubrics or evaluation rubrics related to annotation of the synthetic data is provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No examples of annotation guidance or synthetic data annotation examples are described."
          }
        }
      ]
    }
  },
  {
    "id": "robinson22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9718,
      "completion_tokens": 365,
      "total_tokens": 10083
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the datasets introduced or used."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information indicating quality assurance was conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple non-expert annotators performed QA on the introduced datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report use of AI models as judges for quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No specific automated verification or algorithmic quality assurance procedures for dataset annotations or content are described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any explicit quality assurance process applied to the introduced datasets (e.g., Suba African Voices corpus, Guaran\u00ed Common Voice subsets, or synthesized TTS augmented data). Therefore, no QA process is documented."
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9328,
      "completion_tokens": 497,
      "total_tokens": 9825
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not claim to have created original content entirely from scratch by human contributors. The data used such as Kiswahili authentic recordings, Guaran\u00ed data from Common Voice, and Suba corpus are pre-existing or collected from other sources."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (TTS Augmentation)",
          "reasoning": "The authors generate synthetic audio data by inputting authentic text into pretrained TTS neural models (Microsoft TTS and Google Cloud TTS) from high-resource pivot languages. This synthetic speech data is newly generated by AI models and does not directly copy or transform existing audio data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report the use of machine translation to generate data; even transliteration described is done via hand-crafted phone maps, not automatic translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 (Data setup) and Table 1",
          "reasoning": "Text and audio datasets such as the Helsinki Swahili corpus, Mozilla Common Voice data for Italian and Guaran\u00ed, and AfricanVoices Suba corpus are collected or aggregated from existing resources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Transliteration)",
          "reasoning": "The paper describes deriving transliterated text from the original target language texts using hand-crafted phone maps to approximate orthography for pivot languages. This transliterated text is a modified form of the original data, used as input to the pivot language TTS."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The sources and method of generation of all datasets used or created are documented explicitly."
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9846,
      "completion_tokens": 362,
      "total_tokens": 10208
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 Experimental Setup; Section 4 Results",
          "reasoning": "The novel datasets of synthetic TTS-generated audio paired with text, produced by passing target language text through high-resource pivot language TTS systems, are used to train ASR models from scratch. Experiments involve varying amounts of authentic and synthetic data combined to train an end-to-end ASR system, without mention of pre-training or fine-tuning existing models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 Synthetic data amount; Section 4.4 Application to low-resource languages",
          "reasoning": "The datasets including synthetic TTS-augmented training sets and authentic test sets are used to evaluate and benchmark ASR performance, in terms of word error rate and character error rate. Authentic test sets remain constant to measure improvements from training with synthetic data."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Choice of pivot language; Section 4.3 TTS quality",
          "reasoning": "The datasets are used to analyze the effects of different pivot languages and TTS quality on ASR performance and characteristics of recognition. The paper reports trends and surprising findings, analyzing the relationship between TTS quality and augmentation benefit."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10569,
      "completion_tokens": 556,
      "total_tokens": 11125
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 and throughout the paper",
          "reasoning": "The new datasets introduced for experiments contain entries in multiple distinct human languages: Kiswahili, Guaran\u00ed, Italian, and Suba. The authors use text and speech data for these four languages as part of their augmentation and ASR experiments, demonstrating multilingual coverage."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While bilingual datasets might appear in the context of paired languages, the datasets explicitly described and used are monolingual in separate languages, with experiments conducted across multiple such languages rather than a bilingual dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not consist solely of English content; rather, they focus on other languages like Kiswahili, Guaran\u00ed, Italian, and Suba."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Table 1",
          "reasoning": "Each dataset is monolingual in a non-English language, e.g., Kiswahili, Guaran\u00ed, Italian, or Suba. The data entries are distinctly in each one language, making the individual datasets monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any datasets containing code or programming languages."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the datasets contain mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not involve biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets are clearly specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language data."
        }
      }
    }
  },
  {
    "id": "robinson22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7787,
      "completion_tokens": 185,
      "total_tokens": 7972
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3.1 Data setup and Table 1",
          "reasoning": "The paper provides a link to the data with train/val/test splits and details about subsets for each experiment at https://github.com/n8rob/MultilingualTTSAugmentation, indicating publicly available code and data-related resources for dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data setup and related paragraphs describing authentic and synthetic data preparation, transliteration, and Table 1",
          "reasoning": "The paper documents the dataset creation process in detail, including authentic data sources for Kiswahili, Guaran\u00ed, Suba, and Italian, text corpora used for TTS prompts, synthetic data generation via Microsoft and Google TTS, transliteration methods, and data quantity used, which ensures transparency and completeness in dataset creation."
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8039,
      "completion_tokens": 181,
      "total_tokens": 8220
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The new dataset is simulated two-speaker overlapped utterances created from a corpus of single speaker audio-visual YouTube utterances. The overlapped audio waveform was created by offsetting and adding single speaker audio signals, thus involving both human-recorded audio and algorithmic signal mixing."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The video data consists of mouth-tracks extracted from YouTube videos (human-recorded video data). To simulate overlap, video frames are shifted and non-speech regions are filled with repeated frames, which is a model-generated manipulation, making the origin both human and model generated."
        }
      ]
    }
  },
  {
    "id": "rose22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8839,
      "completion_tokens": 202,
      "total_tokens": 9041
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 mentions that overlapped and single speaker test sets were obtained from human transcribed utterances with aligned mouth tracks taken from YouTube videos, indicating that multiple human experts performed the transcription annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide any explicit description or mention of detailed annotation instructions or guidelines given to annotators for the transcription task."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention in the paper of scoring rubrics or specific evaluation criteria provided for annotation quality or transcription consistency."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not present any examples of annotated data or annotation examples in the guidelines or descriptive text."
          }
        }
      ]
    }
  },
  {
    "id": "rose22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10029,
      "completion_tokens": 321,
      "total_tokens": 10350
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of the dataset annotations was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information or description of quality assurance involving multiple human non-expert annotators is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset is created via automated methods for mixing single speaker YouTube utterances into overlapping speech scenarios, the paper does not describe any automated quality assurance or verification process conducted on annotations or the dataset content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new simulated audio-visual overlapping speech dataset created by mixing single-speaker YouTube utterances with offsets and generated overlap. However, it does not describe any specific quality assurance process for verifying the accuracy or consistency of the annotations or data content. Therefore, no quality assurance process is documented or described."
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9639,
      "completion_tokens": 548,
      "total_tokens": 10187
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states that a training set and test sets of simulated two-speaker overlapped utterances were created from a corpus of single speaker audio-visual YouTube utterances containing aligned face and mouth tracks. The YouTube utterances are collected as original data, and the simulated overlapped utterances are constructed by mixing pairs of these human-created utterances. Thus, the original single-speaker audio-visual utterances are new data collected from human contributors, as described in Section 3.1."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets were not generated entirely by AI or machine learning models. Instead, existing YouTube utterances recorded by humans were used to create the datasets, and overlapping utterances were simulated by mixing these utterances."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being produced by translation from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine-translated data in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper explains that the training data are created by aggregating single-speaker utterances from an existing YouTube corpus. These utterances, collected from different videos, are combined to form overlapped speech utterances by mixing audio and aligning mouth tracks. This constitutes a form of data collation from existing sources without significant modification to the base data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The two-speaker overlapped utterances are derived datasets created by mixing two single-speaker utterances with offsets and combining their videos to simulate overlapping speech. Video frames in non-speech regions are modified by repeating frames forward and backward to ensure continuous visual presence, which is a form of modification and adaptation of the original data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides explicit details about the derivation and creation of the datasets, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10157,
      "completion_tokens": 269,
      "total_tokens": 10426
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper describes creation of a new simulated two-speaker A/V overlapped utterance dataset from single speaker YouTube utterances, which is used as training data for the multi-talker models including the VCAM. The dataset of 20k hours is specifically used for training models from scratch as indicated in Sections 2 and 3."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The paper uses the same newly created simulated overlapped and single speaker datasets as test sets for benchmarking and evaluating the model performance (WER, active speaker detection mAP). This indicates the datasets are used for evaluation and performance measurement."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10880,
      "completion_tokens": 582,
      "total_tokens": 11462
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The new dataset introduced, created from YouTube videos, is described as a corpus of single speaker audio-visual utterances. There is no indication of multiple languages; all content is implied to be in English. Therefore, the dataset is not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The dataset consists of single speaker utterances synthesized into overlapping speech, but no mention is made of two languages per entry. Thus, it is not bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is created from YouTube utterances with aligned face and mouth tracks, as per references [18, 22, 5, 8], which are known to be English language datasets. The paper does not indicate any other languages are included. Hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of a dataset containing only a non-English language is made for the new data introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains audio-visual speech data, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The entries of the dataset are audio-visual speech data without mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves human speech and visual mouth tracks, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is specified to be English, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language speech data (English), so language is present."
        }
      }
    }
  },
  {
    "id": "rose22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8098,
      "completion_tokens": 233,
      "total_tokens": 8331
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any link, URL, or reference to publicly available code related to the dataset creation, data collection, or preprocessing steps. There is no explicit statement indicating that code used for constructing the dataset is available to the public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Simulated audio-visual overlapping speech corpora)",
          "reasoning": "Section 3.1 extensively documents the process of constructing the simulated two-speaker overlapped utterances dataset from a corpus of single speaker A/V YouTube utterances. The paper describes the method of offsetting and mixing audio signals, alignment and offsetting of video mouth tracks, handling of non-speaking video frames by repetition, simulation of single speaker scenarios (TwoFace and OneFace), offset selection for overlap intervals, and size and composition of the training and test sets. This description provides sufficient detail on dataset construction for understanding and potential reproduction of the simulation methodology, although it does not indicate publication of the original single speaker source data."
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7508,
      "completion_tokens": 176,
      "total_tokens": 7684
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 and Section 2",
          "Reasoning": "The kidsTALC dataset introduced in this paper consists of recorded audio from German children's speech collected through examiner-child interactions using human-operated devices (Olympia dictaphone). The speech is naturally spoken by children aged 3 \u00bd to 11 years, captured and manually transcribed by humans, indicating human-generated origin."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The dataset includes manual orthographic and phonetic transcriptions of the audio recordings by trained human transcribers and transcription agencies, ensuring text data originates from human generation directly related to the collected audio speech data."
        }
      ]
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8308,
      "completion_tokens": 214,
      "total_tokens": 8522
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.3 Data Transcription and Annotation",
            "reasoning": "The paper states that transcriptions and annotations are done by trained graduate students of speech language therapy, trained speech language therapists, and a professional transcription agency, indicating multiple human expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Data Transcription and Annotation",
            "reasoning": "A transcription protocol incorporating a three-step procedure for each recording is applied to ensure reliability and consistency, which indicates the presence of detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3 Data Transcription and Annotation",
            "reasoning": "The paper describes use of a transcription protocol and multi-step checking process, but does not mention any formal scoring rubrics or rating scales for the annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any annotation examples or mention examples included in annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9498,
      "completion_tokens": 374,
      "total_tokens": 9872
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes multiple annotators involved in QA rather than a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Data Transcription and Annotation",
          "reasoning": "Quality assurance involves multiple human annotators who are trained graduate students of speech language therapy, trained speech language therapists (experts), and a professional transcription agency. The process includes initial transcription, checking by a different transcriber, and a final check by a professional transcription agency, ensuring adjudication until consensus is reached."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists that QA was done by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as trained speech language therapists or students (experts), not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models being used for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of automated verification or algorithmic QA processes used for annotation validation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A rigorous multi-step human quality assurance process is described, so the dataset does not lack QA."
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9108,
      "completion_tokens": 488,
      "total_tokens": 9596
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Data Acquisition and Section 2.1 Participants",
          "reasoning": "The paper details the collection of a novel dataset called kidsTALC consisting of audio recordings and manual orthographic and phonetic transcriptions of spontaneous German children's speech. The data was collected directly from human participants (children aged 3\u00bd\u201311 years) in examiner-child interactions, recorded in their natural environments (home or kindergarten), with no indication that the data was adapted or translated from existing sources. Section 2 explains that the data was recorded and transcribed by trained humans following specific transcription protocols ensuring high quality. Therefore, the data is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any part of the dataset was generated solely or partially by AI or machine learning models. The speech data consists of real recordings of children's spontaneous speech and manually created transcriptions."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and transcriptions do not involve translation from another language. The speech recordings are originally in German. No mention of translation or bilingual corpus creation is made."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that machine translation was used for generating the dataset or its annotations."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not an aggregation of existing datasets or corpora. It is newly recorded and transcribed from scratch according to the paper."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not derived from existing datasets or transformed from previous sources. Instead, it was newly collected and transcribed manually."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are clearly specified and documented in the paper."
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9626,
      "completion_tokens": 296,
      "total_tokens": 9922
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4. Automatic Speech Recognition",
          "reasoning": "The dataset is used to train an end-to-end automatic speech recognition model from scratch using supervised learning methods, as described in Section 4, where a baseline model is trained using the kidsTALC dataset for phone recognition with connectionist temporal classification (CTC) loss."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4. Automatic Speech Recognition",
          "reasoning": "The dataset is also used as a benchmark for evaluating the performance of the baseline ASR model by measuring the phone error rate (PER) on the train and test subsets."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3. Statistical Analysis",
          "reasoning": "The dataset is analyzed statistically in Section 3 to assess transcription quality (transcriber agreement), age distribution of participants, word and phone distributions, providing insights into the characteristics and challenges of the data."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10349,
      "completion_tokens": 420,
      "total_tokens": 10769
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 2.1 Participants",
          "reasoning": "The kidsTALC dataset contains speech data collected specifically from monolingual German children aged 3 \u00bd to 11 years. The paper explicitly states that the dataset focuses on 'typically developing, monolingual German children' (Section 1) and the participants' eligibility criteria include monolingual German speakers (Section 2.1). Therefore, the dataset entries contain exactly one language which is non-English (German)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data consisting of German child speech with orthographic and phonetic transcriptions, so it clearly contains entries with language."
        }
      }
    }
  },
  {
    "id": "rumberg22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7567,
      "completion_tokens": 146,
      "total_tokens": 7713
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not specified in the paper",
          "reasoning": "The paper mentions using SpeechBrain for the ASR baseline and adjusting the recipe for their dataset, but does not provide any direct link or statement about making the code used for data collection, transcription, or preprocessing publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 through 3",
          "reasoning": "The paper thoroughly documents the dataset creation process including participant recruitment, recording setting, transcription and annotation protocols, phonetic token set, and statistical analysis of the dataset. These details are found primarily in Sections 2 and 3, providing transparency and completeness on how the dataset was constructed."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6494,
      "completion_tokens": 96,
      "total_tokens": 6590
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Experimental Setting",
          "Reasoning": "The kidsTALC corpus is described as consisting of speech of 3 \u00bd \u201311 years old German speaking children, indicating human recorded audio data of children's speech. The corpus is explicitly stated as publicly available and used for phonetic transcription, confirming its human-generated origin as recorded speech audio."
        }
      ]
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7294,
      "completion_tokens": 241,
      "total_tokens": 7535
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3 Experimental Setting; Section 4.2 Relation to token changes during re-annotation",
            "reasoning": "Section 3 states that re-annotations of 500 utterances of kidsTALC were performed by trained speech language therapists, indicating multiple human experts performed the new annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 Experimental Setting",
            "reasoning": "Section 3 describes that for re-annotation, orthographic transcription was used with multiple pronunciations suggested per word and the possibility for transcribers to edit or add new transcriptions, indicating structured instructions were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 Experimental Setting",
            "reasoning": "The paper does not mention any scoring rubrics or formal evaluation criteria provided to annotators for re-annotation; only instructions for transcription choices were described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 Experimental Setting",
            "reasoning": "The paper does not include any mention of example annotations or sample annotated data given to annotators as part of the guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8484,
      "completion_tokens": 311,
      "total_tokens": 8795
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "Section 3 describes that for re-annotation of kidsTALC, annotators were trained speech language therapists, who are subject matter experts. They re-annotated 500 utterances and performed phonetic transcriptions, indicating that quality assurance was performed by single human experts."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The re-annotation process involved multiple human annotators who are trained speech language therapists, thus multiple human experts conducted quality assurance of the kidsTALC annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that any single human non-expert conducted quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper suggests that multiple human non-experts were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model being used as a judge for quality assurance of dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that automatic verification algorithms or rule-based techniques were used to conduct quality assurance on dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents a QA process via expert re-annotation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8094,
      "completion_tokens": 427,
      "total_tokens": 8521
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Setting) and Section 4.2",
          "reasoning": "The paper describes re-annotation of 500 utterances from the kidsTALC corpus by trained human annotators (speech language therapists) creating new phonetic transcriptions. This re-annotation represents original content created entirely from scratch by human contributors and not derived or adapted from existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any completely synthetic data generated solely by models; all datasets used are either existing corpora or human-produced annotations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data produced by human translation from another language is present in the paper."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation from another language is present in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets (TIMIT, MCV, kidsTALC) are used as-is; there is no indication that the authors collected or aggregated data from existing sources to create new datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors utilize existing datasets and provide re-annotations, but do not appear to create derived datasets by modification or transformation of existing corpora beyond annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the origin of the re-annotated data as human-created, clearly outlining the source and method of its generation."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8612,
      "completion_tokens": 503,
      "total_tokens": 9115
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate using the introduced dataset for pre-training models."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The introduced dataset kidsTALC is not used for training models from scratch; the models are pre-existing and the authors do not report training from scratch using kidsTALC."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses pre-trained models and fine-tuned wav2vec2.0 models on other data but does not explicitly state that the newly introduced kidsTALC dataset is used for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning post-training techniques such as RLHF related to the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3 (Experimental Setting), 4 (Results and Discussion), especially 4.1 and 4.2",
          "reasoning": "The newly introduced kidsTALC corpus is used as a test set for evaluating uncertainty estimation methods. The paper holds out test and validation sets from kidsTALC and uses them to benchmark predictive uncertainty and error detection capabilities of the ASR models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The paper uses the kidsTALC dataset to analyze the relationship between model uncertainty and human annotator uncertainty by re-annotating 500 utterances and comparing token changes with model uncertainty estimates."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not use the new dataset as an external knowledge base for augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The new dataset has demonstrated practical use in both evaluation and analysis as documented in the paper."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9335,
      "completion_tokens": 562,
      "total_tokens": 9897
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The new dataset introduced by the authors is kidsTALC, which contains only German children's speech. There is no mention of more than two human languages in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The proposed dataset, kidsTALC, contains speech in exactly one language: German. There is no indication it contains two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "KidsTALC is a corpus of German children\u2019s speech; it is not in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3 (Experimental Setting); Section 1 (Introduction)",
          "reasoning": "The kidsTALC corpus is described as 'a publicly available child speech corpus' containing speech of German-speaking children. Therefore, it contains exactly one non-English language, German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The dataset is speech data and phonetic transcriptions, with no programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "While the paper contains mathematical formulas describing the CTC uncertainty calculation, the dataset entries themselves do not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The dataset consists of human speech recordings and annotations, no biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "No presence of fictional or artificially created languages is mentioned for the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "No section",
          "reasoning": "The language of the dataset is clearly stated as German for kidsTALC; hence, language is specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The kidsTALC dataset is human speech data and thus contains language."
        }
      }
    }
  },
  {
    "id": "rumberg23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6553,
      "completion_tokens": 172,
      "total_tokens": 6725
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not include any link or mention of publicly available code related to dataset construction, collection, preprocessing, or generation. The paper notes using existing datasets (TIMIT, kidsTALC, Mozilla Common Voice) and pretrained models without own training or code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Experimental Setting)",
          "reasoning": "The paper provides information about the used datasets, their composition, and the re-annotation process for kidsTALC including the selection procedure and treatment of annotations. While it does not describe the original dataset collection in detail (as it uses existing datasets), it documents the re-annotation protocol for kidsTALC in sufficient detail relevant to the new annotations introduced for this study."
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7764,
      "completion_tokens": 151,
      "total_tokens": 7915
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data collection",
          "Reasoning": "The MuPTA corpus consists of audio recordings of spontaneous and read speech collected from 30 native Russian speakers using handheld devices, explicitly recorded by humans, as described in Section 3.1."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data collection",
          "Reasoning": "The MuPTA corpus includes video recordings captured simultaneously with audio via smartphones and tablet cameras, recording facial expressions and behavior during speech, thus human-generated video data as per Section 3.1."
        }
      ]
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8564,
      "completion_tokens": 270,
      "total_tokens": 8834
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.2 Data annotation",
            "reasoning": "Section 3.2 states that each speaker (informant) filled in a self-evaluation questionnaire. The annotation is based on self-evaluation by the informants themselves, which corresponds to a single human expert source per example (the subject themselves)."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Data annotation",
            "reasoning": "The self-evaluation questionnaire used is a standard questionnaire of 60 questions [31], which implies the presence of detailed instructions for the subject to fill it, as it is a standardized psychological instrument."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Data annotation",
            "reasoning": "Since the annotation is derived from a standard self-evaluation questionnaire measuring the Big Five traits (reference [31]), scoring rubrics (scoring keys or guidelines) for mapping questionnaire responses to personality trait scores are inherently part of the questionnaire design."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention providing examples in the annotation guidelines or instructions for the annotators (subjects filling the questionnaire). Thus, examples are not confirmed to be present."
          }
        }
      ]
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9754,
      "completion_tokens": 304,
      "total_tokens": 10058
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions that personality traits annotation is based on self-evaluation questionnaire filled by the speakers themselves, but there is no indication that a single human expert performed quality assurance on the annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about multiple experts performing quality assurance or validating the annotations in the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of multiple non-expert human annotators performing quality assurance or validation of the dataset annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or description in the paper that AI models were used for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any automated verification or algorithmic quality assurance performed on the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The MuPTA corpus was annotated via self-evaluation questionnaires filled by the speakers themselves. The paper does not describe any additional quality assurance or validation process for these annotations, nor any external validation or multiple annotator agreement measures. Therefore, no quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9364,
      "completion_tokens": 352,
      "total_tokens": 9716
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection",
          "reasoning": "The MuPTA corpus was collected by recording 30 native Russian speakers performing specific tasks including spontaneous speech and reading scripted sentences. The data consists of original audio-visual recordings created entirely from scratch by human contributors (the speakers), not adapted or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus data consists of human recordings; no data was generated by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data being produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by machine translation systems as per the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MuPTA corpus is not an aggregation of existing sources; it was newly recorded."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the dataset is based on or adapted from existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation process are explicitly documented."
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9882,
      "completion_tokens": 269,
      "total_tokens": 10151
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 4 and 5",
          "reasoning": "The MuPTA corpus is utilized to train and fine-tune the proposed audio-visual neural network models for personality traits assessment using supervised learning methods as described in the methodology and results sections."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The MuPTA corpus is used to evaluate and benchmark the performance of the proposed approach in assessing personality traits, with analysis of metrics such as accuracy and concordance correlation coefficient."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5",
          "reasoning": "The authors analyze the impact of speech type (spontaneous vs. read) on PTA performance and investigate performance trends across different signal lengths and modalities using the MuPTA corpus."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10605,
      "completion_tokens": 390,
      "total_tokens": 10995
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data collection; Table 1",
          "reasoning": "The MuPTA corpus contains data from 30 native Russian speakers. The paper explicitly states that the corpus is in the Russian language only (midly-resourced Russian language), including speech data and scripted sentences in Russian. Table 1 compares MuPTA to other corpora and lists its language as 'Ru' (Russian). Therefore, the dataset entries are monolingual in a non-English language, namely Russian."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ryumina23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7823,
      "completion_tokens": 156,
      "total_tokens": 7979
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 6, Conclusions",
          "reasoning": "The paper states in the Conclusions section that both the source code of the proposed approach and the MuPTA corpus can be found at the web-page https://oceanai.readthedocs.io, indicating that code related to dataset processing and the approach is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3, Data Collection and Data Annotation",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 3, describing data collection devices, recording setup, speech tasks (spontaneous and read speech), synchronization procedures, and annotation via self-evaluation questionnaires. This provides transparency on the dataset construction process."
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 2917,
      "completion_tokens": 243,
      "total_tokens": 3160
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, paragraph describing MuPTA corpus",
          "Reasoning": "The MuPTA corpus is introduced by the authors (reference [3]) as their own multimodal personality traits assessment dataset. Given it includes video modality as used in their evaluation, and such data is recorded from human participants, this video data is human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, paragraph describing MuPTA corpus",
          "Reasoning": "The MuPTA corpus includes audio recordings corresponding to the personality traits assessment tasks. As this data is recorded from humans speaking, it is human generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, paragraph describing MuPTA corpus",
          "Reasoning": "The text modality in the MuPTA corpus comes from speech transcripts of humans. The Whisper model is used for automatic speech recognition, converting human spoken language into text, indicating that the original text content is human generated."
        }
      ]
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 3717,
      "completion_tokens": 220,
      "total_tokens": 3937
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 2",
            "reasoning": "The paper describes that all the processing modules extract features using models such as Face Mesh, Emo-Affectnet, VGG16, Whisper, BERT, and others, indicating the annotation or scoring process is performed automatically by AI models within the framework rather than human annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any annotation instructions provided to human annotators for the newly introduced MuPTA corpus or any data annotation process performed by humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no description or mention of scoring rubrics used for annotation in the MuPTA corpus or any other new dataset introduced by the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide any examples related to annotation or labeling guidelines for the new MuPTA dataset or any other annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 4907,
      "completion_tokens": 389,
      "total_tokens": 5296
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was performed by a single human expert on the newly introduced MuPTA corpus."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts providing quality assurance or annotations for the MuPTA dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper lacks any description of quality assurance conducted by a single non-expert for the new dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided in the paper that multiple non-expert annotators performed quality assurance on the MuPTA dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for feature extraction and personality trait assessment, the paper does not describe using AI models as judges or for quality assurance of the dataset itself."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of an automatic or algorithmic quality assurance process verifying the MuPTA dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe or document any quality assurance process applied to the newly introduced MuPTA corpus. Therefore, it is concluded that no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 4517,
      "completion_tokens": 401,
      "total_tokens": 4918
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: 'Using our framework, we also obtained initial baselines for our MuPTA corpus [3]'",
          "reasoning": "The MuPTA corpus is introduced by the authors as their own dataset, implying it was collected anew, presumably from human participants, as typically collected multimodal human behavior data. The paper mentions it alongside the existing First Impressions v2 corpus but differentiates MuPTA as introduced by the authors. This suggests original content collected from human contributors rather than derived or collated data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data introduced is not generated by AI or machine learning models; rather, models are applied to real human data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was machine-translated."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "MuPTA is presented as a newly introduced dataset rather than aggregated from existing data without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the introduced dataset is constructed by modifying or adapting existing datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the MuPTA dataset is specified as new data introduced by the authors."
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 5035,
      "completion_tokens": 465,
      "total_tokens": 5500
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MuPTA corpus or any new dataset introduced for pre-training models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper of training models from scratch using the new MuPTA corpus. The training is mentioned generally but without clarifying that it is from scratch on this new dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2, last paragraph",
          "reasoning": "The paper states that the OCEAN-AI framework was tested on their new MuPTA corpus and achieved initial baselines. This implies the dataset is used for supervised fine-tuning of the model to predict personality traits."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that reinforcement learning or RL-based methods are applied to the new dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2, last paragraph",
          "reasoning": "The MuPTA corpus is used to establish initial baselines and evaluate the framework's performance, indicating its use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MuPTA corpus specifically for trend or pattern analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as a knowledge base for retrieval or augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The MuPTA corpus is actively used for supervised fine-tuning and evaluation as described in the paper."
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 5758,
      "completion_tokens": 225,
      "total_tokens": 5983
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2, Text information analysis module",
          "reasoning": "The text information analysis module uses the LIWC set intended for psychological analysis of English texts, indicating that the new MuPTA corpus introduced contains English language content only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ryumina24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 2976,
      "completion_tokens": 254,
      "total_tokens": 3230
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2, paragraph starting with 'Source codes [7] and a software documentation [8] of OCEAN-AI are free-available.'",
          "reasoning": "The paper mentions that source code and software documentation for the OCEAN-AI framework are freely available, but this relates to the PTA framework and not explicitly to the code used for constructing the new MuPTA dataset, which is introduced by the authors. There is no explicit mention or link indicating that the code used for collecting or preprocessing the MuPTA dataset itself is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 and throughout the paper, especially the paragraph describing the OCEAN-AI Framework Architecture and the mention of their MuPTA dataset in evaluation.",
          "reasoning": "The paper introduces the MuPTA corpus as a new dataset and references prior publications [3] for more details. While not extensively detailed in this specific paper, some documentation about the dataset creation process is provided, notably in the context of its use and initial baselines. The paper provides enough description to understand the dataset's purpose and general characteristics, implying some degree of documentation on the dataset's creation."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7105,
      "completion_tokens": 156,
      "total_tokens": 7261
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Voice recording",
          "Reasoning": "The CALLS corpus contains speech audio data recorded by the same female speaker acting as an operator, who uttered phone-call lines in a recording studio, thus the audio data are human-recorded."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Crowdsourcing phone-call dialogue lines",
          "Reasoning": "The dialogue lines (text) were written by human crowdworkers via microtask crowdsourcing, following specified instructions to create dialogues between operator and customers, constituting human-generated text data for the corpus."
        }
      ]
    }
  },
  {
    "id": "saito23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7905,
      "completion_tokens": 211,
      "total_tokens": 8116
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 and Section 3.1",
            "reasoning": "Dialogues lines were crowdsourced via microtask crowdsourcing from around 150 workers for complaints and 1400 workers for attentive listening subsets. These crowdworkers are non-experts who produced dialogue lines based on instructions."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Section 2.2 details that crowdworkers were given instructions describing task, persona, dialogue turns, and explicit writing instructions including anonymization and using specific dialogue formats."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention any scoring rubrics or formal evaluation guidelines for the annotation (dialogue line writing) process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No mention of providing example dialogues or annotation examples to the crowdworkers is present in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "saito23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9095,
      "completion_tokens": 313,
      "total_tokens": 9408
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that multiple human experts conducted quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Crowdsourcing phone-call dialogue lines",
          "reasoning": "Quality assurance was done by employing approximately 150 and 1,400 crowdworkers to write dialogue lines. The authors reviewed and excluded spam and non-compliant lines, and proofread all collected lines to remove those with offensive expressions or lacking positive emotions. Since the crowdworkers are non-experts and multiple were involved, quality assurance is conducted by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of any AI model to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of any automated verification or rule-based checks performed as quality assurance on the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes human-based review and filtering steps for quality assurance; thus QA is documented and applied."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8705,
      "completion_tokens": 467,
      "total_tokens": 9172
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 and 2.3",
          "reasoning": "The paper states that dialogue lines for simulated customer-center phone calls were crowdsourced from human workers, who wrote these dialogues based on provided scenarios. The speech recordings of these lines were performed by a single female speaker acting as the operator. Hence, the new dataset consists of original content created from scratch by human contributors via crowdsourcing and human voice recordings, without translation or derivation from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated entirely by AI or machine learning models without referencing existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of the dataset being produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation was applied or stated as part of data generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset scenarios for complaint handling were created by selecting and referencing existing complaint data from the FKC corpus, which contains approximately 5 million text complaints. Although the dialogue lines themselves were written by crowdsourcing, they were based on existing complaint data extracted from the FKC corpus, indicating the dataset incorporates collated content from existing sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset uses complaint data from the FKC corpus to create dialogue scenarios, the dialogues themselves are newly written rather than modified versions of existing dialogues. There is no clear evidence of transformations or adaptations applied to existing dialogue lines."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data collection methodology and source of content."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9223,
      "completion_tokens": 423,
      "total_tokens": 9646
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "In Section 4, the authors describe training EDSS models from randomly initialized parameters using the CALLS corpus for developing empathetic dialogue speech synthesis. They explicitly mention the use of the CALLS corpus for training an acoustic model and a neural vocoder, indicating training from scratch after pretraining on a separate corpus (JSUT)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The paper details fine-tuning the FastSpeech 2 model pretrained on the JSUT corpus with the CALLS corpus training data for EDSS tasks, representing supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning or RL-based post-training methods using the CALLS corpus."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The authors perform MOS tests to evaluate naturalness and speaking-style similarity of synthetic speech generated using models trained with the CALLS corpus, indicating its use for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4",
          "reasoning": "Section 3.4 provides analysis of domain differences between the CALLS and STUDIES corpora in prosodic and textual features, showing use of the CALLS corpus primarily for analyzing characteristics of empathetic spoken dialogues."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CALLS corpus is not described or used as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates multiple practical uses of the CALLS corpus in training, analysis, and evaluation stages."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9946,
      "completion_tokens": 399,
      "total_tokens": 10345
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CALLS corpus introduced in the paper contains only Japanese language data. There is no mention of multiple languages being used in the corpus."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CALLS corpus contains dialogue lines entirely in Japanese. There are no indications that a second human language is included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is recorded in Japanese and the paper extensively discusses Japanese speech data, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 2. Corpus Construction Methodology",
          "reasoning": "The CALLS corpus is a Japanese speech corpus, as stated in the abstract and throughout the paper. All dialogues and speech recordings are in Japanese only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of code or programming language data in the CALLS corpus. The dataset consists of spoken dialogue and text transcripts in Japanese."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical notations are present in the dataset according to the paper."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human dialogue speech only; no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any fictional or artificial languages in the corpus. It only contains Japanese language data."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly described as Japanese, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset explicitly contains spoken and textual Japanese dialogue data containing human language."
        }
      }
    }
  },
  {
    "id": "saito23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7164,
      "completion_tokens": 161,
      "total_tokens": 7325
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not mention any link or repository containing the code used for data collection, preprocessing, or corpus generation. While a project page URL is provided for the corpus, no explicit mention or link to code resources is found."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Corpus Construction Methodology, Sections 2.1 to 2.3",
          "reasoning": "The paper provides a detailed explanation of the corpus construction process, including dialogue scenario design (Section 2.1), crowdsourcing procedure for dialogue lines (Section 2.2), and voice recording methodology (Section 2.3). This clear and transparent documentation enables understanding of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7391,
      "completion_tokens": 117,
      "total_tokens": 7508
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.2; Section 3.2",
          "Reasoning": "The SRC4VC corpus contains 11 hours of speech recorded on smartphones by 100 Japanese speakers, captured through crowdsourcing with humans actively recording their voices using smartphones in their own rooms. The data consists of actual human voice recordings, verified by manual validation, and is explicitly described as smartphone-recorded audio samples, establishing human generation and excluding model or unknown origins."
        }
      ]
    }
  },
  {
    "id": "saito24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8191,
      "completion_tokens": 273,
      "total_tokens": 8464
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.3 Annotation, Table 1",
            "reasoning": "The annotation tasks for speaker-wise recording quality scores and utterance-wise perceived emotion labels were crowdsourced to multiple crowdworkers (non-experts). Table 1 lists 400, 500, and 500 crowdworkers involved in various annotation tasks, indicating multiple human non-experts performed annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Voice recording by crowdworkers",
            "reasoning": "The recording platform included instructions for recording for each subset, which implies detailed annotation instructions were provided to speakers (annotators for recording). This suggests that the crowdsourced annotators were guided by instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 Corpus Analysis, Table 1",
            "reasoning": "The recording quality ratings used a five-scale mean opinion score (MOS) test from 1 (very bad) to 5 (very good), which functions as a scoring rubric guiding the annotators to rate quality consistently."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not mention providing annotation examples or example annotations in the guidelines for the quality rating or emotion labeling tasks for crowdworkers."
          }
        }
      ]
    }
  },
  {
    "id": "saito24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9381,
      "completion_tokens": 327,
      "total_tokens": 9708
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance; the annotations were done via crowdsourcing without mention of expert annotators."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Sections 2.3 and 3.1",
          "reasoning": "Speaker-wise recording quality scores and utterance-wise perceived emotion labels were obtained via crowdsourcing, involving multiple human annotators who are crowdworkers without specified subject matter expertise, indicating quality assurance by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.5",
          "reasoning": "The paper reports computation of correlations between human-annotated recording quality scores and NISQA scores, where NISQA is an AI model that predicts speech quality, reflecting automatic AI-based quality measurement used to assess annotation reliability."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification via code or rule-based algorithmic quality assurance is described for validating dataset annotations or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance exists both through multiple human non-expert annotations and AI model quality estimation; thus, the category N/A does not apply."
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8991,
      "completion_tokens": 462,
      "total_tokens": 9453
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Sections 1, 2.1, 2.2",
          "reasoning": "The SRC4VC dataset consists of 11 hours of speech data recorded by 100 Japanese speakers using their own smartphones, collected via crowdsourcing. The speech samples cover various styles such as read-aloud, expressive, conversational, and singing, and were recorded freshly by human participants specifically for this corpus. The paper explicitly describes that speakers recorded their voices in their own environments using a web-based recording platform. This data is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated by AI or machine learning models; it consists of human voice recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any of the data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation being used in generating the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SRC4VC dataset was recorded specifically for this work and is not a mere aggregation of existing data without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The paper mentions that the corpus includes sentences selected from existing corpora (ITA, JVNV, STUDIES, CALLS) and Japanese songs, which were read or sung by the speakers. Thus, while the audio recordings are newly produced, the verbal content is derived from existing sources with adaptation for recording."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is well documented and described in the paper."
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9509,
      "completion_tokens": 273,
      "total_tokens": 9782
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 - Any-to-Any VC Experiment",
          "reasoning": "The SRC4VC dataset is used as evaluation data in an any-to-any voice conversion task where a model trained on high-quality speech is tested on SRC4VC speakers' voice samples. The study demonstrates how recording quality mismatch affects VC performance, thus using SRC4VC primarily for benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 - Corpus Analysis",
          "reasoning": "The authors perform detailed analyses of SRC4VC including signal-to-noise ratio statistics, recording device diversity, speaker demographics, and annotation results for recording quality and perceived emotions. This indicates usage of the dataset for analyzing trends, patterns, and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10232,
      "completion_tokens": 614,
      "total_tokens": 10846
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SRC4VC corpus contains speech recorded from Japanese speakers only, as repeatedly referenced in the abstract, Sections 1, 2, and 3. There is no mention of more than two languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus includes only Japanese language speech samples, with no indication of a second human language included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not English-only; it is specifically described as Japanese speech corpus. No English speech content was indicated as the core data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 1 Introduction, Section 2 Construction of SRC4VC",
          "reasoning": "The dataset SRC4VC contains speech recorded exclusively in Japanese by 100 Japanese speakers, as noted throughout the paper (e.g., Abstract: \"100 Japanese speakers\", Section 2: \"Japanese copyright-free songs\", Section 3: \"speech corpora related to SRC4VC with Japanese language descriptions\"). Therefore, it is a monolingual, non-English dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code-related content is present in the dataset. The corpus consists solely of speech recordings and their annotations."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are part of the dataset entries. The references to mathematical notation in citations or methods are unrelated to the dataset content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed only of human speech recordings, with no biological sequences or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any constructed or fictional languages in the dataset. All data is real Japanese language speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as Japanese throughout the paper; hence, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are speech recordings containing human language (Japanese); thus, the dataset does contain language."
        }
      }
    }
  },
  {
    "id": "saito24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7450,
      "completion_tokens": 178,
      "total_tokens": 7628
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper mentions that the SRC4VC corpus is open-sourced for research purposes at a given URL, but does not provide any link or mention of code availability related to data collection, preprocessing, or dataset construction. No code repository or code release is cited anywhere in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper provides extensive documentation of the dataset creation process including the design of SRC4VC (Section 2.1), voice recording procedure by crowdworkers via a web platform (Section 2.2), detailed annotation procedures (Section 2.3), and corpus analysis including speech quality and speaker distribution (Section 3). This thorough description provides transparency and completeness of the data collection and annotation."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7245,
      "completion_tokens": 617,
      "total_tokens": 7862
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 CSF2022 Dataset",
          "Reasoning": "The new dataset introduced in the paper consists of videos recorded using a webcam from a professional French cuer uttering sentences in French Cued Speech. The paper explicitly states that videos of 1920x1080 pixels at a target framerate of 60fps were recorded. This video data was captured via human involvement, specifically a human cuer performing and being recorded, and using human-operated recording tools."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 CSF2022 Dataset",
          "Reasoning": "Alongside the video recording, audio was also recorded at 44.1 kHz PCM, 16 bits, from the same human cuer during the performance of French Cued Speech sentences. Though not used for this study, it is part of the new dataset and was recorded via human involvement."
        },
        {
          "Modality": "time series",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Visual Features",
          "Reasoning": "From the recorded videos, visual features consisting of hand and lips pose coordinates were automatically extracted via the Mediapipe software. The 21 hand and 42 lip x/y coordinates per frame were processed and reduced by PCA and clustering (k-means) to generate fixed-dimension time series feature vectors. These feature sequences are algorithmically generated representations, thus model generated rather than human generated."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 CSF2022 Dataset",
          "Reasoning": "The phonetic transcription for the new dataset was created automatically using the LIA-Phon phonetizer and then manually verified and corrected. Thus, the initial phonetic transcription is model generated (automatic phonetic transcription), but then corrected by humans. While the final data includes human revision, the origin includes model generation due to automatic initial transcription."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 CSF2022 Dataset",
          "Reasoning": "The sentences themselves were originally selected from existing text sources such as the SIWIS database, French parliament debates, and French novels. These texts are human-generated language content. The new dataset consists of recordings of these sentences being cued by a human speaker."
        }
      ]
    }
  },
  {
    "id": "sankar23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8045,
      "completion_tokens": 220,
      "total_tokens": 8265
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1 - CSF2022 Dataset",
            "reasoning": "Section 3.1 states that the phonetic transcription was \"verified/corrected manually\" and that \"a set of 20 sentences was manually annotated with the ELAN software\" for temporal segmentation. These manual annotations indicate that an expert performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide any explicit mention of detailed annotation instructions given to the annotator(s) for the manual phonetic transcription correction or segmentation annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no description or reference to scoring rubrics or detailed criteria for annotator decision making in the manual annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not mention or provide example annotations or guidelines including examples for annotators in the manual annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "sankar23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9235,
      "completion_tokens": 335,
      "total_tokens": 9570
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 CSF2022 Dataset",
          "reasoning": "The phonetic transcription of the recorded sentences was obtained automatically and then verified and corrected manually. This manual verification implies that a human annotator, likely an expert given the specialized nature of phonetic transcription in French Cued Speech, performed quality assurance on the annotations. There is no mention of multiple annotators for this task."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple expert annotators or any quality assurance involving multiple experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the manual annotation was performed by a non-expert individual."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple non-expert annotators for quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance was not performed by an AI model as a judge for dataset annotation or content validation."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 CSF2022 Dataset",
          "reasoning": "Phonetic transcriptions were initially obtained automatically using the LIA-Phon phonetizer before manual verification and correction. This indicates an automated process was part of the annotation quality assurance pipeline."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes quality assurance steps involving automatic phonetic transcription followed by manual correction, thus QA processes are documented and applied."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8845,
      "completion_tokens": 402,
      "total_tokens": 9247
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states that a new dataset called CSF2022 was recorded consisting of 1087 sentences by a professional French cuer. The sentences were selected from various French language sources and recorded specifically for this study, indicating that the dataset was created entirely from scratch by human contributors through new recordings."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any part of the dataset was generated by AI or machine learning models. The data consists of human recorded videos."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains translated content produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely collected or aggregated from existing sources. It is newly recorded data, not existing video or data aggregated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the phonetic transcriptions were obtained using LIA-Phon phonetizer and verified manually, the core dataset consists of newly recorded videos and not derived from existing data with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation is clearly specified; thus, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9363,
      "completion_tokens": 473,
      "total_tokens": 9836
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the new dataset being used for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4.1",
          "reasoning": "The new CSF2022 dataset is used to train the proposed automatic cued speech recognition model from scratch, as described in Section 3.1 where the dataset is introduced and in Section 4.1 where model performance is reported after training on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the new dataset for supervised fine-tuning of a pre-trained model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset is used for reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The new dataset is used for evaluation purposes, including performance measurement and benchmarking of the proposed recognition system."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The dataset is used to analyze hand-lip temporal dynamics by extracting and interpreting attention maps and segmentation boundaries, thus serving to study characteristics and patterns of cued speech production."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Practical usages of the dataset for training, evaluation, and analysis are documented in the paper."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10086,
      "completion_tokens": 397,
      "total_tokens": 10483
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The new dataset CSF2022 is recorded for French Cued Speech (LfPC), using sentences selected from French language sources such as French parliament debates and French novels. The phonetic transcription uses 36 phonetic categories to describe French, as stated in Section 3.1. Thus, the dataset entries are monolingual in French, which is a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language data, specifically French Cued Speech with corresponding phonetic transcriptions."
        }
      }
    }
  },
  {
    "id": "sankar23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7304,
      "completion_tokens": 182,
      "total_tokens": 7486
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 (CSF2022 Dataset) and other sections",
          "reasoning": "The paper mentions that the dataset will be made publicly available along with the paper, including raw data and annotations, but does not mention or provide any link to source code related to data collection, preprocessing, or generation. There is no indication that the code used to collect or process the dataset is available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (CSF2022 Dataset)",
          "reasoning": "The paper provides documentation on the dataset creation process, including details about the recording setup, the professional cuer, the sentence sources, recording conditions, frame rate, video resolution, phonetic transcription process, manual corrections, and manual annotation for a subset. This documentation is sufficiently detailed for understanding the dataset creation."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7802,
      "completion_tokens": 129,
      "total_tokens": 7931
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Sections 1 Abstract, 3.1, and 3.2",
          "Reasoning": "The dataset is audio data created by convolving human-generated speech samples from the LibriSpeech dataset with simulated room impulse responses generated by a geometric acoustic solver, resulting in spatial audio with 19 microphone channels and first-order ambisonics. Hence, the original audio is human-generated, but the spatial augmentation and RIR simulation are model-generated, making the final dataset a combination of human-generated and model-generated audio."
        }
      ]
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8602,
      "completion_tokens": 232,
      "total_tokens": 8834
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "The annotations for Spatial LibriSpeech are generated via a simulation pipeline consisting of parametric room generation, room impulse response simulations, and mixing with existing LibriSpeech samples. Labels such as source position, speaking direction, room acoustics, and geometry are derived from the simulations, indicating an automatic process rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention or describe any detailed annotation instructions provided for human annotators since the annotations are generated automatically through simulations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention of scoring rubrics or evaluation criteria for human annotators because the annotations are produced automatically, not by human judgment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No annotation examples are provided or discussed as the labels come from simulation outputs and metadata, not from manual annotation processes requiring examples."
          }
        }
      ]
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9792,
      "completion_tokens": 394,
      "total_tokens": 10186
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert, nor does it describe human annotation or validation by experts."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or indication in the paper of quality assurance performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence that multiple human non-experts performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset generation and validation do not involve quality assurance by an AI model judging data correctness."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Generation Methodology",
          "reasoning": "The dataset is generated via algorithmic simulation of room acoustics and impulse responses, including realistic physical acoustic modeling. Labels and audio samples are produced through automated, reproducible computational processes without manual intervention. The paper describes the use of geometric acoustic solvers and systematic mixing steps, implying automated verification of the simulation pipeline rather than manual annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is documented, and it relies on automated computational generation and simulation rather than no QA at all."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9402,
      "completion_tokens": 383,
      "total_tokens": 9785
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by humans. It uses pre-existing LibriSpeech audio samples and Microsoft Deep Noise Suppression noise samples, which are then augmented."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated entirely by AI or machine learning models; rather, it uses simulated room impulse responses applied to existing audio samples."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any translation of data from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation applied to the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not simply collected or aggregated from existing sources without modification; it involves further processing."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Generation Methodology",
          "reasoning": "Spatial LibriSpeech is derived by augmenting existing datasets (LibriSpeech and Microsoft Deep Noise Suppression Challenge samples) through convolution with simulated room impulse responses and scaling. The data is therefore based on existing sources with applied modifications and transformations (simulated acoustic conditions and mixing)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of data generation for Spatial LibriSpeech is explicitly described."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9920,
      "completion_tokens": 533,
      "total_tokens": 10453
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, especially 4.1 and 4.3",
          "reasoning": "The paper clearly states that models were trained on Spatial LibriSpeech from scratch using the dataset annotations for multiple spatial audio tasks. Section 4 describes training models on the dataset without pre-training, and Section 4.3 discusses training models with subsets of the dataset, confirming its use for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The paper performs supervised fine-tuning of the models pre-trained on Spatial LibriSpeech on the ACE Challenge Dev set to improve target data performance, especially for T30 and DRR estimation, demonstrating post-training supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or description of reinforcement learning or RLHF post-training techniques using Spatial LibriSpeech is found in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Spatial LibriSpeech is used primarily as a training dataset. Evaluations on external datasets like TUT Sound Events 2018 and ACE Challenge are performed, but not primarily on Spatial LibriSpeech as an evaluation dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The authors analyze model embeddings and dataset representations (e.g., UMAP visualizations in Section 4.2), as well as the effect of training on smaller subsets in Section 4.3, indicating the dataset is used for analyzing characteristics and trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for retrieval or augmentation purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates multiple practical uses of Spatial LibriSpeech, including training and analysis, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10643,
      "completion_tokens": 546,
      "total_tokens": 11189
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes the Spatial LibriSpeech dataset as augmenting LibriSpeech samples, which are in English, and noise from the Microsoft Deep Noise Suppression Challenge, without mention of additional languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are not described as containing exactly two languages; only English is indicated."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 3.1 Generation Methodology",
          "reasoning": "Spatial LibriSpeech is generated by augmenting LibriSpeech samples (an English speech corpus), with simulated acoustic conditions and distractor noise, but no other languages are mentioned; hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset originates from LibriSpeech samples which are in English; no non-English language only data is described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content consists of audio data, labels, and simulated audio conditions; no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions as part of the entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only human speech audio and simulated room impulse responses; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses natural English speech from LibriSpeech; no fictional or constructed languages are mentioned."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content is clearly stated as English speech from the LibriSpeech corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language audio (English speech); hence, language is applicable."
        }
      }
    }
  },
  {
    "id": "sarabia23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7861,
      "completion_tokens": 161,
      "total_tokens": 8022
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention in the paper",
          "reasoning": "The paper does not provide any link or mention of publicly available code for the dataset generation pipeline or related preprocessing steps. There is no indication that the code used to generate the Spatial LibriSpeech dataset is shared or accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Generation Methodology and Section 3.2 Dataset Characteristics",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including parametric room generation, room impulse response simulation, and mixing with LibriSpeech samples. Specific details about room distributions, material properties, microphone array modeling, and dataset splits are documented, providing transparency and completeness of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 3733,
      "completion_tokens": 529,
      "total_tokens": 4262
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Recordings",
          "Reasoning": "The MUNDEX corpus contains video recordings of 88 dyads performing board game explanations and gameplay, captured from six cameras at 1920x1080 resolution and 50fps. These videos were recorded with human participants in dyadic interaction settings, thus human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Recordings and Section 2.2.1 Transcription and Acoustic Information",
          "Reasoning": "Separate headset microphones recorded the speech audio of the explainer and explainee during the interactions. This audio data originates from human speech recorded in the sessions, hence human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Transcription and Acoustic Information",
          "Reasoning": "Transcriptions of the recorded speech were created manually with corrections following automatic transcription from BAS Web-service or OpenAI's Whisper, including orthographic transcriptions and annotations for disfluencies and other speech phenomena. Since these transcriptions are manually corrected, they are human generated text data derived from the recordings."
        },
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.2 Multimodal Annotations",
          "Reasoning": "Annotations include head movement, gaze direction, and manual gestures, all of which are temporal behavioral signals tracked over time from the video data. These were annotated by humans observing the recordings, reflecting human-generated time series data."
        },
        {
          "Modality": "other",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.2 Multimodal Annotations",
          "Reasoning": "The corpus includes annotations of non-verbal behavior such as torso movements, facial expressions, and adaptors (e.g., fidgeting). These are behavioral annotations derived from human observation and manual annotation, considered human generated non-standard modality data capturing behavioral phenomena."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2.3 Discourse Annotation",
          "Reasoning": "Discourse functions and understanding signals (e.g., backward looking functions, feedback utterances) are annotated based on dialogue transcripts, manually annotated by humans following modified DAMSL and DIT++ schemes, constituting human generated text annotations."
        }
      ]
    }
  },
  {
    "id": "schade24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 4533,
      "completion_tokens": 219,
      "total_tokens": 4752
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 (Annotations)",
            "reasoning": "Annotations include detailed multimodal and discourse annotations corrected manually and made using established annotation tools and schemes, implying involvement of multiple experts to ensure quality and manage various annotation types."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2.3 (Discourse Annotation)",
            "reasoning": "The discourse annotations are based on modified versions of DAMSL and DIT++ schemes, which serve as formal annotation instructions for annotators to classify utterances into functions and levels of understanding."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2.3 (Discourse Annotation)",
            "reasoning": "The presence of hierarchical categories and detailed classifications of understanding levels in discourse annotations suggests a rubric or coding schema guiding consistent annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not explicitly provide or mention annotation examples in the guidelines or manuals for any annotation type."
          }
        }
      ]
    }
  },
  {
    "id": "schade24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 5723,
      "completion_tokens": 394,
      "total_tokens": 6117
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that quality assurance was performed by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information about quality assurance involving multiple expert annotators; the expertise level of annotators is not detailed."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit information about multiple non-expert annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI tools like OpenAI's Whisper were used for automatic transcription, the paper does not mention using AI models specifically for quality assurance of annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes automatic transcription and annotation tools used (BAS Web-service, OpenFace, OpenAI's Whisper), but does not describe any automated verification processes or algorithmic/rule-based quality assurance of annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe or document any quality assurance procedures for the annotations of the MUNDEX corpus. While initial transcriptions were manually corrected, no specific QA process or its rigor for annotation validation is reported."
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 5333,
      "completion_tokens": 434,
      "total_tokens": 5767
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Recordings",
          "reasoning": "The MUNDEX corpus data was newly created by human participants through recording 88 dyadic interactions involving explanations of a board game by a self-trained explainer to a naive explainee, capturing natural multimodal communication from scratch. This data was not translated, adapted, or derived from pre-existing material but collected as original human-human interaction data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus data was recorded from human interactions; there is no indication that the dataset itself was generated or synthesized by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no statement indicating that the data was produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset was generated by machine translation, as the data consists of original German language interactions."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not a collection or aggregation of existing sources without significant modification; it is newly recorded human interaction data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 Annotations",
          "reasoning": "The rich multimodal annotations such as transcriptions, acoustic information, head movement, gaze, manual gestures, facial expressions, and discourse functions are derived from the original recorded data with modifications and transformation applied to produce various annotation layers."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are well documented and specified."
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 5851,
      "completion_tokens": 241,
      "total_tokens": 6092
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3, Discussion",
          "reasoning": "The MUNDEX corpus is primarily used for analyzing multimodal interaction dynamics and levels of understanding in dyadic explanations, including investigations of listeners' gaze behavior, deictic co-speech gestures, turn-taking dynamics, and non-verbal signals, as described in Section 3. There is no indication that the dataset is actually used for training or evaluation of machine learning models; rather, it facilitates analysis of communication patterns and multimodal behaviors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 6574,
      "completion_tokens": 377,
      "total_tokens": 6951
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Recordings",
          "reasoning": "The MUNDEX corpus recordings consist exclusively of dyadic interactions where all speakers are speakers of German, a non-English language. The paper explicitly states in Section 2.1 that all speakers of the corpus are German speakers, indicating the dataset is monolingual in German."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes transcriptions and annotations of human spoken language, specifically German, thus it contains language."
        }
      }
    }
  },
  {
    "id": "schade24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 3792,
      "completion_tokens": 145,
      "total_tokens": 3937
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Sections 2 and 4",
          "reasoning": "The paper describes the corpus creation process, annotation methods, and tools developed or used for annotation but does not provide any links to repositories or locations where the code related to the dataset construction, data collection software, or preprocessing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (especially 2.1 and 2.2)",
          "reasoning": "The paper provides detailed documentation of the data collection process, participant details, annotation procedures, modalities annotated, tools used or planned for annotation, and discourse annotation schemes employed, giving a comprehensive overview of the dataset creation."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7293,
      "completion_tokens": 123,
      "total_tokens": 7416
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1, Introduction; Section 2, Dataset Description",
          "Reasoning": "The study introduces the CRSS-UTDallas Fearless Steps Apollo 11 audio corpus, a naturalistic multi-channel audio dataset capturing voice communications from the NASA Apollo-11 mission. This dataset consists of around 100 hours of audio manually transcribed with speaker labels collected from multiple communication channels. The data was collected through human-operated recording during the Apollo 11 mission and annotated by professional annotators."
        }
      ]
    }
  },
  {
    "id": "shekar23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8093,
      "completion_tokens": 203,
      "total_tokens": 8296
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2: Dataset Description",
            "reasoning": "The paper states that the selected 100 hours from the UTDallas Fearless-Steps Apollo corpus were manually transcribed by professional annotators for speaker labels, indicating multiple human experts were involved in the annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2: Dataset Description",
            "reasoning": "The paper does not mention providing any detailed annotation instructions for the professional annotators or other human experts; no instructions are described explicitly."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2: Dataset Description",
            "reasoning": "There is no mention or description of scoring rubrics or criteria used in the annotation guidelines in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2: Dataset Description",
            "reasoning": "The paper does not report providing annotation examples or sample annotations to guide the annotators."
          }
        }
      ]
    }
  },
  {
    "id": "shekar23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9283,
      "completion_tokens": 400,
      "total_tokens": 9683
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 Dataset Description",
          "reasoning": "The paper states that the 100 hours of audio data were manually transcribed by professional annotators for speaker labels. The use of 'professional annotators' implies these were experts or persons with relevant expertise conducting the annotation process. There is no indication that multiple experts performed QA collectively or a group consensus method. Therefore, QA is considered to be performed by single human experts."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple expert annotators performing validation or consensus quality assurance on the data annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as professional annotators, indicating expertise, therefore single non-expert human QA is not applicable."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper supports the involvement of multiple non-expert annotators for QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI models being used to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or algorithmic QA methods being applied to ensure annotation quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A QA process is described involving professional annotators manually transcribing and labeling the data."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8893,
      "completion_tokens": 444,
      "total_tokens": 9337
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2, Dataset Description",
          "reasoning": "The paper states that the UTDallas Fearless Steps Apollo corpus was manually transcribed by professional annotators for speaker labels, involving 100 hours of audio data selected from mission-critical events. This indicates that the data annotations and labels were created from scratch by human contributors specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is audio recordings and human annotations. There is no indication that the dataset was generated by AI or machine learning models; rather, models are applied to the dataset for analysis."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation activity or generation of data via translation by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is discussed or applied to the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2, Dataset Description",
          "reasoning": "The dataset is described as a subset of 100 hours drawn from a larger 19,000 hour Apollo corpus, which itself consists of naturalistic audio collected during actual NASA missions. The authors selected and organized this subset and channels based on speech activity, indicating that the data is an aggregation from existing historical sources."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of modified or adapted data derived from other sources; the data is raw or annotated recordings without significant transformations altering the original content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as archival NASA Apollo mission audio recordings manually annotated by humans."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9411,
      "completion_tokens": 527,
      "total_tokens": 9938
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2 Dataset Description; Section 5 Implementation Details",
          "reasoning": "The paper describes using the Fearless Steps Apollo-11 audio corpus subset (100 hours of hand-labeled audio) to train graph attention network models from scratch for speaker classification. It details the training setup, including 70% of the data used for training, a cross-entropy loss, Adam optimizer, and batch sizes, indicating the dataset was used for model training with randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the dataset for fine-tuning pre-trained models; instead, pretrained speaker embeddings are used as features, but training the graph networks is done from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of reinforcement learning based post-training methods using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 Discussion",
          "reasoning": "The dataset is split into training and test sets, and the test set (30% of data) is used for evaluating the speaker classification accuracy of models, thus serving as evaluation data to benchmark performance of the proposed method against baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6 Tracking Speaker-of-Interest on Mission Phases",
          "reasoning": "The authors perform analysis on the dataset to track key speakers of interest across different mission phases, analyzing and visualizing speaker durations and interactions, which is an analytical use of the dataset rather than model training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset serves as a knowledge base for retrieval-augmented generation or similar use cases."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset usage is clearly documented for training, evaluation, and analysis tasks."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10134,
      "completion_tokens": 578,
      "total_tokens": 10712
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Fearless Steps Apollo-11 audio corpus is described as capturing communications from over 400 personnel during NASA Apollo-11 mission, which was conducted primarily in English. There is no indication in the paper of multiple human languages being present in the dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention or imply the presence of exactly two human languages in the dataset entries. The linguistic content is consistently English."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 1 (Introduction), Section 2 (Dataset Description)",
          "reasoning": "The dataset consists of NASA Apollo 11 audio communications involving mission personnel. The paper consistently refers to English speech data (i.e., astronaut and mission specialist English voice communication). There is no indication of any other language spoken or transcribed in the corpus."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any single non-English language being the exclusive content of the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio speech recordings and associated speaker annotations. The paper does not mention inclusion of any programming or structured code content in the dataset itself."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are audio speech recordings and speaker labels; mathematical and logical notations are present only in the paper's methodology and equations, not the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Dataset solely contains human speech from mission personnel without any biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is clearly specified as English; thus unknown language status does not apply."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Dataset is composed of spoken language audio and speaker transcripts, so it indeed contains human language."
        }
      }
    }
  },
  {
    "id": "shekar23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7352,
      "completion_tokens": 223,
      "total_tokens": 7575
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper indicates availability of code for dataset construction.",
          "reasoning": "The paper does not provide any link, appendix, or reference to code repositories related to the dataset construction, preprocessing scripts, or data preparation. It only mentions that the dataset is a subset of the Fearless Steps Apollo 11 audio corpus with manual transcription provided, but there is no indication that dataset construction code has been released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Dataset Description",
          "reasoning": "The paper provides some documentation about the dataset construction process in Section 2. It describes the source corpus (Fearless Steps Apollo 11 audio corpus), the subset selection criteria (100 hours from three mission-critical events), the selection of channels (five channels with most speech activity), the amount of manual transcription by professionals, and the number and duration of speakers and utterances. While this is not exhaustive code-level documentation, it offers transparency and completeness about the dataset creation and selection process sufficient for understanding and reproducibility considerations."
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7460,
      "completion_tokens": 173,
      "total_tokens": 7633
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 ITA Dataset",
          "Reasoning": "The ITA dataset consists of 76 conversational in-class speech recordings from 54 adult learners collected as new speech data for this study, explicitly described as recordings from international students serving as teaching assistants. It is original audio captured from human speakers."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Human labeled Transcripts and Prosody Ratings",
          "Reasoning": "Human transcriptions (human speech recognition - HSR) and prosodic labels were generated by fifteen trained linguists who manually transcribed and rated the ITA speech data, thus these text annotations are human generated."
        }
      ]
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8260,
      "completion_tokens": 216,
      "total_tokens": 8476
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1.1",
            "reasoning": "Section 3.1.1 states that fifteen trained linguists rated the speech for accentedness and intelligibility, indicating multiple human experts performed the prosodic annotations and transcript labelling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "The paper describes that trained linguists annotated prosodic and intelligibility labels but does not provide explicit details about any annotation instructions or protocols given to those annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "Although ratings such as accentedness and intelligibility were given, the paper does not mention specific scoring rubrics or detailed criteria used by annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide or mention examples of annotations or sample annotated data for prosodic or transcript annotations."
          }
        }
      ]
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9450,
      "completion_tokens": 446,
      "total_tokens": 9896
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Human labeled Transcripts and Prosody Ratings",
          "reasoning": "Fifteen trained linguists rated the speech for accentedness and intelligibility and provided prosodic annotations. As linguists are subject matter experts in language and pronunciation, this constitutes quality assurance by multiple human experts. However, since quality assurance is by multiple experts, 'Single Human Expert' does not apply."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Human labeled Transcripts and Prosody Ratings",
          "reasoning": "The dataset is labeled and quality assured by fifteen trained linguists who are experts in the domain. They rated accentedness, intelligibility, and prosodic features, indicating quality assurance is performed by multiple human experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models were used for speech recognition and mispronunciation detection, quality assurance of dataset annotation is not described to be performed by AI as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic rule-based process used specifically to assure the quality of dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has documented quality assurance through multiple expert human annotators; therefore, no QA is not applicable."
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9060,
      "completion_tokens": 519,
      "total_tokens": 9579
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 ITA Dataset",
          "reasoning": "The ITA dataset was collected from 54 adult learners with diverse L1 backgrounds producing conversational in-class speech recordings. The data includes human-annotated transcripts and prosodic ratings completed by trained linguists. The dataset was prepared specifically for this study to analyze L2 speech intelligibility, indicating original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset is described as being originally generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data in the study was produced via human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was described as generated via machine translation from other languages; the ASR transcripts were generated from speech recognition, not translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The ITA dataset is original speech data collected for this study, not a mere aggregation of existing datasets without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Acoustic Model and GOP Feature Extraction; Section 3.3 GOPT Inference and Scoring; Section 3.4 MDD using Wav2vec 2.0",
          "reasoning": "The study derives features such as GOP features using pre-trained acoustic models (Kaldi TDNN-F trained on Librispeech) and applies trained models (GOPT trained on Speechocean762) to the ITA data. Also, mispronunciation detection (MDD) scores leverage pre-trained Wav2vec 2.0 fine-tuned models. Therefore, some data representation used is derived from existing data or models with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the origins of the data clearly, particularly the ITA dataset and modeling techniques."
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9578,
      "completion_tokens": 373,
      "total_tokens": 9951
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Results) and 4.1 (Multicollinearity Test and Regression Analysis)",
          "reasoning": "The ITA dataset is used to assess the performance of multi-level pronunciation scoring systems and mispronunciation detection models, serving as the test set to benchmark and evaluate the correlation between model outputs and human-rated intelligibility scores."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Characterization of L2 Intelligibility using Feature Importance) and Section 5 (Discussion)",
          "reasoning": "The dataset is utilized to analyze the relationships between various pronunciation scores, prosodic features, and intelligibility factors to identify dominant predictors influencing L2 speech intelligibility, thus it is primarily used for analyzing trends and characteristics rather than for training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10301,
      "completion_tokens": 584,
      "total_tokens": 10885
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The ITA dataset described consists of non-native speakers of English with diverse first languages; however, all recordings and transcriptions are in English. There is no indication that the dataset includes speech data in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The dataset involves L2 English spoken by speakers of various L1s, but speech data is only in English. There is no evidence that the dataset contains exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The ITA dataset consists of English conversational speech produced by adult learners from diverse language backgrounds, with transcripts and annotations all in English. The dataset entries contain only English utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No data presented in the paper indicates presence of non-English monolingual speech in the proposed dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset focuses on natural spoken language (English speech) and associated annotations; there is no inclusion of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include content with formal mathematical or logical symbols; it concerns natural language speech data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech data (English) only, without any biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset's language content is well specified as English; there is no lack of documentation regarding language."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language data (English speech); therefore, this label does not apply."
        }
      }
    }
  },
  {
    "id": "shekar23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7519,
      "completion_tokens": 163,
      "total_tokens": 7682
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not mention any publicly available code repository or provide links to code related to data collection, preprocessing, or generation for the ITA dataset or any datasets introduced by the authors."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 ITA Dataset and subsections 3.1.1, 3.1.2",
          "reasoning": "The paper documents the dataset creation process for the ITA dataset in detail, including number of speakers, utterances, details on annotation (human and ASR), types of labels (transcripts, prosodic ratings), and usage of Whisper ASR for transcript generation, providing transparency about dataset composition and labeling procedure."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6701,
      "completion_tokens": 268,
      "total_tokens": 6969
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Methods",
          "Reasoning": "The paper describes speech utterances recorded from 8 children (4 male, 4 female) across grades 1 to 4, using a SHURE KSM30 microphone capturing speech audio during a picture naming task. This audio data is human-generated as it is recorded speech from human children during an experiment conducted by the authors."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Methods",
          "Reasoning": "Subglottal resonance data are recorded from a K&K Sound HotSpot accelerometer positioned on the neck below the thyroid cartilage during the children's speech. This sensor data is recorded from human participants and manually measured, thus human-generated."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Methods and Figure 1",
          "Reasoning": "Ultrasound images of the tongue's midsagittal profile were captured for each child concurrently with microphone and accelerometer data during speech production. These ultrasound tongue images are captured using human-operated ultrasound equipment during data collection, thus are human-generated image data."
        }
      ]
    }
  },
  {
    "id": "shetty23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7501,
      "completion_tokens": 187,
      "total_tokens": 7688
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 Methods",
            "reasoning": "The annotation involved manual extraction of midsagittal tongue contours from ultrasound images and manual measurement of formant frequencies and subglottal resonances using Praat, performed by researchers with expertise in speech pathology and acoustics, indicating a single human expert or experts conducting the annotation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not provide any explicit annotation instructions or guidelines for extracting or measuring acoustic or articulatory features."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "No scoring rubrics or scoring criteria are described or referenced for the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not include annotation examples or samples demonstrating the annotation procedure."
          }
        }
      ]
    }
  },
  {
    "id": "shetty23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8691,
      "completion_tokens": 310,
      "total_tokens": 9001
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator. No information about such QA is provided."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset annotations or content in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used as a judge or for quality assurance in the dataset creation or validation process."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although measurements such as formant frequencies and articulatory features were extracted manually (e.g., formant frequencies measured manually in Praat), there is no description of an automated verification or QA process applied to the datasets."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not explicitly describe any quality assurance process applied to the dataset acquired or annotated. While data extraction techniques are described (manual measurements), there is no reporting on any QA procedure or validation of annotations or measurements."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8301,
      "completion_tokens": 443,
      "total_tokens": 8744
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper explicitly states that data were collected longitudinally from eight typically developing children between grades 1 and 4 using ultrasound imaging, microphone, and neck accelerometer signals during speech tasks. This data was newly collected by the authors from human participants and is original raw data created from scratch, not derived or adapted from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating data via AI or machine learning models; the data consists of recorded acoustic and articulatory features from human subjects."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not aggregated from existing sources; it was collected directly from human participants as original recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 (Articulatory Analysis) and Section 2 (Methods)",
          "reasoning": "The articulatory features such as Tongue Curvature Degree (TCD) and Tongue Curvature Position (TCP) were derived by processing and quantifying ultrasound tongue contours obtained from the novel collected data. Thus, these features are derived data formed by transforming the original ultrasound images."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is documented clearly, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8819,
      "completion_tokens": 325,
      "total_tokens": 9144
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4 (Results and Discussion; Conclusions)",
          "reasoning": "The paper introduces a longitudinal dataset of children's acoustic and articulatory speech features and uses it primarily to analyze developmental trends, sex-based differences, and articulatory-acoustic relationships in speech over time. The dataset is used to study patterns and characteristics rather than for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for analysis and characterization within the study, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9542,
      "completion_tokens": 510,
      "total_tokens": 10052
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2: Methods; Section 3: Results and Discussion",
          "reasoning": "The speech data in the new dataset consists exclusively of utterances by native speakers of Midwestern American English. Words used for vowel analysis are English words such as \"Apple\", \"Vacuum\", \"Teeth\", \"Zebra\", \"Shoe\", \"Zoo\", \"Watch\", and \"Frog\". All participants are native English speakers and the focus is on the English vowel system. Hence, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 2: Methods",
          "reasoning": "The dataset includes articulatory feature measures such as Tongue Curvature Degree (TCD) and Tongue Curvature Position (TCP) which are defined mathematically as ratios of line segment lengths (e.g., TCD = len(ZO)/len(XO), TCP = len(YO)/len(XO)). These formal symbolic expressions of articulatory parameters imply the presence of mathematical and logical notation as part of the dataset's quantified features."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are speech utterances by children and contain linguistic content, so they do contain language."
        }
      }
    }
  },
  {
    "id": "shetty23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6760,
      "completion_tokens": 184,
      "total_tokens": 6944
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not explicitly mentioned in the paper",
          "reasoning": "The paper does not provide any links or references to publicly available code repositories related to data collection, preprocessing, or generation for the dataset. There is no mention of code release or software availability for reproducing the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods) and throughout the paper",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the participant selection criteria, longitudinal data collection from specific grades and ages, methods of acoustic and articulatory data recording (microphone, accelerometer, ultrasound), and specific measurements taken (formant frequencies, subglottal resonances, tongue contour extraction). These details are described mainly in Section 2 and elaborated in the rest of the paper, enabling understanding of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7109,
      "completion_tokens": 91,
      "total_tokens": 7200
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset",
          "Reasoning": "The paper states that the new dataset used for training and validation consists of 325 hours of internal Mandarin speech data obtained from various websites such as YouTube, indicating that the recordings are presumably human-generated audio data collected from real speech sources rather than synthesized or simulated audio."
        }
      ]
    }
  },
  {
    "id": "shi23c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7909,
      "completion_tokens": 262,
      "total_tokens": 8171
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.1 Datasets",
            "reasoning": "The annotation labels (punctuation and endpoint labels) are derived through forced alignment and artificially defined endpoints based on punctuation and silence durations. The endpoint labels are artificially defined in the training set by combining punctuation and silence duration. This process implies an automatic or model-based annotation rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1 Dataset",
            "reasoning": "No explicit mention of annotation instructions provided to human annotators, as the punctuation labels are generated by forced alignment and the endpoint labels are artificially defined labels at the frame level. The paper does not discuss any instructions given to human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 Dataset",
            "reasoning": "The paper does not mention any scoring rubrics or detailed criteria for annotation, since annotation is done via forced alignment and artificial labeling rather than direct human labeling needing rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No examples or samples of annotated data or annotation guidelines are provided in the paper for the labeling scheme, since annotations are automatically generated or artificially defined in training."
          }
        }
      ]
    }
  },
  {
    "id": "shi23c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9099,
      "completion_tokens": 300,
      "total_tokens": 9399
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of multiple human expert annotators performing quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any involvement of multiple non-expert human annotators in quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that an AI model was used as a judge to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "Punctuation labels per frame are obtained by forced alignment, which implies an automated forced-alignment process is used to generate frame-level punctuation labels, representing an automatic process for labeling or verifying annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although the paper does not explicitly describe a quality assurance process, the use of forced alignment to obtain punctuation labels is an automated labeling procedure and thus represents a form of QA."
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8709,
      "completion_tokens": 504,
      "total_tokens": 9213
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The authors specify that they use 325 hours of internal Mandarin speech data collected from various websites such as YouTube, used as training and validation sets. This indicates data collected and curated by the authors or their institution. The paper describes that punctuation labels per frame are obtained by forced alignment, implying manual or automatic alignment on human-generated speech data. There is no indication that the data was translated or generated by models; thus, the data can be considered newly collected original data by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the dataset or its portions were generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the data was produced by translating content from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper of data generated via machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper states the training and validation data were obtained from various websites such as YouTube, indicating collection from existing public sources. The data was aggregated into internal datasets for training and evaluation without explicit mention of significant modification. Thus, the data can be considered collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "Punctuation labels per frame are obtained via forced alignment, which is a process of deriving time-aligned labels from existing textual and audio data. Additionally, endpoint labels are artificially defined by combining punctuation and silence duration. These labels constitute data derived or transformed from the original collected speech data, indicating that parts of the dataset contain derived annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation process are specified sufficiently in the paper."
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9227,
      "completion_tokens": 345,
      "total_tokens": 9572
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.3",
          "reasoning": "The internal Mandarin speech dataset of 325 hours is used for training the proposed semantic VAD model from scratch with the specified multi-task objectives including VAD, punctuation prediction, and ASR auxiliary loss. This is described in Section 3.1 under Dataset and Section 3.3 under Model configuration where training procedures are detailed."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 4",
          "reasoning": "The paper uses internal Mandarin long speech test sets (Test1, Test2, Test3) for performance evaluation of the proposed semantic VAD, measuring latency, character error rate, and detection cost function. This is explicitly described in Section 3.1 and Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 and Figures 4 and 5",
          "reasoning": "The paper conducts visual and quantitative analysis of segmentation timing and judgment proportions using the test sets, especially shown in Section 4, Figure 4, and Figure 5, to analyze characteristics and benefits of the semantic VAD method."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9950,
      "completion_tokens": 498,
      "total_tokens": 10448
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used is described as an internal Mandarin speech dataset only, with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains exactly two human languages. Only Mandarin is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as containing only English. The dataset is specified as Mandarin speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper states in Section 3.1 that the dataset is 325 hours of internal Mandarin speech data used for training and evaluation, indicating it contains only Mandarin, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any programming or structured code-related content is included in the dataset."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas for loss functions and metrics, these are not part of the dataset entries themselves."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not involve biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional, artificial, or constructed languages are mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly specified as Mandarin, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language (Mandarin) data, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "shi23c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7168,
      "completion_tokens": 179,
      "total_tokens": 7347
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not mention or provide any link to publicly available code related to data collection, preprocessing, or dataset generation. There is no indication that the code has been released."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper provides some details about the dataset, including that the training data consists of 325 hours of internal Mandarin speech from various websites such as YouTube. The test data sets are described (meeting, recording pen, live webcast) along with their durations. Punctuation labels per frame are obtained by forced alignment. Table 1 shows label distributions in the training set. These provide some level of documentation on the dataset creation process, although the dataset is internal and sourcing details are brief."
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11435,
      "completion_tokens": 106,
      "total_tokens": 11541
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection, Table 1",
          "Reasoning": "ML-SUPERB gathers data from a wide range of multilingual speech corpora, which are collections of spoken language audio recordings. These datasets originate from human speech recordings with various licenses allowing their use in research, and the benchmark uses subsets of this audio data for training, development, and testing across 143 languages."
        }
      ]
    }
  },
  {
    "id": "shi23g_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12235,
      "completion_tokens": 303,
      "total_tokens": 12538
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1: Data Collection; Section 2.4: Framework and Benchmark Settings",
            "reasoning": "ML-SUPERB benchmark utilizes multiple publicly available multilingual speech corpora for training and evaluation. The benchmark uses an automatic processing pipeline including data preprocessing, training, inference, and evaluation implemented using open-source toolkits (S3PRL for upstream and ESPnet for downstream models) to generate performance metrics as reported. There is no mention of human annotators manually labeling or checking the dataset; instead, data is sourced from existing corpora with licenses for research use, and evaluation relies on automatic procedures."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section or mention",
            "reasoning": "The paper does not provide or reference any annotation instruction guidelines for manual annotators because the dataset compilation and evaluation are done automatically from existing corpora with established labels or transcriptions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section or mention",
            "reasoning": "No scoring rubrics or manual evaluation criteria for annotation quality are described since the benchmark uses automatic metrics such as CER, PER, and accuracy on established datasets without human scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section or mention",
            "reasoning": "There is no indication that manual annotation examples or demonstrations were provided or needed, as the benchmark uses pre-existing labeled data and automated evaluation."
          }
        }
      ]
    }
  },
  {
    "id": "shi23g_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13425,
      "completion_tokens": 285,
      "total_tokens": 13710
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process involving a single human expert for validating dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information or description of a quality assurance process performed by multiple human experts on the datasets used in ML-SUPERB."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any use of a single non-expert human annotator for quality assurance of the dataset annotations or content."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that AI models were used as judges in the quality assurance process of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe any automated verification or rule-based techniques applied as quality assurance to the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document any quality assurance processes applied to the collected datasets; it only lists the sources and licenses but does not describe validation or annotation verification procedures."
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 13035,
      "completion_tokens": 403,
      "total_tokens": 13438
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created any new speech data entirely from scratch by human contributors. Instead, it uses existing publicly available multilingual speech datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of any data being generated by AI or machine learning models for the benchmark dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "ML-SUPERB gathers data from a wide range of existing multilingual speech corpora such as Multilingual Librispeech, Commonvoice, Voxforge, Voxpopuli, Googlei18n, and others, all collected under permissive licenses. The benchmark essentially collates these existing datasets without significant modifications."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the data has been significantly modified, transformed, or adapted from existing sources to form the benchmark dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear documentation on the origins of the data used in ML-SUPERB; therefore, this category does not apply."
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13553,
      "completion_tokens": 357,
      "total_tokens": 13910
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.4 (Downstream model and training details) and Section 3.2 (Experimental Results)",
          "reasoning": "The ML-SUPERB dataset is used to fine-tune pre-trained self-supervised learning (SSL) speech models. The paper specifically states that frozen SSL features are used and a lightweight downstream model is trained on ML-SUPERB data, including 10-minute and 1-hour training sets, confirming supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Throughout the paper, especially Sections 2.4 (Evaluation metric) and 3.2 (Experimental Results)",
          "reasoning": "ML-SUPERB serves as a benchmark dataset used exclusively for evaluating various SSL models on multilingual automatic speech recognition (ASR) and language identification (LID) tasks, as detailed in multiple sections and through performance tables."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 (Layerwise analysis)",
          "reasoning": "The ML-SUPERB dataset is used for analyzing layer importance and model behavior across languages and tasks, as shown by the presented layerwise analysis and weight visualization for models like XLSR-128."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14276,
      "completion_tokens": 558,
      "total_tokens": 14834
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract, Section 2.1 Data Collection, Section 2.3 Multilingual Track",
          "reasoning": "The dataset introduced in the paper, ML-SUPERB, contains speech data for 143 languages ranging from high-resource to endangered languages (Abstract, Section 2.1). It includes multilingual speech corpora such as Multilingual Librispeech, Commonvoice, Voxforge, Voxpopuli, and others covering many languages (Section 2.1). The multilingual track explicitly involves multilingual ASR and LID tasks with combined training data from all these languages (Section 2.3). Thus, the dataset is clearly multilingual with entries across more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes many more than two languages, so it cannot be classified as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although English data is part of the dataset, the dataset is not restricted to English entries only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages, not a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset contains entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech data only, with no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language data."
        }
      }
    }
  },
  {
    "id": "shi23g_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11494,
      "completion_tokens": 190,
      "total_tokens": 11684
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.4 (Framework and Benchmark Settings)",
          "reasoning": "The paper states the use of established toolkits (S3PRL and ESPnet) and mentions that ML-SUPERB will be published as an all-in-one recipe in ESPnet's egstraining collection, but there is no explicit indication or link in the paper stating that the code for dataset collection, preprocessing, or generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Data Collection) and Table 1",
          "reasoning": "The paper provides detailed documentation on dataset construction, including the various source corpora used, their licenses, dataset sizes, statistics in Table 1, training/validation/test splits, inclusion of few-shot languages, and reflections on data selection rationale. This constitutes transparent and comprehensive documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6745,
      "completion_tokens": 182,
      "total_tokens": 6927
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The authors introduced the LibriPhrase dataset derived from LibriSpeech, which consists of audio recordings. These audio data originate from human speech recordings as part of LibriSpeech, which is based on human-read audiobooks. Thus, the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The LibriPhrase dataset includes text transcriptions corresponding to the audio short phrases. These texts represent human-generated written text from the LibriSpeech corpus derived from human-produced materials. The text data stems from human-generated literary sources and manual transcription alignment, therefore it is human generated text."
        }
      ]
    }
  },
  {
    "id": "shin22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7545,
      "completion_tokens": 244,
      "total_tokens": 7789
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Datasets",
            "reasoning": "The LibriPhrase dataset is constructed from the existing LibriSpeech dataset using an automatic process involving the Montreal Forced Aligner (MFA) to determine word-level timestamps and then splitting utterances into shorter phrases programmatically. There is no mention of human annotators; the annotation was done by an automatic forced alignment tool and data processing scripts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "No detailed annotation instructions for human annotators are described since the data annotation was done automatically by the Montreal Forced Aligner and data processing pipeline."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "No scoring rubrics are mentioned or implied, because automatic processing was used without human annotation or scoring criteria."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.1 Datasets",
            "reasoning": "The paper does not provide annotation examples related to human annotation guidelines, as the data processing was fully automatic, using forced alignment and scripted splitting."
          }
        }
      ]
    }
  },
  {
    "id": "shin22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8735,
      "completion_tokens": 313,
      "total_tokens": 9048
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any annotation or quality assurance activities performed by a single human expert on the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no documentation of multiple human experts performing quality assurance for the LibriPhrase dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single non-expert human annotated or verified the data."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple non-expert humans involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that AI models were used as judges or for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 - Datasets",
          "reasoning": "The LibriPhrase dataset was generated by automatically segmenting LibriSpeech utterances into shorter phrases using the Montreal Forced Aligner for word-level time stamps. This forced alignment is an automated process that aligns audio with text. However, the paper does not report any manual correction or human verification for this alignment or the segmented phrases, implying that quality assurance relies on this automated alignment procedure."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some automated quality assurance is performed via forced alignment, so it is not that no quality assurance was applied."
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8345,
      "completion_tokens": 305,
      "total_tokens": 8650
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The LibriPhrase dataset is built from the existing LibriSpeech dataset by extracting and segmenting existing utterances into short phrases of 1 to 4 words, without being created from scratch. This involves collecting and organizing existing speech data without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "LibriPhrase is derived by processing the LibriSpeech corpus using forced alignment to determine word-level timestamps and then splitting long utterances into shorter phrases, representing a transformation and adaptation of existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8863,
      "completion_tokens": 322,
      "total_tokens": 9185
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The LibriPhrase dataset is explicitly introduced and used as a training dataset in an open-vocabulary keyword spotting task to train the proposed cross-modal correspondence detector (CMCD) model from scratch. The paper details its construction and usage for training, as seen in Sections 4.1 and 4.2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2",
          "reasoning": "The LibriPhrase dataset is used for evaluation by constructing positive and negative audio-text pairs to measure the model's performance on keyword spotting under various conditions including easy and hard negatives, as described in Sections 4.1 and 4.2."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and Figure 5",
          "reasoning": "The paper utilizes the LibriPhrase dataset to analyze the impact of keyword length on detection performance and ablation studies on training losses, demonstrating analysis beyond training or simple evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9586,
      "completion_tokens": 268,
      "total_tokens": 9854
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1, Datasets",
          "reasoning": "The new dataset introduced by the authors is LibriPhrase, which is derived from LibriSpeech, an English-language speech corpus of public domain audiobooks. The paper explicitly states the use of the CMU pronouncing dictionary for English grapheme-to-phoneme conversion and the evaluation datasets and examples are in English, indicating that the entries in LibriPhrase are in English only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shin22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6804,
      "completion_tokens": 151,
      "total_tokens": 6955
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code or scripts for the construction or processing of the LibriPhrase dataset or any other dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper documents the dataset creation process of LibriPhrase in Section 4.1, explaining that it is built from LibriSpeech using the Montreal Forced Aligner for word-level time stamps and splitting utterances into shorter phrases of 1 to 4 words. They also specify the training set sizes and how noise is added for robustness."
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7629,
      "completion_tokens": 305,
      "total_tokens": 7934
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Dataset and pre-processing",
          "Reasoning": "The authors introduce a new labeled speech data subset from the JSUT dataset called 'basic5000 subset' consisting of 6.78 hours of speech by a single female speaker, which was manually annotated for TTS labels. This data is human generated because the speech was recorded by humans and labels were manually annotated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 Dataset and pre-processing",
          "Reasoning": "The authors collected a proprietary Japanese speech corpus from six male and eleven female professional speakers totaling 207.96 hours of speech with manually annotated labels, called the LARGE dataset. This new dataset was recorded and annotated by humans, thus is human generated audio data."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1.1 TTS data augmentation and Section 2.3 Text-to-speech data augmentation",
          "Reasoning": "To augment training data for the annotation model, an auxiliary TTS model was first trained on a limited set of paired data and then used to synthesize speech from a large text-only corpus. The resulting synthesized speech paired with generated labels constitutes a model-generated audio dataset created by the authors for training augmentation."
        }
      ]
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8429,
      "completion_tokens": 407,
      "total_tokens": 8836
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1.1",
            "reasoning": "The paper states that the LARGE dataset consists of proprietary Japanese speech corpora recorded by multiple professional speakers with manually annotated labels, which implies annotations were performed by multiple human expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "There is no explicit mention in the paper of detailed annotation instructions being provided to the annotators for the LARGE dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "The paper does not mention any scoring rubrics provided or used as part of annotation guidelines for the LARGE dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "The paper does not provide any indication that annotation examples were given or included in guidelines for labeling the LARGE dataset."
          }
        },
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1.1",
            "reasoning": "The JSUT dataset's phonemic and prosodic labels are described as 'manual TTS labels' and the dataset is a public Japanese speech corpus with annotated labels, indicating annotations were conducted by multiple human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "The paper does not explicitly state that detailed annotation instructions were provided for the JSUT dataset labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "No mention is made of scoring rubrics or guideline rubrics for the annotations of the JSUT dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1.1",
            "reasoning": "No indication is found in the paper that annotation examples or samples were provided with the JSUT dataset annotations."
          }
        }
      ]
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9619,
      "completion_tokens": 291,
      "total_tokens": 9910
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert for the new datasets introduced."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.1 and 3.2.1",
          "reasoning": "The LARGE dataset used as training and evaluation data has manually annotated labels performed by multiple professional speakers (annotators). The paper notes use of manually annotated labels and suggests the annotation was done by multiple human experts, particularly professional speakers, indicating multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information describing quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance performed by an AI model as a judge for dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some automated text processing like Open JTalk is used to generate pseudo labels, this is part of data augmentation rather than a dedicated quality assurance process for verifying annotation accuracy."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9229,
      "completion_tokens": 466,
      "total_tokens": 9695
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset and pre-processing",
          "reasoning": "The paper uses manually annotated labels for the JSUT dataset and a proprietary Japanese speech corpus, both of which are created by human contributors and represent original content not derived or adapted from other sources."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.3 Text-to-speech data augmentation and Section 3.1.1 TTS data augmentation",
          "reasoning": "The paper generates pseudo label-speech paired data using an auxiliary TTS model trained on existing labeled data and text-only corpora, thus creating new speech data synthesized entirely by the TTS model based on text-derived labels."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data is produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe generating data via machine translation systems or translating data from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are either manually annotated or synthetically generated; there is no mention of aggregating data from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 Text-to-speech data augmentation and Section 3.1.1 TTS data augmentation",
          "reasoning": "The pseudo label-speech paired data are derived by transforming text-only corpora into paired data using the auxiliary TTS model and a text processing module, representing data derived from existing text through generation and augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are explicitly described in the paper, with no unspecified or undocumented sources."
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9747,
      "completion_tokens": 260,
      "total_tokens": 10007
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2.1 and 3.2.2",
          "reasoning": "The newly created TTS datasets produced by the authors' annotation model are used to train and fine-tune TTS models using supervised learning methods, as described in the experiments where TTS models are trained with these datasets and evaluated for naturalness and pronunciation."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1.2",
          "reasoning": "The annotated datasets created by the proposed method are used to objectively evaluate the annotation accuracy (e.g., computing CER and F1 score) to benchmark the performance of the annotation model compared to baselines."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10470,
      "completion_tokens": 600,
      "total_tokens": 11070
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1.1 and 3.2",
          "reasoning": "The datasets used and created in the paper are all in Japanese language only. No mention of multiple languages appears for the proposed datasets or experiments."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1.1 and 3.2",
          "reasoning": "The datasets and experimental data described involve only a single language: Japanese. No bilingual language pairs are mentioned or included."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1.1 and overall paper",
          "reasoning": "The datasets introduced are Japanese speech corpora and text data. There is no English content in the datasets introduced by the authors."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Dataset and pre-processing, Section 3.2 Experimental conditions",
          "reasoning": "The new datasets introduced (or augmented) by the authors are all Japanese speech datasets and accompanying Japanese phonemic and prosodic labels. The datasets such as JSUT, LARGE, and JVS are Japanese speech corpora. Therefore, the proposed dataset is monolingual non-English, specifically Japanese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The datasets contain speech samples and text labels (phonemic and prosodic annotation), not programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "While some mathematical notation is used in model formulation, the datasets themselves do not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The datasets contain human speech and text labels only, not biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No fictional or constructed languages are present in the datasets introduced."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The language of the datasets is clearly documented as Japanese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language (Japanese) data and annotations."
        }
      }
    }
  },
  {
    "id": "shirahata24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7688,
      "completion_tokens": 153,
      "total_tokens": 7841
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not mentioned in the paper",
          "reasoning": "The paper does not provide any links or mention that the code used for constructing the annotation models, dataset generation, or data augmentation is publicly available in any repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The paper documents the dataset creation process in detail, including the use of existing datasets (JSUT, LARGE), the method of annotation with the proposed model, the architecture details of the annotation model, and the TTS data augmentation procedure. Experimental conditions and evaluation protocols related to dataset construction and annotation are explicitly described in Sections 2 (Method) and 3 (Experiments)."
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8156,
      "completion_tokens": 357,
      "total_tokens": 8513
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Labelled datasets; Table 1; Section 4.2 Bemba, Nyanja, Tonga, Lozi",
          "Reasoning": "The labelled datasets consist of read speech recordings obtained from human speakers. The paper states these are recorded using the LIG-AIKUMA mobile application by native speakers or students of the University of Zambia reading sourced text, thus human involvement is explicit in both data creation and recording."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Labelled datasets; Nyanja dataset description; Tonga and Lozi datasets description",
          "Reasoning": "The text transcriptions used for read speech are sourced from publicly available literature books or human translations (e.g., translations of English captions into Nyanja by qualified high school teachers). Therefore, the text data is human generated through manual translation or public literary sources."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Unlabelled datasets",
          "Reasoning": "The unlabelled datasets are collected from radio news and talk show recordings sourced from various radio stations and YouTube. These are recordings of natural speech by humans, captured using human-operated equipment and collected by participants during a data sourcing sprint event."
        }
      ]
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8956,
      "completion_tokens": 471,
      "total_tokens": 9427
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.2 Labelled datasets",
            "reasoning": "The labelled data collection involves multiple human annotators: specifically, for the Nyanja dataset, five qualified High School teachers who are native speakers translated text; speakers who recorded the sentences were students of the University of Zambia; text collectors, speakers, and validators are mentioned as annotators. This indicates that multiple human experts were involved in the annotation process (translation, recording, validation)."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.2 Labelled datasets",
            "reasoning": "The paper does not state or provide detailed annotation instructions for the annotators or speakers; it mentions the use of the LIG-AIKUMA mobile application for recording and general data collection methods but no explicit instructions or guidelines documentation is described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.2 Labelled datasets",
            "reasoning": "There is no mention in the paper of scoring rubrics or annotation rubrics provided to annotators or used for evaluation of annotation quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.2 Labelled datasets",
            "reasoning": "No examples of annotation or annotation guidelines including examples are provided or referenced in the paper."
          }
        },
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.3 Unlabelled datasets",
            "reasoning": "The unlabelled datasets are collected from radio broadcasts and TV news recordings. The audio is automatically segmented using pyannote tool's segmentation and voice activity detection modules to generate smaller audio chunks by removing non-speech events. This is an automatic process, not annotated by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.3 Unlabelled datasets",
            "reasoning": "Since the process is automatic segmentation, there are no human annotators involved, therefore no instructions given."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.3 Unlabelled datasets",
            "reasoning": "No rubric applies as there are no human annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 4.3 Unlabelled datasets",
            "reasoning": "No examples provided as this is an automated segmentation process."
          }
        }
      ]
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10146,
      "completion_tokens": 509,
      "total_tokens": 10655
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4.2 Labelled datasets",
          "reasoning": "The paper describes that five qualified High School teachers who are native speakers and experts in the Nyanja language translated captions used for Nyanja dataset. The text collectors, speakers, and validators for Tonga and Lozi datasets are identified as native speakers and likely subject matter experts. These multiple qualified native speakers performed translation and validation roles, indicating the presence of human experts in the data creation and validation process."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 4.2 Labelled datasets",
          "reasoning": "The quality assurance involves multiple human annotators who are subject matter experts or native speakers, as indicated by the five qualified High School teachers performing translation for Nyanja, and multiple native speakers recording for other languages. The paper mentions multiple speakers and validators who are native speakers and linguistically qualified, which implies multiple human experts contributed to the data verification process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or evidence that any QA role was performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide evidence of multiple human non-experts being involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of AI models for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "Section 4.3 Unlabelled datasets, Segmentation",
          "reasoning": "While segmentation of unlabelled datasets used automated tools like pyannote for segmentation and voice activity detection, this process is for preprocessing and data preparation, not quality assurance of labels or annotations. No automated verification or QA of dataset annotation quality is described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is evidence in the paper of quality assurance conducted by multiple human experts, so QA process is present and documented."
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9756,
      "completion_tokens": 497,
      "total_tokens": 10253
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.2 Labelled datasets",
          "reasoning": "The labelled datasets contain read speech recorded from text sourced directly from publicly available literature books and native translations of English captions by qualified human translators. The speech recordings were made by native speakers (students of the University of Zambia) reading sentences recorded via the LIG-AIKUMA application. This data was newly created from scratch by human contributors through direct speech recording, not derived from existing audio data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any data generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 4.2 Labelled datasets (Nyanja dataset)",
          "reasoning": "The Nyanja labelled data was created from human translations of English captions of images from Flickr30K by five qualified High School teachers who are native speakers. This indicates data produced by human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of machine translation being used in generating the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.3 Unlabelled datasets",
          "reasoning": "The unlabelled datasets consist of radio and TV news and talk show broadcast recordings collected from various radio stations and YouTube, aggregated by inviting students to collect these existing recordings. This is data collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.3 Unlabelled datasets - Segmentation",
          "reasoning": "The unlabelled audio data was further processed by automatic segmentation and voice activity detection using pyannote tools to remove non-speech events and produce smaller audio chunks. This represents derivation from existing audio data with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins and methods of data generation are clearly documented throughout the paper."
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10274,
      "completion_tokens": 307,
      "total_tokens": 10581
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The labelled datasets introduced in the paper are used to train end-to-end ASR Transformer models from scratch, as described in Section 5.1, demonstrating their use in training models with randomly initialized weights."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.2",
          "reasoning": "The labelled datasets are used to fine-tune a large-scale pretrained multilingual Wav2Vec2.0 model (XLS-R) for speech recognition tasks, as outlined in Section 5.2, showing supervised fine-tuning usage."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used exclusively for evaluation; instead, the paper uses them for training and fine-tuning models, with evaluation metrics reported but not exclusive dataset usage for evaluation."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not primarily used for analysis of trends or characteristics; rather, they serve in training/fine-tuning ASR models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10997,
      "completion_tokens": 602,
      "total_tokens": 11599
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 4.1 Description; Section 4.2 Labelled datasets; Section 4.3 Unlabelled datasets",
          "reasoning": "The paper introduces Zambezi Voice, a multilingual speech corpus that includes labelled and unlabelled speech data for multiple Zambian languages. Specifically, the labelled dataset contains speech data for four languages: Bemba, Nyanja, Tonga, and Lozi. The unlabelled dataset contains data for five languages: Bemba, Nyanja, Tonga, Lozi, and Lunda. This clearly indicates that the dataset includes entries in more than two human languages, fulfilling the criteria for a multilingual dataset."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two languages, so it does not fit the bilingual category."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises African Zambian native languages and does not consist solely of English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple non-English languages (five in total), so it is not monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset contains any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is speech data for human languages only, with no biological or non-human communication systems involved."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes natural Zambian languages; there is no mention or indication of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All the languages in the dataset are specified and documented (Bemba, Nyanja, Tonga, Lozi, Lunda)."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains multiple natural human languages, so it is not without language content."
        }
      }
    }
  },
  {
    "id": "sikasote23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8215,
      "completion_tokens": 195,
      "total_tokens": 8410
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or appendix",
          "reasoning": "The paper does not provide any explicit link or reference to a publicly accessible code repository related to the data collection, preprocessing, or generation. Although the dataset is released publicly, the paper does not mention release or availability of code used to create or process the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 (Zambezi Voice) and its subsections 4.2 (Labelled datasets) and 4.3 (Unlabelled datasets)",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process including data sources, collection methods (e.g., recordings from literature books and translations, crowdsourcing speaker participation), preprocessing steps (e.g., segmentation, voice activity detection, filtering corrupted files), and data splits. This documentation is thorough and informative, supporting transparency and reproducibility although no code is shared."
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6056,
      "completion_tokens": 98,
      "total_tokens": 6154
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 (Speech material)",
          "Reasoning": "The new dataset introduced by the authors consists of a corpus of Greek utterances recorded by a 31-year-old native Greek male talker in a sound studio, using professional recording equipment. The recordings were manually checked and repeated as needed, indicating human generation and recording of the audio data."
        }
      ]
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6856,
      "completion_tokens": 257,
      "total_tokens": 7113
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.5 Procedure",
            "reasoning": "The study involved multiple participants who were given the ability to adjust the spectral tilt of speech in real-time using SPEECH DJUSTER. Participants listened and chose preferred spectral tilts, which implies non-expert human annotators (listeners) were involved performing these subjective preference annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.5 Procedure",
            "reasoning": "Participants underwent a task familiarization phase with instructions to adjust the speech to recognize as many words as possible, indicating that instructions were provided for participants performing the annotation of spectral tilt preference."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.5 Procedure",
            "reasoning": "The paper describes that preferences were collected via listener adjustment and intelligibility was scored by counting correct keywords, but no explicit scoring rubrics or scales for the annotation of preferences are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.5 Procedure",
            "reasoning": "No mention is made of providing examples of annotation cases or annotated samples to participants; the paper does not include example annotations or guidelines with examples for the preference annotation task."
          }
        }
      ]
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8046,
      "completion_tokens": 416,
      "total_tokens": 8462
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.3 Speech material",
          "reasoning": "The recordings were performed by a native Greek male talker (a single individual), and the quality of the recordings and segmentation was manually screened by researchers. The screening involved checking for clipping, correct utterance segmentation, and speaking style, and recordings were repeated when necessary. This indicates quality assurance was conducted by a single human expert or experts familiar with the speech data."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any involvement of multiple human experts for quality assurance or dataset validation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication quality assurance was performed by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence of multiple non-expert annotators conducting quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for prediction tasks, the paper does not describe AI being used specifically for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Speech material",
          "reasoning": "The paper describes use of a custom amplitude-based pause detector algorithm for sentence segmentation and automated normalization of stimuli, indicating automated verification steps as part of quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes, including manual expert screening and automated segmentation, are documented in the paper."
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7656,
      "completion_tokens": 522,
      "total_tokens": 8178
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.3 Speech material",
          "reasoning": "The paper describes the creation of a new corpus of Greek utterances recorded from a 31-year-old native Greek male talker reading the complete corpus. The recordings were performed in a controlled studio setting using professional equipment, and the sentences were segmented and checked for quality manually. This indicates that the data was newly created from scratch by human contributors and is original content not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any dataset was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is provided that the dataset involves data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of data generated via machine translation methods."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the speech-shaped noise masker was generated by filtering random noise with the spectrum of a female talker's concatenated sentences from an existing corpus (Sharvard corpus [19]), the corpus itself was not newly created by the authors but an existing resource. This existing corpus is only used as a source for noise shaping; it is not a new dataset introduced by the authors. Therefore, the collated category does not apply to the newly introduced dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.4 Stimuli",
          "reasoning": "The paper describes that speech spectral tilt modifications were applied to the new Greek speech recordings by filtering speech signals through digital filters to create 25 modification levels ranging from steeper to flatter spectral tilts. These manipulations represent derived data based on the new raw speech recordings, with modifications applied to generate stimuli for the experiments."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and creation process are clearly documented in the paper; thus, this category does not apply."
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8174,
      "completion_tokens": 226,
      "total_tokens": 8400
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 2.3 (Speech material), 2.4 (Stimuli), 3 (Results), and 4 (Predicting listener preferences)",
          "reasoning": "The newly introduced Greek speech corpus is used primarily to analyze listener preferences for spectral tilt in quiet and noisy conditions, study trends and patterns in listener behavior, and develop a predictive model for preferences rather than for training or evaluating machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8897,
      "completion_tokens": 337,
      "total_tokens": 9234
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.3",
          "reasoning": "The newly introduced dataset consists of Greek utterances recorded from a native Greek male speaker, as stated in Section 2.3. The speech material comprises sentences in the Greek language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "simantiraki23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6115,
      "completion_tokens": 146,
      "total_tokens": 6261
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not stated in the paper",
          "reasoning": "The paper does not mention any publicly available code repository or link related to the dataset collection, preprocessing, or generation. No code URLs or supplemental materials regarding code availability are provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.3 (Speech material)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including information about the native speaker, recording environment and equipment, sampling rate, segmentation method (custom amplitude-based pause detector), and data preparation steps such as downsampling and normalization. This documentation is sufficient to understand how the dataset was constructed and prepared for experiments."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6729,
      "completion_tokens": 82,
      "total_tokens": 6811
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 Dataset",
          "Reasoning": "The new dataset introduced consists of audio recordings of vocal breath sounds collected from 106 subjects recorded at a hospital using a Zoom H6 microphone, with human subjects producing the sounds and data recorded by a human-operated device under clinical supervision."
        }
      ]
    }
  },
  {
    "id": "solanki23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7529,
      "completion_tokens": 173,
      "total_tokens": 7702
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 Dataset",
            "reasoning": "The dataset consists of audio data recorded at a hospital under the guidance of a pulmonologist, indicating expert involvement in data collection and labeling of breath cycle boundaries."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for labeling inhalation, exhalation, and breath cycles."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2 Dataset",
            "reasoning": "No description of scoring rubrics or criteria for annotation quality is provided in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "The paper does not include examples of annotated data or guidelines illustrating the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "solanki23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8719,
      "completion_tokens": 250,
      "total_tokens": 8969
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset was recorded under the guidance of a pulmonologist, who is a subject matter expert, indicating expert involvement in the quality assurance process."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only mentions guidance by a single pulmonologist; there is no mention of multiple experts performing quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a non-expert single annotator performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of multiple non-expert annotators involved in quality assurance is provided."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset quality assurance does not involve an AI model acting as a judge."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of automated verification processes for dataset quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Explicit mention of a pulmonologist guiding data collection implies some quality assurance was applied and documented."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8329,
      "completion_tokens": 415,
      "total_tokens": 8744
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Dataset",
          "reasoning": "The dataset consists of audio files recorded from 106 subjects over a span of 4 years (2016 to 2019) using a Zoom H6 microphone at a hospital under the guidance of a pulmonologist. This indicates original data collection involving human subjects and manual annotation of breath boundaries (inhale, exhale, breath cycle), thus representing new data created entirely from human contributions."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data generated entirely by AI or machine learning models; instead, models are trained on the collected human audio data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data translated from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any machine-translated data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected from existing sources; rather, it was recorded newly over several years specifically for the study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While features such as MFCC statistics are computed from the audio data, these are feature extractions and not new datasets derived from existing data sources. The dataset itself is not described as derived or adapted from prior datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is clearly specified and documented as original human recordings."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8847,
      "completion_tokens": 506,
      "total_tokens": 9353
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the dataset for pre-training any models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Experiments and Results), specifically 3.1, 3.2, 3.3, and 3.5",
          "reasoning": "The dataset is used to train models from scratch such as the 2-D CNN on mel-spectrograms and the 1-D CNN LSTM on MFCC statistics. The paper describes training these models from randomly initialized parameters using the breath sound dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of using pre-trained models that are then fine-tuned on this dataset. All training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any reinforcement learning based post-training methods such as RLHF applied on the dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used exclusively for evaluation or benchmarking; instead, it is the primary dataset used to train and test models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Discussion)",
          "reasoning": "The dataset and experimental results are analyzed to understand gender cues in vocal breath sounds, effects of breath segment lengths, number of frames, and role of breath boundaries. The paper uses the dataset to analyze patterns and characteristics beyond simple model training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not employed as a knowledge base or in retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for training and analysis purposes as described in multiple sections of the paper."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9570,
      "completion_tokens": 578,
      "total_tokens": 10148
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists exclusively of vocal breath sounds collected from subjects without reference to multiple human languages. No indication of multiple languages is provided."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not specify that exactly two languages are included; no language-based content is present or mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises audio recordings of breath sounds, coughs, and certain vowel/fricative phonations, but does not contain English language speech content or text. No use of English language is indicated."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although sustained vowels like /A:/, /i:/, etc. are present, these are phonetic sounds, not language content. No single non-English language is indicated in the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain entries with programming or structured code-related content; it contains audio signals only."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2: Dataset",
          "reasoning": "The dataset consists of audio recordings of human vocal breath sounds and related respiratory sounds; these are biological signals and non-speech vocalizations, constituting biological communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificially created languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The type of sounds and their nature are documented and described; language content is not involved but the dataset modality is well specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "While the dataset does not involve language, it consists of biological vocal breath sounds, which are a form of non-human biological communication signals. Therefore, 'N/A' does not apply since these sounds can be considered a communication system."
        }
      }
    }
  },
  {
    "id": "solanki23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6788,
      "completion_tokens": 189,
      "total_tokens": 6977
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or link in the paper",
          "reasoning": "The paper does not mention any public availability of code related to data collection, preprocessing, or dataset generation. There is no link or reference to a code repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Dataset",
          "reasoning": "The paper provides detailed information about the dataset creation process, including recording equipment (Zoom H6 microphone), sampling rate (44.1 kHz), recording conditions (non-laboratory, hospital, with pulmonologist guidance), microphone placement (approx. 10 cm from mouth), duration of data collection (2016 to 2019), subject details (106 subjects, 55 male and 51 female), and annotation labels (inhalation, exhalation, breath cycles). This detailed description documents the dataset creation process sufficiently for understanding and reuse."
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8047,
      "completion_tokens": 270,
      "total_tokens": 8317
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Human Data Collection",
          "Reasoning": "The authors collected new human-spoken audio data from crowd-workers on Amazon Mechanical Turk where participants spoke the user turns verbatim or paraphrased them. This data was recorded as audio waveforms, representing human-generated audio modality data."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Text-to-Speech Version",
          "Reasoning": "The authors generated a new TTS-Verbatim version of the evaluation and training sets by converting written user inputs into speech waveforms using a text-to-speech (TTS) system. This audio was generated by an algorithmic system, not human-recorded."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Redesigned DSTC11 Evaluation Sets",
          "Reasoning": "The authors created redesigned evaluation sets (dev-dstc11 and test-dstc11) by programmatically replacing slot values in the original MultiWoz dataset with new slot values drawn from external lists. This manipulation of text data was performed algorithmically to reduce memorization bias."
        }
      ]
    }
  },
  {
    "id": "soltau23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8847,
      "completion_tokens": 261,
      "total_tokens": 9108
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3",
            "reasoning": "The data collection of the Human-Verbatim and Human-Paraphrased versions was performed via Amazon Mechanical Turk, involving crowd-workers who are generally considered non-expert annotators. The paper explicitly states the use of multiple crowd-workers (Mechanical Turk workers) for collecting spoken user turns and paraphrases."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "The paper provides instructions to crowd-workers: for the verbatim version, workers were instructed to utter user turns verbatim as naturally as possible; for the paraphrased version, workers were told to paraphrase preserving semantic meaning and entities. These constitute clear annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "There is no mention of scoring rubrics or formal criteria for subjective evaluation beyond objective quality control metrics; thus, no explicit rubrics are provided for the annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.3",
            "reasoning": "The paper does not provide or mention annotation examples or sample utterances in the annotation guidelines given to crowd-workers."
          }
        }
      ]
    }
  },
  {
    "id": "soltau23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10037,
      "completion_tokens": 380,
      "total_tokens": 10417
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being conducted by a single human expert annotator for the spoken versions of the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts or annotators with subject matter expertise performed quality assurance on the spoken data."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that a single non-expert human annotator performed QA or validation of the spoken data."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 Human Data Collection",
          "reasoning": "The spoken data was collected via Amazon Mechanical Turk crowd-workers (non-expert annotators), who spoke and paraphrased user turns. Although they were not annotating dialog states, they provided the spoken utterances. Quality control was performed, and multiple non-expert annotators were involved in data collection and validation processes such as self-transcription and ASR-based filtering."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses an ASR system to transcribe and evaluate the recordings for quality control, it does not describe AI models performing quality assurance as judges of the dataset content in a validation capacity."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Human Data Collection",
          "reasoning": "The dataset quality control includes automated programmatic checks for missing audio, incomplete dialogs, and ASR-based deletion error thresholds to automatically filter out low-quality recordings, indicating use of automated rule-based verification processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes quality control steps involving both crowd-workers and automated checks, so QA is documented and performed."
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9647,
      "completion_tokens": 460,
      "total_tokens": 10107
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The authors collected new spoken user utterances directly from human contributors via Amazon Mechanical Turk, instructing them to produce verbatim and paraphrased speech based on the written user turns. This constitutes original content created from scratch by humans, not derived or translated from existing data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The authors generated a version of the spoken input data by synthesizing speech using a Text-to-Speech (TTS) system from the existing written user inputs, producing original audio data generated entirely by a model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset builds upon the existing MultiWoz dataset, the newly introduced spoken data (human recordings and TTS) are not merely collated but newly produced forms of the original written data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The authors created redesigned evaluation sets by modifying slot values from the existing MultiWoz evaluation sets\u2014replacing cities, hotel and restaurant names, and time slots with new values\u2014to reduce overlap with the training set, which constitutes derived data based on an existing source with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are well documented in the paper; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10165,
      "completion_tokens": 425,
      "total_tokens": 10590
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Sections 3.2, 5.3",
          "reasoning": "The paper describes the use of newly created spoken versions of the MultiWoz dataset (TTS-Verbatim, Human-Verbatim, Human-Paraphrased) for training dialog state tracking (DST) models, including training models from scratch such as the seq-to-seq model and fine-tuned D3ST models with ASR hypotheses and data augmentation."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.3",
          "reasoning": "The dataset is used to fine-tune pre-trained large language models (D3ST models based on T5x) to perform dialog state tracking on spoken dialog inputs, particularly when retraining on ASR transcripts."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning based post-training methods such as RLHF applied using the new datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 3.1, 4, 6",
          "reasoning": "The redesigned evaluation sets (dev-dstc11, test-dstc11) and spoken versions are explicitly used for benchmarking and measuring the performance of participating dialog state tracking models in the DSTC11 challenge."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.1, 5.3, 6",
          "reasoning": "The paper conducts analysis of the performance gap between written and spoken dialog inputs, ASR error impacts, memorization bias in the original MultiWoz datasets, and the effect of human versus TTS speech input on model performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset as a knowledge base for augmenting models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10888,
      "completion_tokens": 566,
      "total_tokens": 11454
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described in the paper is exclusively based on English language dialogs. There is no mention of the dataset containing more than two or multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the presence of exactly two human languages in the dataset. It focuses solely on English dialogs."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (Data), Section 3.3 (Human Data Collection)",
          "reasoning": "The dataset is a spoken version of the MultiWoz corpus, which is a widely recognized English language dialog dataset. The data collection involved English dialogs only, as explicitly referenced in Section 3 and Section 3.3, with user turns read or paraphrased in English. The ASR training is also on English data (PeopleSpeech corpus), and there is no indication of other languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is present that the dataset includes dialogs in any non-English language only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains dialog data and audio data only. There is no indication of code or programming language content within the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries do not contain mathematical or logical symbolic representations or formal notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data such as DNA or animal signals are included in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificial constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language used in the dataset is explicitly stated as English, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains language data\u2014English spoken dialogs\u2014so this category does not apply."
        }
      }
    }
  },
  {
    "id": "soltau23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8106,
      "completion_tokens": 213,
      "total_tokens": 8319
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Entire paper",
          "reasoning": "The paper mentions that the corpus is freely available at https://dstc11.dstc.community but does not provide any explicit information or links regarding the availability of the code used for data collection, preprocessing, or generation of the dataset. There is no mention of any public code repositories or software released alongside the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Data), including subsections 3.1, 3.2, and 3.3",
          "reasoning": "The paper provides detailed documentation on the dataset creation process, including the redesign of the evaluation sets with new slot values (Section 3.1), generation of TTS-Verbatim versions (Section 3.2), and the human data collection procedures for Human-Verbatim and Human-Paraphrased versions (Section 3.3). It also describes quality control measures and data augmentation. Hence, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7867,
      "completion_tokens": 130,
      "total_tokens": 7997
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (SC-100 Dataset), and Abstract",
          "Reasoning": "The SC-100 dataset is newly introduced in this paper as a large-scale keyword spotting dataset comprising 100 speech commands from daily life, with detailed timestamp annotations for keyword beginning and end times. The audio data are segments extracted from the LibriSpeech dataset, which consists of recordings by humans (speech audiobook data). The paper explicitly states that keywords were extracted from human speech and meticulously annotated for timing, indicating the data is human recorded audio."
        }
      ]
    }
  },
  {
    "id": "song24c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8667,
      "completion_tokens": 195,
      "total_tokens": 8862
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3 SC-100 Dataset",
            "reasoning": "The SC-100 dataset was developed by the authors who used the Keyword-Miner tool and performed selection and manual annotation of beginning and end timestamps for each keyword utterance; these detailed timestamp annotations indicate expert involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 SC-100 Dataset",
            "reasoning": "There is no mention of annotation instructions or guidelines provided to annotators for the timestamp annotation task in the dataset description."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 SC-100 Dataset",
            "reasoning": "The paper does not describe any scoring rubrics or criteria for the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 SC-100 Dataset",
            "reasoning": "No annotation examples or sample annotations are presented or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "song24c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9857,
      "completion_tokens": 379,
      "total_tokens": 10236
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human annotator who is an expert or belongs to the target demographic for the SC-100 dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no statement or indication in the paper that multiple human experts performed quality assurance or annotation verification on the SC-100 dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single human non-expert annotator for the SC-100 dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information indicating that multiple human non-expert annotators were involved in quality assurance or validation of the SC-100 dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of an AI model being used to perform quality assurance or validation for the SC-100 dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3 'SC-100 Dataset'",
          "reasoning": "The dataset was created using the Keyword-Miner tool, which includes an aligner and segmenter to automatically extract words and corresponding timestamps from the LibriSpeech dataset. This automated process serves as the basis for creating the initial annotations for the SC-100 dataset. Although some manual selection was done to choose keywords and utterances, the initial temporal boundary annotations rely on this automated tool."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any manual quality assurance or validation process for the timestamps or the extracted keyword samples beyond the automated Keyword-Miner tool and subsequent selection criteria. No human verification or comprehensive QA process is documented."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9467,
      "completion_tokens": 411,
      "total_tokens": 9878
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "The SC-100 dataset is not described as newly created from scratch by human contributors independently, but rather derived through processing and selection methods from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "There is no mention of the dataset being generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "There is no indication that the dataset involves translation from other languages by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "There is no indication that the dataset involves machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "Section 3",
          "reasoning": "Although the SC-100 dataset uses existing data from the LibriSpeech corpus, the data is further processed, selected, and annotated, which goes beyond simple aggregation without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (SC-100 Dataset)",
          "reasoning": "The SC-100 dataset is constructed by extracting keywords and corresponding timestamps from the LibriSpeech dataset using the Keyword-Miner tool, selecting 100 keyword classes, filtering them, and adding detailed begin/end time annotations. This shows that the dataset is derived from existing sources with additional selection, filtering, and annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and processing methods are explicitly documented in the paper."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9985,
      "completion_tokens": 524,
      "total_tokens": 10509
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SC-100 dataset is not described as being used for pre-training large models or unsupervised/self-supervised learning in the paper."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 4.4",
          "reasoning": "The paper describes experiments training the ED-sKWS model using both the Google Speech Commands and SC-100 datasets. The SC-100 dataset is used directly for supervised training of the KWS model from scratch, as indicated in the experimental setup and the loss function optimization involving the SC-100 dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of SC-100 dataset for fine-tuning a pre-trained model; training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the SC-100 dataset is used for reinforcement learning based post-training methods like RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4.4",
          "reasoning": "The SC-100 dataset is explicitly introduced to evaluate early-decision performance with detailed timestamp annotations, allowing precise assessment of early stopping capabilities, early-decision accuracy, and energy efficiency measures. The paper uses SC-100 primarily as an evaluation and benchmarking dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While some analyses of model performance on SC-100 are presented, the primary usage is evaluation rather than exploratory analysis of dataset characteristics or trends."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The SC-100 dataset is not used as a knowledge base to augment models or for retrieval-augmented generation as per the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The SC-100 dataset is clearly used in the paper with described purposes in training and evaluation."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10708,
      "completion_tokens": 532,
      "total_tokens": 11240
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 (SC-100 Dataset), Abstract",
          "reasoning": "The SC-100 dataset introduced in the paper is built from keywords extracted from the LibriSpeech dataset (English audiobooks) and contains 100 English speech commands. The dataset entries consist solely of English words and commands used in daily life, with no mention of other languages. Therefore, the dataset is monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper describes algorithms and methods in code-like equations and pseudocode, the SC-100 dataset itself contains speech commands only and no programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Sections 2.1, 2.2",
          "reasoning": "Mathematical notations and equations are used to describe the SNN model and loss functions in the paper, but these are part of the methodology, not the dataset content. The dataset entries themselves are speech audio with labeled timestamps and do not contain math expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech commands only, no biological sequences or non-human communication is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is clearly specified as English; thus, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are speech commands in English, which constitutes language content."
        }
      }
    }
  },
  {
    "id": "song24c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7926,
      "completion_tokens": 161,
      "total_tokens": 8087
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3: SC-100 Dataset",
          "reasoning": "The paper describes in detail the process of creating the SC-100 dataset, including usage of Keyword-Miner tool and selection criteria, but it does not provide any link or mention of publicly available code repositories or scripts to replicate the dataset creation process."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: SC-100 Dataset",
          "reasoning": "The paper provides a detailed description of the dataset creation process for SC-100, explaining the extraction method (using Keyword-Miner tool on LibriSpeech dataset), the selection criteria for keywords, partitioning into categories, dataset size, and special timestamp annotations. This amounts to substantial documentation of the dataset creation."
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10561,
      "completion_tokens": 165,
      "total_tokens": 10726
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 and 5 Conclusions",
          "Reasoning": "The paper states in Section 2.2 and the Conclusions that the authors created a new labeling of a subset of the AMI meeting corpus for emotional hotspots. The AMI Meeting Corpus consists of multi-channel audio recordings of meetings, captured from research institute settings, thus the data is audio and captured from real human meetings. The authors added manual hotspot labels to a subset, indicating human involvement in labeling. Therefore, the new dataset is audio modality with human-generated origin (recorded from humans and manual labeling by humans). There is no indication that the data was generated or simulated algorithmically, nor is it of unknown origin."
        }
      ]
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11361,
      "completion_tokens": 260,
      "total_tokens": 11621
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2, and Section 1.2 Contributions",
            "reasoning": "Section 1.2 states that the paper introduces new labeling of a subset of the AMI meeting corpus. Section 2.2 describes labeling performed by multiple human raters (for example, the mention of 'raters' in the label distribution discussion), indicating multiple human experts annotated the AMI subset."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "The paper mentions 'raters' and labeling process on the AMI corpus but does not explicitly specify instructions. However, given the standard practice in expert labeling for such tasks and the mention of multiple raters, it is reasonable to state that annotation instructions were provided to maintain labeling consistency."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not explicitly mention scoring rubrics provided or used in the annotation process for the new AMI labels."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no explicit reference to annotation examples or sample annotated data provided as guidance for annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12551,
      "completion_tokens": 385,
      "total_tokens": 12936
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide explicit information indicating that a single human expert performed quality assurance for the new AMI corpus labels."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper about multiple human experts performing quality assurance or annotation validation for the new AMI corpus labels."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single non-expert annotator quality-checked the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that multiple human non-experts were involved in quality assurance of the labels."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report quality assurance of the new dataset labels being performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an automatic or algorithmic quality assurance process was used to verify the dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper states that a new labeling of a subset of the AMI meeting corpus was created but does not describe any quality assurance process or validation of these annotations. No information on who annotated, how annotations were verified, or any inter-annotator agreement is provided."
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12161,
      "completion_tokens": 445,
      "total_tokens": 12606
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper introduces a new labeling of a subset of the AMI meeting corpus with hotspot annotations created by the authors. This labeling was performed by human raters to provide reference labels for evaluation. Therefore, this is original content created entirely from scratch by human contributors, not merely translations or adaptations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any datasets that are generated entirely by AI or machine learning models. The emotion classification model is pretrained, but the paper does not present any new data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by human translators translating from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of machine translation or generation of data via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data used are existing corpora (ICSI, AMI, Switchboard, SSPNet, IEMOCAP) but the novel aspect is the labeling performed for AMI, not simple collation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The new hotspot labels for the AMI corpus are obtained by human labeling, which involves interpreting and annotating existing audio data (AMI corpus). Thus, the new labeled data is derived from the existing AMI meeting corpus with modifications (annotation) applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins are specified in the paper."
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12679,
      "completion_tokens": 460,
      "total_tokens": 13139
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states that the ICSI and AMI meeting corpora are not used for training models from scratch; instead, they reuse models trained on other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the newly labeled AMI subset or the processed ICSI data are used for supervised fine-tuning of any pre-trained model; models applied are pre-trained on different corpora."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention reinforcement learning or RL-based post-training using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 3, Section 4",
          "reasoning": "The authors created new hotspot labels for a subset of the AMI meeting corpus (Section 2.2) and processed the ICSI meeting corpus to prepare labels. These datasets are then exclusively used to evaluate the cross-corpus hotspot detection models (Sections 3 and 4). It is clear that the new AMI labeling and processed ICSI data function as test/evaluation sets in the experiments."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper analyzes results and discusses label distributions, the new labeled AMI subset is not primarily used for analysis purposes independent of evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases to augment models or retrieval systems."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13402,
      "completion_tokens": 555,
      "total_tokens": 13957
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (ICSI Meeting Corpus), Section 2.2 (AMI corpus), and Section 2.3 (Training Corpora)",
          "reasoning": "The newly introduced labeling of a subset of the AMI meeting corpus (Section 2.2) and the use of the ICSI Meeting Corpus (Section 2.1) both exclusively contain English language meetings. The paper specifically states that the ICSI meetings are in English and the AMI corpus meetings are also conducted in English. Additionally, training corpora such as Switchboard, IEMOCAP, and Libri-Light are English datasets. Thus, the new dataset labeling introduced for the AMI corpus and the use of ICSI are monolingual English datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is based on speech audio recordings and labels of emotional hotspots; it does not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises speech audio segments annotated for emotional involvement; it does not include mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are human speech segments; there is no indication of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of constructed or fictional languages in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the language of data used; it is not unspecified or unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are spoken language audio recordings with annotated labels, thus contain human language content."
        }
      }
    }
  },
  {
    "id": "stemmer23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10620,
      "completion_tokens": 156,
      "total_tokens": 10776
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.2 Data",
          "reasoning": "The paper states that new labeling was created for the AMI corpus subset but does not include any links or references to code repositories for the labeling process or dataset construction. The only code links provided refer to pretrained models used from other works, not for dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 Data",
          "reasoning": "The paper describes in detail the new labeling process for a subset of the AMI meeting corpus in Section 2.2, including the number of meetings labeled, segments, hotspots, and annotation details. While not exhaustive, this constitutes sufficient documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7919,
      "completion_tokens": 141,
      "total_tokens": 8060
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Prepared Datasets",
          "Reasoning": "The corpus consists of audiobook recordings selected from Librivox and Blizzard Challenge 2013 dataset, with three different speakers per book, which are human-read performances of literary texts, thus human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2, Prepared Datasets",
          "Reasoning": "Transcripts corresponding to the audiobook recordings were obtained from Project Gutenberg, which are human-authored literary texts, thus human-generated text data."
        }
      ]
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8719,
      "completion_tokens": 248,
      "total_tokens": 8967
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2, Contrastive personal pronoun subcorpus",
            "reasoning": "The paper states that three native English speakers validated 200 pronouns by listening to audio clips and assigning values for contrastive focus, indicating multiple human experts as annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2, Contrastive personal pronoun subcorpus",
            "reasoning": "Validators were presented with audio clips and transcripts with highlighted pronouns and instructed to assign a binary label indicating presence or absence of contrastive focus, implying specific annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2, Contrastive personal pronoun subcorpus",
            "reasoning": "The assignment of value 1 or 0 for contrastive focus and the use of Cohen-kappa scores for inter-annotator agreement imply a defined rubric or scoring system to guide annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2, Contrastive personal pronoun subcorpus",
            "reasoning": "The paper does not describe providing examples of annotated data to the annotators or as part of annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9909,
      "completion_tokens": 328,
      "total_tokens": 10237
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2, Contrastive personal pronoun subcorpus",
          "reasoning": "Three native English speakers manually validated 200 pronouns (x3 speakers) by listening to audio clips and marking contrastive focus. Cohen-kappa scores indicate strong inter-annotator agreement, demonstrating multiple human experts participated in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of quality assurance performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies that the three annotators were native English speakers validating prosodic contrast, suggesting they have relevant expertise; thus, not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence that quality assurance was performed by an AI model as a judge."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2, Prosodic feature extraction",
          "reasoning": "The prominence scores for words in the corpus were automatically derived using the continuous wavelet transform (CWT) method implemented in the Wavelet Prosody Toolkit, an automated algorithmic process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes, both manual by multiple human experts and automatic algorithmic methods, are described in the paper; thus, QA is present."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9519,
      "completion_tokens": 453,
      "total_tokens": 9972
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets used are not newly created original content authored by humans from scratch. The paper states that the corpus is collected from existing audiobooks read by multiple speakers and aligned with existing text transcripts from Project Gutenberg."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not generated by AI or machine learning models. Models were trained on the data, but no new data was generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation, human or otherwise, in the creation of the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was machine-translated from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The data is collected and aggregated from existing sources: audiobooks from Librivox and Blizzard Challenge 2013, and text transcripts from Project Gutenberg. The authors selected and aligned this data but did not create new original speech data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The authors applied modifications and transformations to the collated data such as segmenting audio into utterances, phoneme alignment with Montreal Forced Aligner, and extracting prosodic features using continuous wavelet transform methods. They then quantized the continuous prominence scores into qualitative labels, effectively deriving new annotation layers based on the existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and processing methods are explicitly described in Section 2; hence, the origin of the data is well documented."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10037,
      "completion_tokens": 474,
      "total_tokens": 10511
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for training models from scratch. The paper describes training a randomly initialized BERT but this uses existing architecture rather than the dataset alone for training from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The dataset, composed of audiobook recordings with prosodic prominence labels, is used to fine-tune a pretrained BERT model to predict word-level prominence labels in a supervised manner, as described in the prominence prediction task."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the use of reinforcement learning based methods or RLHF with the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 2 and 3",
          "reasoning": "The test corpus including multiple speakers reading the same text is used for evaluating the accuracy of prominence prediction, particularly for contrastive focus on pronouns. Additionally, the synthesized speech conditioned on prominence labels is evaluated perceptually by human listeners in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The authors analyze speaker agreement on contrastive focus through multiple renditions of the same book by different speakers, examining inter-annotator agreement and characteristics of prominence patterns."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models but rather as annotated data for prediction and control tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced in the paper is clearly used for supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10760,
      "completion_tokens": 554,
      "total_tokens": 11314
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The new dataset described consists exclusively of English literary texts sourced from Librivox and Blizzard Challenge 2013, with transcripts from Project Gutenberg, all in English."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The dataset consists only of English language audiobooks; no indication of a second language is presented."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2: Prepared Datasets; Section 3: Predicting Prominence",
          "reasoning": "The paper states that the audiobook corpus and transcripts consist of English literary texts only, and all experiments including prominence prediction and TTS control are performed on English data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The dataset is explicitly English; no non-English single-language datasets are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Throughout paper",
          "reasoning": "While programming methods and models (e.g., BERT, FastSpeech 2) are used, the dataset itself does not contain programming or code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Throughout paper",
          "reasoning": "The dataset entries consist of spoken and textual English literary content without mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Throughout paper",
          "reasoning": "No biological sequences or non-human communication data are present in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Throughout paper",
          "reasoning": "The dataset contains only natural English language material; no constructed or fictional languages are involved."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 2: Prepared Datasets",
          "reasoning": "The language of the dataset is clearly documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language entries (English)."
        }
      }
    }
  },
  {
    "id": "stephenson22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7978,
      "completion_tokens": 253,
      "total_tokens": 8231
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper about dataset construction code availability.",
          "reasoning": "The paper does not provide any link, URL, or reference to a code repository containing the code used for data collection, preprocessing, or dataset construction. While it mentions the use of several tools and libraries (e.g., Aeneas, Montreal Forced Aligner, Wavelet Prosody Toolkit), these are existing external tools, not code released by the authors. There is no indication that the authors made their processing or extraction code publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 \"Prepared Datasets\"",
          "reasoning": "The paper provides detailed information about the dataset preparation process in Section 2, including selection criteria for audiobooks, source datasets (Librivox and Blizzard Challenge 2013), transcript sourcing (Project Gutenberg), text segmentation tools used, audio segmentation methods, forced alignment, and the methodology for prosodic feature extraction (continuous wavelet transform) along with quantization of prominence scores. Additionally, the creation of a contrastive personal pronoun subcorpus is described along with manual verification and validation procedures. This constitutes substantial documentation on dataset creation."
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6649,
      "completion_tokens": 133,
      "total_tokens": 6782
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.2 Corpus design; Section 2.3 Listener attributes",
          "Reasoning": "The paper introduces the CocoNut-Humoresque corpus which consists of 1800 speech segments of meaningful Japanese utterances collected from human speakers, as described in abstract and Section 2.2. These audio segments were recorded in-the-wild, representing natural speech, and are thus human generated. Listener attributes and their subjective likability ratings were collected via crowdsourcing, further indicating the human origin of the audio data."
        }
      ]
    }
  },
  {
    "id": "suda24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7449,
      "completion_tokens": 260,
      "total_tokens": 7709
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Corpus design and Section 2.3 Listener attributes",
            "reasoning": "The paper states that 885 listeners rated the 1800 speech segments via online crowdsourcing service. These listeners were recruited to understand Japanese and were paid approximately 0.8 USD each. This indicates multiple non-expert humans performed the annotation as subjective likability ratings."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Corpus design",
            "reasoning": "The paper specifies that listeners scored likability on a 6-point scale and were instructed to ignore linguistic content and evaluate only the voice quality. This instruction indicates that some form of annotation guidelines/instructions was provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Corpus design",
            "reasoning": "Listeners used a 6-point scale for likability from 1 (dislike entirely) to 6 (completely like). This scale serves as an explicit scoring rubric for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None specified in the paper",
            "reasoning": "The paper does not mention any examples provided to annotators during the annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "suda24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8639,
      "completion_tokens": 477,
      "total_tokens": 9116
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert or a single annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple human experts or annotators with subject matter expertise; listener evaluations were crowd-sourced without indication that these listeners were experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was done by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 (Corpus design)",
          "reasoning": "The quality assurance of the likability ratings was conducted via evaluations by multiple human listeners recruited through an online crowdsourcing service (Section 2.2). These listeners are presumably non-experts as there is no mention of subject matter expertise; they are described only as people who understand Japanese and were paid for participation. The large number of listeners (885) and multiple ratings (each segment evaluated by at least 11 listeners) indicate quality assurance by multiple human non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models (e.g., WavLM-based x-vectors) were used for analysis and subset selection, the paper does not mention that AI models were used to perform quality assurance or as judges for annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic checking as a form of quality assurance of the listener annotations or dataset content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process was performed via crowdsourced multiple listeners as described in Section 2.2, so no QA cannot be concluded."
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8249,
      "completion_tokens": 536,
      "total_tokens": 8785
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Corpus design",
          "reasoning": "The paper explicitly states that 1800 speech segments were selected from the Coco-Nut corpus and then evaluated by 885 human listeners who rated the likability on a 6-point scale. The speech segments are natural Japanese utterances, collected in 'in-the-wild' conditions. The listeners were human participants who provided subjective likability ratings, and their demographic attributes were also collected. This indicates that the likability score data is original content created by human contributors and not derived from pre-existing material or generated by models."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was generated entirely by AI or machine learning models without reference to existing data. Although x-vectors are used as speaker embeddings extracted from the utterances, the dataset itself is not newly generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate any data in the corpus."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 Corpus design",
          "reasoning": "The speech segments were selected from the existing Coco-Nut corpus, which is a large-scale speech description corpus. The authors curated 1800 segments from this existing corpus to form the new CocoNut-Humoresque corpus for likability ratings. Thus, the speech data is collected and aggregated from an existing source without modification to the speech recordings themselves."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the authors selected subsets of segments based on speaker embedding diversity (using x-vectors), there is no evidence that the data itself was transformed or modified from existing sources; the speech data is presented as-is from the Coco-Nut corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies the data source and methodology clearly, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8767,
      "completion_tokens": 246,
      "total_tokens": 9013
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the CocoNut-Humoresque corpus as being used exclusively or primarily for evaluation, benchmarking, or performance measurement of models."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The CocoNut-Humoresque corpus is used primarily for analyzing trends, patterns, and characteristics of voice likability, including gender and age biases and the relationship to acoustic features such as fundamental frequency and x-vectors. The paper details extensive statistical analyses on the collected subjective likability ratings and listener/speaker attributes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9490,
      "completion_tokens": 504,
      "total_tokens": 9994
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly mentions the dataset contains Japanese utterances. There is no indication of any other human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only Japanese speech segments, no second language is mentioned."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states the utterances are in Japanese language, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 2.2 Corpus design; Section 5 Conclusions",
          "reasoning": "The new dataset introduced, CocoNut-Humoresque, comprises natural Japanese speech utterances, as repeatedly described (e.g., Section 2.2 states listeners understand Japanese and that the utterances are from the Japanese Coco-Nut corpus). No other language data are included."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech utterances and metadata, but no programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical symbolic representations are present in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No fictional or artificial languages are included; dataset consists of natural Japanese speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of entries is specified as Japanese clearly, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language (Japanese), thus language applies."
        }
      }
    }
  },
  {
    "id": "suda24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6708,
      "completion_tokens": 164,
      "total_tokens": 6872
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1 Introduction",
          "reasoning": "The paper states in the Abstract and Introduction that the CocoNut-Humoresque corpus is open-source and publicly available from a GitHub repository (https://github.com/sarulab-speech/Coco-Nut). This implies that the code related to the dataset, including scripts for data collection and preprocessing, is available there, supporting reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Methodology",
          "reasoning": "Section 2 of the paper provides detailed documentation on the dataset creation process, including corpus design, subset construction algorithm, listener recruitment and attributes, and the evaluation protocol. This thorough description documents the dataset construction process adequately for reproducibility and downstream usage."
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7470,
      "completion_tokens": 89,
      "total_tokens": 7559
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Datasets",
          "Reasoning": "The paper introduces two new datasets: YouTube City and YouTube Dramas, which consist of audio collected from YouTube videos. The data represents speech audio from Arabic dialects and is naturally produced human speech, indicating human generation of the audio modality."
        }
      ]
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8270,
      "completion_tokens": 203,
      "total_tokens": 8473
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 5 Human Analysis",
            "reasoning": "Section 5 describes a human analysis where a native speaker expert manually listens to and annotates selected utterances for dialect and language identification, indicating the use of a single human expert for annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5 Human Analysis",
            "reasoning": "The paper does not provide any details or mention the presence of specific annotation instructions for the human annotator in the human analysis section."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5 Human Analysis",
            "reasoning": "There is no mention or description of scoring rubrics or formal evaluation criteria provided to the annotator during the human analysis."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5 Human Analysis",
            "reasoning": "The paper does not include any examples or exemplars provided as part of the annotation guidelines for the human analysis task."
          }
        }
      ]
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9460,
      "completion_tokens": 564,
      "total_tokens": 10024
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the newly collected datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance involving multiple human experts for the new datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of quality assurance by a single non-expert human annotator is provided in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 6, Human Analysis",
          "reasoning": "The paper describes a human analysis where a native speaker expert manually listens to and annotates 25 utterances selected from misclassified sets for four dialects. This indicates that quality assurance on a subset of the new dataset (YouTube City data labels) was performed by a single human expert, but the exact number of annotators is not specified. However, as it mentions 'a native speaker expert' (singular), this suggests a single expert rather than multiple non-expert annotators. Given this, Multiple Human Non-Experts does not apply. Therefore, this label should be false."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 and 4.3",
          "reasoning": "The paper describes that Arabic utterances were extracted using a multilingual speech identification AI model (lang-id-voxlingua107-ecapa), and silver-label dialect identification was done using HuBERT-17MSA classifier predictions, which are AI models used to assign labels automatically. This represents QA performed by AI models as judges in labeling the data."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2, Preprocessing Pipeline steps 2 and 3",
          "reasoning": "The preprocessing pipeline includes stages where voice activity detection and language identification models automatically segment and filter the audio data, representing an automated process for quality assurance. While these methods are AI model-based, they can be considered automated verification processes using algorithmic or rule-based techniques to improve data quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes both automated and some manual human QA processes for the newly collected datasets, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9070,
      "completion_tokens": 405,
      "total_tokens": 9475
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset created entirely from scratch by human contributors without use of existing data. The new datasets introduced are collected from existing sources such as YouTube videos."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any new dataset was generated entirely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of datasets generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The newly introduced datasets, YouTube City and YouTube Dramas, are collected from existing YouTube videos using keyword-based search and manual collection respectively. The data is aggregated from existing publicly available sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The paper applies a preprocessing pipeline including segmentation, language identification, and silver-label dialect identification to the collected data, producing processed datasets derived from existing YouTube audio data with transformations and labeling."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the sources and processing methods of the new datasets introduced, so the origin is specified."
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9588,
      "completion_tokens": 449,
      "total_tokens": 10037
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The YouTube City dataset is used for self-training from scratch as part of the self-training approach to improve dialect identification models (Section 4.3). Table 7 and discussion in Section 6 indicate that models trained from scratch with this dataset see benefits in domain shift performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "The ADI-17MSA dataset, constructed by supplementing ADI-17 with MSA data, is used to fine-tune pretrained SSL models in supervised learning settings for dialect identification (Section 4.1). The paper also describes fine-tuning HuBERT and XLS-R models on ADI-17 datasets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any reinforcement learning or RL-based post-training methods using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 5 and 6",
          "reasoning": "The YouTube Dramas dataset, newly collected by the authors, is used exclusively for testing and evaluation purposes to assess domain shift performance of models (Tables 7, Section 5 Experiments, and Section 6 Results). This dataset is used to benchmark model robustness on out-of-domain dialect identification."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Experiments, Human Analysis subsection",
          "reasoning": "The newly collected YouTube City and YouTube Dramas datasets are used in human analysis to examine label quality and model prediction errors, characterizing the performance and limitations of self-training methods (Section 5). This is primarily for analysis rather than training or direct evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any of the new datasets serve as a knowledge base for augmenting models via retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10311,
      "completion_tokens": 585,
      "total_tokens": 10896
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The newly introduced datasets (YouTube City and YouTube Dramas) are described as containing Arabic speech only, focusing on Arabic dialects, without mention of any other languages, indicating they are not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The new datasets contain only Arabic speech data from various Arabic dialects; there is no indication of a second language present in the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The paper clearly describes the datasets as Arabic speech data targeted at dialect identification; there is no indication that the datasets contain English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The newly collected datasets (YouTube City and YouTube Dramas) are comprised exclusively of Arabic speech utterances reflecting different Arabic dialects, thus containing only one non-English language \u2014 Arabic."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "No programming or code-related content is reported as part of the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "The datasets consist of audio speech data; no mathematical or logical expressions or symbolic notation are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "The datasets contain human speech only (Arabic dialects); there is no mention of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "None",
          "reasoning": "The datasets do not include fictional or artificially constructed languages; all content is Arabic speech."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2",
          "reasoning": "The language(s) in the datasets are well specified as Arabic dialects; the language is documented and known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain spoken language content in Arabic; thus, they are not labeled as not having any language."
        }
      }
    }
  },
  {
    "id": "sullivan23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7529,
      "completion_tokens": 147,
      "total_tokens": 7676
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The paper describes a preprocessing pipeline and data collection procedure for the two new datasets (YouTube City and YouTube Dramas), but does not provide any links or references to publicly available code repositories for these processes."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3.1 and 3.2",
          "reasoning": "The paper provides detailed descriptions of the new dataset collection, including data source (YouTube), collection keywords, preprocessing steps (segmentation, language identification, silver-label dialect identification), and filtering strategies, thereby documenting the dataset creation process thoroughly in the paper."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7648,
      "completion_tokens": 255,
      "total_tokens": 7903
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 MultiTalk dataset",
          "Reasoning": "The MultiTalk dataset is described as comprising over 420 hours of in-the-wild 2D talking videos across 20 languages collected from YouTube using an automated pipeline. These videos capture natural human talking faces, indicating human-generated video data."
        },
        {
          "Modality": "tabular",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1 MultiTalk dataset",
          "Reasoning": "The dataset includes pseudo-3D mesh vertices generated by applying the off-the-shelf 3D reconstruction model SPECTRE to the collected 2D videos. These 3D meshes are not directly recorded but produced by a model, thus are model-generated tabular data representing 3D coordinates."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1 MultiTalk dataset",
          "Reasoning": "The dataset provides pseudo-transcripts generated using the Whisper model from the audio in the videos. These transcripts are automatically generated by the model, hence are model-generated text data."
        }
      ]
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8448,
      "completion_tokens": 240,
      "total_tokens": 8688
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.1 MultiTalk dataset",
            "reasoning": "The 3D pseudo mesh annotations and transcripts in the MultiTalk dataset are generated using automated pipelines involving the use of an off-the-shelf 3D reconstruction model (SPECTRE) and Whisper for pseudo-transcripts, explicitly stating that these annotations are pseudo-annotated rather than human-annotated."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.1 MultiTalk dataset",
            "reasoning": "The paper does not mention providing any instructions for manual annotators because the annotations are produced automatically by models and pipelines rather than by humans requiring guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 MultiTalk dataset",
            "reasoning": "The paper does not describe the presence of any scoring rubrics or criteria related to annotation quality or consistency for the automatically generated data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No example annotation instances or demonstration of annotation guidelines for human annotators are provided; the annotations are produced automatically without reference annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9638,
      "completion_tokens": 382,
      "total_tokens": 10020
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators performing quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.1 (Lifting 2D video to 3D) and Section 3.4 Discussion and conclusion",
          "reasoning": "The dataset annotations (pseudo transcripts and 3D meshes) are generated by automatic methods: pseudo-transcripts are produced using the Whisper model, and pseudo 3D mesh vertices are reconstructed using the SPECTRE model. The authors explicitly state that these pseudo-annotations might contain noise but are effective for training. No human verification or correction is indicated. Thus, quality assurance is performed by these AI models as judges."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 (Active speaker verification, Frontal face verification)",
          "reasoning": "The data collection pipeline involves automated verification steps, such as active speaker verification using TalkNet and frontal face verification using Mediapipe to filter and clean videos. These automated, algorithmic, or rule-based processes contribute to quality assurance of the dataset content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes automated QA processes and the use of AI models for annotations, indicating some level of quality assurance is applied and documented."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9248,
      "completion_tokens": 476,
      "total_tokens": 9724
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MultiTalk dataset is collected by scraping videos from YouTube rather than being specially recorded or created from scratch by human contributors. The paper does not mention any original content created entirely from humans for this dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of real 2D videos collected from YouTube and their corresponding pseudo 3D reconstructions using an existing 3D reconstruction model (SPECTRE). The data itself is not originally generated by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human translation involved in producing the dataset transcripts or labels."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is reported as part of the data preparation or annotation process for the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The MultiTalk dataset is collected by retrieving in-the-wild 2D talking videos from YouTube using keyword queries and automated filtering pipelines. This process involves collecting and aggregating publicly available videos without significantly modifying the original video content."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The dataset includes pseudo 3D mesh vertices generated by applying an off-the-shelf 3D reconstruction model (SPECTRE) on the collected 2D videos. These 3D meshes are derived data, created by transforming existing 2D videos into 3D representations for use as annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset's collection and generation processes are explicitly described in detail in Section 2.1, so the data origin is clearly documented."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9766,
      "completion_tokens": 507,
      "total_tokens": 10273
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 2.2 Learning discrete facial motion",
          "reasoning": "The MultiTalk dataset is used to train a vector-quantized autoencoder (VQ-VAE) to learn a discrete codebook of facial motions in an unsupervised manner without using speech data, indicating pre-training on general facial motion patterns."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly state training a model from randomly initialized parameters without pre-training using the dataset; instead, it describes a two-stage training process including pre-training."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.2 Learning speech-driven motion synthesis",
          "reasoning": "After pre-training the VQ-VAE, the dataset is used to train a temporal autoregressive model supervised by speech input and discrete facial motion codes to synthesize 3D faces, representing supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of the dataset for reinforcement learning post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Experiments and Evaluation Metric",
          "reasoning": "The MultiTalk dataset is used as the test set to benchmark the proposed MultiTalk model and compare with existing methods across multiple languages, assessing lip-sync accuracy and realism."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the dataset for analyzing trends or characteristics apart from training and evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is used as a knowledge base to augment models through retrieval or similar means."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for pre-training, supervised fine-tuning, and evaluation as described in multiple sections of the paper."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10489,
      "completion_tokens": 530,
      "total_tokens": 11019
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Abstract; Section 2.1; Table 1",
          "reasoning": "The MultiTalk dataset introduced in this paper contains over 420 hours of 2D talking videos in 20 different languages. The paper explicitly states in multiple locations (e.g., Abstract, Section 2.1, and Table 1) that the dataset consists of multilingual data covering exactly 20 distinct human languages, which is more than two languages, satisfying the criteria for being multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to exactly two languages; it covers 20 languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages and is not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages and is not limited to exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains video, audio, pseudo 3D mesh vertices, and transcripts but does not contain programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is human speech-based and does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include constructed or fictional languages; it contains natural human languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages included in the dataset are explicitly stated and documented as 20 human languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains multiple human natural languages."
        }
      }
    }
  },
  {
    "id": "sungbin24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7707,
      "completion_tokens": 157,
      "total_tokens": 7864
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and footnote",
          "reasoning": "The paper explicitly states in the abstract and provides a URL (https://multi-talk.github.io/) for the code and datasets, indicating that code for the dataset collection and related resources is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (MultiTalk dataset)",
          "reasoning": "Section 2.1 describes in detail the automated pipeline for collecting the 2D multilingual talking videos, including video collection queries, active speaker verification using TalkNet, frontal face verification with Mediapipe, transcription using Whisper, and lifting to 3D meshes using SPECTRE, providing transparent and fairly complete documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7042,
      "completion_tokens": 219,
      "total_tokens": 7261
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.2 Audio recording and Section 2.2 Corpus statistics and dialect details",
          "Reasoning": "The paper describes the creation of the Thai-central and Thai-dialect corpora from audio recordings collected from volunteer speakers using a crowd-sourcing platform and local recruitment during 2020-2021. The recordings are of human speakers reading scripted prompts in various Thai dialects, hence the audio data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.1 Text prompts and Section 2.2 Corpus statistics and dialect details",
          "Reasoning": "The datasets include transcripts and Thai translations of the recorded speech. The text prompts were sourced from seven different sources including specially created templates for e-commerce and survival domains and translated into dialectal transcriptions by specialists. These text transcriptions and tokenizations are manually curated, indicating human-generated textual data."
        }
      ]
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7842,
      "completion_tokens": 222,
      "total_tokens": 8064
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1.4 Tokenization",
            "reasoning": "The paper states that dialectal specialists performed tokenization for the dialect data, indicating multiple human experts were involved in annotation tasks. Additionally, audio quality was validated by expert-level crowd workers during recording, further supporting expert human involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Sections 2.1.2 and 2.1.4",
            "reasoning": "While the process for recording and tokenization is described, the paper does not explicitly mention providing detailed annotation instructions to the annotators or specialists."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Sections 2.1.2 and 2.1.4",
            "reasoning": "No mention of scoring rubrics or rating scales as part of annotation guidelines is found in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide examples of annotations or mention provision of example data for annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9032,
      "completion_tokens": 377,
      "total_tokens": 9409
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions tokenization for dialect corpora relied on dialectal specialists, but no indication is given that a single expert performed QA alone."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1.4 Tokenization and Section 2.1.2 Audio recording",
          "reasoning": "Dialectal specialists performed tokenization and a separate validation step was done by expert-level crowd workers for the audio recordings, indicating multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of quality assurance by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance involved expert-level annotators and specialists, not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that AI models were used for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.3 Audio Verification",
          "reasoning": "Automated verification techniques including Voice Activity Detection (VAD), Signal to Noise Ratio (SNR), and signal energy thresholds were applied to validate audio quality."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes several QA processes; quality assurance was applied and documented."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8642,
      "completion_tokens": 518,
      "total_tokens": 9160
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 (Data collection procedure) and Abstract",
          "reasoning": "The paper clearly states that the authors collected 840 hours of read speech corpora from volunteers through a crowdsourcing platform and local recordings. The utterances were read by human speakers, and the data includes transcripts and their translations. The collection involved human participants recording original speech data based on prompts from multiple sources, indicating the data was created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by models or AI. The speech data and transcripts all come from human sources."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Text prompts) and Section 2.2 Corpus statistics and dialect details",
          "reasoning": "The paper describes that the Thai-dialect corpora contain E-commerce and Survival sentences translated to their respective dialectal transcriptions, produced by dialectal specialists. These translations are done by human experts to ensure accuracy and tokenization consistency."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of machine translation used to generate any part of the dataset or translations."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Text prompts)",
          "reasoning": "The text prompts for recording were collected from seven different sources such as BEST2010 articles, Dek-D teen webboard, Pantip general webboard, Wikipedia, and others, which implies aggregation of existing textual sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1.1 (Text prompts)",
          "reasoning": "The E-commerce and Survival subsets were generated using templates where nouns, verbs, product names, etc. were replaced to create varied sentences, indicating that some data was created by modifying existing templates, thus derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and creation processes are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9160,
      "completion_tokens": 582,
      "total_tokens": 9742
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "Sections 4.2 and 4.3 describe the use of the Thai-central and Thai-dialect corpora for pre-training models before fine-tuning, demonstrating that these datasets serve as pre-training data to adapt models to the target dialectal languages."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention training models from randomly initialized parameters using the proposed datasets. Instead, it uses pre-trained models (e.g., HuBERT) as starting points."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.2, 4.1, 4.2, 4.3",
          "reasoning": "The paper describes fine-tuning pre-trained models on the Thai-central and Thai-dialect datasets using supervised learning methods for dialect ASR, as outlined in various experimental sections."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of reinforcement learning or RL-based post-training methods such as RLHF using the proposed datasets."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed datasets are not used exclusively for evaluation; rather, they are predominantly used for training (pre-training and fine-tuning). Evaluation is conducted on subsets of the datasets, but this is a consequence of model training rather than the dataset being reserved solely for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.2 and Section 4.4",
          "reasoning": "The authors analyze the dataset's characteristics, such as vocabulary overlap (OOV metrics) between dialects (Section 2.2) and the impact of including the Pattani dialect on overall performance (Section 4.4), indicating use of the dataset for analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for model augmentation or retrieval-augmented generation based on the paper content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates multiple practical uses of the introduced datasets for pre-training, supervised fine-tuning, and analysis."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9883,
      "completion_tokens": 529,
      "total_tokens": 10412
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2, Section 2.2, Table 2, and Table 3",
          "reasoning": "The proposed dataset consists of multiple Thai dialects: the main Thai-central dialect and three local dialects (Khummuang, Korat, Pattani), each representing different regions in Thailand. These dialects are distinct enough linguistically; Pattani is noted as closely related to Malay, an Austronesian language, while Khummuang and Korat are Zhuang-Tai languages. This indicates the dataset includes entries in more than two human languages or dialects."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two dialects/languages rather than exactly two."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset primarily contains Thai and Thai dialect speech, not English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple dialects/languages, not just a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech in various dialects only."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages/dialects of the dataset are clearly documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data, thus language is present."
        }
      }
    }
  },
  {
    "id": "suwanbandit23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7101,
      "completion_tokens": 183,
      "total_tokens": 7284
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Abstract",
          "reasoning": "The paper mentions that the corpus data will be available on Github, but it does not provide any direct link or mention availability of the code used for data collection, preprocessing, or dataset generation. There is no indication that the authors have released the code associated with the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, Data collection procedure (2.1), subsections 2.1.1 to 2.1.4",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the sources of text prompts, procedures for audio recording, audio verification criteria, and tokenization methods with specific details about dialectal tokenization by specialists. This extensive description covers the data collection and preprocessing steps, enabling some level of reproducibility and understanding of the dataset construction."
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8085,
      "completion_tokens": 185,
      "total_tokens": 8270
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Speech collection",
          "Reasoning": "The corpus introduced contains speech samples with different placements of focus, collected from human speakers via a web-based recording application. The speech data were recorded by human speakers recruited through Amazon Mechanical Turk, indicated by detailed recording procedures and quality controls."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Paraphrase collection and Section 2.4 Filtering paraphrases",
          "Reasoning": "The corpus includes paraphrased text corresponding to the recorded speech that reflects the paralinguistic focus information. These paraphrases were generated by native English speakers performing a free-form paraphrasing task on crowdsourcing platform MTurk, hence the text data originate from human generation."
        }
      ]
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8885,
      "completion_tokens": 287,
      "total_tokens": 9172
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2.2 and 2.3",
            "reasoning": "Speech data was collected via Amazon Mechanical Turk from ten native British English workers (Section 2.2.2). Paraphrases were collected from native English speakers recruited via Mechanical Turk as well (Section 2.3). Thus, multiple human non-expert annotators performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2.2 and Section 2.3",
            "reasoning": "Workers were given specific instructions for recording speech emphasizing underlined words and for producing text paraphrases conveying implied meanings as detailed in Section 2.2.2 (recording procedure) and Section 2.3 (paraphrase collection instructions, see Figure 3)."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.4",
            "reasoning": "A subjective evaluation was conducted where workers rated paraphrase quality on scales (accuracy and formatting), and samples below certain thresholds were discarded (Section 2.4). This indicates a rubric or scoring criteria used for paraphrase quality assessment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no explicit mention of provided annotation examples or sample paraphrases included as instructions or guidelines for annotators in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10075,
      "completion_tokens": 564,
      "total_tokens": 10639
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any quality assurance was conducted by a single human annotator who is a subject matter expert. Although qualification tests were applied to select participants, there is no indication that a single expert conducted QA on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple human annotators with subject matter expertise performed quality assurance. The evaluation and qualification of MTurk Workers involved manual checks, but these workers are not described as experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single non-expert performing QA alone. Instead, multiple workers and repeated evaluations were used."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.3 Paraphrase collection, Section 2.4 Filtering paraphrases",
          "reasoning": "Quality assurance involved multiple non-expert human annotators recruited via Amazon Mechanical Turk. For the paraphrase collection and evaluation, multiple native English speakers (non-expert annotators) were involved (e.g., 16 workers for paraphrasing and 16 workers for evaluation), and each paraphrase was evaluated by three different workers. Additionally, manual checking was done during qualification stages. This process indicates multiple non-expert humans performed QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any AI model used as a judge for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 Text design, Section 2.2.1 Recording application",
          "reasoning": "Automatic processes were used as part of the quality assurance. For example, grammatical error correction was applied using the GECToR model to filter text sentences, and the recording application used Google's speech-to-text API to provide instant feedback during recording to encourage clear English speech. However, these automated methods appear to be preliminary filtering rather than direct annotation QA, but they contribute to quality control."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes detailed quality assurance procedures including qualification tests, manual checks, multiple human annotations, evaluations, and some automated filtering steps, so QA process is clearly documented."
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9685,
      "completion_tokens": 498,
      "total_tokens": 10183
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 - Speech collection, Section 2.3 - Paraphrase collection",
          "reasoning": "The dataset consists of speech recordings collected from human speakers via Amazon Mechanical Turk, as described in Section 2.2. The crowdworkers produced speech samples with different placements of focus. Furthermore, the corresponding paraphrases reflecting implied meaning were collected from native English speakers also via Mechanical Turk, as described in Section 2.3. Both speech and paraphrases were created directly by human contributors and are original content generated specifically for this corpus."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated entirely by AI or machine learning models. All the dataset components were created by human contributors."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data involved translation from another language produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of data generated by machine translation from another language in the construction of the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 - Text design",
          "reasoning": "The base captions for text design were selected and filtered from the existing Flickr8k dataset. While the authors carefully filtered and selected items, this portion of the dataset is an aggregation of existing text captions with minimal modification, qualifying as collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 - Text design",
          "reasoning": "The authors filtered the Flickr8k captions to meet criteria such as length, punctuation, and uniqueness, resulting in adapted source sentences used for recordings. Thus, the text data is derived from existing sources with transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides detailed documentation of the data sources and collection methods, so the data origin is known."
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10203,
      "completion_tokens": 238,
      "total_tokens": 10441
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3 (Analysis) and Section 4 (Discussion and conclusion)",
          "reasoning": "The corpus is primarily used for analyzing how focus expressed in speech maps into the linguistic domain, involving both qualitative and quantitative analyses of transformation patterns from paralinguistic to linguistic representation. The paper discusses detailed analysis of lexical and grammatical transformations associated with different parts of speech, demonstrating the dataset's role in revealing trends and characteristics of focus transformation rather than direct training or evaluation of models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10926,
      "completion_tokens": 385,
      "total_tokens": 11311
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 Corpus construction, specifically 2.2 Speech collection and 2.3 Paraphrase collection",
          "reasoning": "The dataset introduced is an English corpus containing speech samples with different focused words and their corresponding paraphrased English text reflecting the implied meaning of the speech. All speech and paraphrase data are exclusively in English as evidenced by participant selection criteria (native English speakers from English-speaking countries) and the instructions that require English utterances and English paraphrases."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "suzuki22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8144,
      "completion_tokens": 173,
      "total_tokens": 8317
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to the dataset construction, data processing, or collection methods."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Corpus construction) and subsections 2.1 to 2.4",
          "reasoning": "The paper contains detailed documentation of the dataset creation process including text design (Section 2.1), speech collection procedures (Section 2.2), paraphrase collection (Section 2.3), filtering and quality control (Section 2.4), as well as analysis in Section 3. The documentation explains data sourcing, filtering, crowdsourcing platform use, participant criteria, evaluation, and payment, providing transparency and completeness for reproducibility."
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6531,
      "completion_tokens": 313,
      "total_tokens": 6844
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data - Experiment 2, lines discussing training data usage from TIMIT corpus",
          "Reasoning": "The new dataset introduced in Experiment 2A is the use of the TIMIT corpus, which contains recordings of a child learning British English including both spoken utterances and their corresponding text transcriptions. This dataset is human generated as it consists of recorded speech from humans (the child and caregivers)."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data - Experiment 2, lines discussing training data usage from LibriSpeech train-clean-100",
          "Reasoning": "In Experiment 2B, the authors use LibriSpeech train-clean-100 as training data, which is a corpus consisting of human-recorded audio speech and their transcriptions. This data is human generated as it consists of real speech recordings from humans."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2.1 Language model inputs - Synthesized speech description",
          "Reasoning": "The authors generate synthetic speech utterances using TTS (Coqui TTS model) trained on LJ SPEECH dataset for Experiment 1C. This synthetic speech is model generated, created by the TTS system rather than a human speaker, and is introduced by the authors to support their method."
        }
      ]
    }
  },
  {
    "id": "sy23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7331,
      "completion_tokens": 207,
      "total_tokens": 7538
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Data and 3.1 Data",
            "reasoning": "The PROVIDENCE dataset is a new child-centred recordings dataset introduced as open archive data used for testing. Child and mother utterances were transcribed phonetically and orthographically by human annotators. The transcriptions and time stamps indicate human involvement, likely multiple experts due to the nature and scale of effort."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not explicitly mention detailed instructions given for the transcription annotation process in PROVIDENCE dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "The paper does not report any scoring rubrics or evaluation criteria used for the annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly described",
            "reasoning": "No mention or examples of annotation guidelines or samples were provided for the transcription annotations."
          }
        }
      ]
    }
  },
  {
    "id": "sy23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8521,
      "completion_tokens": 384,
      "total_tokens": 8905
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert annotator for the introduced datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human expert annotators performed quality assurance on the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no details about quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance conducted by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used in processing and analysis, the paper does not report that AI models were used as judges to perform quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automatic or algorithmic verification process specifically used for quality assurance of dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses publicly available datasets (LibriSpeech, PROVIDENCE, TIMIT) and focuses on experiments and methods rather than introducing new datasets. There is no explicit description of any quality assurance process applied or documented for the datasets used or created in the study."
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8131,
      "completion_tokens": 449,
      "total_tokens": 8580
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created entirely from scratch by human contributors. Instead, the authors use existing datasets such as PROVIDENCE, LibriSpeech-960, and TIMIT for training and testing."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new dataset is generated entirely by AI or machine learning models without reference to or transformation of existing data. The paper focuses on training models and methods to analyze existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any dataset was produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.2 Data",
          "reasoning": "The datasets used for training and testing (LibriSpeech-960, PROVIDENCE, TIMIT) are existing datasets collected previously. The authors aggregate these existing data sources to train language models and evaluate their entropy metric, indicating that the data is collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2.1 Language model inputs",
          "reasoning": "The authors generate discrete acoustic units from speech using clustering on Hubert model features and also generate synthetic speech from orthographic transcriptions. These transformations and adaptations of existing data represent derived data derived from existing speech or text corpora."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of all data used; no unknown or undocumented data sources are present."
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8649,
      "completion_tokens": 288,
      "total_tokens": 8937
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3, Experiment 2",
          "reasoning": "The new dataset composed of paired spoken utterances and their text-based entropy values is used to train a supervised model that predicts entropy from speech features, effectively fine-tuning a pre-trained speech feature extractor (WHISPER) with supervised learning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 3",
          "reasoning": "The child-centered PROVIDENCE dataset (newly processed by the authors) is used to evaluate and benchmark the entropy metrics and speech-based prediction models against standard language development measures."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.3 and Discussion",
          "reasoning": "The dataset is analyzed to observe patterns of entropy changes in children's speech over age and to correlate with traditional language development metrics, focusing on trends and characteristics rather than model training only."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9372,
      "completion_tokens": 557,
      "total_tokens": 9929
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset used for evaluation, PROVIDENCE, contains only English-learning children and adults' speech. The training data is LibriSpeech and TIMIT, both English corpora. No mention of multiple languages beyond English is found for the new datasets introduced."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication of datasets containing exactly two human languages is present. All datasets and corpora mentioned are English-only."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.2 Data; Section 3.1 Data; Abstract",
          "reasoning": "The new datasets introduced use the PROVIDENCE dataset, which contains recordings of English-learning children and their mothers. The language modeling training corpora (LibriSpeech, TIMIT) are also English-only. Therefore, these datasets contain entries with only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with exactly one non-English language are introduced or described."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described consist of child-directed speech audio and transcriptions. No datasets with programming or structured code content are introduced."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe concepts (e.g., entropy formula), the datasets themselves do not contain mathematical or logical symbolic data."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are based on human child-directed speech. No biological sequences or non-human communication data are introduced."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned or used in any introduced datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of all datasets introduced is clearly described as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets introduced clearly contain human language data (English)."
        }
      }
    }
  },
  {
    "id": "sy23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6590,
      "completion_tokens": 162,
      "total_tokens": 6752
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract",
          "reasoning": "The paper explicitly states in the Abstract that the source code of the experiments is released at https://github.com/yaya-sy/EntropyBasedCLDMetrics, indicating that the code related to the experiments and dataset processing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 Data",
          "reasoning": "The paper provides detailed descriptions of the data used, including the training and test datasets (LibriSpeech-960 and PROVIDENCE), the preprocessing steps (e.g., phonemization, speech discretization using k-means clustering on HUBERT features), and data preparation for the language model inputs. This indicates thorough documentation of dataset creation and processing steps within the paper."
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6805,
      "completion_tokens": 83,
      "total_tokens": 6888
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Corpus of gender expansive speakers",
          "Reasoning": "The paper introduces a new corpus called the MAGES Corpus, composed of recordings from 14 gender expansive speakers reading 400 sentences each. This data is audio recorded from human speakers specifically collected for this study."
        }
      ]
    }
  },
  {
    "id": "szekely24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7605,
      "completion_tokens": 243,
      "total_tokens": 7848
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 5.2 and 6.2",
            "reasoning": "Section 5.2 describes a qualitative evaluation involving three nonbinary SGD users (multiple human experts as they belong to the task's demographic and have relevant expertise). Section 6.2 details the community evaluation where these participants evaluated the voices and manipulated features, providing assessments and feedback."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not describe any detailed annotation instructions given to the human evaluators for the community evaluation. The participants explored the voice samples and manipulation interactively without described step-by-step instructions or guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no description of scoring rubrics or formal evaluation scales used by the human evaluators in the community evaluation. Feedback was collected via survey and qualitative responses."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "There is no evidence in the paper of annotation examples or benchmarking samples being provided to the human evaluators to guide their assessments."
          }
        }
      ]
    }
  },
  {
    "id": "szekely24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8795,
      "completion_tokens": 350,
      "total_tokens": 9145
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 5.2 and 6.2",
          "reasoning": "The community evaluation involved three nonbinary Speech Generating Device (SGD) users, who belong to the target demographic, providing qualitative assessments of the synthesized voices and manipulation functionalities. This represents QA performed by single human experts who are members of the target demographic."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance conducted by multiple human experts or annotators. The community evaluation involved three individuals independently, but not as a panel performing annotation or validation collectively on dataset content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that a single non-expert conducted QA on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that multiple non-expert annotators conducted QA."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 4 Objective evaluations",
          "reasoning": "An automatic Mean Opinion Score (MOS) prediction system was used to evaluate synthesis quality automatically, representing AI-model-based QA."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4 Objective evaluations",
          "reasoning": "The objective evaluation also included automated speaker similarity scoring using a state-of-the-art speaker identification model ECAPA-TDNN, providing automatic verification of speaker consistency and identity in synthesized speech, an automated QA process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents multiple QA procedures, including community evaluation by target demographic members and automated objective evaluation, thus QA is present and documented."
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8405,
      "completion_tokens": 430,
      "total_tokens": 8835
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.3 Corpus of gender expansive speakers",
          "reasoning": "The paper introduces a novel corpus recorded by 14 gender expansive human speakers from the mid-Atlantic region of the United States, recruited specifically for this research. This dataset consists of 400 sentences per speaker recorded to cover phonetic coverage, with ethical approval and original recordings collected from human participants."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The synthetic voices generated by the TTS model are outputs of the model, but these are not datasets introduced by the authors; rather, they are synthesized results based on the corpus. The paper does not describe generating an entirely new dataset from the model alone."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data generated by machine translation systems in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not a collection or aggregation of existing data sources without modification; it is original recordings collected from participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While existing x-vector models and pretrained TTS architectures are used for synthesis, the dataset itself is not derived from existing data with modifications; it is original speech recordings from human subjects."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is described explicitly as original recordings from human participants (MAGES corpus)."
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8923,
      "completion_tokens": 315,
      "total_tokens": 9238
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The MAGES corpus of fourteen gender expansive speakers is used to fine-tune a pre-trained Tacotron 2 TTS model using supervised learning. The model training is described as transfer learning starting from a pre-trained gender-ambiguous model, which is then adapted to the new corpus."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 and Section 5",
          "reasoning": "The MAGES corpus is used in objective evaluations to assess consistency and synthesis quality of the TTS outputs. Additionally, the synthesized voices based on this dataset are evaluated by nonbinary SGD users in qualitative community evaluations."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.3 and Section 6.2",
          "reasoning": "The MAGES corpus is analyzed to extract features such as acoustic Vocal Tract Length (aVTL) and to understand variability among gender expansive speakers. The community evaluation also analyzes user perceptions of synthesized voices derived from this dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9646,
      "completion_tokens": 594,
      "total_tokens": 10240
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described, the MAGES corpus, records 14 gender expansive speakers in the mid-Atlantic region of the United States, all speaking English (Section 2.3). There is no indication of multiple languages being represented."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English speech data from gender expansive speakers with no indication of a second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.3 (Corpus of gender expansive speakers)",
          "reasoning": "The MAGES corpus used for training the TTS system comprises recordings of 14 speakers of English, with the speech material taken from the ModelTalker database which covers English diphones and triphones. The corpus is specified as being from speakers in the mid-Atlantic US region and is English-only."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is explicitly described as English speech; no other languages or non-English monolingual datasets are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper discusses technical models (Tacotron 2, x-vectors, PCA), there is no indication that the dataset itself contains entries of programming or structured code."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken utterances; mathematical or logical notation is not part of it, even though the paper discusses mathematical methods applied during modeling."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus contains human speech from gender expansive speakers; no biological sequences or non-human communications are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on English speech from real speakers. There is no mention of fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is clearly stated as English; it is documented and specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data (English spoken utterances). It is not language-free."
        }
      }
    }
  },
  {
    "id": "szekely24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6864,
      "completion_tokens": 176,
      "total_tokens": 7040
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2.3 Corpus of gender expansive speakers",
          "reasoning": "The paper describes the collection of the MAGES Corpus and references an online link to the corpus dataset (https://maxwell-hope.com/mages-corpus/), but it does not include any information or links related to publicly available code for data collection, preprocessing, or generation associated with the dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.3 Corpus of gender expansive speakers",
          "reasoning": "The paper provides detailed documentation about the dataset creation process including recruitment method (word-of-mouth and social media), number of speakers (14 gender expansive speakers), IRB approval, the recording procedure (400 sentences from ModelTalker database), and quality screening steps, thus documenting the dataset creation thoroughly within the paper."
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10413,
      "completion_tokens": 201,
      "total_tokens": 10614
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1, Abstract, and Sections 2.1 and 2.2",
          "Reasoning": "The new dataset 'J-MAC' is constructed from audiobooks read by professional speakers. These audiobooks are recordings produced by humans, as indicated by 'audiobooks read by professional speakers' and the use of commercial audiobook products. Hence, the audio data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data collection",
          "Reasoning": "The text data of J-MAC is derived from out-of-copyright novels available on the web (Aozora project) that serve as reference texts for alignment to the audio. This text is human generated as it is authored literary text, used as ground-truth reference texts for alignment in the corpus construction."
        }
      ]
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11213,
      "completion_tokens": 210,
      "total_tokens": 11423
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 and 2.3",
            "reasoning": "The corpus construction process uses automatic methods such as vocal-instrumental separation (Spleeter), connectionist temporal classification (CTC) segmentation, and voice activity detection (VAD)-based refinement to extract and align audio and text data from existing audiobooks without manual annotation by humans."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "The paper does not provide any mention of annotation instructions or guidelines directed to annotators since the corpus construction is done via automatic processing."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "No scoring rubrics or criteria for manual annotation are described, as no human annotation appears to be employed for the corpus construction."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified",
            "reasoning": "There are no examples of manual annotation provided since the process is automated."
          }
        }
      ]
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12403,
      "completion_tokens": 459,
      "total_tokens": 12862
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by a single human expert or annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating multiple human experts conducted quality assurance on the dataset annotations or alignment."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple non-expert human annotators performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2.3 Sentence-level alignment and Sections 3.2, 3.3: Evaluation in segmentation",
          "reasoning": "The quality assurance of the dataset annotations involves the use of an AI model for alignment. Specifically, a recurrent neural network (RNN)-based Connectionist Temporal Classification (CTC) alignment model trained on another corpus was used to automatically align the sentence timings in audio given the ground-truth text. This end-to-end model enables language-independent alignment and serves as part of the automated QA process by judging alignment accuracy (CTC scores). Thus, an AI model was used as a judge for quality assurance of sentence alignments."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.2 SNR-based cleansing and Section 2.3 Alignment; Section 3.2 Evaluation in background music estimation",
          "reasoning": "The dataset quality assurance involves automated processing steps including vocal-instrumental source separation (using a deep learning model) to classify audio quality (clean, music-inserted, music-overlapped) based on SNR estimates, voice activity detection (VAD) to refine audio-text alignment boundaries, and iterative alignment realignment using CTC segmentation. These algorithmic and signal processing techniques constitute an automatic quality assurance process verifying data cleanliness and alignment correctness without human intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents various automated and AI model-based quality assurance processes; thus, QA is present and documented."
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12013,
      "completion_tokens": 348,
      "total_tokens": 12361
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data collection",
          "reasoning": "The corpus 'J-MAC' was constructed by collecting existing audiobook recordings of out-of-copyright books along with their reference texts available on the Web. The paper states that the audiobooks are existing commercial audiobook products, and the method extracts and aligns these existing audios and texts without creating new content, indicating the data is collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2 SNR-based cleansing and Alignment procedures",
          "reasoning": "The collected audiobook data underwent processing steps including vocal-instrumental separation, connectionist temporal classification (CTC) for alignment between text and audio, and voice activity detection to refine alignment. These steps modify and transform the original collected data, thus deriving a corpus from existing materials with adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12531,
      "completion_tokens": 474,
      "total_tokens": 13005
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.4 Evaluation in speech synthesis",
          "reasoning": "J-MAC dataset was used to train text-to-speech synthesis models from scratch, as described in the speech synthesis evaluation where the training and validation sets were created from J-MAC data and used to train multi-speaker TTS systems."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly mention fine-tuning of pre-trained models using J-MAC; the dataset is primarily used for training models from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or usage of reinforcement learning based post-training techniques, such as RLHF, using the J-MAC dataset is described in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.4 Evaluation in speech synthesis",
          "reasoning": "The J-MAC corpus was used for evaluation by conducting Mean Opinion Score (MOS) tests to assess naturalness of audiobook speech synthesis, thus serving an evaluation purpose."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 and accompanying analysis",
          "reasoning": "The dataset was analysed to study influences of synthesis methods, speakers, and books on speech naturalness, indicating use for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the J-MAC dataset as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly demonstrates multiple practical usages of J-MAC including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13254,
      "completion_tokens": 510,
      "total_tokens": 13764
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced (J-MAC) is described as a Japanese audiobook speech corpus, with no mention of inclusion of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; it focuses solely on Japanese audiobooks."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not English; it is a Japanese audiobook corpus."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction and Section 2 Corpus construction",
          "reasoning": "The introduced dataset, J-MAC, is explicitly described as a Japanese audiobook corpus (see Section 1 and Table 2). The data is collected from Japanese audiobooks and uses Japanese texts and speech exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of recorded speech and aligned text; there is no inclusion of programming or code-based entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of mathematical or logical notation within the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human spoken language audio and text; biological or non-human communication systems are not part of the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are mentioned in the dataset; only Japanese natural language is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language is clearly documented as Japanese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language data; therefore, it is not inapplicable to language coverage."
        }
      }
    }
  },
  {
    "id": "takamichi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10472,
      "completion_tokens": 168,
      "total_tokens": 10640
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.1 Data collection and introduction paragraph",
          "reasoning": "The paper states that J-MAC is open-sourced on their project page (https://sites.google.com/site/shinnosuketakamichi/research-topics/j-mac_corpus), indicating that the code for dataset construction including data collection, cleansing, and alignment is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 and 3 (Corpus construction and evaluations)",
          "reasoning": "The paper provides detailed descriptions of the dataset creation process including data collection criteria, SNR-based cleansing with vocal-instrumental separation, CTC-based alignment and iterative realignment, and VAD refinement. Multiple figures and tables illustrate these processes, evidencing thorough documentation."
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6813,
      "completion_tokens": 296,
      "total_tokens": 7109
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 3.2",
          "Reasoning": "SaSLaW contains spontaneous dialogue speech recordings captured from participants using close-talking microphones and ear-mounted binaural microphones, directly recording what participants speak and listen to, hence human-generated audio data."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 3.2",
          "Reasoning": "Participants wore head-mounted cameras recording egocentric video at 30 fps during conversations, capturing what they watch; these are human-recorded video data from first-person perspective."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Annotation as TTS corpus",
          "Reasoning": "The close-talking microphone audio was segmented and transcribed into text via automatic speech recognition followed by manual correction, creating human-generated text transcripts of the spontaneous speech dialogue."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2, Figure 2",
          "Reasoning": "Head-mounted cameras and microphones are human-operated devices recording signals; impulses responses and ambient noise audio recorded via microphones and sensors are human-collected data capturing environmental sound characteristics during the recordings."
        }
      ]
    }
  },
  {
    "id": "take24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7613,
      "completion_tokens": 381,
      "total_tokens": 7994
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2 and Section 4.1",
            "reasoning": "The paper states that utterances were automatically segmented and transcribed followed by manual correction (Section 3.2). Manual correction of transcripts implies human annotators refined the automatic transcriptions. Furthermore, in Section 4.1, the labeling of environmental noise levels ('quiet', 'moderate', 'noisy') was performed based on noise meter measurements but assigned as labels, suggesting some human oversight or annotation. Hence, multiple human experts were involved in these annotation processes."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 describes that participants were instructed on conversation themes and roles, but regarding annotation, pyannote.audio was used for automatic segmentation and whisper for transcription, followed by manual correction. Although the participant instructions are clear, the paper does not explicitly detail annotation guidelines or detailed instructions for manual correction. However, manual correction suggests some instructions or guidelines were probably followed by annotators. Given no explicit guideline description is present, but manual correction happened, instructions are deemed applicable but not detailed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2 and Section 4.1",
            "reasoning": "There is no mention in the paper about using any formal scoring rubrics or grading scheme for annotations such as quality ratings or scoring. The annotation process involved segmentation, transcription correction, and labeling environmental noise levels based on objective measurement, without reference to rubrics or scoring criteria."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No examples of annotation or explicit sample annotations or guidelines with examples are described or provided in the paper. The paper does not mention or show examples for annotations or labeling procedures."
          }
        }
      ]
    }
  },
  {
    "id": "take24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8803,
      "completion_tokens": 417,
      "total_tokens": 9220
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was conducted by a single human annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of quality assurance involving multiple human annotators with expertise or belonging to the target demographic."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert was responsible for quality assurance or validation of annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe multiple non-expert humans performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models were used for automatic transcription (using whisper) and utterance segmentation (using pyannote.audio), these were part of data processing, not explicitly described as quality assurance procedures."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 and 3.2 Annotation as TTS corpus",
          "reasoning": "Utterance segmentation was performed automatically using pyannote.audio and transcription was done with whisper, followed by manual correction; however, the paper does not specify human QA details, suggesting that the primary quality assurance relied on automated processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an annotation process involving automatic segmentation and transcription followed by manual correction, so some form of QA process exists; thus, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8413,
      "completion_tokens": 428,
      "total_tokens": 8841
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 (Overview of SaSLaW) and Section 3.2 (Recording Configuration and Procedure)",
          "reasoning": "SaSLaW is constructed by recording spontaneous dialogues between human participants engaging in improvisational conversation under controlled noisy environments. The paper clearly states that two participants were recorded speaking real spontaneous dialogue, capturing what they speak, listen to, and watch synchronously with first-person devices. This is original content created entirely by human contributors, not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated solely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No human translation from other languages is mentioned as part of the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used in the data generation process."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not merely collected or aggregated from existing sources without modification. SaSLaW was newly recorded by authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset uses environmental noise samples from DEMAND, the speech data itself is original human recordings and not derived or adapted from other speech corpora. The noise signals are external but do not constitute derived data in the corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is explicitly documented in the paper."
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8931,
      "completion_tokens": 474,
      "total_tokens": 9405
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.2",
          "reasoning": "The SaSLaW dataset is used to fine-tune pre-trained text-to-speech models in a supervised manner to enable environment-adaptive TTS (EA-TTS) that incorporates auditory perception of the target speaker. Section 5.1 details the training conditions where models are fine-tuned on SaSLaW recordings after pre-training on other data, and Section 5.2 shows objective evaluation of these fine-tuned models using the SaSLaW dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper performs objective and subjective evaluations of TTS models trained on SaSLaW, these evaluations use SaSLaW-derived samples, so the dataset is used for training rather than exclusively for evaluation or benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (4.1 and 4.2)",
          "reasoning": "The authors analyze the SaSLaW dataset to examine how spontaneous speech features vary depending on environmental noise and interlocutor coordination. This analysis aims to understand signal characteristics and human speech adaptations, indicating primary usage for analyzing trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes practical usages of SaSLaW dataset in analysis and supervised fine-tuning of TTS models, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9654,
      "completion_tokens": 417,
      "total_tokens": 10071
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 (Recording Configuration and Procedure), Section 3.1 (Overview of SaSLaW), Abstract, and multiple mentions throughout the paper",
          "reasoning": "The SaSLaW corpus is constructed from spontaneous dialogues between human participants, and all examples and usage described in the paper indicate the use of only English language content. The paper mentions using tools such as Whisper for transcription and references a Japanese corpus JSUT only for pretraining, not for SaSLaW. No other languages are mentioned as part of the SaSLaW corpus recordings or transcriptions, implying the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "take24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6872,
      "completion_tokens": 155,
      "total_tokens": 7027
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and footnote with GitHub link",
          "reasoning": "The paper states that the SaSLaW corpus is constructed and published, and it provides a URL link to a GitHub repository (https://github.com/sarulab-speech/SaSLaW) in the abstract area, indicating that the code related to the dataset is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Corpus Construction Methodology)",
          "reasoning": "Section 3 thoroughly describes the recording configurations, equipment used, procedures followed for recording, variation in noise environments, data annotation, and additional data collection for evaluation reproducibility. This detailed description serves as documentation for dataset creation."
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10742,
      "completion_tokens": 154,
      "total_tokens": 10896
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Audio Pair with Difference Dataset, paragraphs describing synthesis of APwD-Dataset",
          "Reasoning": "The paper introduces the APwD-Dataset, which consists of pairs of audio clips synthesized from existing labeled audio data (FSD50K and ESC-50). The dataset includes audio pairs with differences generated by the Scaiper tool, which synthesizes audio by mixing and modifying existing human-recorded samples. Thus, the audio data modality is audio, and it is both human generated (the original audio data recorded by humans) and model generated (the new paired data synthesized algorithmically by Scaiper)."
        }
      ]
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11542,
      "completion_tokens": 289,
      "total_tokens": 11831
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 4.1 Audio Pair with Difference Dataset",
            "reasoning": "The APwD-Dataset was synthesized programmatically using existing audio datasets (FSD50K and ESC-50) with automated procedures and tools (Scaiper) to create pairs of audio clips and differences; the data were not human-annotated but algorithmically generated with manual exclusion only for noisy samples."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 Audio Pair with Difference Dataset",
            "reasoning": "The synthesis procedure described provides explicit rules and steps for data creation, including six defined types of differences and how to generate each, which serve as detailed instructions for producing the dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Audio Pair with Difference Dataset",
            "reasoning": "The paper does not mention any scoring rubrics or criteria for annotators rating or scoring the annotations since the dataset was synthesized automatically rather than annotated by humans with subjective judgments."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.1 Audio Pair with Difference Dataset",
            "reasoning": "Examples of difference descriptions (e.g., \"increase the sound of rain\", \"make car horn lower and add dog bark\") and the types of difference synthesized are explicitly given, illustrating how the auxiliary text differences correspond to audio synthesis."
          }
        }
      ]
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12732,
      "completion_tokens": 392,
      "total_tokens": 13124
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the new dataset (APwD-Dataset). There is no information about any annotator or expert verifying the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper that multiple human experts were involved in quality assurance of the APwD-Dataset. The dataset is synthesized, and no expert annotation or multiple expert reviews are described."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human non-expert performed quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance process conducted by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No AI model was described or implied to be used for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 (Audio Pair with Difference Dataset)",
          "reasoning": "The APwD-Dataset is synthesized automatically by combining and manipulating existing sounds from FSD50K and ESC-50 using the Scaper library, which is an automated soundscape synthesis tool. The dataset construction involves programmatic operations (adding background sounds, events, amplitude changes) to create paired audio with known differences and corresponding textual descriptions. Additionally, noisy and unsuitable data were manually excluded, but the main dataset synthesis and difference assignment are conducted via automated processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is evidence of a systematic process to create and verify dataset contents, including manual exclusion of noisy samples and the use of automated synthesis tools, so QA is not entirely absent."
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12342,
      "completion_tokens": 281,
      "total_tokens": 12623
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Audio Pair with Difference Dataset",
          "reasoning": "The authors constructed the APwD-Dataset by synthesizing pairs of audio clips from existing audio datasets FSD50K and ESC-50. They applied various modifications such as adding or decreasing background sounds and audio events to generate paired audio with differences, along with textual descriptions. Thus, the data is based on existing audio sources with transformations and adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12860,
      "completion_tokens": 380,
      "total_tokens": 13240
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The APwD-Dataset is used to train and test retrieval models using supervised contrastive learning with classification loss, where the dataset pairs audio clips with textual difference descriptions. This is a form of supervised fine-tuning to learn cross-modal representations."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The dataset is used explicitly for evaluating retrieval accuracy (Recall@K) of the proposed method and baseline to benchmark performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3.2",
          "reasoning": "The dataset is used to analyze the learned embeddings via visualization (UMAP) to confirm the relationship between audio differences and text representations."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used for supervised fine-tuning, evaluation, and analysis as documented in multiple sections."
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13583,
      "completion_tokens": 566,
      "total_tokens": 14149
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's newly constructed dataset, APwD-Dataset, contains textual descriptions that are in English only, with no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; only English text descriptions are used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 'Audio Pair with Difference Dataset'",
          "reasoning": "The APwD-Dataset includes text descriptions in the form of imperative sentences in English that describe differences between paired audio clips, as illustrated by examples like 'increase the sound of rain' or 'make car horn lower and add dog bark.' There is no indication of any other languages being present in the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains non-English languages."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or code-related content is included in the dataset entries. The dataset consists of audio clips and natural language text descriptions."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical formulas or formal logical notation are included in the dataset entries; the dataset is audio-text pairs with natural language text."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the audio contains sounds like 'dog barking' and 'chirping birds,' the dataset entries are audio files with textual descriptions, not biological sequences or symbolic representations of non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include fictional or artificially created languages; only English text is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language content is explicitly stated and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language text entries describing audio differences, so it cannot be classified as not containing any language."
        }
      }
    }
  },
  {
    "id": "takeuchi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10801,
      "completion_tokens": 197,
      "total_tokens": 10998
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 (Audio Pair with Difference Dataset)",
          "reasoning": "The paper describes in Section 4.1 the procedure for synthesizing the Audio Pair with Difference Dataset (APwD-Dataset), including the use of FSD50K, ESC-50 datasets and the Scaper tool for synthesis. However, it does not provide any URLs, links, or references to a code repository for dataset creation or code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 (Audio Pair with Difference Dataset)",
          "reasoning": "Section 4.1 provides a detailed explanation of the dataset creation process, including how audio pairs were synthesized, the labels used for synthesis, the procedure of data splitting, manual exclusion of noisy data, types of audio differences applied, and how descriptive text was assigned. Thus, the dataset creation process is well documented in the paper."
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5995,
      "completion_tokens": 209,
      "total_tokens": 6204
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Protocol; Section 2.3 Acoustic low-level feature extraction; Section 2.1 Participants",
          "Reasoning": "The paper describes collecting voice recordings (audio) from participants performing speech tasks. These audio data were recorded from human participants using a built-in microphone in a controlled clinical environment as part of the newly implemented protocol designed by the authors for this study."
        },
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 Protocol; Section 2.1 Participants",
          "Reasoning": "The paper reports collecting handwriting data (drawing trajectory) via a tablet's pen position sampled at 62.5 Hz using a custom MATLAB script during concurrent drawing tasks. These sensor signals are human-generated, as they arise from participant manual drawing movements captured by the tablet sensor system introduced by the authors in this new dataset."
        }
      ]
    }
  },
  {
    "id": "talkar22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6795,
      "completion_tokens": 294,
      "total_tokens": 7089
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 Participants; Section 2.2 Protocol",
            "reasoning": "The dataset was generated by recruiting participants who performed speech and drawing tasks under controlled experimental settings supervised by clinicians and researchers. The cognitive categorization of participants into NCI or MCI was based on an extensive cognitive battery assessed by experts at UMass Chan Medical School, involving consensus conferences. The annotators of cognitive status and data collection were subject-matter experts, not automated or crowd-sourced."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Protocol",
            "reasoning": "The paper provides detailed protocol descriptions of the speech and drawing tasks, specifying the exact tasks (reading stories, listing words with particular constraints, drawing specific shapes and circles) and timing, indicating that annotators (participants) followed clear instructions during data collection."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.1 Participants",
            "reasoning": "The cognitive impairment labels (NCI vs MCI) were derived from a validated battery of neuropsychological tests with consensus classification methodology, implying existence of scoring rubrics and criteria for categorizing participants."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in paper",
            "reasoning": "The paper does not provide explicit annotation examples or illustrative cases in the text or appendices for annotators or data labeling."
          }
        }
      ]
    }
  },
  {
    "id": "talkar22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7985,
      "completion_tokens": 282,
      "total_tokens": 8267
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 Participants",
          "reasoning": "Participants' cognitive status (NCI or MCI) was classified via a consensus conference using a validated neuropsychological battery, indicating expert human judgment involved in annotation."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Participants",
          "reasoning": "Cognitive impairment classification was reached through a consensus conference involving multiple experts reviewing neuropsychological testing results, indicating multiple human experts performed quality assurance on cognitive status labels."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single non-expert performed quality assurance on annotations."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that multiple non-experts performed quality assurance on annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models were used for classification, QA of the annotations (i.e., cognitive status labels) was not conducted by AI."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification processes for quality assurance of dataset labels were described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process involving expert consensus was documented for cognitive status labels, so QA is not absent."
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7595,
      "completion_tokens": 421,
      "total_tokens": 8016
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Protocol and Section 2.1 Participants",
          "reasoning": "The paper reports new data collected from human participants diagnosed with Parkinson's Disease (PD). The authors designed a new speech and drawing protocol which was executed by seventeen participants (nine NCI and eight MCI), wherein simultaneous speech and drawing tasks were recorded in a clinical setting. These recordings and participant responses constitute original content created by human subjects specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as generated by AI or machine learning models; rather, models are applied to the collected data for classification purposes."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation of data from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that machine translation was used or that data originated from content in other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not gathered by aggregating pre-existing sources but collected anew via controlled experimental protocols."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While features are derived from raw audio (e.g., acoustic features extracted from recordings), the underlying dataset of participant speech is original human-collected data, not based on existing datasets or sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as original data collection from human participants following a newly designed protocol."
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8113,
      "completion_tokens": 306,
      "total_tokens": 8419
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.5 and 3.1",
          "reasoning": "The collected speech and drawing dataset was used to train Gaussian mixture models (GMMs) to classify participants as NCI or MCI. The dataset served as labeled training data for supervised learning to develop models discriminating cognitive impairment status."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4",
          "reasoning": "The dataset was used to evaluate model performance in discriminating NCI from MCI, reporting area under the curve (AUC) metrics and analyzing which features and tasks produce better discrimination, thus serving an evaluation purpose as well."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1 and 4",
          "reasoning": "The dataset was used primarily to analyze acoustic features and motor coordination patterns associated with cognitive impairment, to explore the physiological implications of these features, and to study differences in performance under single and dual task conditions."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8836,
      "completion_tokens": 505,
      "total_tokens": 9341
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Participants",
          "reasoning": "The participants were explicitly selected as English speakers (Section 2.1 Participants). All speech data collected involved reading and listing words in English as part of the protocol (Sections 2.2 Protocol, Tables 1 and 2). There is no mention of any other language used or recorded within the dataset."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of acoustic speech recordings and drawing data; no dataset entries involving programming language or code are present. Although computational methods and scripts (e.g., MATLAB) are mentioned, these do not constitute dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical methods for feature extraction and modeling, the dataset itself consists of speech and drawing data, not mathematical or logical symbolic expressions as entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech and drawing tasks data, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is based on natural human language (English) and there is no use of constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly stated as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data clearly involves speech recordings in English, so it contains language data."
        }
      }
    }
  },
  {
    "id": "talkar22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6054,
      "completion_tokens": 143,
      "total_tokens": 6197
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link in the paper",
          "reasoning": "The paper does not mention or provide any links to publicly available code repositories related to the data collection, preprocessing, or dataset generation. There is no indication that code for dataset creation or processing is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper documents the dataset creation process including participant recruitment, inclusion and exclusion criteria, experimental protocols involving speech and drawing tasks, recording details, and acoustic feature extraction methods. There is a detailed description of the data collection and processing steps, enabling transparency and completeness regarding dataset creation."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7014,
      "completion_tokens": 196,
      "total_tokens": 7210
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Dataset construction and Table 1 Real samples distribution",
          "Reasoning": "The authentic speech samples are audio data collected from 162 distinct speakers of five Latin American Spanish accents. These are recorded samples sourced from a previous study [21], representing human-generated natural speech in audio format."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3 Dataset construction and Table 2 Spoof samples distribution",
          "Reasoning": "The spoof samples in the dataset are audio data generated using six different spoofing strategies, including three voice conversion (VC) algorithms (StarGAN, CycleGAN, diffusion model), one text-to-speech (TTS) system (Microsoft Azure TTS), and two combined TTS-VC approaches. These are model-generated synthetic audio samples created algorithmically."
        }
      ]
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7814,
      "completion_tokens": 193,
      "total_tokens": 8007
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3 Dataset construction",
            "reasoning": "Spoof samples generated by three selected voice conversion architectures were chosen by two human judges who rated the speech conversion quality; thus, multiple human experts were involved in annotation and evaluation process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 Dataset construction",
            "reasoning": "The paper does not mention any detailed annotation instructions provided for the human judges or annotators during the selection or labeling of samples."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 Dataset construction",
            "reasoning": "No scoring rubrics or detailed evaluation guidelines for annotation are described in the paper for the human judges or any other annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 Dataset construction",
            "reasoning": "No annotation examples or sample annotations are provided or referenced in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9004,
      "completion_tokens": 353,
      "total_tokens": 9357
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset samples."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3 Dataset construction",
          "reasoning": "The selection of voice conversion architectures for spoof generation was performed by two human judges who rated speech conversion quality. Although their expertise is not explicitly stated, they conducted subjective assessments indicating multiple human non-experts participated in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance procedures for the dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset construction involved human judging for spoof quality selection, which counts as a form of quality assurance."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8614,
      "completion_tokens": 518,
      "total_tokens": 9132
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset construction), paragraph 1",
          "reasoning": "The dataset's authentic speech samples were sourced from [21], which consists of recordings of 162 distinct speakers from various Latin American Spanish accents. These samples were originally recorded by human speakers and subsequently collected for this dataset, thus representing original human-produced speech data rather than adapted or translated content."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset construction), paragraph 3 and Table 2",
          "reasoning": "The spoof samples were generated by applying six different speech synthesis strategies, including voice conversion algorithms (StarGAN, CycleGAN, diffusion model) and Microsoft Azure TTS, as well as combinations of TTS and VC methods. These synthetic samples were generated entirely by AI models using the text and voice data, constituting new data generated from models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation being used to produce data in the dataset is made in the paper."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset construction), paragraph 1",
          "reasoning": "The authentic speech samples were sourced from an existing dataset [21]. The authors aggregated these existing authentic samples from multiple Latin American accents into the HABLA dataset without significant modification to the original recordings, indicating a collated data component."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (Dataset construction), paragraphs 2-4 and Table 2",
          "reasoning": "The spoofed samples are derived from existing authentic samples through modification by speech synthesis algorithms (VC, TTS). This constitutes data based on existing sources with transformations applied, qualifying as derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and generation methods of the dataset; thus, the data origin is known and documented."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9132,
      "completion_tokens": 518,
      "total_tokens": 9650
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or mention the dataset being used exclusively for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Experiments",
          "reasoning": "The paper describes training anti-spoofing models from scratch using the HABLA dataset (Spanish dataset) to evaluate their performance (Section 4). The dataset is split into training, validation, and test sets, and models trained from scratch showed improved performance on the Spanish data."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4, Experiments",
          "reasoning": "The paper discusses fine-tuning pre-trained English models using the HABLA Spanish dataset to create further-pretrained (FT) models, showing improved cross-language performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description or evidence of reinforcement learning or RL-based post-training methods using this dataset is provided."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4 and 4.1, Experiments and Experiments results",
          "reasoning": "The HABLA dataset is used for benchmarking and measuring the performance of pre-existing and newly trained anti-spoofing models, comparing their error rates and accuracies."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1, Experiments results; Section 5, Conclusions",
          "reasoning": "The paper uses the dataset to analyze performance differences across accents, spoofing strategies, and languages, providing insights on model effectiveness and challenges."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models or retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper fully documents usage of the HABLA dataset in training, fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9855,
      "completion_tokens": 551,
      "total_tokens": 10406
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The HABLA dataset includes only Spanish language entries across different Latin American regional accents but does not include multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is not composed of exactly two languages; it only contains Spanish."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is explicitly described as consisting of Spanish language samples only, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 3 Dataset construction",
          "reasoning": "The paper introduces the HABLA dataset as the first voice anti-spoofing dataset in the Spanish language with samples of various Latin American Spanish accents including Argentinian, Colombian, Peruvian, Venezuelan, and Chilean. All spoken content is in Spanish; hence, the dataset is monolingual in a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication in the paper that the dataset includes programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of audio speech samples; no mathematical or logical expressions are included."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No fictional or constructed languages are included in the dataset; all speech is natural Spanish."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the samples is clearly documented as Spanish; therefore, it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language speech data and so it is not without language content."
        }
      }
    }
  },
  {
    "id": "tamayoflorez23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7073,
      "completion_tokens": 158,
      "total_tokens": 7231
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 Dataset construction",
          "reasoning": "The paper provides a public URL for downloading the dataset itself (https://zenodo.org/record/7370805) but does not mention the release or availability of the source code used for dataset construction, data processing, or spoof sample generation scripts."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Dataset construction",
          "reasoning": "The paper contains a detailed description of the dataset creation process including the data sources, selection criteria, audio format and sampling rates, distribution across accents and genders, details on spoof sample generation using multiple voice conversion and TTS methods, and the resulting dataset statistics. This thorough description provides sufficient documentation of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7639,
      "completion_tokens": 147,
      "total_tokens": 7786
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Speech dataset",
          "Reasoning": "The new dataset used to train the TTS module is constructed using clean speech data from the VCTK dataset (manually recorded human speech) of multiple speakers combined with measured room impulse responses (RIRs) from the REVERB challenge database and Aachen impulse response database. This results in reverberant speech data created by convolving human-recorded clean speech with human-measured RIRs, where each speaker is paired with a unique environment. Thus, the dataset comprises audio modality speech data that is originally human generated and captured."
        }
      ]
    }
  },
  {
    "id": "tan22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8439,
      "completion_tokens": 237,
      "total_tokens": 8676
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2 and 3.1",
            "reasoning": "The environment embedding extractor is trained on environment-labeled data constructed by convolving clean speech from LibriTTS speakers with measured RIRs from BUT ReverbDB, generating reverberant speech for multiple speakers per environment, and training uses a batch sampling and loss function for leveraging these labels. This process is an automatic method without mention of human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "The paper does not mention any detailed annotation instructions or guidelines given to annotators for this dataset, indicating no human annotators with instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "No mention is made of rubrics or scoring criteria for annotation in regard to this dataset since no manual annotations were performed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not specified in the paper",
            "reasoning": "No annotation guidelines or examples are described for this dataset as it is automatically generated without manual labeling."
          }
        }
      ]
    }
  },
  {
    "id": "tan22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9629,
      "completion_tokens": 370,
      "total_tokens": 9999
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the constructed datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process involving multiple human experts; no expert annotation or validation is reported."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of QA by a single non-expert annotator is found in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss any QA process involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the system includes neural network components and uses embedding extractors, these do not constitute an AI model performing quality assurance on dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No explicit automatic quality assurance processes or verification of dataset annotations or content are described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance or validation process applied to the introduced datasets. The datasets are constructed by combining existing datasets through signal processing (e.g., convolution with RIRs) without additional QA steps reported."
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9239,
      "completion_tokens": 364,
      "total_tokens": 9603
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The datasets used include multiple public datasets such as the VCTK dataset (clean speech data), measured RIRs from REVERB challenge database and Aachen impulse response database. These are existing sources aggregated for the purpose of this study without significant modification. The authors combine these existing datasets to create specific speaker-environment combinations for TTS training."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 2.2 and 3.1",
          "reasoning": "Derived data is created by convolving clean speech utterances from multiple speakers (from LibriTTS or VCTK) with measured room impulse responses (RIRs) from existing RIR datasets (BUT ReverbDB, REVERB challenge database, Aachen impulse response database). This process produces reverberant speech data that are transformations of existing clean speech data, thus constituting derived data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9757,
      "completion_tokens": 265,
      "total_tokens": 10022
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 3",
          "reasoning": "The paper describes the training of the speaker embedding extractor, environment embedding extractor, and the TTS module using the specially constructed datasets (Sections 2.1, 2.2 and 3.1). These datasets are used to train models from scratch, as indicated by the neural network architectures and loss functions described without mention of pre-training or fine-tuning from pre-trained models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the use of newly constructed datasets to train embedding extractors and the TTS model, demonstrating practical utility in training for environment-aware speech synthesis."
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10480,
      "completion_tokens": 595,
      "total_tokens": 11075
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses clean speech data from the VCTK dataset and LibriTTS 'train-clean-100' subset, both of which are English speech datasets. There is no mention of any other human languages involved."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that exactly two human languages were present in the newly proposed datasets; only English data are used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Speech dataset), Section 2.2 (Environment embedding extractor)",
          "reasoning": "The newly constructed datasets for training embedding extractors and the TTS module are based on LibriTTS 'train-clean-100', VCTK, and reverberant speech generated from clean English utterances. It is explicitly stated that clean speech data are used from VCTK and LibriTTS datasets, which contain English speech only. No other languages are mentioned or used."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No non-English language datasets are introduced or used in the new datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of speech utterances and reverberation impulse responses, no programming or structured code data are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical formulas to describe loss functions and model architectures, the datasets themselves do not include mathematical or logical symbolic representations as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets involve human speech audio and reverberation impulse responses only; no biological or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or usage of fictional or artificially constructed languages in any proposed dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets are clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The proposed datasets contain speech utterances in English language; hence they contain language data."
        }
      }
    }
  },
  {
    "id": "tan22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7698,
      "completion_tokens": 234,
      "total_tokens": 7932
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or mention in the paper",
          "reasoning": "The paper does not provide any link, URL, or archival location for the code related to the construction of the datasets used for training the speaker and environment embedding extractors or the TTS module, nor for the process of generating the constructed datasets from existing speech and RIR data."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 and Section 3.1",
          "reasoning": "The paper thoroughly documents the dataset construction process for the new datasets used to train the environment embedding extractor and the TTS module. It specifies the source datasets used (e.g., LibriTTS, BUT ReverbDB, VCTK, REVERB challenge, Aachen impulse response database), the method of dataset construction (e.g., generating reverberant speech by convolving clean speech with measured RIRs), the number of speakers, environments, and utterances per pair, and how the training/test splits are done. This level of documentation is sufficient to reproduce the dataset construction process based on the information provided."
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7179,
      "completion_tokens": 176,
      "total_tokens": 7355
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Datasets, paragraph 'We created our TED-EN-JP dataset as follows.'",
          "Reasoning": "The TED-EN-JP dataset is created from 628 hours of TED English speech recorded from human presentations, as stated in the data download step, indicating human-generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Datasets, paragraph 'We created our TED-EN-JP dataset as follows.'",
          "Reasoning": "The TED-EN-JP dataset includes human-created transcription texts (English transcriptions) and their Japanese translations, both originally produced by humans as indicated by the data download and alignment steps."
        }
      ]
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7979,
      "completion_tokens": 270,
      "total_tokens": 8249
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1: Dataset creation - TED-EN-JP dataset",
            "reasoning": "The TED-EN-JP dataset was created by downloading existing TED English speech, transcriptions, and Japanese translations, then aligning texts and audio using tools such as Vecalign and Gentle forced aligner in an automated manner. The process involves automated alignment and filtering rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1: Dataset creation - TED-EN-JP dataset",
            "reasoning": "The paper does not mention any instructions provided to annotators or humans performing manual annotation. The dataset creation process relies on automatic alignment tools."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1: Dataset creation - TED-EN-JP dataset",
            "reasoning": "There is no indication of scoring rubrics or evaluation criteria being used in annotation guidance for the TED-EN-JP dataset, as the creation is via automatic alignment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1: Dataset creation - TED-EN-JP dataset",
            "reasoning": "The paper does not provide or mention any annotation examples or guidelines illustrating annotation standards; creation is through automated alignment methods."
          }
        }
      ]
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9169,
      "completion_tokens": 340,
      "total_tokens": 9509
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or mention of multiple human experts performing quality assurance on the datasets presented in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information about multiple non-expert human annotators validating or performing quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any AI model being used to perform quality assurance or validation of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1, Paragraph titled 'Text alignment' and 'Audio to text alignment'",
          "reasoning": "The TED-EN-JP dataset was created through an automated process involving alignment tools and forced alignment with thresholds to filter data. Specifically, sentence alignment was performed with Vecalign, paragraph merging and timestamp checking were automated, and audio-to-text alignment used the Gentle forced aligner with filtering based on word accuracy percentage. This indicates automated verification and filtering processes were applied as quality assurance steps."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated process for dataset creation and filtering; hence, the absence of any quality assurance does not apply."
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8779,
      "completion_tokens": 468,
      "total_tokens": 9247
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any new dataset was created entirely from scratch by human contributors without reliance on existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any datasets were generated solely by AI or machine learning models without reference to existing sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention that the translations used in the original TED-EN-JP dataset were produced by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that any translations were produced using machine translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The TED-EN-JP dataset was created by downloading existing TED English speech data with transcriptions and Japanese translations, then aligning these texts and aligning audio to text using existing tools. This process involves collecting and aggregating existing data sources without data generation or translation indicated, thus the dataset is collated from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The TED-EN-JP dataset was derived from existing corpora (TED talks with English transcriptions and Japanese translations) by applying processing steps including paragraph and sentence alignment, forced alignment of audio to text, filtering based on alignment quality, and merging data to produce a dataset suitable for speech recognition tasks. This constitutes a derived dataset based on existing data with some modifications and processing."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and creation process for the new TED-EN-JP dataset is documented in detail in the paper, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9297,
      "completion_tokens": 289,
      "total_tokens": 9586
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.2 Training and Section 3 Experiments",
          "reasoning": "The paper describes training the proposed Transformer-based multimodal ASR model from scratch with labeled datasets consisting of speech and source language text to output correct transcriptions, optimized in an end-to-end supervised manner."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Experiments (3.1 Datasets and 3.3 Experimental Results)",
          "reasoning": "The newly created TED-EN-JP dataset is used as a test set to evaluate and benchmark the proposed model's performance compared to the baseline model."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 Experimental Results",
          "reasoning": "The dataset is used to analyze and understand model performance improvements, as seen in the reporting of word error rates, attention weight visualization, and examples of corrected transcription outputs."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10020,
      "completion_tokens": 595,
      "total_tokens": 10615
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The authors introduced their original TED-EN-JP dataset consisting of English speech, its English transcription, and Japanese translation. Along with the TED-EN-JP dataset, they also used MuST-C datasets that include English speech and translations in Dutch and German, and CoVoST 2 dataset with English speech and Japanese translation. Thus, the proposed datasets contain entries involving more than two human languages: English, Japanese, Dutch, and German."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although each dataset pair involves two languages, the authors introduced multiple new datasets (including TED-EN-JP) that collectively cover more than two languages. Therefore, the dataset collection as a whole is considered multilingual rather than strictly bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "All introduced datasets involve at least two languages (source speech in English and target text in Japanese, German, or Dutch). There is no dataset introduced that contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset introduced contains entries solely in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of natural language speech and text (transcriptions and translations); there is no indication of programming or structured code content in the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are composed of speech and text data without any mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets include human speech and text, with no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or constructed languages is made regarding the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the datasets are clearly specified: English, Japanese, Dutch, and German."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain natural language data including speech and textual transcriptions and translations."
        }
      }
    }
  },
  {
    "id": "taniguchi22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7238,
      "completion_tokens": 155,
      "total_tokens": 7393
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Datasets, paragraph about original TED-EN-JP dataset creation",
          "reasoning": "The paper describes the process of creating the original TED-EN-JP dataset in section 3.1 but does not provide any links or references to publicly available code repositories for the data collection, preprocessing, or alignment processes."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The paper provides detailed documentation on the original TED-EN-JP dataset creation process, including data download, text alignment using Vecalign, audio-to-text alignment using Gentle forced aligner, and filtering criteria, all described in Section 3.1."
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8347,
      "completion_tokens": 173,
      "total_tokens": 8520
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Griots",
          "Reasoning": "The Griots corpus consists of audio and video recordings of live performances by griots, collected on location in Mali by a professional audio technician and videographer. This data is human-generated in the sense that it was recorded by humans capturing traditional oral performances, not synthesized or generated by models."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Griots",
          "Reasoning": "Transcriptions of the Griots audio recordings were produced by Malian doctoral candidates in linguistics using ELAN and internal transcription guidelines. These transcriptions are manually created by humans, not model-generated."
        }
      ]
    }
  },
  {
    "id": "tapo24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9147,
      "completion_tokens": 397,
      "total_tokens": 9544
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "The griots corpus was transcribed by five Malian doctoral candidates in linguistics, who can be considered human experts in the Bambara language context."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The transcribers were trained in using ELAN and in the authors' internally defined guidelines for how to render Bambara using the Latin alphabet with supplemental IPA characters, indicating the presence of annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no mention in the paper of any scoring rubrics or explicit scoring guidelines used by the transcribers during annotation of the griots corpus."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not reference any example annotations or example transcripts being provided as part of the guidelines or training for the transcribers."
          }
        },
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "The fieldwork corpus transcriptions were performed over the last ten years by several Bambara-speaking linguists at CNRS, indicating multiple human experts annotated this data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention specific written instructions or guidelines provided to annotators for the fieldwork corpus transcription."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No mention of scoring rubrics or evaluation criteria used during transcription of the fieldwork corpus is present in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No example annotations or example transcripts are referenced for the fieldwork corpus annotation process."
          }
        }
      ]
    }
  },
  {
    "id": "tapo24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10337,
      "completion_tokens": 432,
      "total_tokens": 10769
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1, Fieldwork",
          "reasoning": "The fieldwork corpus was transcribed by several Bambara-speaking linguists at the Centre national de la recherche scientifique (CNRS), indicating multiple human experts contributed to the transcription process. While multiple experts are involved, a single expert might have also been involved in quality checking individual transcriptions, but this is not explicitly stated. However, since multiple experts are involved, this label is less appropriate than multiple experts."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The fieldwork corpus transcriptions were performed by several Bambara-speaking linguists (experts) at CNRS (Section 3.1). The griot corpus transcription involved five Malian doctoral candidates in linguistics (Section 3.2) who are subject matter experts. Thus, multiple human experts were involved in quality assurance for both corpora."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of quality assurance or transcription by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.2, Griots",
          "reasoning": "For the griots corpus, while the transcribers were trained linguistics doctoral candidates, it is noted that in most cases, the transcribers did not have extensive prior experience with writing or reading Bambara. This suggests multiple human non-experts were involved in transcription and quality assurance for this corpus."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of AI models being used for quality assurance or as judge in validating the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of automated verification or code-based quality assurance for dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes human transcription and quality assurance processes for both datasets; thus, QA is present and documented."
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9947,
      "completion_tokens": 497,
      "total_tokens": 10444
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3, specifically 3.1 Fieldwork and 3.2 Griots",
          "reasoning": "The two corpora were collected by human effort: the fieldwork corpus consists of archival recordings collected by linguists, journalists, and anthropologists in the 1970s-1990s and later transcribed by Bambara-speaking linguists; the griots corpus was recorded recently with thirty griots performing, with transcription performed by trained human transcribers. Both datasets constitute original, culturally significant speech data created or collected by humans and transcribed by human contributors without indication of being translated or derived from other data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention in the paper indicates that any of the datasets were generated by AI or machine learning models. All data comes from human-produced speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of recordings and transcriptions of Bambara speech; there is no indication that the data was translated from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of machine translation for producing the datasets."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Fieldwork",
          "reasoning": "The fieldwork corpus consists of archival recordings from the 1970s-1990s which were digitized and transcribed recently. Thus, it is a collation of existing recordings without significant modification, serving as a gathered resource."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence suggests the datasets were derived from existing sources with modifications; while the fieldwork corpus is old recordings aggregated, there is no mention of significant transformation or adaptation applied to create the new dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper adequately documents the origin of the datasets."
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10465,
      "completion_tokens": 292,
      "total_tokens": 10757
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 Method, Section 5 Results",
          "reasoning": "The newly introduced Bambara speech corpora (fieldwork and griots) are used to fine-tune pre-trained multilingual ASR models (Wav2Vec2 XLSR and Whisper) using supervised learning methods to improve transcription accuracy."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Results",
          "reasoning": "The datasets are used for testing and evaluating ASR model performance, measuring word error rates (WER) and character error rates (CER) to benchmark accuracy across different training configurations and domains."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Results, Section 6 Discussion & Future Work",
          "reasoning": "The datasets are analyzed to understand the impact of domain differences, audio quality, and training configurations on ASR performance, providing insights into challenges and opportunities in low-resource language documentation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11188,
      "completion_tokens": 492,
      "total_tokens": 11680
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves primarily Bambara with occasional code-switching with French but does not contain more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Fieldwork, and Section 3 Data overview",
          "reasoning": "The fieldwork corpus includes recordings with Bambara and code-switching with French, meaning exactly two human languages are present in the dataset entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets described contain no entries in English; the language focus is on Bambara and French."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although Bambara is the primary language, the dataset contains code-switching with French in the fieldwork corpus, so it is not exclusively monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's datasets consist of speech recordings and transcriptions and contain no programming or code-related entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of math or logical notation is present in the datasets."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that constructed or fictional languages are included."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages present in the datasets are explicitly identified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets clearly contain language data, so this label does not apply."
        }
      }
    }
  },
  {
    "id": "tapo24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8406,
      "completion_tokens": 160,
      "total_tokens": 8566
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention found in the paper.",
          "reasoning": "The paper does not contain any explicit links, references, or mentions of public availability of code for dataset construction, data collection, or preprocessing. No repositories or URLs are provided related to code for dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Data), subsections 3.1 Fieldwork and 3.2 Griots",
          "reasoning": "The paper provides detailed descriptions of the datasets, including the data collection context, transcription process, speaker information, domains, audio characteristics, transcription protocols, and ethical approvals. These descriptions serve as documentation of the dataset creation process, supporting transparency and reproducibility to an extent."
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7515,
      "completion_tokens": 152,
      "total_tokens": 7667
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (ViCapPunc Dataset)",
          "Reasoning": "The ViCapPunc dataset is a newly introduced large-scale public dataset by the authors for Vietnamese capitalization and punctuation recovery. It consists of text data from 50,000 question and answer pairs crawled from reputable Vietnamese healthcare websites (Vinmec, Alobacsi, ISOFHCARE). This crawling of human-generated content from medical domain websites constitutes human-generated text data. There is no indication that this text data is generated or simulated by models. The source is explicitly stated as crawled from online healthcare sites, confirming it is human-generated text."
        }
      ]
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8315,
      "completion_tokens": 279,
      "total_tokens": 8594
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3, Paragraph 1 and Table 1",
            "reasoning": "The dataset ViCapPunc is constructed by crawling 50,000 question-answer pairs from reputable Vietnamese healthcare websites and then preprocessed automatically to assign capitalization and punctuation labels according to a scheme described in Table 1. There is no explicit mention of human annotators performing manual labeling; the paper describes labeling as a process following predefined rules on transformed data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "The paper does not mention any annotation guideline instructions provided to annotators. Since the labeling process is described as rule-based and performed automatically on the crawled data, no instructions for manual annotation are reported."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3",
            "reasoning": "There is no description or evidence of rubrics or scoring criteria for annotation. Since the labels are assigned automatically based on defined label schemes, rubrics for manual annotators are not applicable or provided."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3, Table 1",
            "reasoning": "The dataset format and labeling scheme are illustrated in Table 1, which shows example lines of text with corresponding capitalization and punctuation labels. This serves as annotation examples in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9505,
      "completion_tokens": 367,
      "total_tokens": 9872
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert annotator for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper about quality assurance involving multiple human expert annotators for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple human non-expert annotators validated the dataset annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that AI models were used as judges for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or detailed description on the use of automatic processes for quality assurance of dataset annotations is provided."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper presents the ViCapPunc dataset collected by crawling question and answer pairs from online healthcare sites and describes preprocessing and labeling steps; however, it does not document any explicit quality assurance or validation process for the annotations or content."
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9115,
      "completion_tokens": 537,
      "total_tokens": 9652
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 ViCapPunc Dataset",
          "reasoning": "The dataset was created by crawling 50,000 question and answer pairs from reputable Vietnamese healthcare websites such as Vinmec, Alobacsi, and ISOFHCARE. This data was collected from real human-generated content (questions and answers) from medical domain websites, reflecting spoken language context. The paper describes pre-processing and labeling steps performed on the raw text, which aligns with human effort producing original content distinct from existing public datasets. Thus, the dataset is original content created by human contributors from scratch, not translated or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was generated entirely by AI or models. The dataset is based on crawled human-written text rather than synthetic data generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information or indication in the paper suggests that the dataset is derived by translating data from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of dataset content being produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although data was collected from multiple medical question-answer websites, these sources are original human-generated content rather than existing datasets or corpora. The paper describes crawling data rather than aggregating existing datasets, and the data was transformed and labeled, indicating more than simple collation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 ViCapPunc Dataset",
          "reasoning": "The collected raw text was pre-processed by converting to lowercase, removing special characters except certain punctuations, and labeled with capitalization and punctuation tags. These transformations and the labeling process constitute modifications and adaptations from the original raw web data, making the dataset derived in the sense that it is not raw data directly used but processed and annotated for the task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and source of the dataset is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9633,
      "completion_tokens": 319,
      "total_tokens": 9952
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5 Experiments",
          "reasoning": "The ViCapPunc dataset introduced by the authors is used to fine-tune pre-trained language models (e.g., XLM-R, vELECTRA) with supervised learning methods. The training procedure and results reported in Section 5 show the dataset is used for supervised fine-tuning of these models for capitalization and punctuation recovery tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 Experiments",
          "reasoning": "The ViCapPunc dataset is also used for evaluation and benchmarking the proposed JointCapPunc model performance by measuring precision, recall, and F1 scores on validation and test splits as detailed in Section 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Experiments (Ablation study)",
          "reasoning": "The authors conduct error analysis and ablation studies on their dataset, analyzing the impact of different model components on performance. This shows the dataset's use for analyzing trends and characteristics related to model effectiveness."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10356,
      "completion_tokens": 562,
      "total_tokens": 10918
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset ViCapPunc is described as containing only Vietnamese language data. There is no indication of more than two human languages included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset ViCapPunc is only in Vietnamese language; there is no mention of inclusion of exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not in English; it is specifically Vietnamese medical domain spoken language data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3, ViCapPunc Dataset",
          "reasoning": "The dataset ViCapPunc is a large-scale public dataset for Vietnamese capitalization and punctuation recovery, containing spoken language from the Vietnamese medical domain. The dataset is explicitly stated to be in Vietnamese. For example, it contains 50,000 question and answer pairs crawled from Vietnamese healthcare websites and is lowercased Vietnamese text."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content is natural language text (Vietnamese), with capitalization and punctuation labels. There is no indication or description of programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the model section describes mathematical notations to represent model components, the dataset itself consists purely of natural language sentences without mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human Vietnamese language text from medical conversation transcripts. There is no mention of biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains naturally occurring Vietnamese language and does not include fictional or artificial languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is explicitly documented as Vietnamese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains natural language text in Vietnamese, so language is present."
        }
      }
    }
  },
  {
    "id": "thithuuyen22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7574,
      "completion_tokens": 175,
      "total_tokens": 7749
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 1 Introduction",
          "reasoning": "The paper explicitly states in the Abstract and Introduction that they publicly release their dataset and the implementation of their model at https://github.com/anhtunguyen98/JointCapPunc, indicating that code related to data collection, preprocessing, and generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 ViCapPunc Dataset",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 3, including data source descriptions (crawling from Vietnamese healthcare websites), dataset statistics, preprocessing steps (lowercasing, removing special characters, segment length considerations), label definitions for capitalization and punctuation, and dataset splits into train, validation, and test sets, demonstrating transparent and complete dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10637,
      "completion_tokens": 108,
      "total_tokens": 10745
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2, 4.1",
          "Reasoning": "The new dataset consists of recordings of 28 participants reading aloud oral passages and performing shadowing tasks. These recordings are human-produced audio data obtained by recording the participants reading and shadowing passages aloud. The passages include various accents and are recorded specifically for this study, as detailed in Sections 3.2 and 4.1."
        }
      ]
    }
  },
  {
    "id": "tomita24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11437,
      "completion_tokens": 299,
      "total_tokens": 11736
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1, Section 3.2",
            "reasoning": "Participants were recruited and selected through a screening process conducted by the authors. The annotation involves measuring and analyzing shadowing disfluencies and pronunciation gaps, which indicates expert involvement rather than crowd-sourced or automatic annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2, Section 3.1, Section 3.2",
            "reasoning": "The paper describes detailed protocols for the shadowing exercises, including the steps participants followed (e.g., script-shadowing, reading aloud) and how the recordings were collected. This constitutes instructions given to participants and annotators to ensure consistency."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2, Section 3.2",
            "reasoning": "A clear quantitative metric of listening disfluency (LD) is defined based on Dynamic Time Warping (DTW) scores between different shadowed audio signals. The authors also specify an LD threshold value (1.5) for judging listening difficulty, which serves as a scoring rubric for interpreting results."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit annotation examples are provided in the paper.",
            "reasoning": "The paper does not present explicit annotation examples or exemplars illustrating how annotations are applied step-by-step or showing sample annotated data for training or guideline purposes."
          }
        }
      ]
    }
  },
  {
    "id": "tomita24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12627,
      "completion_tokens": 500,
      "total_tokens": 13127
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance conducted by a single human expert for the dataset. The participants are speakers reading passages, and the data collection involved screening and recording. No expert validation of annotations or content is described."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the dataset or annotations. The paper does not document any manual expert review or validation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator. No such process is mentioned."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence is provided that multiple non-expert annotators conducted quality assurance. The data collection and analysis are described without mention of such QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although Kaldi ASR toolkit and DTW are used in analysis, the paper does not describe AI models performing quality assurance or judgment on dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2 Recording of World Englishes samples; Section 2.2 Shadowing as a tool to measure listening disfluency",
          "reasoning": "The dataset's quality and annotation validation appear to rely on automated processes such as Dynamic Time Warping (DTW) used to measure pronunciation gap and listening disfluency, and use of Kaldi speech recognition toolkit configurations. The paper details these algorithmic methods for measuring and analyzing data but does not mention human QA. Thus, quality assurance is based on automated verification via algorithmic techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes automated algorithmic methods for analyzing the dataset, indicating that some QA process is applied. Therefore, it is not correct to say no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12237,
      "completion_tokens": 455,
      "total_tokens": 12692
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.2, Data collection",
          "reasoning": "The paper describes that 28 participants read aloud selected passages and their oral passages were recorded. These recordings of World Englishes samples consist of original speech data created by human participants reading passages aloud. The data are not translated, adapted, or derived from pre-existing speech data but are newly recorded from scratch by human speakers."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset generated entirely by AI or machine learning models without human involvement in data creation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the datasets being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data was generated by translating content using machine translation systems as indicated in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions existing corpora and publicly available passages, the new dataset consists of newly recorded speech samples, not aggregated existing data sets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2 Recording of World Englishes samples",
          "reasoning": "The passages read aloud by participants were originally from existing test materials (Eiken Grade-2 Tests) with GA accent, and were converted to RP versions using text-to-speech synthesis. The participants then read these passages aloud to create new data. Therefore, the data is derived from existing sources with modifications (e.g., choice of passages, conversion between accents) and human speech recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data collection and origin are clearly described."
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12755,
      "completion_tokens": 222,
      "total_tokens": 12977
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The newly introduced dataset consists of recordings of World Englishes speakers and their shadowing performances, which are used primarily to analyze the directional listening fluency and communicability among speakers. The dataset supports visualization techniques (communicability charts) and analysis of patterns in listening disfluency and pronunciation gap rather than training or evaluation of machine learning models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13478,
      "completion_tokens": 533,
      "total_tokens": 14011
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Table 1 and Section 3.1",
          "reasoning": "The newly introduced dataset consists of audio recordings from 28 participants whose first languages span multiple language families and subfamilies, including Chinese (Sino-Tibetan), Japanese and Korean (Trans-Eurasian), French, Italian, Spanish, Hindi, Serbian, Ukrainian (Indo-European subfamilies), Hungarian (Uralic), and Malay (Dravidian). This indicates that the dataset entries involve more than two human languages represented by different speakers' accents and backgrounds."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although all passages recorded are English texts (from Eiken tests), the dataset focuses on speakers from diverse L1 backgrounds; since the entries represent various first languages influencing English pronunciation, the dataset is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises audio recordings and their analysis, but does not include mathematical or formal logical expressions as dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human speech and listening behaviors; no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages and accents of speakers are specified and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains human language speech data."
        }
      }
    }
  },
  {
    "id": "tomita24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10696,
      "completion_tokens": 146,
      "total_tokens": 10842
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not mention any code repository or provide links to code used for data collection, preprocessing, or analysis. No statements about publicly available code are found in the document."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 Data collection and 4 Analysis and visualization",
          "reasoning": "The paper documents the dataset creation process in detail, including participant recruitment and screening (Section 3.1), recording procedures of World Englishes samples (Section 3.2), and subsequent analysis and visualization methods (Section 4). This provides transparency on how the data were collected and processed, enabling reproducibility to some extent."
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7585,
      "completion_tokens": 161,
      "total_tokens": 7746
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The authors introduce a new in-house Vietnamese dataset named VBSF001 consisting of approximately 10 hours of audio recordings from a professional Southern female speaker, explicitly described as human-recorded speech data used for training and evaluation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The VBSF001 dataset includes normalized text corresponding to the audio utterances, which are manually collected and normalized by a rule-based TTS front-end module, implying the text originates from human-generated scripted sentences for TTS training."
        }
      ]
    }
  },
  {
    "id": "tran22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8385,
      "completion_tokens": 198,
      "total_tokens": 8583
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.3 MOS Test",
            "reasoning": "The paper states that for the Mean Opinion Score (MOS) test evaluation, 20 testers who are all Vietnamese native speakers from different regions listened to the audio samples. This implies multiple human non-expert annotators judged the audio quality."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any detailed annotation instructions given to the MOS testers or annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There is no mention in the paper of scoring rubrics or detailed scoring criteria used for the MOS evaluations by the testers."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide or mention any examples given to annotators for guidance during MOS testing or other annotations."
          }
        }
      ]
    }
  },
  {
    "id": "tran22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9575,
      "completion_tokens": 338,
      "total_tokens": 9913
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance being conducted by a single human expert on the new dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts validated or performed QA on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of multiple non-expert humans conducting quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention of any automated verification or rule-based quality assurance process for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new Vietnamese speech dataset named VBSF001 but provides no details on any quality assurance or validation process applied to this dataset."
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9185,
      "completion_tokens": 389,
      "total_tokens": 9574
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper states that they use an in-house Vietnamese dataset named VBSF001 with a professional Southern female accent, consisting of approximately 7000 utterances (~10 hours). It is described as a dataset recorded by humans and not derived from any existing resource or translated data, indicating original content created entirely by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any datasets entirely using AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was created by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described as in-house and recorded rather than collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of the dataset being based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and generation method of the dataset is explicitly described, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9703,
      "completion_tokens": 258,
      "total_tokens": 9961
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper explicitly states that their in-house Vietnamese dataset VBSF001 is used for training their speech synthesis model from scratch, as described in the training details in Section 3.1. The dataset is split into training, validation, and test sets and used for training their TTS models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 Latency and Quality",
          "reasoning": "The dataset is also used for evaluation purposes where the authors conduct Mean Opinion Score (MOS) tests and latency measurements on samples from the test set to benchmark the proposed model's performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10426,
      "completion_tokens": 529,
      "total_tokens": 10955
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only introduces a single dataset, VBSF001, which is described as a Vietnamese dataset with a professional Southern female accent. No other languages are mentioned."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains entries in exactly two human languages. The dataset is specified only to contain Vietnamese."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The released dataset, VBSF001, is specifically stated to be Vietnamese speech data, not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper states that the in-house dataset VBSF001 is Vietnamese, with a professional Southern female accent. It consists of Vietnamese utterances only and no other languages are noted."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is described purely as a speech dataset (audio and corresponding text) with no programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas describing training losses and models, these are not part of the dataset but part of the methodology."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech in Vietnamese; no biological sequences or non-human communication data is mentioned."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificial languages in the dataset. It is a natural language Vietnamese dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly documented as Vietnamese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data in Vietnamese; therefore, it does contain language."
        }
      }
    }
  },
  {
    "id": "tran22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7644,
      "completion_tokens": 195,
      "total_tokens": 7839
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Data and Abstract",
          "reasoning": "The paper mentions that the authors release a Vietnamese single accent dataset used in their experiments and provides a URL to the dataset on HuggingFace, but does not provide any link or mention of releasing any code associated with the dataset construction or preprocessing. There is no indication that the code for dataset collection or preprocessing is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The paper provides a detailed description of the dataset used (VBSF001), including number of utterances (about 7000), total duration (~10 hours), speaker accent (professional Southern female), sampling frequency (22 kHz), train/validation/test splits, and a brief description of text normalization and the data preparation process. Therefore, the dataset creation process is documented in sufficient detail for reproducibility evaluation."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7799,
      "completion_tokens": 92,
      "total_tokens": 7891
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Datasets",
          "Reasoning": "The paper introduces a private dataset named ASound, a crowdsourced dataset collected by the authors for respiratory illness detection recorded using mobile phones, which inherently involves human participation in recording their respiratory sounds. The data modality is audio as it contains respiratory sound samples from human participants."
        }
      ]
    }
  },
  {
    "id": "tran23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8599,
      "completion_tokens": 218,
      "total_tokens": 8817
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1 Datasets",
            "reasoning": "Section 4.1 describes the ASound dataset as a crowdsourced dataset collected using mobile phones with participants assigned anonymized identifiers for personalization. The collection involved user consent and was crowdsourced, implying multiple human non-expert participants contributed data."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide explicit information or details about detailed instructions given to annotators or participants for labeling or data annotation in the ASound dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no mention of rubrics or scoring criteria provided for annotation or labeling in the ASound dataset within the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide examples or annotation guidelines comprising examples for the ASound dataset annotations."
          }
        }
      ]
    }
  },
  {
    "id": "tran23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9789,
      "completion_tokens": 370,
      "total_tokens": 10159
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the involvement of a single human expert for quality assurance of the introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts performed quality assurance or annotation verification on the new ASound dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide details indicating that a single non-expert human annotator conducted quality assurance for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that multiple non-expert human annotators performed quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by an AI model acting as a judge on the dataset annotations or contents."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe an automated verification or algorithmic process used as quality assurance for the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces the ASound dataset as a crowdsourced collection with some anonymized profile information but does not document or describe any quality assurance processes or validation methods for dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9399,
      "completion_tokens": 433,
      "total_tokens": 9832
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets, paragraphs discussing the ASound dataset",
          "reasoning": "The authors collected a new crowdsourced respiratory sound dataset named ASound with 4495 samples recorded using mobile phones. Additionally, they conducted a second collection phase with unique anonymized identifiers (ASound-P), indicating original data created entirely from scratch by human contributors, not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models. The datasets consist of real-world human recorded sound samples."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of datasets generated by machine translation exists in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses several existing public datasets (Coswara, COUGHVID, ICBHI), these are not new datasets introduced by the authors; therefore, collated is not applicable to datasets introduced in this work."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe creating new datasets based on modifications or transformations of existing datasets. The ASound dataset is described as newly collected, not derived."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin of their newly introduced dataset (ASound) through crowdsourced human collection, so the data origin is documented."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9917,
      "completion_tokens": 713,
      "total_tokens": 10630
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1, Algorithm 1, and Section 4.3.1",
          "reasoning": "The authors use their new dataset ASound-P in the pre-training phase where a transformer-based model is pre-trained via masked contrastive learning to learn personalized profile embeddings. Specifically, ASound-P samples with profile information are used to pre-train the profile extractor model as detailed in the training pipeline (Algorithm 1 lines 5 to 7). This pre-training is confirmed to improve performance in downstream tasks (Table 1) and is a core aspect of their framework (Section 3.1 and 4.3.1)."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training the model solely from randomly initialized parameters using the new dataset alone without pre-training. Instead, training involves pre-training and fine-tuning with personalization."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.3.1",
          "reasoning": "After the pre-training (profile extraction), the ASound dataset variants are used to fine-tune downstream classification models in a supervised manner. Section 3.2 describes end-to-end training with the extracted profile features concatenated to classification features, optimizing cross-entropy loss. The performance improvements shown in Table 1 demonstrate supervised fine-tuning on ASound and its subsets."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper makes no mention of reinforcement learning or RL-based post-training using the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.3 (Tables 1, 2, 3, 4)",
          "reasoning": "The ASound dataset, particularly its variants ASound-P and ASound-NP, is used for extensive evaluation and benchmarking of the proposed RoPADet method. Performance metrics including AUC on ASound-P and ASound-NP (Table 1) and cross-dataset evaluation (Table 3) confirm its usage for evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3.3 and Table 4",
          "reasoning": "The dataset is used for analysis of performance fairness across user sub-groups (age and gender) in Table 4 and discussed in Section 4.3.3 to show fair classification outcomes. This analysis helps understand the impact of personalization."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that ASound or its variants are used as a knowledge base for retrieval-augmented generation or similar."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The ASound dataset, a new private dataset introduced by the authors, is heavily utilized throughout the paper for pre-training, supervised fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10640,
      "completion_tokens": 476,
      "total_tokens": 11116
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that the new dataset contains samples in more than two human languages. There is no mention of multilingual linguistic coverage for the ASound dataset introduced by the authors."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the new dataset has samples in exactly two human languages. The linguistic coverage of the dataset is not described as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "The paper presents the ASound dataset collected for respiratory illness detection. The dataset is described in the context of respiratory sound recordings with no explicit mention of multiple languages or multilingual content. Since speech samples are from participants likely speaking English (or at least the paper does not specify otherwise), and no other languages are mentioned, it is reasonable to interpret the language coverage as monolingual in English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that the dataset entries are in a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries described are sound recordings, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or formal logical expressions or symbolic representations are part of the dataset entries. Mathematical notation appears only in model formulation, not the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries consist of human respiratory sound recordings, not biological sequences like DNA or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly describes the dataset as containing respiratory sound recordings collected from human subjects, thus the language and nature of entries are known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains audio recordings of human speech and cough sounds, which are linguistic or paralinguistic data; thus, the dataset entries contain language."
        }
      }
    }
  },
  {
    "id": "tran23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7858,
      "completion_tokens": 159,
      "total_tokens": 8017
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 4.1",
          "reasoning": "The abstract explicitly states that source codes, reproducible baselines, and their new dataset are made publicly available at https://github.com/ReML-AI/RoPADet, indicating that code related to data collection, preprocessing, and dataset generation is accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Datasets",
          "reasoning": "Section 4.1 provides a description of the newly collected ASound dataset, including the total number of recordings, collection method (crowdsourced via mobile phones), presence of profile information such as anonymized identifiers, and the reasoning behind the data collection. This constitutes documentation on dataset creation process."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8555,
      "completion_tokens": 116,
      "total_tokens": 8671
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Table 1, and Section 3.1",
          "Reasoning": "The HEBDB dataset is described as roughly 2500 hours of natural and spontaneous speech recordings, collected from testimonies and podcasts in Hebrew. The data is raw recordings of in-the-wild audio from human speakers, explicitly stated as natural speech and spontaneous. Thus, the audio modality is human generated, as it is directly recorded from human speech."
        }
      ]
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9355,
      "completion_tokens": 232,
      "total_tokens": 9587
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 3.1 and 3.2",
            "reasoning": "The dataset HEBDB's pre-processed version is automatically transcribed using a pre-trained Whisper ASR model (an AI model). Data filtering uses a forced aligner (MMS model) and automatic confidence scoring to filter samples, indicating no human annotations were performed for transcription or quality assessment."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not mention any annotation instructions provided to human annotators since the transcriptions are produced by an AI model automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No scoring rubrics for human annotators are described; the only scoring applied is automatic confidence scoring by forced aligner, which is an AI process, not human annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper provides no details about examples for human annotations or guidelines since the annotations are automatic transcriptions from AI models."
          }
        }
      ]
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10545,
      "completion_tokens": 366,
      "total_tokens": 10911
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance being performed by a single human expert annotator on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of multiple human expert annotators conducting quality assurance or validation on the dataset annotations or transcriptions."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of multiple non-expert human annotators performing quality assurance or annotation verification."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1 Pre-processing and Section 3.2 Data filtering",
          "reasoning": "The dataset transcriptions were generated automatically using a pre-trained Whisper ASR model, which is an AI model. Furthermore, a forced aligner (MMS model) was used to generate confidence scores for each utterance to filter out lower-quality data, effectively acting as AI-based quality assurance."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 Pre-processing and Section 3.2 Data filtering",
          "reasoning": "The pre-processing pipeline includes automatic processes such as voice activity detection (VAD) using silero-vad model and automated filtering based on confidence scores from forced alignment. These algorithmic and rule-based processes contribute to quality assurance by removing low-quality speech segments and transcripts."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes multiple automated processes used as quality assurance mechanisms; hence, this label does not apply."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10155,
      "completion_tokens": 449,
      "total_tokens": 10604
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of recordings sourced from existing podcasts and testimonies; it does not represent original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data itself is not generated by AI or machine learning models; rather, models are used for transcription and preprocessing but not to generate the speech data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human translation involved in creating the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention machine translation of data; the transcription is done with an ASR model and transliteration tools are used only for alignment."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 (HEBDB dataset), Table 1",
          "reasoning": "The dataset is aggregated from multiple existing recorded sources such as podcasts and testimonies, without indication of the recordings being created by the authors. Thus, it is collected from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 (Pre-processing), Section 3.2 (Data filtering)",
          "reasoning": "The authors apply resampling, VAD-based segmentation, automatic transcription using a pre-trained ASR model, normalization (e.g., removing irrelevant tokens), diacritization, transliteration, and filtering based on confidence scores. These steps indicate the dataset is transformed and adapted from the original raw collected audio, making it a derived dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper sufficiently documents and describes the data sources and generation method."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10673,
      "completion_tokens": 499,
      "total_tokens": 11172
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 4.1 Baseline system, HuBERT model training description",
          "reasoning": "The dataset is used in self-supervised pre-training of the HuBERT-base model, where the authors train the model from the HEBDB dataset in an unsupervised manner before fine-tuning."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters without pre-training on this dataset; both provided baselines use pre-training or supervised fine-tuning instead."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4.1 Baseline system and Section 4.2 Results",
          "reasoning": "The dataset is used for supervised fine-tuning of both the HuBERT model (after pre-training) and the Conformer model, using the pre-processed and filtered version of HEBDB for ASR training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using the dataset for any reinforcement learning based post-training methods such as RLHF."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset itself is not used exclusively for evaluation; evaluation is done on the Fleurs benchmark, a separate dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analysis of trends or patterns; the paper focuses on releasing the dataset for model training and demonstrating baseline results."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used as a knowledge base to augment models via retrieval or similar mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has documented usage in pre-training and supervised fine-tuning as described in the paper."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11396,
      "completion_tokens": 565,
      "total_tokens": 11961
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 and Abstract",
          "reasoning": "The HEBDB dataset is composed solely of Hebrew language speech recordings; there is no indication that it includes more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3 and Abstract",
          "reasoning": "The dataset entries are described only as Hebrew speech with no mention of any second human language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3 and Abstract",
          "reasoning": "The dataset focuses on Hebrew speech processing; English content is not described as part of the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 3; Section 3.1",
          "reasoning": "The HEBDB dataset contains only Hebrew language spoken data as per multiple mentions in the abstract and dataset description (Section 3). The text transcriptions are in Hebrew, and the speech recordings are Hebrew, confirming the dataset is monolingual in Hebrew, a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains natural speech audio and accompanying transcriptions, without any indication of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention or indication of mathematical or logical notation or symbolic representations in the dataset contents."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is focused on human speech in Hebrew; no non-human or biological communication data is described."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset includes Hebrew only and does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset language is explicitly stated as Hebrew and well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains spoken and transcribed Hebrew language, thus it includes language."
        }
      }
    }
  },
  {
    "id": "turetzky24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8614,
      "completion_tokens": 195,
      "total_tokens": 8809
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3 (Pre-processing)",
          "reasoning": "The abstract states that dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/. The paper describes in detail the preprocessing pipeline and mentions usage of open-source packages and models. This indicates that code related to preprocessing and dataset creation is available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (HEBDB dataset) and Subsections 3.1 (Pre-processing) and 3.2 (Data filtering)",
          "reasoning": "The paper provides detailed documentation on dataset collection, composition, preprocessing steps including resampling, segmentation using VAD, transcription methods, and data filtering using forced alignment with confidence thresholds. Statistical summaries of raw and preprocessed data and rationale for preprocessing decisions are documented, demonstrating transparency and completeness in dataset creation."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9137,
      "completion_tokens": 721,
      "total_tokens": 9858
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, Tables 1 and description in 'Aivaliot' subsection",
          "Reasoning": "The Aivaliot corpus is a newly introduced dataset compiled by the authors from oral narratives elicited from 18 elderly native speakers in 2002-2003. The audio recordings are human speech captured by researchers, explicitly stated as part of the Asia Minor Archive compiled within research projects. This confirms audio data modality and human-generated origin."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, Tables 1 and 'Aivaliot' subsection",
          "Reasoning": "The Aivaliot corpus was transcribed and annotated manually by two native speakers of the dialect, with detailed orthographic transcription. This indicates the presence of text data modality created by human annotators, confirming human-generated origin."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Cretan' subsection and Table 1",
          "Reasoning": "The Cretan corpus was compiled from 32 tapes of radio broadcasts recorded and aired between 1998-2001. The broadcasts contain radio narratives by a named speaker (Ioannis Anagnostakis). These are human speech audio recordings collected with permission from the library. Thus, this is audio data of human-generated origin."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Cretan' subsection",
          "Reasoning": "Initial transcripts for the Cretan dataset were produced using the Whisper Large-v2 model but were then manually corrected in collaboration with a native speaker/transcriber. The finalized transcriptions therefore include human-generated text data, despite initial model-generated drafts."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Griko' subsection and Table 1",
          "Reasoning": "The Griko corpus was collected during a field trip by linguists in 2013. The linguistic content is audio recordings of nine native speakers. These are human speech audio recordings recorded digitally in the field, hence audio modality and human-generated nature."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Griko' subsection",
          "Reasoning": "The Griko audio files were manually segmented, transcribed, glossed in Italian and annotated by trained linguists. This makes the text transcripts human-generated annotations accompanying the audio data."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Messenian' subsection and Table 1",
          "Reasoning": "The Messenian corpus was collected by new interviews conducted in 2023-2024 with residents from Kalamata and nearby villages resulting in collected narratives audio of six speakers. This new data is human-generated speech audio recordings."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 Resources, 'Messenian' subsection",
          "Reasoning": "Initial transcripts of Messenian audio were produced using the Whisper Large-v3 model but these transcripts were then manually corrected by a native speaker of the dialect. This yields human-generated transcription text for the dataset."
        }
      ]
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9937,
      "completion_tokens": 208,
      "total_tokens": 10145
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3 Resources (Aivaliot, Cretan, Griko, Messenian subsections)",
            "reasoning": "Transcriptions for the dialect datasets (Aivaliot, Cretan, Griko, Messenian) were made or corrected by native speakers and trained linguists, implying multiple human experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly discussed",
            "reasoning": "The paper does not explicitly describe the presence of detailed annotation instructions for the transcribers or annotators of the new datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly discussed",
            "reasoning": "The paper does not mention scoring rubrics or criteria for the transcription or annotation process in the dialect corpora."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly discussed",
            "reasoning": "The paper does not indicate the presence of annotation examples or guidelines provided to the annotators."
          }
        }
      ]
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11127,
      "completion_tokens": 482,
      "total_tokens": 11609
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3 Resources, Aivaliot; Griko; Messenian corpus description",
          "reasoning": "The Aivaliot corpus was transcribed and annotated by two native speakers of the dialect (implying experts), and the Griko corpus was transcribed, segmented, and annotated by trained linguists (experts). For the Messenian corpus, transcripts were manually corrected by a native speaker of the dialect, considered an expert. These descriptions indicate quality assurance by single or multiple human experts per dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3 Resources, Aivaliot; Griko corpus description",
          "reasoning": "Aivaliot data annotations involved two native speakers of the dialect, indicating multiple human experts. The Griko corpus was collected by two linguists and annotated by trained linguists, also multiple human experts ensuring QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates QA conducted by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper indicates QA conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models like Whisper were used to produce initial transcriptions for Cretan and Messenian, these outputs were subsequently manually corrected by native speakers, indicating that AI models were not solely responsible for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA performed through automated verification of code or formulas, nor algorithmic or rule-based techniques for annotation QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents human-based QA procedures involving experts for all new datasets introduced. Therefore, it does not apply."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10737,
      "completion_tokens": 605,
      "total_tokens": 11342
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (Resources), especially 3.1 Aivaliot, 3.2 Cretan, 3.3 Griko, 3.4 Messenian",
          "reasoning": "The paper describes that the Aivaliot corpus was compiled from narratives elicited from elderly speakers, collected with consent and transcribed by native speakers. The Cretan corpus was gathered from radio broadcasts and manually corrected transcripts from native speakers. The Griko corpus was collected during field trips by linguists who manually segmented and transcribed the audio. The Messenian corpus was recently collected via interviews, with transcription manually corrected by a native speaker. These processes indicate original data creation through human recording, transcription, and annotation, not derived or adapted from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists in the paper that any of the datasets were generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets were not produced by translating content from another language by human translators; they are original speech recordings and transcriptions of dialect speakers."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any datasets were created by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3 (Resources), especially 3.1 Aivaliot and 3.2 Cretan",
          "reasoning": "Some datasets, like the Aivaliot corpus, were compiled from existing narratives collected during previous projects and stored in archives, thus representing collected or aggregated data from existing sources without significant modification. Similarly, the Cretan corpus was gathered from existing radio broadcasts collected by an audiovisual department, indicating collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4 (Benchmark Description), Data Segmentation and Dataset Creation",
          "reasoning": "For Cretan and Messenian corpora, initial transcriptions were generated using the Whisper automatic speech recognition models and then manually corrected by native speakers. This process involves modification and adaptation of model-generated transcriptions rather than purely original human transcription. Also, audio recordings were segmented and normalized, indicating transformations applied to the original audio and transcripts to prepare the datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper provides clear descriptions and provenance of the data sources and generation methods; thus, data origin is documented."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11255,
      "completion_tokens": 480,
      "total_tokens": 11735
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are not used for pre-training models; the paper describes fine-tuning and evaluation but no pre-training on these datasets."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report training models from random initialization on these datasets; only fine-tuning pre-trained models is discussed."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Benchmark Description) and Section 5 (Results)",
          "reasoning": "The paper fine-tunes pre-trained ASR models (XLS-R-greek and Whisper-medium) on the new dialect datasets with supervised learning to adapt to the specific dialects, as explicitly described in Sections 4 and 5."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning based post-training or RLHF techniques being applied using the datasets."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Results)",
          "reasoning": "All four new dialect datasets are used for benchmarking and performance measurement of ASR models, including zero-shot inference and fine-tuned evaluation, as shown in Section 5."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 6 (Discussion)",
          "reasoning": "The datasets are used to analyze the challenges posed by dialectal variation on ASR performance and to discuss linguistic characteristics affecting results, as detailed in the discussion section."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used to augment models via retrieval or as an external knowledge base."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents clear usage of the datasets for fine-tuning, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11978,
      "completion_tokens": 596,
      "total_tokens": 12574
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset contains dialects of Greek only; each dataset corresponds to a single non-English dialect. Although the paper mentions influences from other languages (e.g., Turkish and Italian), the dataset entries themselves are not mixed or annotated with multiple languages concurrently within entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Each dataset entry is focused on one dialect (e.g., Aivaliot, Cretan, Griko, or Messenian). There is no indication that the data contains exactly two human languages per entry."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain Greek dialects and no English-only content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3 Resources (especially 3.1 Aivaliot, 3.2 Cretan, 3.3 Griko, 3.4 Messenian)",
          "reasoning": "The datasets are transcriptions and audio recordings of four varieties of Greek dialects: Aivaliot, Cretan, Griko, and Messenian. Each dataset consists entirely of one dialect, all non-English, mostly using Greek alphabet for transcription (except Griko uses Latin alphabet). Therefore, each dataset is monolingual in a single non-English language variety."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No programming or structured code-related content is present in the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets contain speech and transcription data only, no mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of natural Greek dialects, not constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages of the datasets are explicitly documented and specified as Greek dialects."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Since the datasets contain transcribed speech in Greek dialects, they all contain human language."
        }
      }
    }
  },
  {
    "id": "vakirtzian24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9196,
      "completion_tokens": 183,
      "total_tokens": 9379
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 (Resources) and Section 4 (Benchmark Description)",
          "reasoning": "The paper describes the datasets' origins, collection methods, and preprocessing steps in detail, but it does not mention the availability of any associated code repository or provide links to code used for data collection, preprocessing, segmentation, or dataset creation. No URLs or references to public code are included."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Resources), Section 4 (Benchmark Description)",
          "reasoning": "The paper provides comprehensive descriptions of each new dataset's origin, collection procedures, transcription methods, ethical considerations (e.g., speaker consent, ethics committee approval), metadata annotations, and data segmentation techniques. It documents the process of converting audio to text segments and creating training/dev/test splits, which supports reproducibility of dataset construction."
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6545,
      "completion_tokens": 232,
      "total_tokens": 6777
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3; Section 3.1; Section 3.2",
          "Reasoning": "The Fair-Speech dataset consists of approximately 26.5K utterances in recorded speech by 593 people in the United States who self-recorded and submitted audio commands. The speech data was collected from paid participants speaking prompted commands related to voice assistant use cases. This data is explicitly recorded from humans and is thus human-generated audio."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The dataset includes verbatim transcriptions of the utterances, produced through multi-pass human transcription and curation by external vendors and internal linguistic experts. These transcriptions are human-generated textual data corresponding to the audio utterances."
        }
      ]
    }
  },
  {
    "id": "veliche24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7345,
      "completion_tokens": 260,
      "total_tokens": 7605
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 describes multi-pass human transcription by external vendors with a three-way agreement, and data curation involved audits by cultural-specific linguistic vendors and internal linguistic experts; this indicates multiple human annotators who are trained transcribers or non-experts performing transcription and annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The description of multi-pass annotation and auditing process with linguistic vendors and internal experts implies detailed transcription instructions were provided to ensure verbatim transcription and consistency in handling colloquial words, repeated words, and entity capitalization."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no mention in the paper of scoring rubrics or explicit scoring guidelines provided to annotators during transcription; the term 'scoring' refers to internal evaluation of annotation WER by experts, not rubrics given to annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not mention any example annotations or transcription examples provided to annotators as part of the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "veliche24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8535,
      "completion_tokens": 412,
      "total_tokens": 8947
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.2 Data transcription",
          "reasoning": "The paper states that 200 random rows were scored by an internal linguistic expert to obtain the annotation WER (1.47%), indicating quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.2 Data transcription",
          "reasoning": "The multi-pass annotation process involved multiple external vendors performing transcriptions until three-way agreement was reached, and disagreements (10%) were resolved by internal linguistic engineers. Various internal teams performed cross-functional validations and audits by cultural-specific linguistic vendors occurred, indicating multiple human experts were involved in quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of quality assurance performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any AI model was used as a judge for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process is described as part of the quality assurance for the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a detailed, multi-pass human transcription and cross-validation process, so quality assurance was clearly performed."
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8145,
      "completion_tokens": 416,
      "total_tokens": 8561
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 3 Corpus contents",
          "reasoning": "The paper clearly states that the Fair-Speech dataset consists of audio recordings of approximately 26.5K utterances from 593 people in the United States, who were paid to record and submit audio commands. These are original audio recordings produced by human speakers responding to prompts, not derived from existing datasets. Participants self-identified their demographics and the data collection process involved participants producing their own content, not reading given text prompts, to ensure natural speech patterns."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human recorded audio; no mention is made of data generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translated data or materials translated by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation systems for dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The Fair-Speech dataset is not aggregated from existing datasets but rather collected through new recordings made by participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications or adaptations; the data is original speech recordings from participants."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The method of data generation is explicitly described; thus, this category is not applicable."
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8663,
      "completion_tokens": 285,
      "total_tokens": 8948
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 Corpus contents; Section 4 Speech recognition experiments; Section 5 Results",
          "reasoning": "The paper explicitly states that the Fair-Speech dataset is intended to be used as an evaluation tool to uncover gaps or biases in ASR models. Multiple models are evaluated on this dataset in Section 4 and results demonstrating performance differences across demographic groups are analyzed in Section 5, confirming its role in benchmarking and performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 Results",
          "reasoning": "Beyond evaluation, the dataset is used primarily for analyzing trends and patterns in ASR performance across demographic sub-groups, as detailed in Section 5, where the authors conduct statistical and model-based analyses including mixed-effects Poisson regression to understand fairness and disparity factors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9386,
      "completion_tokens": 508,
      "total_tokens": 9894
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3 Corpus contents, Linguistic variation and Table 1",
          "reasoning": "The dataset includes utterances from speakers whose native languages include English, Spanish, Mandarin, and other languages in smaller percentages. This indicates that the dataset contains entries in more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains more than two languages, not exactly two, so it is not classified as bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the majority of the data is from native English speakers, the dataset also includes other languages, thus it is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Since the dataset contains English plus other languages, it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains transcribed spoken voice commands, with no mention of mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human speech data and demographics, with no biological sequences or non-human communication present."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages being part of the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Language information is clearly documented in the dataset, including native languages of speakers."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly includes human language speech data."
        }
      }
    }
  },
  {
    "id": "veliche24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6604,
      "completion_tokens": 121,
      "total_tokens": 6725
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "None",
          "reasoning": "The paper does not mention any publicly available code repository or links related to data collection, preprocessing, or generation for the Fair-Speech dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 and subsections",
          "reasoning": "The paper provides detailed documentation about the dataset creation, including participant demographics, data collection methods with paid participants consenting to use of their data, recording conditions, transcription process, and demographic distributions. This documentation appears comprehensive and supports transparency and reproducibility in dataset creation."
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8017,
      "completion_tokens": 510,
      "total_tokens": 8527
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The NOTSOFAR recorded meeting dataset consists of 280 unique English meetings recorded in 30 different rooms with 4-8 attendees each. The recordings were captured using multiple devices with both single-channel and multi-channel microphone setups. These are real acoustic recordings involving human speakers in natural meeting scenarios, thus the audio data originates from human involvement and human recording tools."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1",
          "Reasoning": "The meeting dataset includes high-quality, multi-judge human transcriptions of the recorded meetings. The transcription process was conducted without machine aid and involved multiple professional human transcribers to ensure accuracy and minimize bias. Hence, the textual transcriptions are human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The NOTSOFAR simulated dataset is a 1000-hour training dataset synthesized using 15,000 real acoustic transfer functions convolved with clean speech from an external corpus. It is generated through simulation methods involving mixing up to three speakers, applying room impulse responses, and adding noise, i.e., it is model generated audio data."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.2",
          "Reasoning": "The simulated dataset uses clean speech drawn from the Librivox corpus, filtered for quality through an algorithmic Mean Opinion Score estimation. This clean speech originates from a human-generated corpus but is filtered and augmented by automated processing steps for simulation, which is part of the model-generated pipeline. However, since the final data is audio, and no textual content is directly presented or annotated as part of the simulated dataset, textual data is not directly introduced here. Therefore, no new text data is explicitly introduced as part of the simulated dataset besides that implied from the audio."
        }
      ]
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8817,
      "completion_tokens": 249,
      "total_tokens": 9066
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1, 'Multi-judge annotation for accurate segmentation and transcription' paragraph",
            "reasoning": "The paper specifies that for most meetings, transcription was carried out independently by two professional transcribers, with a third judge resolving non-consensus cases, involving multiple human experts (professional transcribers) for accurate annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1, 'Bias-free transcription process' and multi-judge annotation paragraphs",
            "reasoning": "The paper describes a transcription process designed to avoid machine bias and ensure transcription accuracy, implying detailed instructions to annotators to perform annotations without machine aid, ensuring unbiased labels."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not mention any scoring rubrics used as part of the annotation guidelines; rather, it focuses on multiple annotators and adjudication rather than rubric-based scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide or mention any specific annotation examples provided to annotators in the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10007,
      "completion_tokens": 376,
      "total_tokens": 10383
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The quality assurance for the NOTSOFAR recorded meeting dataset involved multiple human annotators who are professional transcribers. Most meetings were transcribed independently by two transcribers, and a third judge resolved non-consensus cases. This multi-judge annotation process aimed to ensure high-quality ground-truth transcription and segmentation. Although the paper does not explicitly confirm that the annotators were subject matter experts, the description of 'professional transcribers' and the detailed annotation process indicates that multiple human experts performed QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given in the paper that QA was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies professional transcribers and a multi-judge annotation process, implying expertise rather than non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used to perform quality assurance of the datasets."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While automatic segmentation and spectral subtraction were employed initially, the QA process is described as involving human correction and transcription. The automatic processes were part of preprocessing, not quality assurance of annotation quality. Hence, the automated methods do not constitute a QA step for annotation correctness."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A detailed QA process with multiple human annotators is described; thus, QA was applied and documented."
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9617,
      "completion_tokens": 462,
      "total_tokens": 10079
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The NOTSOFAR Recorded Meeting Dataset consists of 280 unique meetings involving real human participants speaking English in various conference rooms, with multi-judge human annotation processes for transcription and segmentation performed without machine bias. The data is original and created entirely from scratch by human contributors, not derived or translated from pre-existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced do not contain content generated entirely by AI or machine learning models; the simulated data uses recorded real acoustic transfer functions convolved with human speech but does not generate new speech content autonomously."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset recordings and transcripts are English and were generated natively; no indication is given that any content was created by translating from another language via humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the datasets was produced by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The recorded meeting dataset and simulated dataset are not merely aggregated from existing sources; they were collected and created with specific processes for this challenge."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The simulated training dataset is derived from existing clean speech recordings (Librivox corpus) selected by MOS filtering and real measured acoustic transfer functions (ATFs), which are convolved and processed to generate mixtures simulating realistic environments. Thus, it is adapted and transformed from pre-existing speech and measured data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sources and generation methods for both datasets are well-documented."
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10135,
      "completion_tokens": 352,
      "total_tokens": 10487
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The NOTSOFAR simulated training dataset, consisting of approximately 1000 hours of simulated multi-channel audio, is designed for training data-driven speech separation and enhancement methods from scratch with clean supervision signals."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The recorded meeting dataset of 280 distinct meetings with high-quality transcriptions and speaker labels is used to fine-tune or supervise training of DASR models, providing supervised learning data for speech recognition and diarization."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the datasets for reinforcement learning based post-training methods like RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 and Section 2",
          "reasoning": "The recorded meeting dataset is explicitly described as a benchmark dataset for evaluation and benchmarking of DASR systems, enabling reliable performance measurement with a large number of distinct meetings and associated metrics like tcpWER."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While metadata enables error analysis, the dataset is primarily intended for training and evaluation, and explicit use solely for analysis is not described."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not described as serving as a knowledge base for retrieval-augmented generation or related purposes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10858,
      "completion_tokens": 544,
      "total_tokens": 11402
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states that the recordings are exclusively in English (Section 3.1), with no mention of other languages; hence, the dataset is not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English as specified (Section 3.1), with no indication of a second human language; hence, it is not bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1, 'Oriented toward far-field' paragraph",
          "reasoning": "The recorded meeting dataset is explicitly stated to contain English language meetings spoken by native or near-native speakers, with no other languages recorded or mentioned."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset uses English exclusively; no non-English monolingual data is introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes entries with programming or structured code-related content; the paper focuses on speech meeting data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes some mathematical notation to describe methods, the dataset itself does not contain entries with mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech in meeting scenarios only; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include any fictional or artificially created languages; only natural human English speech is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the dataset are explicitly stated and documented as English; no ambiguity or unknown language presence."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains speech data in English, so it does contain language."
        }
      }
    }
  },
  {
    "id": "vinnikov24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8076,
      "completion_tokens": 195,
      "total_tokens": 8271
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 1 and Section 4",
          "reasoning": "The paper mentions that an open-source baseline system including inference, training, data handling, and evaluation code is provided (Section 1), but there is no explicit mention or link to the release of the code used to construct or collect the datasets themselves, such as data collection scripts or preprocessing used to create the recorded and simulated datasets."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (3.1 and 3.2)",
          "reasoning": "The paper provides extensive documentation on the dataset creation processes for both the recorded meeting dataset and the simulated training dataset in Section 3. This includes detailed descriptions of recording conditions, participant details, acoustic complexities, annotation procedures, metadata, simulation methodologies including the real-room acoustic transfer functions, clean speech selection, and mixing process, all contributing to a thorough and transparent dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7505,
      "completion_tokens": 123,
      "total_tokens": 7628
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2 and 3 (Experiment design and Data processing and analysis)",
          "Reasoning": "The new dataset is a corpus of radar data recorded using three different types of antennas positioned on speakers' cheeks while two German native speakers uttered 25 phonemes. The radar signals, representing transmission spectra as complex vectors sampled at 100 Hz, constitute sensor data captured via human subjects performing tasks. The dataset was recorded specifically for this study to compare antenna types and positions."
        }
      ]
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8305,
      "completion_tokens": 208,
      "total_tokens": 8513
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 and Section 3",
            "reasoning": "The data consists of phoneme labels segmented from audio recordings using Praat by humans, likely experts in phonetics or speech processing, as segmentation of phonemes requires expertise. The corpus was recorded with precisely controlled conditions by two native German speakers and segmented correspondingly."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No explicit instructions are described in the paper regarding consistent annotation or guidelines for phoneme segmentation beyond use of Praat segmentation software."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No rubrics or scoring criteria for annotation quality are described or mentioned in relation to the phoneme segmentation or annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not provide any annotation examples or illustrative examples of the annotation guidelines or segmented phonemes."
          }
        }
      ]
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9495,
      "completion_tokens": 305,
      "total_tokens": 9800
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that a single human expert performed quality assurance on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts performed quality assurance or annotation validation on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that a single non-expert conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence in the paper supports that multiple non-expert annotators were involved in quality assurance for the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models as judges for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that automatic verification or code-based QA was performed on the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper documents the dataset recording and experimental procedure in detail but does not describe any quality assurance process for validating the dataset annotations or their correctness. The segmentation of phonemes was performed with Praat using simultaneous audio recordings, but no manual or expert verification or QA steps are described. Therefore, no QA process is documented or applied."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9105,
      "completion_tokens": 395,
      "total_tokens": 9500
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Experiment design",
          "reasoning": "The dataset consists of recordings of phoneme utterances produced by two native German speakers in two sessions for each configuration, using different antenna types and positions. The corpus is described as being the same as in prior work [14], but the actual radar data recordings were newly acquired by human speakers performing the utterances in the experiment. Thus, the data is original human-produced speech-related radar signals recorded from scratch by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models without reference to raw experimental data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not produced by translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation for data generation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not aggregated from existing sources; it was newly recorded for this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though processed with feature extraction and segmentation, the core data is original radar recordings, not based on existing datasets with modification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is clearly documented: human speakers produced the utterances recorded through radar in the described experimental setup."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9623,
      "completion_tokens": 440,
      "total_tokens": 10063
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 (Data processing and analysis) and Section 4 (Results and discussion)",
          "reasoning": "The newly recorded dataset consisting of 8280 tokens across nine antenna setups, speakers, and sessions was used exclusively for phoneme recognition classification tasks to evaluate the performance of different antenna types and positions. The dataset serves as the benchmark to measure recognition rates and assess the effectiveness of the radar-based SSI under various configurations. The paper uses nested cross-validation and statistical tests to analyze recognition rates, confirming that this dataset is used for evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Results and discussion)",
          "reasoning": "Beyond evaluation, the dataset is used to analyze patterns and trends in recognition performance related to antenna type, antenna position, speaker differences, and feature sets. Linear mixed effect models and likelihood ratio tests are applied to the recognition rates derived from this dataset to study the significance of these variables, indicating its role in analyzing characteristics of the radar-based SSI system."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates practical usage of the newly recorded dataset for evaluation and analysis of the radar-based silent speech interfaces."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10346,
      "completion_tokens": 540,
      "total_tokens": 10886
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists solely of phoneme utterances by native German speakers; there is no indication of multiple languages used."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only German language phonemes, not two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The corpus and recorded phonemes are from native German speakers; the dataset is not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 (Experiment design), Abstract",
          "reasoning": "The proposed dataset is a corpus of 25 German phonemes uttered by native German speakers; the paper explicitly mentions use of German phonemes and native German speakers, indicating a monolingual dataset in the non-English German language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or code-related content is included as dataset entries; the dataset consists of radar recordings representing phoneme utterances."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper includes some mathematical models and analyses, the dataset itself contains no mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech-related radar data, not biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset involves natural language phonemes (German), not constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset's language is clearly specified as German phonemes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains entries of spoken human language (German phonemes) represented via radar signals, so language is present."
        }
      }
    }
  },
  {
    "id": "vitormenezes22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7564,
      "completion_tokens": 173,
      "total_tokens": 7737
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any links, URLs, or references to publicly available code or repositories related to the dataset creation or preprocessing steps. No indication that the code used for data collection, segmentation, feature extraction, or classification is shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 (Experiment design) and 3 (Data processing and analysis)",
          "reasoning": "The paper provides detailed information about the dataset creation process. Section 2 describes the antenna types, positions, and the speech corpus recorded, including the number of subjects, sessions, and total tokens recorded. Section 3 describes how data was processed, segmented, and features extracted. This constitutes thorough documentation of the dataset creation process within the paper."
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5828,
      "completion_tokens": 193,
      "total_tokens": 6021
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 Data",
          "Reasoning": "The paper explicitly states that the experiments use the Switchboard Telephone Corpus and the DailyDialog corpus as data sources. However, these are existing datasets, not introduced by the authors in this paper. Therefore, they do not qualify as new datasets introduced by the authors."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.3 Language model surprisal",
          "Reasoning": "The authors trained a TurnGPT language model from scratch on equal amounts of DailyDialog and Switchboard data to obtain surprisal estimates. This trained model and its output surprisals are model-generated data, but these surprisal estimations are not described as new datasets. Therefore, no new dataset of this modality introduced by the authors is reported."
        }
      ]
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6628,
      "completion_tokens": 258,
      "total_tokens": 6886
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.4 Behavioural study and Participants",
            "reasoning": "The 52 participants recruited for the acceptability judgement task were native English speakers from Prolific Academic, a crowd-sourcing platform, indicating multiple human non-expert annotators rather than experts or AI models."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 The Human Judgement Task",
            "reasoning": "Participants were given a detailed task description, being asked to rate how plausible a turn is in the dialogue context on a scale of 1-5. The description of the task as a graded acceptability rating task implies the presence of clear instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 The Human Judgement Task",
            "reasoning": "The rating scale from 1 to 5 ('Very Unlikely' to 'Very Likely') serves as a scoring rubric guiding the annotation judgement."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not mention providing annotators with examples of acceptable or unacceptable turns for the judgement task, or any sample annotations in guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7818,
      "completion_tokens": 424,
      "total_tokens": 8242
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single human expert or annotator with subject matter expertise."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts or annotators with domain expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a quality assurance process was conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.4 (Behavioural study and Participants)",
          "reasoning": "The behavioural study involved 52 participants recruited from Prolific Academic who are native English speakers but not described as experts. They provided acceptability judgments used as data, effectively acting as multiple human non-expert annotators contributing to data quality through their collective ratings."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While an AI model (TurnGPT) was used to generate surprisal estimates, it was not used as a judge or for quality assurance of the dataset annotations, but as a measurement tool."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance applied to the dataset annotations or stimuli beyond model computations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is a quality assurance process implied through data collected from multiple human participants (non-experts) in the behavioral study, thus this label does not apply."
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7428,
      "completion_tokens": 463,
      "total_tokens": 7891
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.4",
          "reasoning": "The paper describes a novel human judgement task where participants rated the acceptability of dialogue turns. The stimuli for this task were generated by sampling context-response pairs from existing dialogue corpora, and human participants provided new, original ratings for these stimuli during the study, thereby creating new data originating from human contributions."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No data are described as being generated entirely by AI or machine learning models. The language models were used to estimate surprisal on existing dialogue data, but did not generate new dialogue data or new experimental stimuli."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced via human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that data were generated by machine translation from another language."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "The dialogue data used for generating stimuli were derived from existing corpora: the Switchboard Telephone Corpus and the DailyDialog corpus. These datasets were compiled and curated previously and reused here without significant modification, qualifying as collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4",
          "reasoning": "Stimuli for the human judgement task were created by deriving context-response pairs from existing dialogues, including the generation of negative samples by sampling turns with a range of conditional surprisals. This constitutes adaptation and transformation of existing data to create experimental stimuli."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The methods and origins of the datasets used and created are explicitly described in the paper."
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 7946,
      "completion_tokens": 304,
      "total_tokens": 8250
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The paper describes training the TurnGPT language model from scratch on combined data from the DailyDialog and Switchboard corpora, using this new training dataset to obtain surprisal estimates for investigating dialogue acceptability judgments."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.4 and Section 4.1",
          "reasoning": "The stimuli generated from the new sampling of dialogue turns in the DailyDialog and Switchboard corpora were used in a human behavioral judgment task to evaluate acceptability and to analyze the correlation between surprisal and human perception, thereby serving an evaluation role."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and Section 5",
          "reasoning": "The paper uses the collected human acceptability judgments and surprisal estimates from the newly processed dataset to analyze relationships between different surprisal operationalizations and human perception, as well as differences between spoken and written dialogues."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8669,
      "completion_tokens": 413,
      "total_tokens": 9082
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 Data",
          "reasoning": "The datasets used are the Switchboard Telephone Corpus and the DailyDialog corpus, both of which contain English dialogue data only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 3.2 Data",
          "reasoning": "The datasets do not contain exactly two human languages; both are monolingual English corpora."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.2 Data",
          "reasoning": "The Switchboard Telephone Corpus (spoken dialogue) and DailyDialog corpus (written dialogue) are both described as English language datasets representing spoken and written English dialogues respectively."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.2 Data",
          "reasoning": "No indication is given that any non-English language data is included in the datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "There is no mention or indication that the datasets include programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "Although the paper discusses mathematical formulas and metrics, the datasets themselves do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "The datasets contain human language dialogue, with no biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "-",
          "reasoning": "There is no mention or evidence that constructed or fictional languages are included in the dataset entries."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Section 3.2 Data",
          "reasoning": "The language of the datasets is explicitly stated as English and documented; no unspecified languages are present."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets are composed of natural language dialogues and thus do contain language."
        }
      }
    }
  },
  {
    "id": "wallbridge22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5887,
      "completion_tokens": 228,
      "total_tokens": 6115
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit mention",
          "reasoning": "The paper does not provide any explicit reference, link, or mention of code repositories related to the dataset creation, data preprocessing, or stimulus generation used for their novel acceptability judgement task or TurnGPT training. Therefore, it appears that code is not publicly shared or described for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.2 (Data), Section 3.4 (Behavioural study and Participants) and Section 3.3 (Language model surprisal)",
          "reasoning": "The paper documents the dataset sources clearly, describing the use of pre-existing datasets (Switchboard, DailyDialog), and the process of stimulus generation for the human judgement task (e.g., sampling contexts by cumulative surprisal, selection of true and negative turns). The preprocessing steps (removal of punctuation except turn-segmentation, vocabulary choices) and details of the TurnGPT model training are described. These descriptions provide transparency about dataset creation and experimental method, supporting reproducibility, even though no code is shared."
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7653,
      "completion_tokens": 201,
      "total_tokens": 7854
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2 (paragraphs 1-3) and Figure 1",
          "Reasoning": "The novel dataset SONYC-FSD-SED introduced in this paper is composed of 10-second audio soundscapes simulating environmental sound events. Background audio clips are human-generated data recorded by the Sound of New York City (SONYC) acoustic sensor network (real acoustic sensor recordings). Foreground sound events are sampled from the FSD50K dataset, which contains human-labeled sound events. These foreground sounds are mixed with the background clips to create simulated soundscapes using Scaper, a soundscape synthesis library. Thus, the dataset contains audio data that originate both from human-generated recordings and human-labeled examples, but the final dataset is partially model-generated (simulated via algorithmic sound mixing and manipulation) to create the long-term temporal environmental soundscape scenarios."
        }
      ]
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8453,
      "completion_tokens": 255,
      "total_tokens": 8708
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2: Dataset Creation",
            "reasoning": "The SONYC-FSD-SED dataset is synthesized by simulating long-term temporal characteristics of sound events with ground-truth labels. The paper describes a process of sampling and mixing background clips from the SONYC sensor network and foreground clips from the FSD50K dataset using the Scaper tool and probability distributions to generate 10-second soundscapes. The dataset creation involves deterministic sampling procedures and synthetic mixing rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit information provided",
            "reasoning": "The paper does not describe any annotation instructions or guidelines given to human annotators for the SONYC-FSD-SED dataset since the dataset is synthetically generated rather than manually labeled."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit information provided",
            "reasoning": "No scoring rubrics or evaluation guidelines regarding annotations are mentioned for the synthetic dataset creation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit information provided",
            "reasoning": "No examples or annotation examples are provided or needed given the synthetic nature of dataset creation. The paper only describes the generation and sampling methodology."
          }
        }
      ]
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9643,
      "completion_tokens": 415,
      "total_tokens": 10058
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that a single human expert conducted quality assurance on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance performed by a single human non-expert on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although an urban sound event classifier pretrained model is used to filter background clips (Section 2), this usage is part of the dataset creation, not explicitly described as a quality assurance process for validating annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The dataset annotations are constructed by algorithmic simulation of sound event occurrences using von Mises probability density functions, with systematic sampling and mixing of background and foreground clips via the Scaper tool. The process involves formula-based statistical modeling and automated procedural data synthesis. Thus, quality is ensured by the automated verification and procedural rigor of these code-driven methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an algorithmic simulation and systematic curated sampling process for dataset creation, indicating some form of quality assurance via automated procedures."
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9253,
      "completion_tokens": 445,
      "total_tokens": 9698
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by human contributors. Instead, it uses existing recordings from the SONYC acoustic sensor network and the FSD50K dataset."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not generated purely by AI or machine learning models without reference to existing data. The dataset involves soundscape synthesis by mixing existing audio clips, but not fully AI-generated data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no use of machine translation in data generation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2, Dataset description",
          "reasoning": "The dataset uses existing audio recordings from the SONYC sensor network and the FSD50K dataset, which are collected from real-world environments. These data are aggregated and used as background and foreground sounds respectively."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2, Dataset description",
          "reasoning": "The authors simulate sound event temporal characteristics by modeling occurrence probabilities with von Mises distributions and sample sound events accordingly. They generate new soundscapes by mixing background audio clips from SONYC with foreground audio from FSD50K clips using pitch-shifting, time-stretching, and noise mixing (SNR adjustments). This results in a dataset derived from existing sources with transformations and adaptations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is explicitly described and documented in Section 2."
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9771,
      "completion_tokens": 345,
      "total_tokens": 10116
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1, 4.1",
          "reasoning": "The SONYC-FSD-SED dataset is used to train the few-shot prototypical network models via episodic training (Section 3.1) for few-shot sound event detection. Experiments (Section 4.1) demonstrate training and evaluation of models on this dataset, indicating supervised fine-tuning use."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2, 4.3, 4.4",
          "reasoning": "The dataset is used extensively for evaluation and benchmarking of proposed few-shot learning and active learning methods in Sections 4.1 through 4.4, where performance metrics are reported and sampling strategies compared."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2, 4.4, 4.5",
          "reasoning": "The dataset is used to analyze model behavior, support set diversity, temporal sampling windows effects, and generalization trends of few-shot learning combined with active learning (Sections 4.2, 4.4, 4.5)."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10494,
      "completion_tokens": 624,
      "total_tokens": 11118
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset SONYC-FSD-SED is composed of environmental and urban soundscapes created by mixing background recordings from the SONYC sensor network and foreground sound events from the FSD50K dataset. These sound event recordings do not contain spoken language content or any indication that they are English-language speech. Therefore, the dataset entries are not English content but audio sound recordings of various environmental sounds."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 2 (Dataset description)",
          "reasoning": "The dataset consists of environmental acoustic recordings created from background environmental sounds and foreground sound events from FSD50K. There is no indication these are natural human language speech recordings in any language, so not considered as monolingual (non-English) language content."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes mathematical formulas and describes data sampling processes, the dataset itself does not contain programming or structured code-related content as entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": true,
          "reference": "Section 2, paragraph describing the use of von Mises probability density function",
          "reasoning": "The paper describes sampling occurrence probabilities using mathematical formulas such as the von Mises probability density function with explicit equations. These mathematical representations are part of the dataset modeling and generation process."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains environmental audio recordings and sound events, but no biological sequences (e.g., DNA) or non-human communication systems such as animal calls explicitly represented symbolically or in sequence data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and its contents are clearly described; there is no ambiguity about the nature of language content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The SONYC-FSD-SED dataset consists of 10-second environmental and urban soundscape audio clips, created by mixing background sounds and foreground events. There is no textual or human language content contained within the dataset entries."
        }
      }
    }
  },
  {
    "id": "wang22aa_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7712,
      "completion_tokens": 177,
      "total_tokens": 7889
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2, Dataset description",
          "reasoning": "The paper states that the dataset SONYC-FSD-SED will be freely available online, but it does not provide any direct link, nor does it mention publicly available code repositories for data generation or construction. There is detailed description of the dataset creation process, but no explicit mention of code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2, Dataset description",
          "reasoning": "The paper provides a comprehensive and detailed description of how the SONYC-FSD-SED dataset was created, including the selection and filtering of background and foreground sound clips, the simulation of occurrence probabilities using von Mises distributions, the mixing process using the Scaper library, and the final dataset statistics. This documentation is sufficient for understanding how the dataset was constructed."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8786,
      "completion_tokens": 338,
      "total_tokens": 9124
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 (COSPRO-2mix), Section 2.1 (COSPRO)",
          "Reasoning": "COSPRO-2mix is synthesized by mixing segmented utterances from the COSPRO corpus, which is collected and annotated by the Institute of Linguistics, Academia Sinica. Thus, the original audio recordings are human-recorded speech, i.e., human generated audio data, and then mixed according to WSJ0-2mix procedures to form the new dataset COSPRO-2mix."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3 (EAT & EAT-2mix)",
          "Reasoning": "The EAT-2mix test sets are created by mixing English utterances and code-mixing utterances recorded from college students in Taiwan using microphone recordings. This data is collected from human speakers and then mixed to form test sets; thus, the audio data is human generated."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.4 (TAT), Section 2.5 (TAT-2mix)",
          "Reasoning": "TAT-2mix is created by mixing utterances from the Taiwanese Across Taiwan (TAT) corpus which includes recordings on six different channels using microphones and smartphones from 200 speakers. This dataset was specifically tailored and synthesized by the authors for their experiments, thus constituting new dataset creation with human-generated audio modality."
        }
      ]
    }
  },
  {
    "id": "wang22c_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9586,
      "completion_tokens": 219,
      "total_tokens": 9805
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1, 2.2, 2.4, and 2.5",
            "reasoning": "COSPRO and TAT datasets were designed, collected, and annotated by researchers at Academia Sinica as stated in Section 2.1 and 2.4; the collection and careful curation indicate involvement of expert human annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 and 2.5",
            "reasoning": "Segmentation and mixing procedures used forced alignment and followed detailed methods described, suggesting that instructions existed for uniform data processing and annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any scoring rubrics or criteria used during annotation\u2014only data collection and processing procedures."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of annotations or instructions are explicitly provided or described in the paper, thus no evidence of annotation examples."
          }
        }
      ]
    }
  },
  {
    "id": "wang22c_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10776,
      "completion_tokens": 451,
      "total_tokens": 11227
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance for any of the new datasets was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the datasets introduced by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process performed by a single non-expert human annotator for the new datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of any quality assurance conducted by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models as judges for the quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "Section 2.2 and 2.5",
          "reasoning": "While the datasets COSPRO-2mix and TAT-2mix involved processing such as forced alignment and segmenting using Kaldi (Section 2.2) and following mixing procedures, these are standard automated processes for data preparation but are not described as QA or verification steps explicitly validating the quality or correctness of annotations. The paper does not explicitly state an automated quality assurance process."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any explicit quality assurance processes performed on the new datasets introduced (COSPRO-2mix and TAT-2mix), nor is there mention of any form of human or automatic assessment for verifying annotation quality. Therefore, no quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10386,
      "completion_tokens": 471,
      "total_tokens": 10857
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 2.4",
          "reasoning": "COSPRO and TAT are human-collected corpora designed and recorded entirely by human contributors; COSPRO was designed, collected, and annotated by the Institute of Linguistics, Academia Sinica (Section 2.1), and TAT is a speech corpus recorded by 200 speakers across Taiwan with multiple channels (Section 2.4). These corpora consist of original speech recordings not derived or adapted from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated purely by AI or model synthesis."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any datasets are produced through human translation from one language to another."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Datasets like COSPRO and TAT were recorded and collected explicitly for this research purpose and are not described as aggregates of existing datasets without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2, 2.3, 2.5",
          "reasoning": "COSPRO-2mix, EAT-2mix, and TAT-2mix datasets were derived by mixing utterances from the original corpora (COSPRO, EAT, and TAT) following the mixing procedure of WSJ0-2mix, producing new mixed datasets. These are modifications or transformations of existing raw recordings to create speech separation test sets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and generation methods of all discussed datasets are explicitly described."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10904,
      "completion_tokens": 411,
      "total_tokens": 11315
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The newly constructed datasets COSPRO-2mix and TAT-2mix (including several channel-specific variants) are used as training sets to train speech separation models from scratch, e.g., the DPT-Net, Conv-TasNet, and DPRNN. This is explicitly described in Section 4 where models are trained on these datasets and evaluated on various test sets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The authors use the newly created COSPRO-2mix and TAT-2mix datasets for evaluation purposes to measure the performance impact of language and channel variability on speech separation models. Multiple test sets derived from these datasets serve as benchmarks in their experiments, as shown in Tables 2 and 3."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 and 4.3",
          "reasoning": "The paper analyzes trends and characteristics of speech separation performance across different channels using TAT-2mix. They perform pairwise PCA to visualize and understand relationships between channels and their impact on model performance, which is an analytical use of the datasets rather than purely training or evaluation."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents how the newly introduced datasets COSPRO-2mix and TAT-2mix are utilized in training, evaluation, and analysis of speech separation models; therefore, no category applies that indicates lack of practical usage."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11627,
      "completion_tokens": 732,
      "total_tokens": 12359
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2.2 (COSPRO-2mix), Section 2.3 (EAT & EAT-2mix), and Section 2.5 (TAT-2mix)",
          "reasoning": "The paper introduces COSPRO-2mix derived from COSPRO which contains Mandarin Chinese speech, EAT-2mix which includes English speech and English/Mandarin code-mixing utterances, and TAT-2mix which contains Taiwanese speech. The paper specifically mentions using these datasets to investigate language impact, including bilingual cases like English/Mandarin code-mixing. Hence, the proposed datasets contain exactly two human languages in entries where code-mixing occurs (English and Mandarin)."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.3 (EAT-2mix English test set), Section 4.1 (Impact of Language experiments)",
          "reasoning": "The EAT-2mix English test set consists of English-only utterances from microphone recordings as stated in Section 2.3. Additionally, WSJ0-2mix (not newly created but used as baseline) is English-only. The EAT-2mix English test set is used to evaluate the models, indicating monolingual English data is included."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 (COSPRO), Section 2.5 (TAT-2mix)",
          "reasoning": "COSPRO contains Mandarin Chinese speech exclusively, and TAT contains Taiwanese (Southern Min) speech exclusively, making these newly created datasets monolingual non-English datasets. This is explicitly stated in the paper that COSPRO is Mandarin and TAT is Taiwanese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced contain natural spoken language audio data; no programming or structured code content is included within the datasets."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses mathematical notation to describe metrics and models, the datasets themselves contain only speech audio data without mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets focus exclusively on human speech and do not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention in the paper that the datasets contain any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the proposed datasets are specifically identified and documented (Mandarin, Taiwanese, English), so language origin is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "All proposed datasets contain speech in human languages, so it is not applicable that they contain no language."
        }
      }
    }
  },
  {
    "id": "wang22c_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8845,
      "completion_tokens": 205,
      "total_tokens": 9050
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "End of Section 1 (Introduction) and Section 2 (Datasets for Speech Separation)",
          "reasoning": "The paper states in the Introduction that the details and source codes are open-sourced, indicating that code related to data synthesis is publicly available. This is confirmed in Section 2 where datasets such as COSPRO-2mix and TAT-2mix are described as synthesized, implying the use of shared scripts for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Datasets for Speech Separation), subsections 2.2 COSPRO-2mix and 2.5 TAT-2mix",
          "reasoning": "The paper provides detailed explanations on how new datasets were constructed, including selection of speakers, segmentation procedures using forced alignment, mixing procedures aligned with WSJ0-2mix, and description of channels for the TAT-2mix dataset. These constitute thorough documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7491,
      "completion_tokens": 117,
      "total_tokens": 7608
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Depression corpus",
          "Reasoning": "The paper introduces a new Chinese clinical depression corpus collected from 131 participants during Hamilton Rating Scale for Depression (HAMD) interviews. The data consists of recorded speech audio from face-to-face clinical interviews, recorded by a professional audiologist using a dual channel recording device, saved as WAV files at 16kHz. This is explicitly described in Section 2 as newly collected data for this study."
        }
      ]
    }
  },
  {
    "id": "wang22q_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8291,
      "completion_tokens": 253,
      "total_tokens": 8544
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2. Depression corpus",
            "reasoning": "The paper states that physicians organized face-to-face interviews with participants in a clinical setting to collect recordings during the HAMD interviews. Since physicians conducted the interviews and completed questionnaires for depression rating, annotation (depression status labeling) is performed by clinical experts."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2. Depression corpus, Table 1",
            "reasoning": "The annotation (clinical labeling) was guided by the Hamilton Rating Scale for Depression (HAMD), with clinicians asking set questions listed in Table 1 and completing questionnaires according to symptoms. These structured question guidelines constitute instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2. Depression corpus",
            "reasoning": "The paper defines depression status based on HAMD scores with explicit thresholds (HAMD score higher than eight indicates depression), indicating the use of scoring rubrics to define depressive symptom presence and severity."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not provide or mention any examples of annotated data or annotation examples for the HAMD dataset transcription or labeling."
          }
        }
      ]
    }
  },
  {
    "id": "wang22q_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9481,
      "completion_tokens": 310,
      "total_tokens": 9791
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2, Depression corpus",
          "reasoning": "In Section 2, the paper states that physicians organized face-to-face interviews with participants in a quiet room, during which the speech was recorded. Physicians conducted these interviews, asking questions based on HAMD and patients' own situations. Since physicians are subject matter experts and were directly involved in conducting the interviews, their involvement constitutes quality assurance as single human experts ensuring that the recorded speech aligns with clinical diagnostic procedures."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human experts performing quality assurance or annotations on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information suggesting multiple non-experts performed quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "AI models were used for speech transcription and classification tasks, but not for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based quality assurance processes for dataset annotation or content quality are described."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process was described involving physicians conducting interviews to collect data, thus QA is present."
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9091,
      "completion_tokens": 411,
      "total_tokens": 9502
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2: Depression corpus",
          "reasoning": "The paper clearly states that the authors collected a new Chinese clinical depression corpus from real clinical practice at Beijing Huilongguan Hospital. The dataset consists of 131 participants' speech recorded during face-to-face Hamilton Rating Scale for Depression (HAMD) interviews conducted by physicians. This data was generated entirely by human contributors (patients and clinicians) in clinical settings and was not translated, adapted, or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset includes any data generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected natively in Mandarin Chinese; there is no mention of data being produced by human translators from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of machine translation being used to produce any part of the dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being collected or aggregated from existing sources without significant modification; rather, it was newly collected by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset is based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is explicitly described, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9609,
      "completion_tokens": 482,
      "total_tokens": 10091
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training a model from randomly initialized parameters on the new dataset; instead, it uses the dataset for fine-tuning a TDNN-based model."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Sections 3.5 and 4.2",
          "reasoning": "The proposed new Chinese clinical depression corpus is used to train a TDNN-based depression detection model. The paper describes extracting features (MFCC) from the corpus and training the neural network in a supervised manner, utilizing cross-entropy loss. This indicates the dataset is used for supervised fine-tuning or supervised training of the model."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of reinforcement learning or RL-based post-training techniques using this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3",
          "reasoning": "The dataset is also used for evaluating the performance of the proposed depression detection system by conducting five-fold cross-validation and reporting accuracy, precision, recall, and F1 scores both at utterance and speaker level."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.4",
          "reasoning": "The dataset is used for analyzing the verbal expressions differences between depressed and healthy subjects through transcription and statistical comparison of word frequency and part-of-speech categories."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly demonstrates multiple practical uses of the dataset, including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10332,
      "completion_tokens": 350,
      "total_tokens": 10682
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 Depression corpus",
          "reasoning": "The dataset is a newly collected Chinese clinical depression corpus with speech recordings from participants who are fluent Mandarin speakers. The interviews were conducted in Mandarin, as stated in Section 2. Therefore, the dataset entries contain exactly one non-English language, Mandarin Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wang22q_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7550,
      "completion_tokens": 172,
      "total_tokens": 7722
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or statement in the paper provides a link or mentions availability of code for dataset construction or processing.",
          "reasoning": "The paper describes the dataset collection process and preprocessing steps in the sections '2. Depression corpus' and '3. Methods', but does not provide any URL, link, or mention of publicly available code repositories related to data collection or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Depression corpus) and Section 3 (Methods)",
          "reasoning": "The paper documents the dataset creation process including participant recruitment, recording conditions, data preprocessing steps such as voice activity detection, segmentation into 3-second clips, and feature extraction with MFCC. Detailed demographic information and interview procedures are described, providing sufficient transparency about dataset construction."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7609,
      "completion_tokens": 219,
      "total_tokens": 7828
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.3.1 Recording Preparation",
          "Reasoning": "The audio data was recorded directly from human speakers using the microphone array of the Azure Kinect device, capturing natural speech uttered by 64 native Mandarin speakers."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.3.1 Recording Preparation, Figure 4",
          "Reasoning": "The dataset includes color images (RGB) captured by the Azure Kinect during recording sessions of human speakers reading sentences, thus the images directly originate from human recording."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract, Section 3.3.1 Recording Preparation, Figure 4(b)",
          "Reasoning": "Depth images were captured by the Azure Kinect device while speakers read the text; these depth images represent sensor data produced by human performance during recording."
        }
      ]
    }
  },
  {
    "id": "wang23o_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8409,
      "completion_tokens": 247,
      "total_tokens": 8656
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.4 Data Annotation",
            "reasoning": "The dataset annotation uses automated tools: the open-source grapheme to phoneme (G2P) model to generate phoneme sequences and Montreal Forced Aligner (MFA) for forced alignment to generate phoneme durations. There is no mention of human annotators performing the phoneme-level annotations, indicating the process is automatic."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.4 Data Annotation",
            "reasoning": "The paper describes the annotation methods but does not provide any annotation instructions or guidelines for annotators, consistent with the annotation being performed by automatic tools rather than humans."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.4 Data Annotation",
            "reasoning": "No scoring rubrics or evaluation criteria for annotation quality are described in the listed annotation procedure; the process is fully automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.4 Data Annotation",
            "reasoning": "The paper does not present any annotation examples or samples in the annotation section or appendices; annotations are derived automatically from audio and text."
          }
        }
      ]
    }
  },
  {
    "id": "wang23o_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9599,
      "completion_tokens": 392,
      "total_tokens": 9991
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process being conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset annotations or content."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any single non-expert individual conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1.2 Manual Selection",
          "reasoning": "The dataset texts are manually filtered to remove immoral, violent, or sensitive sentences. This manual filtering implies that multiple human non-expert annotators - likely the authors or research assistants - performed quality assurance on the text data by screening inappropriate sentences. There is no indication that these annotators were experts in linguistics or annotation, so they are best described as non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the use of any AI model to perform quality assurance or validation of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Text Acquisition Pipeline and Section 3.4 Data Annotation",
          "reasoning": "The dataset employs an automatic pipeline for initial text acquisition and cleaning, including sentence segmentation and scoring based on di-phone and tri-phone coverage metrics. Additionally, annotations for phoneme sequences and duration alignment were generated using automatic tools such as the open-source G2P model and Montreal Forced Aligner (MFA). These automatic processes serve as automated verification steps during data preparation and annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes both manual and automatic processes for quality assurance; thus, QA is documented and performed."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9209,
      "completion_tokens": 488,
      "total_tokens": 9697
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Text Acquisition Pipeline",
          "reasoning": "The dataset's raw text was collected from authentic Chinese sources such as social media, news, novels, and poetry, then manually filtered and cleaned by human contributors to select suitable reading materials. The audio-visual data was recorded by native Mandarin speakers reading these texts, thus the content and recordings are original and created by humans without adapting or deriving from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any portion of the dataset was generated entirely by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the data was produced by translating from another language into Mandarin by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or statement suggests any machine translation was used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Text Acquisition Pipeline",
          "reasoning": "The raw text material was collated from various existing Chinese media sources such as Weibo, People's Daily, novels, and poetry. While cleaned and filtered, this collection process aggregates existing texts without fundamental modification to the source content."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1 Text Acquisition Pipeline and Section 3.4 Data Annotation",
          "reasoning": "Although the raw texts are collated from existing sources, the dataset applies filtering, sentence segmentation, and scoring algorithms to select and balance the material. Furthermore, detailed phoneme-level annotations, including tone sequences and phonetic segmentation, are derived using language dictionaries and forced alignment tools, representing derived and adapted data based on the original texts."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method are sufficiently described in the paper."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9727,
      "completion_tokens": 464,
      "total_tokens": 10191
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using the MA VD dataset for pre-training large models in an unsupervised or self-supervised manner. Instead, it is used in supervised settings."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training any models from scratch using the MA VD dataset; the baseline experiments utilize pre-trained models for feature extraction."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Baseline Experiment)",
          "reasoning": "In Section 4, the authors fine-tune a pre-trained Mandarin-Wav2Vec2 model and ResNet-18 model using supervised learning on the MA VD dataset to evaluate audio-visual speech recognition performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning based post-training techniques such as RLHF are mentioned or used with the MA VD dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 (Result and Analysis)",
          "reasoning": "The MA VD dataset is used for evaluation in baseline experiments to measure character error rate (CER) to benchmark the effectiveness of different modalities on AVSR tasks."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not primarily use the dataset for analyzing trends or patterns beyond evaluation metrics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base for augmenting models via retrieval or other mechanisms."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is practically used in supervised fine-tuning and evaluation as demonstrated in Section 4."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10450,
      "completion_tokens": 448,
      "total_tokens": 10898
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only Mandarin language materials and there is no indication of more than two human languages present."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is explicitly described as Mandarin only, without presence of a second human language in its entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains Mandarin data, not English; thus it is not monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 3 Our Mandarin Audio-Visual Dataset with Depth Information; Table 1",
          "reasoning": "The dataset is clearly presented as a large-scale Mandarin audio-visual dataset containing utterances all recorded in Mandarin by native Chinese speakers (Section 1). Table 1 also shows the dataset is 'Mandarin' language only. The raw text sources and annotations confirm only the use of Mandarin language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that code or programming language content is part of the dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is linguistic audio-visual data; mathematical or logical notation content is not mentioned as part of the dataset itself."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Dataset involves human speech data; there is no biological sequence or non-human communication data included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not contain fictional or constructed languages; it uses natural Mandarin language only."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language used in the dataset is clearly identified as Mandarin and well documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language (Mandarin) audio and text, so language data is present."
        }
      }
    }
  },
  {
    "id": "wang23o_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7668,
      "completion_tokens": 212,
      "total_tokens": 7880
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and last paragraph of Section 1 Introduction",
          "reasoning": "The paper states explicitly in the abstract that \"The dataset and code will be released at https://github.com/SpringHuo/MA VD.\" and reiterates the intention to make the corpus open-source and free for research use in Section 1. This indicates that the code for dataset related tasks will be publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 (Our Mandarin Audio-Visual Dataset with Depth Information) and subsections 3.1 to 3.4",
          "reasoning": "The paper provides detailed documentation about the dataset creation process, including text acquisition pipeline (3.1), manual selection of text (3.1.2), speaker description (3.2), recording setup and environment (3.3.1 and 3.3.2), and data annotation process (3.4). This extensive information demonstrates transparency and completeness in documenting the dataset creation."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7739,
      "completion_tokens": 347,
      "total_tokens": 8086
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1; Section 3; Table 1",
          "Reasoning": "The GLOBE dataset consists of 535 hours of high-quality English speech audio recorded from 23,519 speakers worldwide with 164 accents. The data originates from human voice recordings collected in the Common Voice dataset and then filtered, enhanced, and curated by the authors to remove low-quality samples and improve transcription accuracy, thus the audio modality is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1; Section 3.2; Table 1",
          "Reasoning": "Each audio utterance in the GLOBE corpus is associated with corresponding text transcriptions. The text data originates from the original Common Voice transcriptions, which are then validated and improved by the authors using Whisper ASR model and alignment filtering to ensure transcription accuracy. This text metadata is human-generated as it corresponds to human speech content."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 1; Section 3.3; Table 1",
          "Reasoning": "The GLOBE dataset includes detailed speaker metadata in tabular form such as speaker accent, age, and gender. Some of this metadata is completed or predicted by the authors using trained speaker information prediction models, thus a portion of this tabular data is human generated (original speaker metadata) and some are model generated (filled in by prediction models). Therefore, both human- and model-generated labels are present in this modality."
        }
      ]
    }
  },
  {
    "id": "wang24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8539,
      "completion_tokens": 245,
      "total_tokens": 8784
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1, 3.2, 3.3",
            "reasoning": "The GLOBE dataset is created by filtering and enhancing the original Common Voice dataset using signal-to-noise ratio analysis, bandwidth measurement, voice activity detection, transcription verification with Whisper and other automatic systems, and speaker metadata prediction models as described in Sections 3.1 to 3.3. These processes are automatic, including filtering, alignment, metadata prediction, and post-processing."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any detailed annotation instructions given to human annotators; rather, data processing relies on automatic filtering and modeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not report any scoring rubrics used in annotations; the processes are automatic and do not involve rubric-based scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "There are no examples of annotation provided, as annotation is performed via automatic methods and models rather than human annotators following a guideline."
          }
        }
      ]
    }
  },
  {
    "id": "wang24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9729,
      "completion_tokens": 484,
      "total_tokens": 10213
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was performed by a single human expert annotator for the GLOBE dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any indication that multiple human experts performed quality assurance or annotation validation for the GLOBE dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.3 (Speaker Information Refinement and Speech Post-processing)",
          "reasoning": "In Section 3.3, the paper discusses manual cleaning of accent labels and removal of invalid entries such as meaningless accent labels and merging rare accents. This suggests that humans (likely non-expert annotators) performed cleaning and refinement of metadata labels. Given the scale and nature of the task and the lack of claim of expert annotators, this implies multiple human non-experts were involved in the quality assurance process for metadata."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.3 (Speaker Information Refinement and Speech Post-processing)",
          "reasoning": "The paper describes training three speaker information prediction models (for accent, age, and gender) with high accuracies to automatically populate missing speaker metadata labels. This use of trained AI models to predict and fill in missing annotations reflects an AI-model-based quality assurance process."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2 (Speech Sample Pre-processing and Filtering; Speech Text Alignment)",
          "reasoning": "The data filtering steps for speech quality (SNR thresholds, bandwidth thresholds, long internal silence detection using voice activity detection) are automated processes applying algorithmic and rule-based criteria for sample selection and rejection. Additionally, in Section 3.2, automated transcription with the Whisper model followed by edit distance calculations to filter utterances with transcription inaccuracies represents automated verification techniques used in quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple stages of quality assurance including manual cleaning, automated filtering, and AI-model-based annotation prediction. Thus, there is evidence of a quality assurance process."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9339,
      "completion_tokens": 443,
      "total_tokens": 9782
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the data as being created entirely from scratch by human contributors. The GLOBE corpus is based on an existing dataset (Common Voice) which was collected from volunteer contributors, but was then processed and refined by the authors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any of the data was generated entirely by AI or machine learning models. The authors used models for metadata prediction but did not generate new speech data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no reference to any machine translation used for generating dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 1 (Abstract), Section 2, Section 3",
          "reasoning": "The GLOBE dataset is created by collecting and aggregating existing speech data from the Common Voice dataset, which contains recordings from volunteer contributors worldwide."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3 (Data Processing Pipeline)",
          "reasoning": "The authors applied extensive filtering, quality improvement, metadata correction, and post-processing (e.g., filtering low-quality samples, re-aligning text and utterance, predicting missing metadata, speech enhancement) to the Common Voice dataset, thus creating a dataset derived from an existing source with modifications and enhancements."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly specifies the origin and processing of the dataset, so the data source is documented."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9857,
      "completion_tokens": 508,
      "total_tokens": 10365
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), especially 4.1.2 Experimental results and descriptions",
          "reasoning": "The GLOBE dataset is used to train speaker adaptive TTS models from scratch as demonstrated by training the modified YourTTS model on the GLOBE dataset. The experiments show that models trained on GLOBE achieve better speaker similarity and comparable naturalness to those trained on other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe or demonstrate fine-tuning a pre-trained model using the GLOBE dataset. Training described is from scratch rather than fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or demonstration of reinforcement learning based post-training methods like RLHF using the GLOBE dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Experiments for Ground-Truth Speech Samples), Table 2",
          "reasoning": "The GLOBE dataset is used in evaluation, benchmarking model performance in speaker adaptive TTS systems, to assess speech naturalness, word error rates, speaker similarity, and diversity."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 (Data Processing Pipeline) and 4 (Experiments), including analysis of speaker metadata and diversity",
          "reasoning": "The authors analyze speaker accent distribution, speaker metadata quality, and perform speaker embedding distribution analysis to assess accent diversity and dataset quality."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not indicated to serve as a knowledge base for retrieval-augmented generation or similar augmentations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset's practical usages are clearly described for training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10580,
      "completion_tokens": 536,
      "total_tokens": 11116
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The GLOBE dataset contains only English speech data from speakers worldwide with various English accents, but does not include multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper clearly states that GLOBE is an English corpus and does not contain entries in exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 2 Relevant English Multi-speaker Corpus; Table 1",
          "reasoning": "The paper explicitly defines GLOBE as a high-quality English corpus containing speech data with various English accents from around the world. All data entries are in English only, as per the descriptions and statistics provided (Table 1)."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains English content only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset is composed of speech utterances and transcriptions in English without mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains human speech data only, no biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention of fictional or artificially created languages; dataset only covers real-world English accents."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language content (English) and accents are clearly specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken English utterances and their transcriptions, thus contains language."
        }
      }
    }
  },
  {
    "id": "wang24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7798,
      "completion_tokens": 211,
      "total_tokens": 8009
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section describes code availability for dataset construction; no code repository link provided in the paper.",
          "reasoning": "The paper mentions the dataset will be publicly released and provides a website link for the GLOBE dataset, but there is no explicit mention or link to code repositories for the data processing pipeline or dataset construction code. The description of the data processing pipeline is detailed but does not indicate that the code is publicly shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 'Data Processing Pipeline' and subsections 3.1, 3.2, 3.3 describe dataset creation process.",
          "reasoning": "The paper provides a thorough description of the dataset construction process, including detailed procedures for speech sample pre-processing and filtering, speech text alignment, and speaker metadata refinement and post-processing. These are documented in Section 3 and its subsections, outlining the filtering criteria, transcription techniques, and metadata prediction models used to refine and curate the dataset."
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8104,
      "completion_tokens": 119,
      "total_tokens": 8223
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3. Data and Task",
          "Reasoning": "The authors collected a new dataset consisting of thousands of human judgments of the pragmatic similarity between pairs of utterances, where the utterance pairs originate from unstructured conversations among students, task-oriented dialogs, and toddler utterances. These are recorded audio clips, thus the modality is audio. The data clearly comes from human speech recordings, and the human judges provided similarity ratings, indicating human involvement in data creation."
        }
      ]
    }
  },
  {
    "id": "ward24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8904,
      "completion_tokens": 239,
      "total_tokens": 9143
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3, Data and Task",
            "reasoning": "The paper states that a total of 689 utterance pairs were each judged by 6 to 9 judges, who were hand-picked for demonstrated sensitivity and adeptness with language nuances. This indicates multiple human experts were involved in annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3, Data and Task",
            "reasoning": "Judges were asked specifically to indicate 'How pragmatically similar are the two clips, in terms of the overall feeling, tone, and intent?' which implies they were provided clear instructions about the annotation task."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3, Data and Task",
            "reasoning": "The use of a 1 to 5 scale for judging pragmatic similarity implies an annotation rubric defining the scoring scale was used."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3, Data and Task",
            "reasoning": "The paper does not mention providing any annotation examples to the judges, nor does it describe example annotations or calibration samples used during annotation."
          }
        }
      ]
    }
  },
  {
    "id": "ward24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10094,
      "completion_tokens": 319,
      "total_tokens": 10413
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3, Data and Task",
          "reasoning": "The dataset was created from human judgments by multiple judges (6 to 9 per utterance pair) assessed over three sessions. The judges were hand-picked for sensitivity and adeptness with language nuances, indicating expert knowledge, and the process involved multiple such experts providing judgments to ensure high-quality annotation with good inter-annotator agreement."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper specifies the judges were hand-picked for sensitivity and adeptness, implying expertise rather than non-expert annotators."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators were selected for expertise and sensitivity to linguistic nuances, thus not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance of the dataset was performed via AI models as judges; AI models were used later as predictive models, not as annotators."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification process of annotations or dataset quality is described in the paper."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a careful human annotation process with multiple expert judges and inter-annotator agreement, thus QA process is documented and applied."
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9704,
      "completion_tokens": 410,
      "total_tokens": 10114
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3: Data and Task",
          "reasoning": "The paper describes a new collection of data consisting of human judgments of pragmatic similarity between pairs of utterances. These judgments were obtained in controlled sessions, with human judges rating hundreds of utterance pairs created from original spoken conversations and dialogs. The utterances themselves are from natural conversations, not pre-existing datasets, and the similarity judgments were made directly by human annotators without reference to prior data, indicating original, human-created data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No new data generated entirely by AI or machine learning models is presented or introduced in the paper."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No human translation of data from other languages is indicated for this new dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation of data from other languages is indicated for this new dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The utterance pairs and corresponding human judgments are not merely collected or aggregated from existing sources; rather, they were specifically created and collected for this study."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The new dataset is not described as derived from existing sources with modifications; it appears to be original, not transformations or adaptations of pre-existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly documented and specified."
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10222,
      "completion_tokens": 304,
      "total_tokens": 10526
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4",
          "reasoning": "The new dataset of human judgments of pragmatic similarity between utterance pairs is used to train models predicting these judgments from audio features. Specifically, feature selection and model training to predict similarity scores involve supervised learning using this collected dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 5.1, and Table 1",
          "reasoning": "The dataset is used for evaluating and benchmarking various models (e.g., HuBert-based models) by correlating predicted similarity scores with human judgments, serving as ground truth for performance measurement."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2, Section 5, and Section 6",
          "reasoning": "The dataset facilitates qualitative and quantitative analyses of model errors, strengths, weaknesses, and generality across languages and dialects, supporting deeper understanding of pragmatic similarity."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10945,
      "completion_tokens": 670,
      "total_tokens": 11615
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 3 Data and Task",
          "reasoning": "The paper introduces a new dataset of human judgments of pragmatic similarity between utterance pairs in English and Spanish only, specifically American English and Mexican/American-border Spanish. No mention or inclusion of more than two languages is present, thus it is not multilingual in the sense of more than two languages."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3 Data and Task",
          "reasoning": "The new dataset consists of utterance pairs judged for pragmatic similarity in two human languages: American English and Mexican/American-border Spanish. This is explicitly stated with data collected in three sessions: two English sessions and one Spanish session, representing exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3 Data and Task",
          "reasoning": "While partial data is for English only, the dataset as a whole includes Spanish data as well. The paper treats data from both languages as part of the new dataset, so it is not exclusively monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3 Data and Task",
          "reasoning": "The dataset includes Spanish utterances, which are non-English, but the dataset also contains English utterances. Therefore it is not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset comprises human spoken utterances and human similarity judgments. There is no mention or indication of any entries containing programming or structured code-related contents."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset contains audio utterance pairs and human judgments of pragmatic similarity, with no indication of entries including mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The dataset is of human spoken language utterances only, with no mention of biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "No mention or use of fictional or artificially created languages is made in the dataset. All languages are natural human languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Entire paper",
          "reasoning": "The languages of the dataset entries are explicitly stated as English and Spanish. There is no ambiguity or lack of documentation about the languages."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains utterances with human language (English and Spanish), so language is present."
        }
      }
    }
  },
  {
    "id": "ward24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8163,
      "completion_tokens": 238,
      "total_tokens": 8401
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3 'Data and Task' and Section 7 'Summary, Implications, and Future Work'",
          "reasoning": "The paper mentions the dataset of thousands of human judgments collected by the authors, describing the data collection conditions and details in Section 3. However, the paper does not provide any indication or links to code repositories for the dataset construction or annotation processes. The available code link at https://github.com/andysegura89/Pragmatic_Similarity_ISG pertains to model code, not the dataset creation."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 'Data and Task'",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 3, including the number of utterance pairs, the number of human judges, the nature of the utterances (English and Spanish, from student conversations and toddler speech), the judgment scale used, and properties such as task-agnostic judgments, judge training and quality, and the mix of lexically identical versus different pairs. This comprehensive description serves as documentation of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6974,
      "completion_tokens": 219,
      "total_tokens": 7193
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 5, and 7",
          "Reasoning": "The dataset includes recorded speech from three human narrators (Perec Zylberberg, Sara Blacher-Retter, and Leib Rubinov), whose audiobooks and narrated short stories were originally recorded by humans and later digitized. The authors collected and segmented this found audio data to form the speech component of the dataset."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1, 3.3, 4.4, and 5",
          "Reasoning": "The text transcripts in the dataset are derived from OCR scans of books and corresponding manual transcriptions. These texts were hand-corrected by a human expert for transcription accuracy and to match exactly what the narrators produced, including disfluencies. The text was therefore human generated or corrected to serve as accurate speech-text pairs."
        }
      ]
    }
  },
  {
    "id": "webber22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7774,
      "completion_tokens": 261,
      "total_tokens": 8035
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 4.4 and Section 5",
            "reasoning": "The dataset's text was hand-corrected by the third author, a specialist in Yiddish, indicating annotation was performed by a single human expert. Additionally, the corrections coordinated by Refoyl Finkel (a separate human expert) for another speaker's data are included. Hence, a single expert or expert-led correction process was involved in the data annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not explicitly describe any detailed annotation instructions provided to the human expert annotator(s) performing the hand corrections; it only states that corrections were made by an expert."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "There is no indication or description of any scoring rubrics or criteria used during the text correction or annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention in the paper",
            "reasoning": "The paper does not provide or mention annotation examples provided to annotators; the corrections appear to be done by a single expert following expert knowledge rather than guided by examples."
          }
        }
      ]
    }
  },
  {
    "id": "webber22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8964,
      "completion_tokens": 250,
      "total_tokens": 9214
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 4.4 Improvements to the dataset",
          "reasoning": "The paper states that the published final dataset includes only texts that have been manually corrected by the third author, who is a specialist in Yiddish. This indicates quality assurance performed by a single human expert annotator who ensured transcription accuracy and consistency with YIVO orthographic norms."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Sections 3.2 Automated text preprocessing and 3.4 Utterance-level segmentation",
          "reasoning": "Automated preprocessing steps such as Unicode normalization, removal of pointing, respelling loshn-koydesh words, and utterance-level segmentation with Aeneas were applied, indicating quality assurance through automatic, algorithmic processes."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8574,
      "completion_tokens": 448,
      "total_tokens": 9022
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not stated to contain any content created entirely from scratch by human contributors. The content originates from pre-existing audiobooks and texts, not newly created original material."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset content was generated entirely by AI or machine learning models. The models were trained on the dataset but did not generate the source data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset including content produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to generate any dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset was created by collecting found data: audiobooks and OCR digitized books from existing sources such as the Yiddish Book Center and University of Haifa resources. These materials were aggregated without being originally created for this purpose."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 3.2, 3.3, 3.4, 4.4, and 5",
          "reasoning": "The collected data was hand-corrected and processed (e.g., text hand-corrections, orthographic normalization, manually adjusted segmentation, and respelling) to produce a cleaned and consistent dataset suitable for TTS training. These modifications constitute derivations from the original collated materials."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the data sources and the preprocessing steps, so the data origin is specified."
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9092,
      "completion_tokens": 273,
      "total_tokens": 9365
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4",
          "reasoning": "The dataset was used to train FastSpeech2 TTS models from scratch, as described in Section 4 where the authors build single-speaker TTS models using the dataset and report evaluation results on these models."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset was used to evaluate the resulting TTS system, the dataset itself is not exclusively used for evaluation or benchmarking, but primarily for training the system."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.3 and 4.4",
          "reasoning": "The dataset and results from TTS models trained on it were analyzed to understand the effect of transcription quality and orthography on speech naturalness and intelligibility, guiding iterative improvements to the dataset itself."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9815,
      "completion_tokens": 603,
      "total_tokens": 10418
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are all in Yiddish, which is a single language. Although Yiddish contains loanwords from other languages, the dataset is focused on Yiddish text and speech only."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages. It only contains Yiddish language entries."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content; the dataset is exclusively Yiddish speech and text."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract and Section 3.1; Section 5",
          "reasoning": "The dataset is a Yiddish text-to-speech dataset containing speech and aligned text in Yiddish exclusively. Although Yiddish includes words of Hebrew, Aramaic, Germanic and Slavic origin, the language itself is singular and non-English. The paper explicitly states it is the first Yiddish TTS dataset and system (Abstract) and details that all recordings and texts are in Yiddish (Section 3.1). The final dataset contains utterances in Yiddish from three speakers (Section 5)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "Section 3.2 (mention of Python library)",
          "reasoning": "While the paper mentions a Python library for text preprocessing, the dataset entries themselves are not code or programming language content but are Yiddish text and speech."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that the dataset contains mathematical or logical notation entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human language data only, no biological or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Yiddish is not a constructed language; the dataset does not include any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified and documented as Yiddish."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken and written language data (Yiddish), so it does contain language."
        }
      }
    }
  },
  {
    "id": "webber22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7033,
      "completion_tokens": 186,
      "total_tokens": 7219
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 5",
          "reasoning": "The paper explicitly states in Section 5 that along with the dataset, the authors have published corrected transcripts, scripts for downloading and segmenting audio, scripts for normalizing and respelling text, and model files accessible through the GitHub page https://github.com/REYD-TTS. This indicates that all code related to data collection, preprocessing, and generation is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 and 5",
          "reasoning": "The paper provides a detailed description of the dataset creation process in multiple sections: Section 3 describes the source material, automated and manual text preprocessing, utterance-level segmentation, and forced alignment; Section 5 describes the final dataset contents and the accompanying resources released for reproducibility. This offers transparent and comprehensive documentation of the dataset construction process."
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6299,
      "completion_tokens": 80,
      "total_tokens": 6379
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3, Data",
          "Reasoning": "The paper states that they recruited 63 UK-based speakers to read scripted utterances over the telephone line, resulting in a dataset of 12.7 hours of telephone audio, which is thus human recorded speech data."
        }
      ]
    }
  },
  {
    "id": "wei24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7099,
      "completion_tokens": 205,
      "total_tokens": 7304
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3 Data",
            "reasoning": "The dataset was created by recruiting 63 UK speakers to read scripted utterances; each utterance was transcribed by human annotators considering deviations from the script, indicating multiple human annotators performed transcription and annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3 Data",
            "reasoning": "The paper does not explicitly mention any detailed annotation instructions given to the annotators for transcribing or annotating named entities in the dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3 Data",
            "reasoning": "There is no mention of scoring rubrics or formal evaluation criteria provided to annotators for the transcription or named entity identification."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3 Data",
            "reasoning": "The paper does not describe the provision of example annotations or sample transcriptions as part of guidelines for annotators."
          }
        }
      ]
    }
  },
  {
    "id": "wei24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8289,
      "completion_tokens": 463,
      "total_tokens": 8752
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single human expert annotator. No information is provided about expert annotators involved in transcription validation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts or experts from the target demographic conducted quality assurance on the dataset. The transcriptions are described but annotator expertise is not specified."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single human non-expert conducted quality assurance on the data. No details on singular annotator QA are provided."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3 (Data)",
          "reasoning": "The dataset was transcribed by multiple human annotators; specifically, 63 speakers read scripted utterances and each utterance has been transcribed taking into account deviations from the script. While there is no explicit statement that transcribers were experts, the plural form and description implies multiple human non-expert annotators performed the transcription and validation. The paper does not indicate these annotators had subject matter expertise or belonged to the target demographic."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of AI models as judges or quality assurance tools for the dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of automated verification processes such as code or formula verification being used for data quality assurance."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is described in terms of human transcription of utterances and checking for deviations from scripts, so the dataset is not without QA."
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7899,
      "completion_tokens": 409,
      "total_tokens": 8308
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3, Data",
          "reasoning": "The authors explicitly state that they recruited 63 human speakers to read scripted utterances created for this study, resulting in 7,031 unique utterances. These utterances were crafted to represent commercial banking enquiries involving account names, reflecting original content specifically created for this research. There is no indication that this data was translated, adapted, or derived from pre-existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any data generated by translating content from another language using machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not collected or aggregated from existing sources without significant modification; it was expressly designed and created by the authors."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data is based on existing sources with modifications or adaptations; rather, it is newly created original scripted utterances and recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation method is fully documented in Section 3."
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8417,
      "completion_tokens": 327,
      "total_tokens": 8744
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4, 5",
          "reasoning": "The dataset is used to fine-tune the Whisper model with in-domain data for better recognition of named entities, especially in scenarios involving novel or previously unheard names. The paper describes fine-tuning with a learning rate of 1e-5 and batch size 8 on this dataset and evaluates the impact on recognition performance."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3, 5",
          "reasoning": "The dataset is used for evaluation and benchmarking of various models and tuning strategies across different scenarios (seen-and-heard, seen-but-unheard, unseen-and-unheard). This includes cross-evaluation data splits and unscripted data to assess real-world performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.2, 6",
          "reasoning": "The paper uses the dataset to analyze model performance trends such as recall and WER under different prompt conditions, prompt list lengths and positions, and strategies for prompt tuning. This analysis provides insight into model behavior and attention mechanisms."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9140,
      "completion_tokens": 555,
      "total_tokens": 9695
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's proposed dataset consists of utterances recorded by 63 UK-based speakers in a commercial banking context with scripted and unscripted data. There is no mention of multiple languages; only English utterances are discussed."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only English language utterances. There is no indication that two languages are included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3 Data",
          "reasoning": "The dataset consists of 7,031 unique utterances spoken by UK-based speakers, all in English, focused on commercial banking queries. The examples cited, such as 'search the overdue invoices for Alpha Limited,' are English language utterances. There is no indication or mention of any other human languages in the data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is English, not non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of spoken utterances and transcriptions related to commercial banking tasks. There is no mention or presence of code, programming language, or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the dataset or examples of mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human spoken English utterances; there is no mention of biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that constructed or fictional languages are used in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language is clearly stated and documented as English; thus, the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are all spoken utterances with transcriptions and thus contain language."
        }
      }
    }
  },
  {
    "id": "wei24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6358,
      "completion_tokens": 148,
      "total_tokens": 6506
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No explicit section or mention of code availability",
          "reasoning": "The paper does not mention any link or repository to access code for dataset collection, preprocessing, or generation. No URLs or references to code release are given anywhere in the text, suggesting the code is not publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 Data",
          "reasoning": "The paper provides a detailed description of the new dataset, including the number of utterances, speaker demographics, acoustic conditions, stratification of data into clusters, data collection procedure (recorded over telephone lines), and transcription approach. This documentation supports understanding and potential reproduction of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9765,
      "completion_tokens": 107,
      "total_tokens": 9872
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1.1 Smartphone app Plapper; Section 2.1.3 Speech material",
          "Reasoning": "The new dataset is the Plapper corpus introduced by the authors, consisting of self-recorded read speech audio recordings collected via the smartphone app Plapper from 1,522 German speakers. This data was recorded by human participants speaking, thus originates from human generated audio data."
        }
      ]
    }
  },
  {
    "id": "weirich24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10565,
      "completion_tokens": 204,
      "total_tokens": 10769
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1.4 and 2.1.5",
            "reasoning": "The annotation (labeling) of audio recordings was done semi-automatically using WebMa with hand-corrections. The paper indicates hand-correction was performed by experts familiar with the task as it involves phonetic labeling requiring expertise."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "The paper does not explicitly state that detailed annotation instructions were provided to annotators during hand-labeling correction, nor that instructions were distributed as guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No scoring rubrics or systematic annotation guidelines for labeling quality or decisions are described in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit mention",
            "reasoning": "No example annotations or labeling examples are described or referenced in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "weirich24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11755,
      "completion_tokens": 338,
      "total_tokens": 12093
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance performed by multiple human experts or annotators."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or mention of quality assurance by a single non-expert human annotator in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although automated pitch extraction was performed using Praat scripts, no AI model was used or described for quality assurance purposes."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and Section 2.1.4",
          "reasoning": "The paper reports that audio recordings were semi-automatically labeled using WebMa and specific segments were hand-corrected, and that a custom Praat script was used for pitch extraction, indicating an automated (algorithmic) process with some manual correction but no elaboration on further quality assurance steps. This suggests that quality assurance was primarily through an automatic process combined with targeted manual correction."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Some quality assurance is implied through semi-automatic labeling and manual correction, so it is not accurate to state that no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11365,
      "completion_tokens": 410,
      "total_tokens": 11775
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1.2 Participants; 2.1.3 Speech material",
          "reasoning": "The dataset consists of speech recordings collected directly from human participants who self-recorded read speech via the smartphone app Plapper. The participants are from Germany and recorded themselves reading specific sentences, generating new, original human-spoken data created from scratch through human contribution. This is explicitly stated as data collected via voluntary self-recordings and not adapted or derived from previously existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models. The data originates from human speech recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any translation of data by humans from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate or modify the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not taken from existing sources or aggregated from other datasets; it was newly collected from participants."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is not based on existing sources with modifications or adaptations; it is original speech data recorded by participants for the study."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and method of data generation are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11883,
      "completion_tokens": 317,
      "total_tokens": 12200
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3 and 4",
          "reasoning": "The Plapper corpus dataset is used primarily for analyzing trends and patterns in fundamental frequency (f0) variation as a function of gender, age, self-ascribed gender spectrum, sexual orientation, and region in German read speech. The paper presents statistical models and discusses phonetic and sociolinguistic analyses based on this new corpus to investigate social and biological influences on speech characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12606,
      "completion_tokens": 286,
      "total_tokens": 12892
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1.1 and 2.1.3",
          "reasoning": "The Plapper corpus contains recordings of spoken German only. Participants from Germany recorded themselves reading sentences in German, as stated in Sections 2.1.1 (smartphone app designed to sample the German-speaking population in Germany) and 2.1.3 (speech material consists of self-recorded read speech). No other languages are mentioned as part of the dataset, confirming that it is monolingual and non-English (German)."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "weirich24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9824,
      "completion_tokens": 174,
      "total_tokens": 9998
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section or link provided",
          "reasoning": "The paper does not mention or provide any link to code repositories or explicit code used for data collection, preprocessing, or dataset generation. While methods and processing steps are described, no source code is shared publicly or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Method), especially 2.1 (Smartphone app Plapper), 2.1.1 to 2.1.5",
          "reasoning": "The paper provides a detailed description of the data collection process using the Plapper smartphone app, participant recruitment, the speech material, acoustic analysis procedures including extraction of fundamental frequency, and the statistical analysis approach. This coverage documents the dataset creation process including recording, labeling, segmentation, preprocessing, and analysis."
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8154,
      "completion_tokens": 155,
      "total_tokens": 8309
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The dataset comprises over 85 hours of segmented speech utterances from long-form audio recordings found online (the 'Letters' recordings). These recordings represent natural human speech collected from publicly available sources and were manually verified and processed for TTS training."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "Corresponding text transcripts for the found speech recordings were used. These transcripts were human-generated as they represent the original textual data aligned to the speech and were lightly normalized but not algorithmically generated."
        }
      ]
    }
  },
  {
    "id": "wells23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8954,
      "completion_tokens": 203,
      "total_tokens": 9157
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.3 Acoustically-driven data set selection",
            "reasoning": "The dataset creation involves automatic segmentation, alignment and selection based on acoustic unit sequences extracted via a HuBERT model and k-means clustering, with a greedy algorithm for data selection; these are deterministic computational processes rather than human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "No mention of human annotators receiving instructions; segmentation and data selection is performed by an automatic process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "No scoring rubrics or criteria for human annotator judgements are discussed as the data selection is automated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.3",
            "reasoning": "No annotation examples are provided since this is an automatic data selection process without human annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "wells23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10144,
      "completion_tokens": 323,
      "total_tokens": 10467
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human expert performing quality assurance on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human non-experts involved in quality assurance of the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for various tasks (e.g., training TTS models), the paper does not describe the use of AI models as judges for quality assurance of the dataset annotations or contents."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset construction involved an automated process of alignment and segmentation using character-based HMM-GMM acoustic models trained via Kaldi, followed by forced alignment, ASR decoding, and Smith-Waterman text alignment. These algorithmic and rule-based steps represent automated verification and quality assurance of the speech-text alignment without human annotation verification."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance is documented in the paper via an automatic alignment and verification process; thus, QA is present."
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9754,
      "completion_tokens": 380,
      "total_tokens": 10134
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the authors created new speech or text data from scratch by human contributors. Instead, they use 'found' data, specifically publicly available online Scottish Gaelic recordings and transcripts."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No original data was generated by AI or machine learning models themselves independent of existing data; models are trained based on the found data but do not generate new foundational speech or text data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No machine translation is used or mentioned to produce any portion of the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 (Corpus Preparation), especially Sections 2 and 2.2.",
          "reasoning": "The corpus is created by collecting and segmenting over 85 hours of Scottish Gaelic recordings found online (the Letters recordings), along with their transcripts. This data is existing material publicly available but collated and processed by the authors to create the segmented corpus for TTS training."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Sections 2.1, 2.2, and 2.3.",
          "reasoning": "The authors apply transformations and adaptations such as speech segmentation, forced alignment, selection of subsets based on acoustic unit coverage, and acoustic unit extraction via HuBERT features and k-means clustering on the found data to derive training subsets. This indicates that the data is modified and processed from its original source."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and processing of the data are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10272,
      "completion_tokens": 253,
      "total_tokens": 10525
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Model specification) and Section 5 (Results and discussion)",
          "reasoning": "The datasets of segmented Scottish Gaelic speech utterances (2h and 8h subsets selected) are used to train FastPitch acoustic models from scratch using character, phone, or acoustic unit inputs."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Listening test design) and Section 5 (Results and discussion)",
          "reasoning": "Held-out test sets derived from the discovered and segmented Scottish Gaelic speech corpus are used for subjective evaluation and listening tests to assess the performance of the trained TTS models."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10995,
      "completion_tokens": 402,
      "total_tokens": 11397
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 and 3.2",
          "reasoning": "The dataset primarily contains Scottish Gaelic speech and text, but the texts also include some non-Gaelic words, mostly snippets of English (Section 3.1). They apply an English G2P model to these non-Gaelic words (Section 3.2). Therefore, the dataset entries contain two human languages: Scottish Gaelic and English."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although Scottish Gaelic is the main language, the presence of English words in the dataset prevents it from being purely monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wells23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8213,
      "completion_tokens": 186,
      "total_tokens": 8399
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Conclusion",
          "reasoning": "The abstract explicitly states, 'We release our corpus building recipe so that others may easily apply our work to new languages.' The conclusion repeats that they 'release our corpus building and model code so that others may easily apply our work to new languages.' This indicates that the code used for constructing the dataset is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Recording corpus, Segmentation, and Data selection)",
          "reasoning": "Section 2 details the process of dataset creation, including recording data sourcing, segmentation, alignment, and acoustic unit-based data selection methods. The documentation is quite comprehensive, describing the iterative alignment procedure, acoustic model training, forced-alignment steps, and data set selection using discrete acoustic units. Therefore, the paper provides thorough documentation on the dataset creation process."
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11017,
      "completion_tokens": 170,
      "total_tokens": 11187
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Data Collection), especially 3.3 (Speech Elicitation and Collection), and Section 3.4 (Ethics)",
          "Reasoning": "The paper introduces a new dataset comprising audio recordings collected from adult human subjects undergoing acute pain inducement via a Cold Pressor Task. Speech was elicited by having participants read out loud randomized sentences, with audio recorded via microphones. This implies that the data modality is audio and the data was generated through human recording of natural speech during an experiment conducted by the authors. The dataset is explicitly described as newly collected and curated by the authors. No mention is made of model-generated or simulated audio. Hence, the dataset is audio modality, human generated."
        }
      ]
    }
  },
  {
    "id": "williams24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11817,
      "completion_tokens": 290,
      "total_tokens": 12107
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Non-Expert",
            "reference": "Section 4 and Section 3.3",
            "reasoning": "Pain level labels were assigned manually by the authors based on subjects' self-reported pain statements recorded during the experiment. The paper states that utterances were manually reviewed and labels were extrapolated from self-reported pain levels, indicating non-expert humans (authors) performed the annotation, not domain experts or AI models."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly described in the paper",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators for labeling pain levels. Labels were derived directly from participants' pain statements, without description of separate guideline instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 and Section 5",
            "reasoning": "Pain labels were assigned based on clinically relevant pain scales (1-10 self-reported numeric scale), and mapped to binary and three-class categories using specified thresholds (e.g., 1-3 mild/no pain, 4-6 moderate, 7-10 severe), effectively serving as rubrics for classification."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not described in the paper",
            "reasoning": "The paper does not provide example annotations or labeling examples as guidelines for annotators. Labeling was directly guided by participant pain reporting."
          }
        }
      ]
    }
  },
  {
    "id": "williams24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13007,
      "completion_tokens": 340,
      "total_tokens": 13347
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention multiple human expert annotators performing quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4 - Speech Data Pre-Processing",
          "reasoning": "All 1,947 utterances were manually reviewed by the authors for quality control to remove utterances with issues such as cut off words, noise, or interruptions. Manual review was performed by multiple authors of the paper; however, these reviewers are not explicitly stated to be experts in annotation or the target demographic, so they are considered non-experts with respect to quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of AI models for the purpose of quality assurance of annotations or data quality."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although voice activity detection (VAD) was applied automatically to trim recordings, this was part of preprocessing and not explicitly described as a quality assurance step to validate annotation or data content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a manual quality control process performed by multiple human reviewers (authors), so it is not the case that no quality assurance was applied or documented."
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12617,
      "completion_tokens": 411,
      "total_tokens": 13028
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 Data Collection",
          "reasoning": "The dataset was created by recruiting 15 human participants who were subjected to a cold pressor task to induce acute pain. The participants read out aloud sentences specifically selected for this study, with recordings made under controlled conditions. The speech data is original and collected from scratch by human subjects performing the task and reading the sentences during the experiment."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not generated or synthesized by any AI or machine learning models; rather, it consists of direct human speech recordings during the experiment."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that any data was produced by human translation from another language is provided; the dataset consists of English speech recorded from participants."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence of machine translation being used to generate the dataset is mentioned."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not collected from existing sources or aggregated; it was directly recorded from a novel experimental protocol."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While acoustic features were extracted from the recordings, the underlying data (speech audio) is not derived from existing datasets; no modifications or transformations were applied to produce the dataset itself."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the dataset is clearly documented and described in detail in the paper."
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13135,
      "completion_tokens": 335,
      "total_tokens": 13470
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.2, 5.3",
          "reasoning": "The dataset is used to train machine learning models such as SVM, Logistic Regression, and MLP classifiers from scratch to predict pain levels from speech features as described in Sections 5.2 and 5.3. There is no mention of pretrained models being fine-tuned; models are trained using this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.3",
          "reasoning": "The dataset is used for evaluation purposes, with performance metrics such as F1 scores reported for both binary and three-class pain classification tasks (Section 5.3). The dataset serves as the evaluation benchmark for the classifiers trained."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.3 (Feature Analysis)",
          "reasoning": "The dataset is used extensively for analyzing acoustic feature contributions to pain classification using SHAP values and PCA, to understand the relationship between speech features and pain levels (Section 5.3 Feature Analysis). This analysis clarifies how different features affect model predictions, contributing to explainability."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13858,
      "completion_tokens": 501,
      "total_tokens": 14359
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described is collected exclusively in English. There is no mention or indication of more than two human languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains entries only in English and does not include a second language."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.3 Speech Elicitation and Collection",
          "reasoning": "The paper explicitly states that the dataset consists of English speech only, where participants read aloud sentences from the Harvard Sentences, which are English sentences. The dataset is an English speech-only pain dataset as mentioned in the abstract and Section 3.3."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is in English, not a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech utterances only; no entries contain programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence that the dataset entries contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human speech recordings; it does not include biological sequences or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language is clearly documented as English; thus, the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries do contain language in the form of English speech."
        }
      }
    }
  },
  {
    "id": "williams24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11076,
      "completion_tokens": 145,
      "total_tokens": 11221
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Not specified",
          "reasoning": "The paper does not mention or provide any links, references, or supplemental materials indicating that the code for data collection, preprocessing, or dataset construction is publicly available or shared."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3: Data Collection and Section 4: Speech Data Pre-Processing",
          "reasoning": "The paper provides detailed documentation about the dataset creation process, including participant recruitment criteria, pain inducement protocol (Cold Pressor Task), speech elicitation methods, audio recording setup, data preprocessing steps such as quality control and labeling, and ethical considerations. This information adequately documents the dataset construction process."
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8182,
      "completion_tokens": 80,
      "total_tokens": 8262
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data",
          "Reasoning": "The paper states that they used an existing cognitive and physical load dataset corpus presented in [7], so no new dataset was created by the authors. No new dataset was introduced; the datasets used are pre-existing."
        }
      ]
    }
  },
  {
    "id": "wu23i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8982,
      "completion_tokens": 256,
      "total_tokens": 9238
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1",
            "reasoning": "The data used are existing cognitive and physical load datasets combined from prior works (as stated in Section 3.1), where audio samples and labels are used for training and testing. Labels correspond to whether the speaker was under cognitive or physical load. There is no explicit mention of manual annotation for this new combined dataset. Hence, the labeling corresponds to the experimental task conditions rather than annotations by human experts."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not report providing annotation instructions to human annotators for the labeling of the new combined dataset. The labels originate from experimental task conditions, and no annotation guidelines are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No scoring rubrics or formal annotation criteria are described for this combined dataset, as labels stem from the task condition rather than subjective annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "No annotation examples are provided or referenced, since the labeling is derived from experimental conditions, not human annotation."
          }
        }
      ]
    }
  },
  {
    "id": "wu23i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10172,
      "completion_tokens": 373,
      "total_tokens": 10545
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the new datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence in the paper that multiple human experts were involved in quality assurance of the datasets."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information suggesting that a single non-expert human annotator conducted quality assurance on the datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or description of quality assurance performed by multiple non-expert human annotators in relation to the datasets used."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using any AI model as a judge or for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance processes used on dataset annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper uses existing datasets referenced in [7], and does not introduce any new datasets nor documents any quality assurance procedures on the data. No QA process is described or conducted for these datasets."
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9782,
      "completion_tokens": 437,
      "total_tokens": 10219
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses an existing cognitive and physical load dataset corpus as presented in [7], consisting of five datasets with 111 speakers and nine languages. There is no indication that the authors collected or created a new dataset entirely from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data is derived from existing speech recordings; no data was artificially generated or created solely by models as original data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data acquired through human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data acquired through machine translation from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1 Data",
          "reasoning": "The authors used an existing corpus consisting of five separate datasets collected from multiple languages and speakers, combined for their experiments. The data is aggregated from these existing sources without creating new raw data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.4 Decreasing input audio length, Section 3.5 Utterance-level speaker embeddings",
          "reasoning": "The authors derived new data representations by chunking original audio files into shorter segments (3-5 seconds) and by computing speaker embeddings (using ECAPA and Resemblyzer) from full-length audio. These processes involve transformation and adaptation of existing data rather than completely new data generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly documented as existing datasets combined and processed as described."
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10300,
      "completion_tokens": 316,
      "total_tokens": 10616
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2, 3.3, and 3.5",
          "reasoning": "The combined voice stress datasets are used to fine-tune pre-trained audio embedding models by training downstream classifiers (SVM, MLP, Transformer) for binary classification tasks of voice stress detection. The paper describes training and evaluation procedures on combined and chunked datasets using supervised methods."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3, Table 1",
          "reasoning": "The datasets are explicitly split into training, validation, and test sets to evaluate cross-dataset voice stress detection performance and generalization. Performance metrics (UAR) are reported to benchmark and validate the models' effectiveness."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 3.5 and 3.6",
          "reasoning": "The paper conducts detailed analyses on how speaker embeddings and different types of audio embeddings (utterance-level, framewise) affect voice stress detection performance, including individual variability analysis across combined datasets."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11023,
      "completion_tokens": 241,
      "total_tokens": 11264
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The paper states in Section 3.1 that the dataset corpus consists of five existing datasets with a total of 111 speakers from nine languages. This indicates that the combined dataset used by the authors contains entries from more than two human languages, making it multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wu23i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8241,
      "completion_tokens": 157,
      "total_tokens": 8398
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No specific section",
          "reasoning": "The paper does not provide any link or mention of publicly available code related to data collection, preprocessing, or dataset construction. The focus is on models and evaluation using existing datasets, with no indication that code for dataset construction is released."
        },
        "documentation": {
          "Has Documentation": false,
          "reference": "No specific section",
          "reasoning": "The paper states that it uses an existing voice stress dataset corpus from prior work [7], consisting of five datasets, 111 speakers, nine languages, and 12 hours duration. There is no detailed description of dataset construction or data collection within this paper, only reference to prior datasets. Therefore, documentation about dataset creation is not provided in this paper."
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9679,
      "completion_tokens": 141,
      "total_tokens": 9820
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3 Dataset generation pipeline",
          "Reasoning": "The CodecFake dataset is newly introduced in the paper and is created by re-synthesizing speech from the VCTK corpus using 15 pre-trained neural audio codec models. The original VCTK speech data is human-generated (recorded from human speakers), and the speech is then model-generated through codec-based resynthesis. Therefore, the audio data modality is a mixture of human generated source recordings and model generated synthesized speech, specifically designed to comprise deepfake audios from codec-based speech synthesis systems."
        }
      ]
    }
  },
  {
    "id": "wu24p_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10479,
      "completion_tokens": 237,
      "total_tokens": 10716
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.3, Dataset generation pipeline",
            "reasoning": "The CodecFake dataset is generated by encoding real speech from the VCTK corpus into discrete codes using pre-trained codec encoders and then re-synthesizing speech using the respective codec decoders. This resynthesis process is performed programmatically using existing models without mention of human annotators or manual labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.3, Dataset generation pipeline",
            "reasoning": "The paper does not mention any annotation instructions for human annotators since the dataset is generated automatically by codec model resynthesis."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.3, Dataset generation pipeline",
            "reasoning": "There is no indication of any scoring rubrics provided during dataset creation as the process is automatic synthesis without human annotation or scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.3, Dataset generation pipeline",
            "reasoning": "No examples for annotators are provided because the dataset generation is automatic; no manual annotation examples exist."
          }
        }
      ]
    }
  },
  {
    "id": "wu24p_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11669,
      "completion_tokens": 289,
      "total_tokens": 11958
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the CodecFake dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process involving a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence of quality assurance provided by multiple non-expert human annotators mentioned in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that an AI model was employed to perform QA on the dataset."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.3 Dataset generation pipeline",
          "reasoning": "The dataset was generated automatically by encoding and decoding speech through pre-trained neural audio codec models using an automated resynthesis procedure. The process relies on algorithmic transformations without human annotation or review, thus constitutes an automatic process for quality assurance of the dataset content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automatic process for dataset generation, so QA is performed, albeit automatically."
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11279,
      "completion_tokens": 463,
      "total_tokens": 11742
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not created entirely from scratch by humans; rather, it is derived from existing speech data (VCTK corpus) processed by codecs."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2, CodecFake Dataset Generation Pipeline",
          "reasoning": "The CodecFake dataset is generated by resynthesizing existing speech data (VCTK corpus) through 15 pre-trained neural audio codec models. This involves encoding real speech into discrete codes and decoding it back into speech using neural audio codec models, which are machine learning models. Hence, the resulting data is generated by models transforming existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or mention of any translation activity performed by human translators in the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence or claim that the dataset involves translating content from one language to another via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset uses existing VCTK speech data, it does not simply aggregate or collect existing recordings; it transforms them via codec resynthesis, indicating modification rather than direct collation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1, Codec-based speech generation; Section 2.3, Dataset generation pipeline",
          "reasoning": "The CodecFake dataset is derived from the original VCTK corpus by applying transformations using pre-trained neural audio codec models to produce resynthesized speech. This process represents modifications and transformations of existing data, creating derived content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11797,
      "completion_tokens": 370,
      "total_tokens": 12167
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1 Implementation details; Section 2.3 Dataset generation pipeline",
          "reasoning": "The CodecFake dataset is used to train anti-spoofing models by fine-tuning the AASIST-L pre-trained backbone model with a supervised learning setup, as described in Section 3.1 and 2.3. Models are trained using the CodecFake subsets combined with original VCTK utterances for supervised anti-spoofing training."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2 Evaluation details; Section 2.3 Dataset generation pipeline",
          "reasoning": "The CodecFake dataset is also used as an evaluation benchmark, particularly its test subsets, to measure the performance of anti-spoofing models against deepfake audios generated by codec-based speech synthesis systems. This usage is described in Section 3.2 and Section 2.3."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 1 Introduction; Section 2 CodecFake",
          "reasoning": "The paper performs a comprehensive analysis using the CodecFake dataset to assess the effectiveness of anti-spoofing models when presented with codec-based synthesized speech. This analytical use is indicated in the introduction and in the description of the dataset in Section 2."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12520,
      "completion_tokens": 649,
      "total_tokens": 13169
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.2 and Table 1",
          "reasoning": "The CodecFake dataset is created by resynthesizing speech from the VCTK corpus using 15 different pre-trained neural audio codec models. The training data for these codec models include multiple languages: English (e.g., Librispeech, LibriTTS), Chinese (e.g., AISHELL, zh-en datasets), and potentially others such as Common Voice (which supports multiple languages) and MUSDB, AudioSet, Jamendo, which may include multilingual content. Specifically, codec models F5 and F6 are trained on both English and Chinese (zh-en) datasets, indicating bilingual data, and other datasets used for training codec models include multiple languages. Therefore, the resulting dataset contains entries with more than two human languages. Hence, it is appropriate to classify CodecFake as multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CodecFake dataset contains more than two languages due to the diverse training data of the codecs used, including English, Chinese, and possibly others; thus, the dataset is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset includes English speech from VCTK, the use of codec models trained on Chinese and other language data means the dataset is not exclusively English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains English and other languages, so it is not exclusively a single non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio waveforms synthesized from codec-based speech generation, not code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human speech audio data only."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or constructed languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Languages of the dataset are documented through the data origins of the codecs and VCTK corpus."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language audio data derived from human languages; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "wu24p_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9738,
      "completion_tokens": 192,
      "total_tokens": 9930
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 2; Section 3.1",
          "reasoning": "The paper states in the abstract: 'We will release all code and data,' indicating intent to make the code publicly available. Additionally, in Section 2 and 3.1, the dataset generation and training procedures mention usage of pre-trained codecs and detailed pipeline, implying code for dataset generation exists. Also, a project URL is given as https://codecfake.github.io suggesting code/data availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (CodecFake)",
          "reasoning": "Section 2 describes in detail the dataset creation procedure, design principles, codec adoption, and the dataset generation pipeline, including the choice of base corpus (VCTK), the splitting strategy, and the use of 15 codec models. Hence the dataset creation process is well documented within the paper."
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 5892,
      "completion_tokens": 169,
      "total_tokens": 6061
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The new dataset is constructed by the authors, consisting of synthesized single-channel noisy audio samples generated by convolving clean speech from the TIMIT corpus with 108 real-world Room Impulse Responses (RIRs) (32 recorded by the authors and 76 from the EchoThief Impulse Response Library), and adding noise including environmental sounds and pink/babble noise at various Signal-to-Noise Ratios. Hence, the audio modality is present, and its origin is both human generated (speech recordings, recorded RIRs, real noise recordings) and model generated (synthesized noisy audio samples created by convolution and noise addition as part of data synthesis)."
        }
      ]
    }
  },
  {
    "id": "xiang23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 6692,
      "completion_tokens": 223,
      "total_tokens": 6915
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "AI Model",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The dataset is described as synthesized noisy audio samples created by adding noise and reverberations to clean audio using various RIRs and noise types. This synthesis process indicates that the STI ground truth labels were generated automatically by simulation rather than by human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "There is no description or mention of providing annotation instructions to annotators for the dataset, since the data labeling involved automated synthesis and calculation rather than human annotation requiring instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 4.1 Dataset",
            "reasoning": "Since the ground truth STI values are computationally derived from simulation methods without human raters, no scoring rubrics are mentioned or needed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No annotation examples are provided because dataset labels are automatically generated; examples for human annotations are not applicable or discussed."
          }
        }
      ]
    }
  },
  {
    "id": "xiang23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 7882,
      "completion_tokens": 267,
      "total_tokens": 8149
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single non-expert human annotator conducted quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence that multiple non-expert annotators were involved in quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic quality assurance processes applied to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not document or describe any quality assurance process for the constructed dataset, which is synthesized using clean speech from TIMIT, real-world RIRs, and noises. Therefore, no quality assurance process is applied or documented."
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 7492,
      "completion_tokens": 407,
      "total_tokens": 7899
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not created entirely from scratch by human contributors. Instead, it was synthesized using existing resources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset uses 108 real-world room impulse responses (RIRs) collected from offices and the EchoThief Impulse Response Library, along with the TIMIT dataset for clean speech and TUT Acoustic Scenes dataset for noise, all existing sources. These existing components were collected and aggregated for use in the new dataset."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The authors generated the noisy audio samples by convolving clean speech with RIRs and adding noise at various SNR levels, thus transforming and combining existing data to produce a synthesized dataset covering the full range of STI values."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data sourcing and generation process of the dataset is clearly specified in Section 4.1."
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8010,
      "completion_tokens": 289,
      "total_tokens": 8299
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset; Section 4.2 Experimental Details",
          "reasoning": "The constructed dataset consisting of over 590,000 synthesized noisy audio samples with corresponding STI ground truth and clean audio is used to train the proposed eSTImate model from scratch. The training is conducted with this dataset, as described in the experimental setup."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 STI Evaluation; Section 4.4 Ablation Study; Section 4.5 Framework Comparison",
          "reasoning": "The dataset is used as a test set with known STI ground truth values to evaluate and benchmark the performance of the proposed model and compare it against previous methods. The test results demonstrate low MAE and RMSE, showing the dataset's role in evaluation and benchmarking."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 8733,
      "completion_tokens": 416,
      "total_tokens": 9149
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is constructed using speech from the TIMIT dataset, which contains only English sentences from speakers of American English dialects; no mention of any other human languages is given."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the dataset as containing two human languages; only English speech from TIMIT is used."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The dataset is synthesized using the TIMIT dataset as the clean speech corpus, which consists of English speech samples; no other languages are mentioned. Hence, the dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly one non-English language; it exclusively uses English speech from TIMIT."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio recordings with associated STI labels, not programming or structured code data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper contains mathematical equations to describe methods and formulations, the dataset itself contains audio data, not mathematical or logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain biological or non-human communication data; it contains human speech audio data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or fictional languages in the dataset; only natural spoken English is used."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages used in the dataset are clearly specified\u2014the TIMIT dataset contains English speech; thus, language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of human speech audio, so it contains language data."
        }
      }
    }
  },
  {
    "id": "xiang23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 5951,
      "completion_tokens": 143,
      "total_tokens": 6094
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper describes the dataset construction process in Section 4.1 but does not provide any links or references to publicly available code repositories for dataset construction or preprocessing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "Section 4.1 provides a detailed description of how the dataset was constructed, including source datasets (TIMIT for clean speech, TUT Acoustic Scenes for noise), RIRs used, noise types and SNR levels, and the overall size and distribution of the synthesized dataset, thus documenting the dataset creation process adequately."
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7482,
      "completion_tokens": 104,
      "total_tokens": 7586
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection",
          "Reasoning": "The SSBPR dataset consists of 7570 snoring audio recordings collected from 20 adult patients during overnight polysomnography at a sleep medicine center, recorded using a subminiature lavalier microphone placed 3cm from the mouth. This indicates the audio data were directly recorded from human patients using human-operated equipment."
        }
      ]
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8282,
      "completion_tokens": 210,
      "total_tokens": 8492
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The paper states that sleep stages were scored by three experienced experts and verified/scored by two certified doctors with RPSGT certification, indicating multiple human experts performed the annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The annotation process follows the AASM Manual V2.6 for scoring sleep stages, which implies detailed and standardized instructions were used for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "Sleep stages were scored according to the AASM Manual V2.6, which provides standardized scoring rules and rubrics for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide or mention explicit annotation examples within the guidelines or appendices for the snoring sound annotations or sleep body position annotation."
          }
        }
      ]
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9472,
      "completion_tokens": 377,
      "total_tokens": 9849
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes annotation involving multiple personnel including a certified technician and two certified doctors, not a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Annotation",
          "reasoning": "The annotation was performed by a certified technician initially, and then the final scoring was conducted by two certified doctors who verified the PSG scoring. These annotators are experts as they are certified with RPSGT and have specific expertise in sleep data annotation. Therefore, multiple human experts validated the annotation."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance was not performed by a single non-expert; expert annotators were involved."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The quality assurance involved certified expert annotators, not non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report that any AI model was used for quality assurance of annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of automated or algorithmic verification for quality assurance in annotation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a thorough multi-expert annotation and verification process, so quality assurance was applied."
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9082,
      "completion_tokens": 435,
      "total_tokens": 9517
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection and Section 2.2 Data Annotation",
          "reasoning": "The SSBPR dataset was collected from 20 adult patients undergoing overnight polysomnography (PSG) at a sleep medicine center. Snoring audio was recorded directly via a subminiature lavalier microphone placed near patients' mouths during natural sleep. The data was annotated by experienced certified technicians and doctors based on simultaneous PSG signals including EEG, EOG, EMG, body position sensors, and video monitoring. This indicates the data is originally collected and annotated by human experts from scratch, not adapted or derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any part of the dataset was generated by AI or machine learning models. The dataset is based entirely on real clinical recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translation from other languages in the creation of the dataset."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used in the dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely an aggregation of pre-existing data sources; it involves original data collection and annotation."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not based on existing datasets with modifications; instead it is newly collected clinical data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the source and data collection methods of the dataset."
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9600,
      "completion_tokens": 273,
      "total_tokens": 9873
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3. Baseline Method",
          "reasoning": "The paper describes training a baseline model (AST) from scratch on the SSBPR dataset for sleep body position classification using supervised learning with snoring sound spectrograms as inputs."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 Results",
          "reasoning": "The dataset is used to evaluate the accuracy of the baseline model in classifying sleep positions based on snoring sounds assessed by accuracy metrics reported in Table 2 and confusion matrix in Figure 3."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 Results and Section 4 Conclusions",
          "reasoning": "The authors analyze classification accuracy differences across genders and sleep positions, and discuss potential pathophysiological reasons and dataset characteristics affecting performance."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10323,
      "completion_tokens": 457,
      "total_tokens": 10780
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of snoring audio recordings and associated labels; the paper does not mention the dataset containing any programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical symbolic expressions are present in the dataset entries; the data are audio recordings labeled by sleep body position."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection and Abstract",
          "reasoning": "The dataset contains recorded snoring sounds, which are biological signals produced by human physiology during sleep. These are sounds generated by human biological processes, thus qualifying as biological communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes no fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's content and nature are clearly described as snoring sounds collected from human subjects; language categorization is not ambiguous."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The dataset entries consist of audio recordings of snoring sounds representing biological signals, not language content or text. Hence, the dataset does not contain any human language."
        }
      }
    }
  },
  {
    "id": "xiao23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7541,
      "completion_tokens": 185,
      "total_tokens": 7726
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4 Conclusions, final paragraph",
          "reasoning": "The paper states that the dataset will be released at https://github.com/xiaoli1996/SSBPR but does not mention any release of the code used for data collection, preprocessing, or dataset construction. There is no explicit indication or link provided for the code repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Datasets (particularly subsections 2.1 Data Collection and 2.2 Data Annotation)",
          "reasoning": "The paper provides detailed documentation about the dataset creation process, including information on subject recruitment (20 adult patients), ethics approval and consent, recording setup (microphone type, placement, sampling rate), annotation methodology using synchronized PSG signals and expert scorers, and data distribution details. These details offer transparency and completeness regarding dataset creation."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6926,
      "completion_tokens": 153,
      "total_tokens": 7079
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.3, Abstract",
          "Reasoning": "The FakeSound dataset is a new dataset introduced by the authors in this paper for the deepfake general audio detection task. It consists of general audio clips derived from the AudioCaps dataset (human recorded audio) which are then manipulated via an automated pipeline. The pipeline uses models such as AudioLDM1/2 and AudioSR to generate deepfake audio segments by inpainting masked regions. Thus, the dataset comprises both human-generated original audio and model-generated manipulated audio segments, explicitly created for model training and evaluation as detailed throughout Section 2 and Section 2.3."
        }
      ]
    }
  },
  {
    "id": "xie24d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7726,
      "completion_tokens": 218,
      "total_tokens": 7944
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2: FakeSound: Deepfake General Audio Dataset",
            "reasoning": "The dataset FakeSound is created using an automated manipulation pipeline consisting of grounding, masking, regenerating, and replacing key audio segments. This pipeline does not involve human annotators but rather model-based processes to generate the fake audio and align annotations precisely."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "The paper does not describe any annotation instructions given to human annotators because the dataset generation and annotation are automatic."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "Since annotations are automatically generated via the manipulation pipeline, there is no mention of grading or scoring rubrics for annotation quality."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No explicit section or mention",
            "reasoning": "The paper does not mention providing annotation examples, as annotations are algorithmically created via the automated pipeline rather than through manual labeling."
          }
        }
      ]
    }
  },
  {
    "id": "xie24d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8916,
      "completion_tokens": 450,
      "total_tokens": 9366
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper indicating that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance performed by a single human non-expert on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 5.2 Subjective Evaluation, and Table 2",
          "reasoning": "The paper describes a subjective evaluation where 10 human evaluators listened to samples from the dataset and annotated whether the audio was deepfake or not, and located deepfake regions. These evaluators appear to be human non-experts as there is no indication they are domain experts. This evaluation serves as a quality check on the difficulty and realism of the dataset samples, thus constituting a quality assurance process involving multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset is generated using AI models (e.g. AudioLDM1/2, AudioSR), the paper does not mention using AI models as judges or for quality assurance purposes on the dataset annotations or labels."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2 FakeSound: Deepfake General Audio Dataset, especially 2.1 Ground & Mask and 2.2 Regenerate & Replace",
          "reasoning": "The dataset generation process is described as an automated pipeline involving an audio-text grounding model to locate key segments, masking chosen segments, and then regenerating those segments with generative models and super-resolution models. This automated manipulation pipeline generates deepfake general audio samples and their corresponding precise timestamps, which serve as exact annotations. This process constitutes an automatic quality assurance mechanism ensuring the correctness of annotations without requiring human intervention."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents quality assurance processes, including automated pipeline generation and subjective human evaluation; thus QA is present."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8526,
      "completion_tokens": 449,
      "total_tokens": 8975
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not created entirely from scratch by human contributors. Instead, it leverages an existing dataset (AudioCaps) as a base."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2, Section 2.3",
          "reasoning": "The FakeSound dataset is generated by applying an automated manipulation pipeline that uses AI models, including AudioLDM1/2 (inpainting generative models) and AudioSR (super-resolution), to regenerate masked audio segments. Thus, significant portions of the final dataset are newly generated by AI models rather than being purely sourced from existing recordings."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication of translation from another language involving human translators in the dataset creation."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of use of machine translation systems to generate or transform data for this dataset."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the original data used for manipulation is from AudioCaps, the resulting dataset is not simply collated but significantly manipulated by generation models."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1, Section 2.2, Section 2.3",
          "reasoning": "The dataset is derived from existing audio data (AudioCaps). The key segments are detected and masked based on grounding models, then replaced with model-generated inpainted audio segments, effectively modifying and transforming the original audio data to create deepfake samples."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset origin and generation method are clearly documented throughout the paper."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9044,
      "completion_tokens": 510,
      "total_tokens": 9554
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2, Section 5.1",
          "reasoning": "The FakeSound dataset is used as the primary training data for developing a deepfake general audio detection model. Section 2 describes dataset construction and manipulation for training, and Section 5.1 details the training setup using the dataset, indicating it is used to train the detection model from its initialized state (except for the frozen feature extractor)."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate the use of the dataset for fine-tuning a pre-trained model in a supervised manner. The proposed model uses frozen pre-trained feature extractor but is trained with FakeSound from scratch for the detection layers."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based post-training methods using the FakeSound dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.3, Section 6",
          "reasoning": "FakeSound includes several test sets (Test-Easy, Test-Hard, Test-Zeroshot) explicitly designed for comprehensive evaluation of deepfake detection models, as described in Sections 2.3 and 6. The model and human evaluations use these test sets to benchmark performance."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used for analyzing trends or characteristics outside of training and evaluation contexts."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation in this work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The FakeSound dataset is clearly used for training and evaluation of a deepfake detection model, thus it has practical utility described in the paper."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9767,
      "completion_tokens": 567,
      "total_tokens": 10334
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The paper describes the FakeSound dataset as manipulated general audio samples from AudioCaps, which contains audio captions in English. There is no indication of multiple human languages involved."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no mention in the paper of the FakeSound dataset including exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.3 Dataset Metadata",
          "reasoning": "The dataset uses AudioCaps as the source dataset, which contains English captions and associated English language descriptions. No indication of other spoken human languages is given. Hence, the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No non-English monolingual language content is mentioned in the paper with respect to the dataset."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of audio data and includes no entries of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper contains mathematical formulas in the methodology and evaluation metrics, the dataset entries themselves do not contain mathematical or formal logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset comprises general audio manipulated for deepfake detection, not biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset contains any fictional or artificially created languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The linguistic content of the dataset entries is clearly documented as based on English audio captions and audio clips. Language is not unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries are audio content which includes human language (English) and other environmental sounds. Therefore, it is not without any language."
        }
      }
    }
  },
  {
    "id": "xie24d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6985,
      "completion_tokens": 227,
      "total_tokens": 7212
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2.2 and 2.3 (paragraphs about the manipulation pipeline and dataset metadata) and footnote with GitHub URL",
          "reasoning": "The paper states in Section 2.2 that the FakeSound dataset along with the training and evaluation code is available at https://github.com/FakeSoundData/FakeSound, indicating that the code used for data collection, preprocessing, and generation is publicly accessible. Hence, the code repository is linked explicitly for dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (FakeSound: Deepfake General Audio Dataset)",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 2, including an explanation of the manipulation pipeline (grounding, masking, regeneration, and replacement), the models used (AudioLDM1/2, AudioSR), dataset metadata (Table 1 with training and test splits), and how the manipulated segments are generated and limited. This thorough description covers dataset generation methodology, enabling reproducibility and comprehension of dataset construction."
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11468,
      "completion_tokens": 131,
      "total_tokens": 11599
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2. Laughter data collection",
          "Reasoning": "The paper introduces a new large-scale in-the-wild Japanese laughter corpus composed of about 3.5 hours of laughter utterances collected from online videos featuring human speakers (casters and YouTubers). The data consists of single-speaker laughter recordings obtained by manual segmentation and denoising from these videos, thus it originates from human recorded audio."
        }
      ]
    }
  },
  {
    "id": "xin23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12268,
      "completion_tokens": 243,
      "total_tokens": 12511
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2: Laughter data collection",
            "reasoning": "The paper states that about 1500 workers were requested to label videos with categories indicating single-speaker laughter, multi-speaker laughter, and others through a listening test with crowd-sourcing, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2: Laughter data collection",
            "reasoning": "The paper mentions a listening test with multiple workers labeling videos with specific categories, which implies that instructions were provided to the annotators on how to perform the labeling."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2: Laughter data collection",
            "reasoning": "The labeling involved three categories (single-speaker laughter, multi-speaker laughter, and others), which serve as a rubric or labeling scheme guiding the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific mention in the paper about providing annotation examples to annotators.",
            "reasoning": "The paper does not explicitly mention providing examples of laughter types or annotation examples in the guidelines for annotators."
          }
        }
      ]
    }
  },
  {
    "id": "xin23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13458,
      "completion_tokens": 429,
      "total_tokens": 13887
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human expert performing quality assurance on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts performed quality assurance on the laughter corpus data."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2, Laughter data collection",
          "reasoning": "The paper states that about 1500 workers (crowd-sourcing) labeled the detected videos with three categories ('single-speaker laughter', 'multi-speaker laughter', 'others'). This indicates that multiple human non-experts conducted labeling as a form of quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 2, Laughter data collection",
          "reasoning": "An open-sourced pretrained laughter detection model (an AI model) was first used to discover videos that possibly contain laughter, which forms part of the data selection and quality assurance process."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While a source separation model (Demucs) was used to extract vocals and reduce noise, the paper does not describe this as a quality assurance step for the dataset annotation; rather it is a preprocessing step."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents quality assurance steps involving multiple human non-experts and an AI model, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 13068,
      "completion_tokens": 463,
      "total_tokens": 13531
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2 Laughter data collection",
          "reasoning": "The paper describes the creation of a large-scale in-the-wild Japanese laughter corpus collected from videos on YouTube. The laughter utterances were manually segmented after crowdsourced human labeling to identify single-speaker laughter. This process indicates that the laughter data is original content created by human contributors, not derived from existing datasets or translations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The laughter corpus itself is not generated by AI or machine learning models; they used models only for processing and representation, not for creating the original laughter data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the laughter data was produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the laughter data was generated through machine translation."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 Laughter data collection",
          "reasoning": "The authors collected data by crawling existing videos from YouTube, which is an aggregation of existing public content. They used an open-sourced laughter detection model and human labeling to select relevant utterances but did not modify the original laughter utterances extensively beyond segmentation and noise reduction."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Laughter data collection",
          "reasoning": "The laughter utterances underwent processing such as source separation (using Demucs model) to reduce noise and isolate vocals. This processing represents a transformation of the collected original videos, thus the final corpus is derived from existing source videos with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is clearly described and documented."
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13586,
      "completion_tokens": 280,
      "total_tokens": 13866
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2 and Section 4.1",
          "reasoning": "The proposed large-scale in-the-wild Japanese laughter corpus is used to train the text-to-speech model from scratch using pseudo phonetic tokens (PPTs) derived from the corpus. The TTS model is trained on this dataset (Section 4.1) to synthesize laughter, as described in Section 3.2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The corpus is split into train, validation, and test sets, with the test set used exclusively for evaluation of the laughter synthesis models (Section 4.2). Objective and subjective evaluations are conducted on the test utterances from the corpus."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14309,
      "completion_tokens": 496,
      "total_tokens": 14805
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The proposed dataset contains only Japanese laughter utterances collected from Japanese speakers, and there is no indication that multiple languages are included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only one language (Japanese), so it is not bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not English-only; it consists of Japanese laughter utterances."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2 Laughter data collection",
          "reasoning": "The proposed corpus is described as an in-the-wild Japanese laughter corpus collected from YouTube videos, with non-Japanese videos discarded during preprocessing. Thus, all entries are Japanese laughter utterances."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio laughter utterances without code or programming language content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains audio laughter data, no mathematical or logical symbolic notations are part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human laughter audio; there is no biological sequence data or non-human communication."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed or fictional languages are included; dataset is human Japanese laughter."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly stated and documented as Japanese laughter."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human laughter speech recordings in the Japanese language, so it contains language."
        }
      }
    }
  },
  {
    "id": "xin23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11527,
      "completion_tokens": 241,
      "total_tokens": 11768
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 2",
          "reasoning": "The paper states in the abstract: 'We publicate the proposed corpus and the code implementation of the proposed method.' Also, at the bottom of page 2, footnotes provide the URLs (sites.google.com and github.com) where both the corpus and code are publicly available. Although the code repository likely includes the code for the proposed laughter synthesis method, it is reasonable to conclude the code related to dataset collection and preprocessing is publicly provided to enable reproducibility given the explicit mention and links."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2: Laughter data collection",
          "reasoning": "Section 2 provides a detailed description of the dataset creation process, including data sources (YouTube videos of casters and YouTubers), crawling procedures, use of a pretrained laughter detection model, crowd-source labeling to filter for single-speaker laughter, manual segmentation criteria, denoising with a pretrained Demucs v3 model, and statistics on the final corpus size and speaker count. This documentation is sufficiently transparent and detailed to enable understanding and reproduction of the data collection protocol."
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8776,
      "completion_tokens": 127,
      "total_tokens": 8903
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Datasets",
          "Reasoning": "The paper introduces a self-recorded dataset consisting of 1,097 crying sounds collected by recruiting new mothers as volunteers who recorded and annotated babies' cries using mobile phones in indoor environments. This data is clearly audio modality, human-recorded and annotated, thus Human Generated is true, Model Generated is false, and origin is known."
        }
      ]
    }
  },
  {
    "id": "xu22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9576,
      "completion_tokens": 217,
      "total_tokens": 9793
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3.1 Datasets",
            "reasoning": "The self-recorded dataset was annotated by some new moms as volunteers who labeled the crying sounds based on their own experience, indicating multiple human non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1 Datasets",
            "reasoning": "The paper states that the moms labeled the recorded sounds based on their own experience but does not mention any detailed annotation instructions provided to them."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 Datasets",
            "reasoning": "No mention is made in the paper of scoring rubrics or standardized criteria used by the annotators in the annotation process of the self-recorded dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1 Datasets",
            "reasoning": "The paper does not provide any indication or examples given to annotators to guide the labeling of the self-recorded infant cries."
          }
        }
      ]
    }
  },
  {
    "id": "xu22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10766,
      "completion_tokens": 460,
      "total_tokens": 11226
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "For the public CRIED dataset, the paper specifies that two experts in the field of early speech-language development annotated the vocalizations based on audio-video clips. This indicates that multiple human experts performed annotation; however, regarding quality assurance, the mention of expert annotators highlights at least expert involvement in annotation and likely quality assurance."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The CRIED dataset was annotated by two experts in the field, indicating that multiple human experts performed the annotation and quality assurance. The paper explicitly states the annotation by two experts, which supports this label."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any annotation or quality assurance performed by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The self-recorded dataset was annotated by new moms based on their own experience. The paper notes that these moms 'somehow lack experience,' implying they are non-experts. Multiple moms participated in annotation, constituting multiple human non-experts performing the annotation and quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of an AI model being used as a judge or quality assurance process for dataset annotation."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance process applied to the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance processes are described for both datasets, so this label is not applicable."
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10376,
      "completion_tokens": 414,
      "total_tokens": 10790
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 (Datasets)",
          "reasoning": "The paper states that the authors recruited new mothers as volunteers to record and annotate new babies' cry sounds using mobile phones in indoor environments, resulting in a self-recorded dataset consisting of 1097 crying sounds. This indicates original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any synthetic data or data generated entirely by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any dataset was produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any machine translation used in producing any datasets."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper uses the existing CRIED database, it is a pre-existing dataset and not newly introduced by the authors. There is no indication the authors aggregated or collected data from multiple existing sources to create a new dataset."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The differential time-frequency log-Mel spectrogram features are derived transformations of audio signals, but these are feature extractions and transformations applied to datasets rather than new datasets themselves."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and generation method of the self-recorded dataset are explicitly described; therefore, the origin is documented and does not qualify for N/A."
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10894,
      "completion_tokens": 333,
      "total_tokens": 11227
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.3 Results, Table 2",
          "reasoning": "The self-recorded dataset is used to train the infant cry recognition model from scratch without indication of pre-training on this dataset. The experimental results in Section 3.3 show training and evaluating the model on this dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.3 Results, Table 2",
          "reasoning": "The paper reports that fine-tuning the vision transformer model pretrained on ImageNet using the CRIED and self-recorded datasets achieves better performance, indicating these datasets are used for supervised fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 Results, Table 2 and Table 3",
          "reasoning": "The proposed self-recorded dataset is used to evaluate and validate the effectiveness of the proposed infant cry recognition method alongside the standard CRIED dataset."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3 Results, Figure 2 and accompanying text",
          "reasoning": "The self-recorded dataset is used for analyzing classification performance patterns such as confusion among classes, which informs understanding of infant cry recognition challenges."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11617,
      "completion_tokens": 623,
      "total_tokens": 12240
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The proposed self-recorded dataset and the CRIED dataset contain infant cry vocalizations without mention or indication of multiple human languages. The labeling is based on cry types or needs, not language."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not involve exactly two human languages; the cry data is unlabeled by language, focusing on infant cries and not spoken language."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The datasets are related to infant cries, which do not contain spoken English content or words; thus, they are not monolingual English datasets."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "Section 3.1",
          "reasoning": "The datasets contain infant cry sound recordings without linguistic content or spoken non-English language. The babies' cries are not linguistically classified."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets consist of audio recordings of infant cries with labels about crying type or need, with no programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Although the paper uses mathematical notation to describe features and models, the datasets themselves do not contain mathematical or logical symbolic data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The proposed self-recorded dataset and the CRIED dataset contain infant cry vocalizations, which are biological vocal signals, representing non-linguistic communication by infants, classified based on different expressed needs or emotions."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The datasets do not include any fictional or artificially created languages; only natural infant cry sounds are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The nature of the dataset is clearly specified as infant cry recordings; the language content is inherently non-linguistic infant vocalization, so it is not unknown but non-linguistic."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains vocalizations that represent infant communication via cries, which are forms of biological communication, thus not language-less."
        }
      }
    }
  },
  {
    "id": "xu22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8835,
      "completion_tokens": 171,
      "total_tokens": 9006
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Datasets and elsewhere",
          "reasoning": "The paper mentions a self-recorded infant cry dataset collected from volunteers recording babies' cries with mobile phones, but no code repository or link is provided for data collection, preprocessing, or annotation code related to this dataset. No details about sharing the code or tools used for data collection are given."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Datasets",
          "reasoning": "The paper describes the self-recorded dataset including number of samples, classes, recording conditions (mobile phones in indoor environments), sample rates, and annotation process based on moms' labeling experience, which provides some documentation on dataset creation. However, details about exact collection protocol, consent, or ethical approvals are not mentioned."
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6529,
      "completion_tokens": 127,
      "total_tokens": 6656
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 and 3.2",
          "Reasoning": "The paper introduces the VoxTube dataset consisting of over 4 million utterances from more than 5,000 speakers extracted from Creative Commons licensed YouTube videos. The data consists exclusively of audio segments extracted from these videos. These audios originate from human speech recorded in videos, thus are human generated recordings. The paper clearly states the audio extraction and processing pipeline, emphasizing audio only data and no use of visual information from videos."
        }
      ]
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7329,
      "completion_tokens": 204,
      "total_tokens": 7533
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.2",
            "reasoning": "The data collection and labeling for the new VoxTube dataset is performed using a fully automated unsupervised approach described in Section 3.2, involving a pipeline that extracts audio, extracts embeddings using a pre-trained model, applies clustering and filtering algorithms without human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No instructions for human annotators are described since the process is fully automated without human annotation involved."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No scoring rubrics or guidelines for human annotation are described or applicable as the labeling is done via clustering algorithms automatically."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "No annotation examples are provided since the labeling process is automatic and no manual labeling examples exist."
          }
        }
      ]
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8519,
      "completion_tokens": 420,
      "total_tokens": 8939
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication or description of quality assurance involving multiple human experts in the dataset collection or validation process."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors do not mention quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No quality assurance process involving multiple non-expert human annotators is described in the paper."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.2 Collection pipeline",
          "reasoning": "Quality assurance is partly done through the use of a pre-trained speaker embedding extractor model (ResNet48 trained on VoxCeleb2) applied for audio embedding extraction and subsequent filtering of clips by clustering embeddings to identify predominant speakers. The AI model is effectively used as a judge to select and label predominant speaker segments."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.3 Filtration",
          "reasoning": "The filtration pipeline uses hierarchical agglomerative clustering (HAC) with cosine similarity and defined thresholds, operating automatically without human intervention. This automatic clustering process acts as an algorithmic validation and filtration step to select relevant speaker segments."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is a clearly described and implemented quality assurance process based on AI models and automated clustering algorithms."
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8129,
      "completion_tokens": 436,
      "total_tokens": 8565
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The VoxTube dataset is collected from existing YouTube videos with CC BY licenses and not created entirely from scratch by human contributors. The data is not newly recorded or manually crafted speech content."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not represent content generated by AI or machine learning models; rather, it consists of speech recordings from real-world sources."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the data involves human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation process used to create the data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.1, 3.2",
          "reasoning": "The VoxTube dataset is collected by aggregating audio data from multiple existing YouTube videos under CC BY licenses. The collection pipeline extracts audio and associated embeddings but does not create new speech data; the data is collected from existing open-source media."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.2, 3.3",
          "reasoning": "The dataset undergoes an automated unsupervised labeling process involving clustering, embedding extraction with a pre-trained speaker recognition model, filtering, and duplicate removal. These are transformations and adaptations applied on the collected raw audio data, making the dataset derived from the original YouTube sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and generation process are clearly described in the paper sections detailing collection and filtration."
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8647,
      "completion_tokens": 238,
      "total_tokens": 8885
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.3.3 (Training), Section 5 (Results)",
          "reasoning": "The VoxTube dataset is used to train speaker recognition models from scratch with a ResNet48 architecture, as described in Section 4.3.3 where training setups are given, and Section 5 where the trained models using VoxTube data show performance results. The models are trained from scratch using VoxTube data alone as well as in combination with VoxCeleb2."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9370,
      "completion_tokens": 439,
      "total_tokens": 9809
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 3.1 Description",
          "reasoning": "The VoxTube dataset contains utterances pronounced in more than 10 languages, including English, Russian, Spanish, and Portuguese among others. This is explicitly mentioned in the abstract and Section 3.1, indicating the dataset includes multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains speech audio data and does not include programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on audio utterances for speaker recognition and does not contain mathematical or logical symbolic expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset comprises human speech audio and does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly reported and detailed."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries clearly contain spoken human languages."
        }
      }
    }
  },
  {
    "id": "yakovlev23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6588,
      "completion_tokens": 194,
      "total_tokens": 6782
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 3 (Description and Collection pipeline) and Footnote 1",
          "reasoning": "The paper provides a URL (https://github.com/IDRnD/VoxTube/) where the VoxTube dataset can be downloaded, indicating availability of data and possibly the related code. The description in Section 3 outlines the data collection and filtering pipeline in sufficient detail, suggesting that code related to data collection and preprocessing is available at the given project webpage/repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (Description and Collection pipeline and Filtration subsections)",
          "reasoning": "The paper thoroughly documents the dataset creation process, including metadata filtering, audio extraction, embedding extraction using a pre-trained model, hierarchical clustering filtration steps, duplicates removal, and statistics of the dataset collected. The detailed step-wise description provides transparency and completeness about the dataset creation, which supports reproducibility."
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7422,
      "completion_tokens": 157,
      "total_tokens": 7579
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2",
          "Reasoning": "The new dataset's audio involves speech segments narrated by two standard Mandarin speakers (human recorded) and two synthesized speech streams generated by shifting the fundamental frequency of the original speech using Adobe Audition software, thus comprising both human generated and model generated audio data."
        },
        {
          "Modality": "time series",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3",
          "Reasoning": "The dataset includes EEG recordings collected from human subjects via 64-channel EEG amplifier during the auditory attention task, resulting in time series data that are human generated (recorded from human participants)."
        }
      ]
    }
  },
  {
    "id": "yan24b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8222,
      "completion_tokens": 316,
      "total_tokens": 8538
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2 (Methods) and Section 3 (Results)",
            "reasoning": "The data was collected from human participants during the four-talker auditory attention experiment in an anechoic chamber as described in the methods sections. EEG data was recorded from human subjects attending to one of four talkers. This indicates human subjects provided annotations (attended talker labels), and the study likely involved domain experts designing and conducting the experiment. No indication of multiple annotators or crowdsourcing is given; thus, the annotators can be categorized as single human experts (researchers) overseeing the annotation process."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 (Stimuli and experimental procedure)",
            "reasoning": "Subjects were given instructions to attend to the speech from one loudspeaker while ignoring the others, with visual feedback and relevant instructions presented on a monitor. This indicates instructions for the annotation (attended talker identification) were explicitly provided to human subjects."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not mention any scoring rubrics or detailed criteria to guide annotation other than general instructions to attend to certain talkers. The evaluation metrics are described for decoding accuracy but not for annotator guidance."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No example annotations or annotation examples are described as part of the data collection or annotation protocols in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "yan24b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9412,
      "completion_tokens": 284,
      "total_tokens": 9696
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance process performed by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information in the paper regarding multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of quality assurance conducted by multiple non-expert humans in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While AI models are used for decoding tasks (ASAD models), the paper does not indicate that AI models performed quality assurance on the annotations or dataset content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe automated verification processes or algorithmic rule-based techniques for quality assurance on dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new four-talker EEG dataset but does not describe any quality assurance process applied to validate or verify the dataset annotations or contents. No QA procedures or checks are documented in the text."
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9022,
      "completion_tokens": 466,
      "total_tokens": 9488
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The paper states that the new AAD database was created by recording EEG data from human subjects attending to four-talker speech scenarios constructed using speech segments narrated by human speakers and some pitch-shifted versions. The stimuli were based on narrations from the book 'Twenty Thousand Leagues under the Sea' read by human speakers, with pitch shifting applied to create variations. The EEG data was recorded from human listeners under controlled experimental conditions. Thus, the data (EEG responses and stimuli combinations) were newly created by human contributors from scratch, not derived or translated from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not simply aggregated or collected from existing sources without modifications; it involved recording new EEG data under experimental conditions."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "Although the speech stimuli were based on original narrations, two of the four talkers' speeches were created by shifting the fundamental frequency (F0) of the original speech with speech synthesis software, which is a form of data derived by transformation of existing speech data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9540,
      "completion_tokens": 443,
      "total_tokens": 9983
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 2.8 and 3.3",
          "reasoning": "The newly introduced four-talker EEG database is used to train deep neural network models (e.g., DenseNet-3D) from scratch for auditory spatial attention detection (ASAD). Section 2.8 describes the training of models on the new dataset using 5-fold cross-validation, and Section 3.3 reports the performance results, indicating the dataset's use for model training from random initialization."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 2.8 and 3.3",
          "reasoning": "The DenseNet-3D model is initialized (fine-tuned) by bootstrapping weights from a pre-trained DenseNet-2D model prior to training on the new four-talker EEG dataset. This supervised fine-tuning approach uses the dataset to adapt a pre-trained model to the four-talker scenario, as described in Sections 2.8 and 3.3."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.3 and 3.2",
          "reasoning": "The new four-talker EEG dataset is used to evaluate and benchmark the performance of multiple ASAD models and stimulus reconstruction methods. Section 3.2 reports decoding accuracy for stimulus reconstruction, while Section 3.3 presents ASAD model accuracies, indicating the dataset's role in performance evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is used to analyze neural responses via temporal response functions (TRFs), comparing attended and unattended speech representations. Section 3.1 details this analysis, highlighting the dataset\u2019s role in investigating cortical processing characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10263,
      "completion_tokens": 547,
      "total_tokens": 10810
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset only involves speech stimuli in one language, Mandarin Chinese, with multiple talkers but no mention of multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech stimuli from speakers all speaking in Mandarin Chinese, not two different languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2.2",
          "reasoning": "The speech corpus used is a Chinese translation of the book 'Twenty Thousand Leagues under the Sea', and the speakers are standard Mandarin speakers, indicating the dataset is not English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The dataset uses speech stimuli from four talkers all speaking Mandarin Chinese (a non-English language). This is explicitly stated in the description of audio materials."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the implementation code is available on Github, the dataset itself does not contain entries of programming languages but rather EEG and audio data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "Mathematical expressions and notation appear in the paper describing methods, but the dataset entries themselves consist of speech and EEG signals, not symbolic mathematical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of human speech and EEG signals, rather than biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "There is no indication that the dataset includes fictional or artificially constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language and speech content are clearly specified as Mandarin Chinese."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes natural human language speech content."
        }
      }
    }
  },
  {
    "id": "yan24b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7481,
      "completion_tokens": 128,
      "total_tokens": 7609
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "End of Section 1 (Introduction)",
          "reasoning": "The paper explicitly states that both the implementation code and the new four-talker database are available on public repositories, providing URLs to GitHub and Zenodo."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Methods)",
          "reasoning": "The paper provides a thorough description of the data collection environment, experimental procedure, stimuli, EEG data acquisition and processing, speech envelope extraction, and stimulus reconstruction. The detailed explanation of the experimental setup and preprocessing procedures indicates comprehensive documentation of dataset creation."
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 10412,
      "completion_tokens": 177,
      "total_tokens": 10589
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 4 Experimental results; Section 5 Conclusions",
          "Reasoning": "The authors explicitly introduce a new large-scale dataset called Audioset-TSD based on the Audioset dataset for target sound detection (TSD). Audioset is a human-curated audio event dataset, and the TSD version involves audio clips annotated for target sounds within mixtures. The data modality is audio since it deals with sound event detection and reference/mix audio clips. The dataset is human generated because it is derived from Audioset, which is based on human-labeled audio data. The paper clearly states that they establish a new dataset called Audioset-TSD in addition to using the pre-existing URBAN-TSD."
        }
      ]
    }
  },
  {
    "id": "yang22e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 11212,
      "completion_tokens": 241,
      "total_tokens": 11453
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 1 Abstract, Section 4.2 Experimental results",
            "reasoning": "The paper states that the Audioset-TSD dataset is established by the authors based on Audioset; however, there is no mention of human annotation or manual labeling processes. The dataset appears to be constructed automatically from the existing Audioset using algorithmic procedures, as there is no description of human annotators or annotation protocols."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "The paper does not provide any description of detailed annotation instructions for the Audioset-TSD dataset, nor does it mention any annotation guidelines or protocols."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "There is no information in the paper indicating that scoring rubrics or detailed criteria were used or provided for annotating the Audioset-TSD dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No specific section",
            "reasoning": "The paper does not present annotation examples or any similar illustrative material related to the Audioset-TSD dataset."
          }
        }
      ]
    }
  },
  {
    "id": "yang22e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 12402,
      "completion_tokens": 276,
      "total_tokens": 12678
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance of dataset annotations was performed by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of multiple human experts validating or annotating the datasets introduced."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance process conducted by a single non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper lacks details about multiple non-experts performing quality assurance on the datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used as a judge for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated verification or rule-based quality assurance process for the datasets is described in the paper."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces two datasets (UrbanSound-based URBAN-TSD and Audioset-based Audioset-TSD) but does not document any quality assurance process, annotation methodology, or validation procedure for the dataset annotations or contents."
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 12012,
      "completion_tokens": 396,
      "total_tokens": 12408
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental results and Abstract",
          "reasoning": "The paper establishes a new dataset named Audioset-TSD based on the existing Audioset dataset (reference [3]). This indicates that the dataset is collected or aggregated from an existing source (Audioset) without evidence of original content creation from scratch. Similarly, the URBAN-TSD dataset is based on UrbanSound dataset [18] which is also a prior dataset. Hence, the datasets introduced are collated from pre-existing datasets."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental results and Abstract",
          "reasoning": "The new datasets (URBAN-TSD and Audioset-TSD) are built based on existing datasets (UrbanSound and Audioset). Though they are based on existing sources, the paper mentions that they are established (implying some modifications, such as labeling for target sound detection task). Therefore, these datasets are derived from existing datasets with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin and preparation of the datasets are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 12530,
      "completion_tokens": 339,
      "total_tokens": 12869
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 (Experimental results)",
          "reasoning": "The authors use the new Audioset-TSD dataset to train their RaDur model from scratch for target sound detection, as shown in the experimental comparisons with TSDNet. The dataset is used directly to train the detection model with no indication of prior pre-training on other datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.2 (Experimental results)",
          "reasoning": "The new datasets URBAN-TSD and Audioset-TSD are used to evaluate and benchmark the performance of the proposed RaDur method against previous approaches by reporting segment-based and event-based F-scores and average precision. This confirms their role in evaluation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Experimental results) and Section 4.3 (Ablation studies)",
          "reasoning": "The URBAN-TSD and Audioset-TSD datasets are used to analyze the effect of event duration on detection performance and to conduct ablation studies on hyper-parameters, demonstrating the datasets' use for detailed analytical purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 13253,
      "completion_tokens": 588,
      "total_tokens": 13841
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper introduces two new datasets: URBAN-TSD and Audioset-TSD, both based on existing UrbanSound and Audioset datasets, respectively. There is no mention or indication that these datasets contain data in multiple human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the proposed datasets have exactly two human languages. The datasets pertain to sound event detection, not multilingual language data."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experimental setup and evaluation metrics",
          "reasoning": "The datasets are based on UrbanSound and Audioset, which are large-scale audio event datasets primarily collected and labeled in English contexts. Given no mention of other human languages and the use of English references (e.g., references to UrbanSound, Audioset, and sound labels like 'chop', 'bouncing'), the datasets can be considered monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of any non-English language exclusive datasets."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of audio recordings and labels related to sound events; no code or programming language data entries are included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets themselves contain audio data and annotations, not mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the datasets focus on environmental sound events, they do not specifically include biological sequences or non-human communication such as DNA or chemical signaling."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No constructed languages are involved in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) of the datasets entries are explicitly known or implied to be English in context."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets contain human language content in the form of sound event labels and annotations, thus not applicable to 'N/A' category."
        }
      }
    }
  },
  {
    "id": "yang22e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 10471,
      "completion_tokens": 193,
      "total_tokens": 10664
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 5, Conclusions; Footnote 1 at the end of the paper",
          "reasoning": "The paper explicitly states in the Conclusions section that the source code and dataset of this work have been released, and provides a GitHub URL (https://github.com/yangdongchao/RaDur) indicating public availability of the code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 Dataset and Experimental Setup paragraphs and throughout Section 4 (Experiments)",
          "reasoning": "The paper documents the dataset creation process by explaining that they built two datasets: URBAN-TSD (based on UrbanSound) and Audioset-TSD (based on Audioset). They describe the datasets' construction and refer to prior datasets upon which these were based. Though details are relatively brief, the dataset curation and processing are described in the Experiments section."
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7097,
      "completion_tokens": 136,
      "total_tokens": 7233
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 4 Experiments; Footnote 1",
          "Reasoning": "The paper introduces a new ASR error correction dataset by applying internal ASR and Text-to-Speech (TTS) engines to a single-turn dialogue dataset called TOP. This process involves human-generated original text data (the TOP dataset) and model-generated ASR outputs (via internal ASR and TTS systems). Thus, the dataset consists of text modality data combining human-generated original transcriptions and model-generated ASR transcriptions for ASR error correction tasks."
        }
      ]
    }
  },
  {
    "id": "yang22g_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7897,
      "completion_tokens": 234,
      "total_tokens": 8131
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 1 Introduction, Section 4 Experiments",
            "reasoning": "The new dataset is created by applying internal ASR and Text-to-Speech (TTS) engines to the existing TOP single-turn dialogue dataset, effectively simulating ASR output errors automatically rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section describing annotation instructions for the new dataset",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators, as the dataset is generated via automated ASR and TTS engines application, not manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No explicit section describing scoring rubrics for annotations",
            "reasoning": "Similarly, no scoring rubrics are described since the dataset generation process is automatic and does not involve human raters requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No appendix or section providing annotation examples",
            "reasoning": "No examples of annotation guidelines or examples are provided in the paper, as manual annotation is not part of the dataset creation process."
          }
        }
      ]
    }
  },
  {
    "id": "yang22g_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9087,
      "completion_tokens": 384,
      "total_tokens": 9471
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert for the introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about multiple human expert annotators performing quality assurance on the introduced dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper lacks any mention of quality assurance conducted by a single human non-expert for the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication or description provided about multiple human non-expert annotators performing QA on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report using an AI model for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the dataset is generated using internal ASR and TTS engines (Section 4), no explicit automated quality assurance or verification process for the dataset is described."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new ASR error correction benchmark dataset constructed by applying internal ASR and TTS engines on the TOP dialogue dataset (Section 1, Section 4). However, no information or documentation is provided regarding any quality assurance or validation process for this dataset."
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8697,
      "completion_tokens": 378,
      "total_tokens": 9075
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any dataset was created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any part of the dataset was created by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No dataset produced by machine translation is mentioned."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced is not described as merely collected or aggregated from existing sources without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4, paragraph 1 and 5 Conclusions",
          "reasoning": "The paper states that the authors apply internal ASR and Text-to-Speech (TTS) engines to a single-turn dialogue dataset (TOP) to produce a benchmark ASR error correction dataset. This indicates that the new dataset is derived by transforming existing data (TOP dataset) through ASR and TTS systems to simulate errors and create paired data for correction tasks."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly stated and documented in the paper."
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9215,
      "completion_tokens": 286,
      "total_tokens": 9501
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments)",
          "reasoning": "The newly produced ASR error correction dataset based on the TOP single-turn dialogue dataset is used for supervised fine-tuning of the proposed constrained decoder models to evaluate their performance for ASR error correction tasks."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4 and 4.2 (Experiments and Results Analysis)",
          "reasoning": "The dataset is also used as a public benchmark dataset to evaluate and compare the effectiveness and inference speed of the proposed models against existing state-of-the-art methods."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 (Results Analysis) and Table 5",
          "reasoning": "The dataset is used for analyzing trends in error types and the effectiveness of corrections over various error categories such as grammatical errors, similar sound errors, and entity errors."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9938,
      "completion_tokens": 480,
      "total_tokens": 10418
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4, Dataset description and Table 2",
          "reasoning": "The newly created dataset is based on the TOP dataset, described in Section 4, which focuses on navigation and events in English. The ASR output and reference texts are in English as evidenced by the example sentences (e.g., 'cheapest airfare from tacoma to orlando'). There is no mention of other languages covered in the new dataset, indicating it is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains transcribed spoken language text data; no code or programming language data entries are involved."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset content is related to ASR error correction on natural language text and does not include mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological or non-human communication data such as DNA sequences or animal signals are described in the dataset."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset includes any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset definitely contains human language text."
        }
      }
    }
  },
  {
    "id": "yang22g_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7156,
      "completion_tokens": 199,
      "total_tokens": 7355
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 1",
          "reasoning": "The abstract states that the code and datasets are made publicly available, providing a GitHub link (https://github.com/yangjingyuan/ConstDecoder). This implies that code related to the dataset creation, including the process of generating the ASR error correction dataset from a single-turn dialogue dataset using internal ASR and TTS engines, is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 1 and Section 4",
          "reasoning": "The paper documents the dataset creation process in Section 1 and Section 4, where it details the methodology to produce the ASR error correction dataset from the TOP dataset using internal TTS and ASR engines. Section 4 describes the datasets, specifically mentioning the creation process of the new TOP dataset variant with the applied ASR and TTS. The paper thus provides sufficient documentation on the dataset creation process."
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 11406,
      "completion_tokens": 195,
      "total_tokens": 11601
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2.1; Section 2. Dataset Description",
          "Reasoning": "The MagicData-RAMC dataset consists of 180 hours of Mandarin conversational speech data recorded from native speakers using mainstream smartphones at 16 kHz sampling rate, as explained in Section 2.1 and the Abstract. Since recordings are made by human participants speaking and using human-operated mobile devices, the audio data is human generated."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 2 Dataset Description",
          "Reasoning": "The dataset includes accurate transcriptions and annotations such as voice activity timestamps and speaker demographic information, all manually labeled and proofed. The transcriptions are created by human annotators as described in Section 2 and the Abstract, indicating human-generated text data."
        }
      ]
    }
  },
  {
    "id": "yang22h_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 12206,
      "completion_tokens": 188,
      "total_tokens": 12394
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 2.1 Collection setup",
            "reasoning": "Section 2.1 states that all speech data are manually labeled using a platform built by Magic Data Technology Co., Ltd., with each recording labeled by one annotator and transcriptions proofed by professional inspectors, indicating human experts performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "The paper does not explicitly mention detailed annotation instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2",
            "reasoning": "There is no explicit description of scoring rubrics or rubric-based quality criteria for annotations in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "No examples or sample annotations are presented or discussed in the paper as part of annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "yang22h_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 13396,
      "completion_tokens": 494,
      "total_tokens": 13890
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that 'each recording is labeled by one annotator' and that 'transcriptions are proofed by professional inspectors,' but does not specify that annotators or inspectors are subject matter experts or belong to the target demographic. Therefore, there is no evidence that the QA was conducted by a single human expert."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper mentions that transcriptions were proofed by professional inspectors, it does not state that these inspectors or multiple annotators are subject matter experts or members of the target demographic. There is no explicit mention of multiple human experts performing QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": true,
          "reference": "Section 2.1 Collection setup",
          "reasoning": "The dataset was annotated with each recording labeled by one annotator (Section 2.1). There is no indication that the annotator was an expert, so it is reasonable to consider the annotator as a non-expert individual performing the initial annotation."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Collection setup",
          "reasoning": "Transcriptions were proofed by professional inspectors to ensure labeling and segmentation quality (Section 2.1). Although described as professional inspectors, the paper provides no evidence these inspectors were subject matter experts or part of the target demographic. Hence, supervision by multiple human non-experts is indicated."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that AI models were used for quality assurance of dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of automated verification or rule-based techniques used for quality assurance of the dataset annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a clear manual annotation and proofreading process; thus, QA was applied and documented."
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 13006,
      "completion_tokens": 376,
      "total_tokens": 13382
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Collection setup",
          "reasoning": "The dataset MagicData-RAMC consists of 180 hours of conversational speech data manually recorded from native Mandarin speakers using smartphones. The speech and transcriptions are manually labeled and proofed, indicating that the data are original human-produced conversational speech content created from scratch and not derived from existing datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any AI or machine learning model generated or synthesized the speech data or transcripts."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that the data were created by translating content from other languages using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that machine translation was used to generate the data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated or collected from existing sources. Instead, it was recorded freshly from human speakers."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data were adapted or modified from existing datasets; the content is original human conversational recordings."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes the data origin and collection process."
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 13524,
      "completion_tokens": 263,
      "total_tokens": 13787
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is used as part of the training data combined with MAGICDATA-READ to train an end-to-end automatic speech recognition (ASR) model from scratch, demonstrating supervised training use."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3",
          "reasoning": "The dataset is used to evaluate baseline systems on ASR, speaker diarization, and keyword search tasks to benchmark performance."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 2.4 and 2.5",
          "reasoning": "The paper provides detailed analysis and statistics of the dataset's structure, speaker information, utterance statistics, and comparison with other datasets, indicating use for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 14247,
      "completion_tokens": 588,
      "total_tokens": 14835
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 2 and throughout the paper",
          "reasoning": "The MagicData-RAMC dataset contains only Mandarin Chinese conversational speech data; there is no indication of the presence of more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 2 and dataset description",
          "reasoning": "The dataset consists solely of Mandarin Chinese speech data, with no mention of exactly two languages involved."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 2 and dataset description",
          "reasoning": "The dataset is in Mandarin Chinese and does not contain English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 2 Dataset Description; Section 2.2 Speaker information",
          "reasoning": "MagicData-RAMC is described as a high-quality rich annotated Mandarin conversational speech dataset, collected from native Mandarin Chinese speakers, and all transcriptions and annotations are in Mandarin Chinese only."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "No section indicates presence of code",
          "reasoning": "The dataset pertains to human conversational speech and transcripts; there is no mention of programming languages or code snippets included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "No mention within the dataset description",
          "reasoning": "The dataset contains conversational speech in Mandarin, with manual transcriptions, voice activity timestamps, topics, and speaker information but no mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "No mention of biological or non-human communication data",
          "reasoning": "The dataset involves only human Mandarin speech and transcripts, with no non-human communication or biological sequences included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "No indication in any section",
          "reasoning": "The dataset consists of spontaneous Mandarin Chinese dialogs; no constructed or fictional languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "Dataset description clearly states Mandarin language",
          "reasoning": "The language of the dataset is explicitly specified as Mandarin Chinese; it is not unknown or unspecified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset clearly contains conversational speech data and transcriptions, thus includes language data."
        }
      }
    }
  },
  {
    "id": "yang22h_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 11465,
      "completion_tokens": 168,
      "total_tokens": 11633
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or mention in the paper",
          "reasoning": "The paper does not provide any link, URL, or reference to publicly available code repositories or scripts related to the data collection, preprocessing, or generation processes for the MagicData-RAMC dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (Dataset Description) and subsections 2.1 to 2.5",
          "reasoning": "The paper provides a thorough description of the dataset creation process including collection setup (devices, environments, sampling rate), labeling procedures (manual annotation, proofreading, segmentation), speaker information, partitioning details, utterance statistics, and topic labels. This comprehensive documentation is contained mainly in Section 2 and its subsections, providing transparency about the dataset construction."
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7801,
      "completion_tokens": 172,
      "total_tokens": 7973
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Mandarin Lombard Corpus)",
          "Reasoning": "The corpus contains speech recordings of 28 native Mandarin speakers reading 'Grid'-like sentences at four noise levels. This data is audio modality as it consists of audio recordings, and it is human generated since it was recorded from human speakers under controlled conditions as described in the paper."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3 (Mandarin Lombard Corpus)",
          "Reasoning": "The corpus includes word and phoneme level transcriptions obtained via forced alignment based on human-generated speech data. These transcriptions are text modality and originate from human speech content, thus human generated."
        }
      ]
    }
  },
  {
    "id": "yang22i_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8601,
      "completion_tokens": 304,
      "total_tokens": 8905
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 3, Mandarin Lombard Corpus",
            "reasoning": "The corpus was created by recruiting 28 native Mandarin speakers, who were students at Wuhan University, to read scripted sentences. These annotators are native speakers but not described as experts or professionals conducting annotation beyond speech recording. Their task involved reading sentences rather than complex annotation, implying multiple human non-experts performed the data collection/annotation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1.2, Speaker recruitment",
            "reasoning": "The paper states that speakers were required to take a hearing test and were instructed to read sentence sets following researcher directions (e.g., taking breaks, re-recording errors). This indicates that instructions were given to the annotators (speakers) for the speech recording process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3, Mandarin Lombard Corpus",
            "reasoning": "No indication of any scoring rubrics or formal rubric-based evaluation guidelines for the annotators is provided. The only evaluation described relates to STOI score for analysis, not annotation scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire paper",
            "reasoning": "There are no explicit annotation examples or exemplars provided for the annotators (speakers) in the paper. The speech was scripted with predefined sentences, but no examples of the annotation process or detailed exemplar annotations are shown."
          }
        }
      ]
    }
  },
  {
    "id": "yang22i_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9791,
      "completion_tokens": 336,
      "total_tokens": 10127
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by a single human expert on the dataset or its annotations."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper indicating that multiple human experts conducted quality assurance on the dataset or annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe QA conducted by a single human annotator without subject matter expertise."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although multiple speakers contributed speech data, the paper does not indicate that multiple non-expert annotators performed quality assurance on the dataset or annotations."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of an AI model for quality assurance of the dataset or annotations."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3, Paragraph 2 (use of Montreal Forced Aligner)",
          "reasoning": "The authors used the Montreal Forced Aligner tool to produce phonetic transcriptions with word and phoneme boundaries automatically. This represents an automatic process applied as part of the annotation and verification pipeline for the dataset. There is no mention of manual correction or human verification of the alignments, so this automatic process constitutes the quality assurance mechanism for the annotations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Although automated alignment was used for transcription, there is evidence of some quality assurance via this automatic process, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9401,
      "completion_tokens": 415,
      "total_tokens": 9816
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 and Section 3",
          "reasoning": "The paper clearly states that they recruited native Mandarin speakers who read newly created 'Grid'-like sentences in different background noise conditions and recorded these utterances. The sentences were structured and randomly generated as new content for this corpus, and the speech data was collected fresh from human contributors, not adapted or derived from existing datasets. This process is described in Section 2.1 (Sentence selection, Speaker recruitment, Recording setup) and Section 3 (Mandarin Lombard Corpus)."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was collected from human speakers reading sentences; there is no indication that data was generated or synthesized by AI or machine learning models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the data was produced by translating content from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication that the data was produced by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus was not collected or aggregated from existing sources; it was recorded from scratch."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not adapted or derived from an existing dataset; it is original recordings, not transformations of prior data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and method of generation are explicitly documented."
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9919,
      "completion_tokens": 230,
      "total_tokens": 10149
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4 (Analysis of Lombard Effect)",
          "reasoning": "The newly introduced Mandarin Lombard corpus is primarily used for analyzing the Lombard effect characteristics, such as vowel duration, formants, and acoustic parameters, comparing the effects in Mandarin versus English. The paper includes detailed statistical analysis (Section 4) to characterize speech under different noise levels, without any mention of using the dataset for training or evaluation of ML models."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10642,
      "completion_tokens": 590,
      "total_tokens": 11232
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced, the Mandarin Lombard Grid, contains speech data only in Mandarin Chinese and no other human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include exactly two human languages; it is solely Mandarin Chinese speech data."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content; it is focused on Mandarin Chinese."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3, \"Mandarin Lombard Corpus\", and throughout the paper notably in the Abstract and Introduction.",
          "reasoning": "The newly introduced dataset is a corpus of Mandarin Chinese speech containing Lombard speech styles induced by noise at different SPLs. The paper explicitly states it is a Mandarin Lombard corpus and details the speakers as native Mandarin speakers. Thus, the dataset is monolingual with the language being Mandarin Chinese, which is a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain any programming or structured code-related content. While tools like Montreal Forced Aligner and Praat are used for processing, the dataset itself is audio and transcriptions of Mandarin speech."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are speech recordings and their transcriptions; although statistical tests and acoustic parameters are analyzed in the paper, the dataset does not contain mathematical or formal logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is composed of human speech in Mandarin. It does not contain biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of natural Mandarin Chinese speech and does not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is specified clearly as Mandarin Chinese, so it is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains language data in the form of Mandarin speech and transcriptions."
        }
      }
    }
  },
  {
    "id": "yang22i_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7860,
      "completion_tokens": 210,
      "total_tokens": 8070
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and Section 3 (Mandarin Lombard Corpus)",
          "reasoning": "The paper explicitly states that the corpus is being made freely available for download at https://github.com/ASP-WHU/Mandarin-Lombard-Grid, which is a public GitHub repository hosting the dataset. Although the paper does not detail all code related to data collection and preprocessing, the availability of the corpus and the linked repository suggests that at least data and possibly related materials are openly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2 (Lombard Style Classification) and 3 (Mandarin Lombard Corpus)",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including sentence selection, speaker recruitment, recording setup, noise conditions, number of speakers and utterances, and transcription methods (e.g., forced alignment). It also describes the noise stimulation and recording environment, and dataset contents. This comprehensive documentation supports reproducibility."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8590,
      "completion_tokens": 173,
      "total_tokens": 8763
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 5.1 Corpus Details",
          "Reasoning": "The authors introduce a new speech command recognition dataset collected for 41 Mandarin commands targeting air conditioner control. The training set consists of approximately 95k utterances recorded manually on smartphones in near-field quiet conditions from 1744 different speakers (human generated). Additionally, the dataset is augmented by applying Room Impulse Response simulation using pyroomacoustics to increase data by 10 times via simulated reverberation and added noise, which is model generated data. Furthermore, 400k negative samples from human voice sources and environment noise are added. Thus, the dataset consists of both human recorded audio and simulated augmented audio. This is explicitly described in Section 5.1."
        }
      ]
    }
  },
  {
    "id": "yang22m_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9390,
      "completion_tokens": 268,
      "total_tokens": 9658
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 5.1",
            "reasoning": "The paper describes the dataset as collected from 1744 different speakers recording speech commands. This indicates multiple humans (non-expert speakers) were involved in providing the speech utterances, but there is no mention of expert annotation or labeling processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 5.1",
            "reasoning": "There is no information in the paper indicating that detailed annotation instructions were provided to the speakers or annotators related to the new dataset. The corpus is described as recorded and augmented data rather than manually annotated with guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 5.1",
            "reasoning": "The paper does not mention any scoring rubrics or evaluation guidelines provided to annotators for labeling or classification within the new dataset. The dataset appears to be collected raw speech commands rather than labeled via annotators using rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 5.1",
            "reasoning": "No annotation examples or guidelines are provided or described for the dataset collection or labeling process in the paper. The focus is on data collection and augmentation, with no mention of sample annotations or examples for annotation."
          }
        }
      ]
    }
  },
  {
    "id": "yang22m_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10580,
      "completion_tokens": 347,
      "total_tokens": 10927
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided about multiple human experts performing quality assurance on the collected dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No description of multiple non-expert annotators performing quality assurance is provided in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that an AI model was used to perform quality assurance on the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No automated quality assurance process or algorithmic verification is described for the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper introduces a new speech command dataset collected from 1744 speakers and records evaluation data but does not describe any quality assurance process or procedure for validating the dataset annotations or content."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10190,
      "completion_tokens": 450,
      "total_tokens": 10640
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 5.1 Corpus Details",
          "reasoning": "The authors collected a new Mandarin speech command dataset specifically for their experiments, containing 41 commands with recordings from 1744 speakers and about 95k utterances. The dataset is described as recorded on smart phones in near-field quiet environments, indicating human contributors recorded original speech data specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not generated by any AI or machine learning models; it consists of human-recorded utterances and augmented data created using room impulse responses and noise addition, not synthetic speech generation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was produced by translating speech or text from another language by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as collected or aggregated from existing sources without modification. The authors specifically state that the dataset is collected for the SCR task, not compiled from other data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 5.1 Corpus Details",
          "reasoning": "The training corpus is augmented by applying room impulse response simulation and adding noise of various kinds to the original human-recorded utterances, which is a common data augmentation practice that transforms the original data to enhance robustness."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data collection and augmentation process is clearly documented in Section 5.1, so the origin of the dataset is specified."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10708,
      "completion_tokens": 436,
      "total_tokens": 11144
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the newly collected dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.2",
          "reasoning": "The authors collected a sizable speech command dataset specifically for the SCR task and trained their acoustic models from scratch using this dataset, as described in 'Corpus Details' and 'Experimental Setups'."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using a pre-trained model or fine-tuning models supervised using the collected dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RLHF methods leveraging this dataset in the paper."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5.1 and 5.5",
          "reasoning": "The collected evaluation set is explicitly used for evaluating and benchmarking the SCR system's performance and confusion errors."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not primarily used just to analyze trends or patterns but rather for training and evaluation purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as serving as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the practical usage of the newly collected dataset for training and evaluation."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11431,
      "completion_tokens": 499,
      "total_tokens": 11930
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "Section 5.1",
          "reasoning": "The dataset introduced contains only Mandarin commands; no indication of multiple languages is given."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "Section 5.1",
          "reasoning": "No evidence or mention of exactly two languages in the dataset."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "Section 5.1",
          "reasoning": "The dataset is described as consisting of Mandarin commands; no English-only dataset is introduced."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The dataset contains 41 Mandarin commands only, indicating a monolingual dataset in a single non-English language, Mandarin."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset consists of speech utterances representing Mandarin commands, no programming or structured code content is included."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "While the paper contains mathematical formulas describing the method, the dataset itself does not contain mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset does not include biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No constructed or fictional languages are mentioned in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The language of the dataset is clearly specified as Mandarin; thus, not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken Mandarin command utterances, so it does contain language content."
        }
      }
    }
  },
  {
    "id": "yang22m_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8649,
      "completion_tokens": 141,
      "total_tokens": 8790
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No code repository or link mentioned in the paper",
          "reasoning": "The paper does not provide any link or mention of code availability related to the dataset construction, data preprocessing, or generation. There is no indication that such code is publicly accessible."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 5.1 Corpus Details",
          "reasoning": "The paper dedicates Section 5.1 to describe the dataset collected by the authors, detailing the corpus size, number of commands, recording environment, speaker variation, data augmentation techniques, and negative sample sources. This documentation provides transparency about the dataset creation process used for experiments."
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7329,
      "completion_tokens": 154,
      "total_tokens": 7483
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 3.1, especially 3.1.1 and 3.1.2 (Proposed Method: Relation-based Counterfactual Data Augmentation)",
          "Reasoning": "The new dataset entries are generated by the authors using automatic data augmentation methods. Token-level augmentation modifies nouns with substitutions based on WordNet to create counterfactual sentence pairs of different classes, and sentence-level augmentation generates sentences via a generator model iteratively trained and filtered by model confidence. The resulting augmented sentence pairs for NLI tasks are thus model-generated synthetic data, not manually written by humans, as explicitly described in Section 3.1."
        }
      ]
    }
  },
  {
    "id": "yang23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8129,
      "completion_tokens": 263,
      "total_tokens": 8392
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 3.1 (Proposed Method)",
            "reasoning": "The paper describes automatic token-level and sentence-level data augmentation methods using spaCy, WordNet substitutions, and a trained generator model without any mention of human annotators generating new data for the augmentation. Thus, the annotation (in this case, generation and labeling of augmented data) is performed automatically."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly provided",
            "reasoning": "There is no explicit mention of detailed annotation instructions for annotators in the process of generating or labeling the augmented data, since data augmentation is automated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly provided",
            "reasoning": "The paper does not describe any scoring rubrics or criteria for annotators to follow, as the augmented data labeling is derived programmatically using lexical relations (e.g., synonym, antonym) and model confidence thresholds."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1 and Table 2",
            "reasoning": "The paper provides concrete examples of generated sentences for different classes in Table 2 and illustrates augmentation procedures in Figure 1, effectively serving as examples of the augmentation and labeling process."
          }
        }
      ]
    }
  },
  {
    "id": "yang23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9319,
      "completion_tokens": 342,
      "total_tokens": 9661
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the newly introduced augmented dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that multiple human experts performed quality assurance on the dataset created by the authors."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single human annotator without expertise on the augmented dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any involvement of multiple human non-expert annotators in quality assurance for the new dataset."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1.2 Sentence-level Data Augmentation",
          "reasoning": "The paper states that for sentence-level data augmentation, a classifier model and a generator model are trained, and confidence-based filtering using model confidence threshold is applied to select generated samples. This process involves using AI models to judge the quality of the generated data, thereby performing quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While confidence-based filtering is mentioned, the paper does not describe an automated verification of code or formulas as a quality assurance method for the dataset; rather, QoA is through model confidence filtering, which is AI model based."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes an AI model based filtering process for augmented data, indicating QA is present. Therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8929,
      "completion_tokens": 509,
      "total_tokens": 9438
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not introduce any new dataset created entirely by human annotators from scratch. Instead, it uses existing datasets such as CF-SNLI and SNLI for training and evaluation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 3.1 (Proposed Method), specifically Sections 3.1.1 (Token-level Data Augmentation) and 3.1.2 (Sentence-level augmentation process described in Figure 1b and text)",
          "reasoning": "The authors generate new counterfactual sentence pairs using automatic augmentation methods. Token-level augmentation applies substitutions based on WordNet to generate sentences with altered relations, while sentence-level augmentation uses a generator model trained on the original data to generate new sentences for each class, filtered by model confidence. This data is newly generated by AI/machine learning models without human annotation, making it 'New Data from Model'."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of data generated by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using machine translation to generate or adapt data."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The augmented data is not merely aggregated or collected from existing sources; it is generated or derived through augmentation methods."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1.1 (Token-level Data Augmentation)",
          "reasoning": "The token-level generated data is derived from existing sentence pairs by selectively substituting words (nouns) with synonyms, hypernyms, antonyms, hyponyms, or co-hyponyms according to predefined rules to produce counterfactual examples. This is a transformation of existing data and thus fits 'Derived'."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data generation process is clearly documented and described; thus N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9447,
      "completion_tokens": 350,
      "total_tokens": 9797
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4.1, Section 4.2",
          "reasoning": "The paper describes that the newly generated counterfactual sentence pair datasets (via token-level and sentence-level augmentation) are used to fine-tune pre-trained language models such as BERT and RoBERTa. Specifically, these augmented datasets serve as the training data for supervised fine-tuning of models with both contrastive learning and cross-entropy objectives. This is explicitly detailed in Section 3.2 (Relation-based Contrastive Learning) and evaluated in Section 4.1 (Experiment Setup) and Section 4.2 (Results)."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Section 4.2",
          "reasoning": "The paper uses the proposed counterfactually augmented dataset to evaluate model robustness and performance, for example using CF-SNLI test sets and other general NLI datasets to benchmark different models and training methods, as described in Sections 4.1 and 4.2. Hence, the dataset is also used for evaluation purposes."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10170,
      "completion_tokens": 500,
      "total_tokens": 10670
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Experiment Setup",
          "reasoning": "The authors use the CF-SNLI dataset, which is based on SNLI, a large annotated corpus for natural language inference in English (Section 4.1). The augmentation methods such as token substitutions rely on English lexical resources like WordNet, and examples throughout the paper are all in English. There is no mention of any other natural language being present in the dataset or generated during augmentation; thus, the dataset contains only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset and augmentation involve natural language sentence pairs, not programming or structured code content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper contains mathematical formulas explaining loss functions, the dataset entries are natural language sentence pairs without mathematical or logical symbolic notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human natural language sentence pairs with no biological or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss or include fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language(s) used in the dataset are clearly specified and documented as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain natural language sentences, specifically English, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "yang23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7388,
      "completion_tokens": 169,
      "total_tokens": 7557
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or statement provides code availability information.",
          "reasoning": "The paper does not mention any code release, repository, or link for the dataset creation or augmentation code. There is no explicit indication that the code used for the relation-based counterfactual data augmentation or contrastive learning is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 (Proposed Method - Relation-based Counterfactual Data Augmentation)",
          "reasoning": "The paper thoroughly describes the dataset creation process related to their new augmented data, including token-level and sentence-level augmentation methods, substitution rules with WordNet relations, the sentence generation and filtering process, and how data pairs are constructed for contrastive learning. This provides transparency and completeness of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6550,
      "completion_tokens": 97,
      "total_tokens": 6647
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3 (implied from methodology)",
          "Reasoning": "The paper describes EEG signals collected by a wireless mobile EEG system from subjects in real-life scenarios (walking and sitting). EEG data is a sensor signal modality recorded from human participants in experimental conditions designed by the authors, indicating human-generated sensor data collected for this study."
        }
      ]
    }
  },
  {
    "id": "yang23r_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7350,
      "completion_tokens": 225,
      "total_tokens": 7575
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1 and Experimental Setup description in the paper",
            "reasoning": "The EEG data used for the new dataset was collected from subjects in real-life scenarios (walking and sitting) under controlled experimental conditions as detailed in Section 3.1. The collection and annotation involve expert experimenters setting up and labeling data, which implies human expert involvement."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not explicitly describe any detailed annotation instructions provided to annotators regarding the labels or categories for the new EEG dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not mention any scoring rubrics or criteria used for annotating or labeling the EEG data in the new dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No annotation examples provided in the paper",
            "reasoning": "No examples of annotated data or labeling guidelines are given for the new dataset within the paper."
          }
        }
      ]
    }
  },
  {
    "id": "yang23r_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8540,
      "completion_tokens": 270,
      "total_tokens": 8810
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert for the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information provided about multiple human experts performing quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple non-expert human annotators conducted quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although AI models are used for classification and attention detection tasks, they are not described as performing quality assurance on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process for quality assurance of the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied to the newly introduced EEG dataset collected during real-life scenarios. There is no mention of validation, annotation verification, or quality checks conducted by humans or machines."
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8150,
      "completion_tokens": 448,
      "total_tokens": 8598
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data collection and preprocessing, and Section 3 Experiments and results",
          "reasoning": "The paper describes EEG data collected from human subjects using a wireless mobile EEG system in real-life scenarios involving sitting and walking. This data consists of human physiological signals recorded during auditory attention tasks, and is original content created by human participants specifically for this study."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The study does not introduce any datasets generated entirely by AI or machine learning models without reference to existing data. All data corresponds to EEG signals recorded from human subjects."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data generated by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any dataset created by aggregation or collection from existing sources without modification. The data used is newly collected EEG data from their own experiments."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper applies various signal processing and filtering methods (e.g., bandpass filtering into sub-bands) on the raw EEG data for analysis, these processed signals are not presented as separate new datasets but rather as analysis results. The new dataset is the raw EEG recordings themselves."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origin of the data is clearly specified as newly recorded EEG signals from human subjects, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8668,
      "completion_tokens": 316,
      "total_tokens": 8984
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, 4.1",
          "reasoning": "The paper describes training auditory attention detection (AAD) models from EEG data captured in real-life scenarios to decode directional focus of attention. The dataset is used to train models such as CSP-based classifiers and CNNs, as indicated by the performance results in Table 1 and method descriptions."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1, Table 1",
          "reasoning": "The dataset collected by the authors is used to evaluate and benchmark the performance of different AAD methods (SR, CCA, CNN, CSP) for auditory attention detection in real-life scenarios, as shown in Table 1 and comparative analyses."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2",
          "reasoning": "The authors use the dataset to analyze the contribution of different EEG frequency bands (\u03b4, \u03b8, \u03b1, \u03b2) to AAD accuracy in sitting and walking states, demonstrating characteristic patterns in attention decoding."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9391,
      "completion_tokens": 582,
      "total_tokens": 9973
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the proposed dataset contains entries with more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that the proposed dataset contains entries with exactly two human languages."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "General description of experimental setting and stimuli, Sections 1 and 4",
          "reasoning": "The dataset involves EEG signals recorded while subjects attended to auditory stimuli in real-life scenarios. The paper is written in English and does not mention multiple languages or any non-English language content in the stimuli. The auditory stimuli and attention decoding experiments likely use English speech or speech envelopes, which implies the dataset entries contain only English content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only one non-English language; the paper uses English as the language for description and the experiments imply English speech."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses algorithms and mathematical models, it does not present a dataset containing entries with programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "Section 2.2 and 2.3 Mathematical descriptions of CSP and classification methods",
          "reasoning": "The paper contains mathematical formulations describing CSP and classifiers but does not introduce a dataset that contains entries in mathematical or formal logical expressions as data entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains EEG recordings related to human cognitive processes and attention, but does not involve biological sequences or non-human communication systems."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language content of the dataset is known and implied to be English in the auditory stimuli and experimental context."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset includes EEG data recorded during auditory linguistic stimulation, so it contains human language content."
        }
      }
    }
  },
  {
    "id": "yang23r_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6609,
      "completion_tokens": 154,
      "total_tokens": 6763
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "",
          "reasoning": "The paper does not provide any link, URL, or mention of publicly available code for data collection, preprocessing, or dataset generation. There is no indication of any code repository or supplementary material for code provided."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 Data collection; Section 3 Methodology",
          "reasoning": "The paper describes the EEG data collection process in real-life scenarios with a wireless mobile EEG system, including details about experimental conditions (sitting and walking), frequency bands explored, and the methods (CSP) applied. This documentation allows understanding of how the dataset was created and processed, although no explicit mention of dataset release is found."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8001,
      "completion_tokens": 104,
      "total_tokens": 8105
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Dataset",
          "Reasoning": "The paper introduces a new 16,600-hour in-house dataset of de-identified audio recordings for the Alexa keyword detection task. These recordings were obtained from real user interactions (human involvement) and processed into audio features (LFBE spectrograms). The dataset is explicitly collected by the authors and used for both training and evaluation."
        }
      ]
    }
  },
  {
    "id": "yang23y_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8801,
      "completion_tokens": 192,
      "total_tokens": 8993
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.1",
            "reasoning": "Section 3.1 states that the keyword labels were based on human annotation and underwent a quality check inspection, implying annotation was done by humans with expertise in keyword identification for the Alexa dataset."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not describe any detailed annotation instructions provided to annotators for labeling keywords in the dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "There is no mention in the paper of scoring rubrics or formal evaluation guidelines for the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.1",
            "reasoning": "The paper does not provide annotation examples or sample labeled data to illustrate the annotation process for the keyword spotting dataset."
          }
        }
      ]
    }
  },
  {
    "id": "yang23y_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9991,
      "completion_tokens": 289,
      "total_tokens": 10280
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify that multiple human experts performed the quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that a single human non-expert was responsible for quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the paper indicates that an AI model was used to perform quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based process for quality assurance of the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper states that keyword labels were based on human annotation and underwent a quality check inspection (Section 3.1), but provides no details about the quality assurance process, annotator expertise, number of annotators, or methods used for quality control. Therefore, no quality assurance process is documented or described explicitly for the dataset."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9601,
      "completion_tokens": 397,
      "total_tokens": 9998
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper states in Section 3.1 that the authors collected a 16,600-hour de-identified in-house dataset specifically for the Alexa keyword detection task, with human annotation and quality check inspection. This indicates original content created entirely from scratch by human contributors rather than sourced from existing public datasets."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated purely by AI or machine learning models without reference to or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any of the data was produced by human translators translating from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data generated by machine translation systems."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The in-house dataset was collected specifically for this task and not described as aggregated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as based on existing sources with modifications; it appears to be newly collected data specific to the task."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is clearly specified in Section 3.1; therefore, this category does not apply."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10119,
      "completion_tokens": 552,
      "total_tokens": 10671
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset; Section 3.2 Implementation",
          "reasoning": "The 16,600 hours in-house Alexa keyword spotting dataset is used for pre-training the student models in a self-supervised manner with knowledge distillation, as described in the methodology and implementation sections. The dataset is leveraged for pre-training models using self-supervised objectives (Wav2vec 2.0 style) along with proposed distillation techniques."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe training models from randomly initialized parameters on this dataset without pre-training or knowledge distillation. The student models are trained with knowledge distilled from teacher models, not strictly from scratch."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3.2 Implementation; Section 4 Results",
          "reasoning": "The same dataset is used for supervised fine-tuning of the student models after pre-training and distillation, with the addition of a linear classifier for keyword spotting, using human-annotated keyword labels. This is explicitly described in Section 3.2 and evidenced in evaluation in Section 4."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention use of reinforcement learning or RL-based post-training techniques such as RLHF on the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.1 Dataset; Section 3.3 Evaluation Metrics; Section 4 Results",
          "reasoning": "A portion of the dataset (85 hours for validation and 85 hours for testing) is explicitly held out and used for evaluation and benchmarking of the models on normal and noisy conditions. Evaluation metrics such as false acceptance rate at fixed false rejection rate are computed on this data."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 4.2 Single-View vs Dual-View; Section 4.3 Teacher Codebook vs Codebook from Scratch; Section 4.4 Layer Selection from Teacher Model",
          "reasoning": "The dataset is further used for extended analysis and ablation studies to investigate the impact of different distillation techniques and layer selection, as detailed in the analysis sections of the Results."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models in retrieval or similar frameworks in this work."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset introduced is actively used for pre-training, fine-tuning, evaluation, and analysis of models as documented in the paper."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10842,
      "completion_tokens": 537,
      "total_tokens": 11379
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the dataset containing multiple human languages; it only references an in-house Alexa keyword spotting dataset without details of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains entries in exactly two human languages; no such bilingual data description is provided."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset is described as an in-house Alexa keyword dataset. The teacher model is trained on LibriSpeech (a primarily English corpus), and there is no mention of other languages in the dataset. The context and use in Alexa keyword spotting strongly imply the data is English speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that the dataset contains a single non-English language exclusively."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is audio recordings processed into feature representations; no programming languages or code snippets are included as dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the paper includes equations and formulas describing methods, these do not pertain to dataset entries but to modeling techniques."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of human speech audio recordings; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any constructed or fictional languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset's language is explicitly implied to be English; the dataset language is specified and not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of human speech utterances, so it includes language data."
        }
      }
    }
  },
  {
    "id": "yang23y_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8060,
      "completion_tokens": 196,
      "total_tokens": 8256
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper describes the dataset as 16,600 hours of de-identified audio recordings collected in-house for Alexa keyword detection. There is no mention of any released code or link to code repositories for data collection, preprocessing, or dataset construction. No code is publicly provided or referenced."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3.1 Dataset",
          "reasoning": "The paper provides a description of the dataset including the size (16,600 hours), data source (Alexa keyword detection task), processing steps (64-D LFBE spectrogram with specified window and shift size), train-validation-test split details, and evaluation conditions (normal and playback). Although details on the data collection process are limited to stating 'de-identified audio recordings in various front-end conditions,' the dataset creation and characteristics are documented in sufficient detail relevant to reproducibility."
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8179,
      "completion_tokens": 157,
      "total_tokens": 8336
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data recording and annotation",
          "Reasoning": "The MSceneSpeech dataset consists of real audio recordings performed and recorded by professional recording artists according to daily life scenarios, using interpretative performances rather than synthetic generation, indicating human involvement in audio creation."
        },
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.1 Data recording and annotation",
          "Reasoning": "The accompanying text scripts for each scenario were carefully selected and curated by humans to align with the contextual backdrop, and text annotations were verified and manually corrected, implying the text data is human generated and carefully prepared."
        }
      ]
    }
  },
  {
    "id": "yang24d_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8979,
      "completion_tokens": 259,
      "total_tokens": 9238
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Single Human Expert",
            "reference": "Section 3.1 - Data recording and annotation",
            "reasoning": "The dataset recording artists are described as professional recording artists performing interpretative renditions of the scripts, indicating expert speakers. For text verification, manual comparison and corrections were performed after automatic transcription, implying expert human oversight."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 - Data recording and annotation",
            "reasoning": "The paper states that recording artists were given texts and scene context and required to deliver content through interpretative performances with clear style distinctions. This implies detailed instructions were provided to the professionals to achieve desired prosodic effects and style differences."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.1 - Data recording and annotation",
            "reasoning": "There is no mention of scoring systems or rubrics used for annotation or quality control in the dataset; only procedures like verification of texts and performance guidelines are described, without a rubric or scoring criteria."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Entire Paper",
            "reasoning": "The paper does not provide any annotation examples or formatted guidelines showcasing annotated samples or example annotations for the dataset. No appendix or section mentions examples."
          }
        }
      ]
    }
  },
  {
    "id": "yang24d_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10169,
      "completion_tokens": 424,
      "total_tokens": 10593
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "The dataset recordings were conducted by professional recording artists who performed interpretative readings of the texts. Post-recording, there was manual verification and correction of text annotations to ensure accuracy. Given the involvement of professional artists and manual correction, it is reasonable to interpret that quality assurance was conducted by single human experts."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention involvement of multiple human experts jointly performing quality assurance."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication exists that quality assurance was done by a single non-expert human."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state any involvement of multiple non-expert humans for quality assurance."
        },
        "AI Model": {
          "is_applicable": true,
          "reference": "Section 3.1",
          "reasoning": "Automatic transcription and verification was performed using Whisper ASR to check alignment between audio and text annotation, acting as an AI model-based quality assurance step."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 3.2",
          "reasoning": "Automated filtering was done using text similarity metrics and automatic segment splitting based on punctuation and duration constraints. Additionally, ASR-based proofreading filtered segments with less than 80% similarity, indicating an automated rule-based quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents multiple quality assurance steps including human expert checks, AI model use, and automated processes."
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9779,
      "completion_tokens": 375,
      "total_tokens": 10154
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3.1 Data recording and annotation",
          "reasoning": "The MSceneSpeech dataset was created by professional recording artists who performed and recorded original scripts curated specifically for various daily life scenarios. The data is not translated or derived from pre-existing material; it is original content meticulously recorded with expressive nuance to capture prosodic variety relevant to the intended scenarios."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any portion of the dataset was generated entirely by AI or machine learning models without reference or transformation of existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any data was created via machine translation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing datasets or sources; it was newly recorded as described."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that the data is based on existing sources with modifications or adaptations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin and method of generation of the dataset."
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10297,
      "completion_tokens": 476,
      "total_tokens": 10773
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the MSceneSpeech dataset for pre-training large models. Pre-training is conducted on a separate aggregated public dataset."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MSceneSpeech dataset is not used to train models from scratch; it is used for fine-tuning pre-trained models as stated in Section 5.1."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The paper explicitly states that after pre-training on a large mixed dataset, supervised fine-tuning is performed on the MSceneSpeech dataset (around 15 hours) to enable style-related speech synthesis and prosody transfer."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any reinforcement learning based post-training methods such as RLHF using the MSceneSpeech dataset."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The MSceneSpeech dataset is not solely used for evaluation or benchmarking; it is primarily used for supervised fine-tuning."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.3",
          "reasoning": "The paper performs analysis of prosodic features across scenes, comparing pitch variance and prosody richness in MSceneSpeech versus other datasets to highlight the advantages of their dataset."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or evidence that MSceneSpeech is used as a knowledge base for retrieval-augmented generation or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has multiple practical uses in the paper, including supervised fine-tuning and analysis."
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11020,
      "completion_tokens": 235,
      "total_tokens": 11255
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3, Dataset Description",
          "reasoning": "The MSceneSpeech dataset is explicitly described as a Mandarin speech dataset in Section 3, with recordings by professional speakers in Mandarin for various daily life scenarios. No other human languages are included in this dataset according to the description."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "yang24d_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8238,
      "completion_tokens": 296,
      "total_tokens": 8534
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract; Section 5.1 (Experimental setup)",
          "reasoning": "The paper states explicitly in the abstract that the MSceneSpeech dataset and audio samples of their baseline model are available at a public URL (https://speechai-demo.github.io/MSceneSpeech/). Additionally, in Section 5.1, it mentions the code and models are available on GitHub for the baseline, indicating related code resources are publicly accessible. While it mainly refers to code for the baseline model, the availability of the dataset and audio samples at the public site implies data preparation or access code is provided, supporting reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3 (MSceneSpeech Dataset), specifically Sections 3.1 (Data recording and annotation) and 3.2 (Data Processing)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process, including the selection of scenes, text script preparation, recording by professional artists with interpretative performances rather than mere recitation, verification and correction of text annotations (including use of the Whisper ASR system and manual corrections). It describes data processing steps such as segmenting audio clips by punctuation and duration constraints, automatic ASR proofreading to filter low-quality alignments, and manual refinements. The dataset split strategy into training and testing subsets is also explained. This level of detail provides transparency and completeness in dataset creation documentation."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8989,
      "completion_tokens": 201,
      "total_tokens": 9190
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and Table 3",
          "Reasoning": "The dataset is built upon the original GigaSpeech ASR corpus, which consists of 10,000 hours of English speech audio recorded by humans. The paper explicitly mentions using the original speech audio from GigaSpeech, which is human-generated through recorded English speech."
        },
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 2.2",
          "Reasoning": "The training set's translated text target side is obtained by translating GigaSpeech transcripts using a strong machine translation (MT) model developed by the authors. This translation is model generated. The test set translations, however, are produced by human translators, but since the test set is separate, only the training set's text translations are considered model generated here."
        }
      ]
    }
  },
  {
    "id": "ye23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9789,
      "completion_tokens": 196,
      "total_tokens": 9985
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 Test Sets",
            "reasoning": "The test sets of GigaST were produced by human translators who produced translations by looking at the transcriptions, indicating human experts performed these annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned in the paper",
            "reasoning": "The paper does not provide any information about detailed annotation instructions given to human translators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.1 Training Set (Human evaluation)",
            "reasoning": "For human evaluation of the translation quality, a rubric of rating translations from 0 to 6 is described and used, indicating such rubrics are available for evaluators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No examples of annotation cases or example annotations are provided in the paper for the human translators."
          }
        }
      ]
    }
  },
  {
    "id": "ye23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10979,
      "completion_tokens": 349,
      "total_tokens": 11328
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 2.2 Test Sets",
          "reasoning": "The test sets in GigaST are produced by human translators, and these translators are described as professionals. This indicates that quality assurance on test data translation is performed by single human experts who translated and verified each sample individually."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Training Set",
          "reasoning": "For human evaluation of the MT-generated training data, 6 evaluators (3 for En-Zh and 3 for En-De), all human professionals, rated the translation quality, indicating multiple human experts were involved in quality assurance of the pseudo-labeled training data."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that a single non-expert conducted quality assurance was found in the paper."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper only mentions professional human translators and evaluators for quality checks; no information supports multiple non-experts performing quality assurance."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using AI models as judges or for quality assurance of the dataset translations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although automatic metrics like BLEU were used to evaluate MT quality, there is no description of automated quality assurance processes on the dataset annotations themselves."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents human expert QA processes for the test set translations and human expert evaluation of MT-generated training data, thus QA is applied and documented."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10589,
      "completion_tokens": 500,
      "total_tokens": 11089
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Test Sets",
          "reasoning": "The paper states that the test sets for English-Chinese and English-German were produced by human translators translating the original English transcriptions. This original translation was done by humans viewing the transcripts to produce target language translations, as described in Section 2.2."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any dataset generated entirely by AI or machine learning models from scratch without referencing or transforming existing data."
        },
        "Human Translation": {
          "is_applicable": true,
          "reference": "Section 2.2 Test Sets",
          "reasoning": "The test sets are explicitly described as human translations of English transcripts into German and Chinese, designed to ensure high quality and avoid alignment errors. This fits the category of human translations."
        },
        "Machine Translation": {
          "is_applicable": true,
          "reference": "Section 2.1 Training Set",
          "reasoning": "The training set target translations were obtained by translating the English transcripts of GigaSpeech using strong neural machine translation models that were trained and refined via data filtering, knowledge distillation and back translation. This indicates the training labels are machine translated, as explicitly stated."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not simply collected or aggregated from existing sources without modification. It is constructed by translation, either machine or human, of existing transcripts."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Dataset Creation",
          "reasoning": "The new GigaST speech-to-text translation corpus is derived from the existing GigaSpeech English ASR corpus by applying machine translation to the transcripts for training data and human translations for test data. Hence, the dataset is based on an existing source with modifications (translations) applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly documents the data creation pipeline and origins for both training and test sets, so the origin is clearly specified."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11107,
      "completion_tokens": 492,
      "total_tokens": 11599
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.2, Section 4 (Conclusion)",
          "reasoning": "The GigaST dataset is explicitly used as a training dataset for training speech-to-text translation models from scratch, as described in Section 3.2 where various Speech-Transformer and SSL-Transformer models are trained on GigaST subsets of increasing size. The conclusion also states that the corpus is suitable for training ST systems."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using GigaST for fine-tuning pre-trained models; rather, it is used directly as a training corpus."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of reinforcement learning or RL-based fine-tuning using the GigaST dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 2.2 (Test Sets), Section 3.1 (Evaluation), Tables 4 and 5",
          "reasoning": "GigaST includes human-translated test sets for En-Zh and En-De used exclusively for evaluation and benchmarking of ST models, demonstrated in multiple results tables."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.4 (Analysis on Speech Representations)",
          "reasoning": "The dataset is used to analyze the effects of pre-trained speech encoders and model size on performance, contributing to understanding model behavior."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used as a knowledge base to augment models through retrieval or similar techniques."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is clearly used for multiple practical purposes including training, evaluation, and analysis."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11830,
      "completion_tokens": 586,
      "total_tokens": 12416
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The GigaST dataset contains translations from English into two separate target languages, Chinese and German, but the dataset entries are bilingual pairs, not containing more than two languages in a single entry."
        },
        "Bilingual": {
          "is_applicable": true,
          "reference": "Section 2 Dataset Creation (especially 2.1 Training Set and 2.2 Test Sets)",
          "reasoning": "The GigaST dataset is created by translating English speech transcripts into two different languages: German and Chinese. Each entry consists of source English speech and a corresponding translated text either in German or Chinese. Therefore, each dataset entry pairs exactly two human languages (English source and one target language), making it bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains translated text into German and Chinese, so it is not limited to English-only content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset pairs English speech with either German or Chinese translation, hence it is bilingual, not monolingual non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech and natural language translation texts, with no indication of programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of mathematical or formal logical expressions being part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset involves human speech and translations, no biological sequences or non-human communication systems are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset languages are real human natural languages (English, German, Chinese), no constructed or fictional languages are present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages involved in the dataset entries are clearly documented as English for the source speech and German or Chinese for the translations."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language content in English, German, and Chinese, so 'no language' does not apply."
        }
      }
    }
  },
  {
    "id": "ye23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9048,
      "completion_tokens": 148,
      "total_tokens": 9196
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 4 Conclusion",
          "reasoning": "The paper states in the abstract and conclusion that they release the training scripts on NeurST and provide the dataset publicly at https://st-benchmark.github.io/resources/GigaST, indicating that code related to dataset creation and training is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 Dataset Creation",
          "reasoning": "Section 2 provides a detailed description of the translation process used to create the pseudo speech-to-text translation corpus, including the machine translation system, training data, data filtering, multi-resolution training, and human evaluation steps, which documents the dataset creation process comprehensively."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7549,
      "completion_tokens": 450,
      "total_tokens": 7999
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.1",
          "Reasoning": "Tencent Corpus is a newly introduced dataset by the authors, containing speech clips with various simulated impairments and live recordings. The data includes artificially degraded speech based on publicly available clean speech datasets (Magic Data, ST Mandarin, AiShell 100h) which are human-generated audio recordings. These clean speech clips are further processed with algorithmic simulations to add impairments, noise suppression, packet loss concealment, and reverberation effects (both simulated and live room recordings). Therefore, the final data includes both human-generated audio and model-generated (simulated) audio samples."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.4",
          "Reasoning": "PSTN Corpus is introduced as a new dataset in this challenge. It uses clean reference files from human-generated audiobook recordings (Librivox) and adds background noise clips from Audioset, Freesound, and DEMAND corpora. Noise is added algorithmically with varying SNR to simulate real phone call scenarios. The clean speech is human-generated while the noisy versions are model-generated via data processing."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 and 4.4 combined with Section 4 introduction",
          "Reasoning": "The paper explicitly states only NISQA Corpus is publicly available prior, thus it is not a new dataset introduced by the authors. IU Bloomington Corpus is also not created by the authors but included as additional data. Therefore, only the Tencent Corpus, PSTN Corpus, and the newly created TUB Corpus are new datasets introduced or created by the authors. TUB Corpus is described in Section 4.2 as newly created: clips from unimpaired signals are selected and synthetic degradations applied (codecs, bandwidth, background noises, packet loss), constituting model-generated augmentations applied on human-generated audio recordings. Hence, TUB Corpus includes human-recorded audio that is model-processed to simulate impairments."
        }
      ]
    }
  },
  {
    "id": "yi22b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8349,
      "completion_tokens": 237,
      "total_tokens": 8586
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 4.1",
            "reasoning": "The subjective scoring procedure for the Tencent Corpus was conducted via crowdsourcing with more than 24 listeners per clip, which after cleaning yields more than 20 subjective scores per clip averaged to obtain MOS. Crowdsourcing typically involves multiple non-expert annotators."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1",
            "reasoning": "The subjective quality scores were obtained through crowdsourcing following ITU-T Rec P.808, which prescribes standard instructions and procedures for such tests."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1 and 2",
            "reasoning": "The subjective assessment was conducted similarly to ITU-T Rec P.808, which includes rating scales with rubrics for mean opinion scores (MOS), i.e., 1 to 5 scale ratings."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not mention any specific examples being provided in the annotation guidelines or to annotators for the Tencent Corpus."
          }
        }
      ]
    }
  },
  {
    "id": "yi22b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9539,
      "completion_tokens": 446,
      "total_tokens": 9985
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not specify any quality assurance conducted by a single human expert for the annotation or validation of the datasets."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper that multiple human experts annotated or validated the datasets. The subjective ratings were crowdsourced and do not specify expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate quality assurance by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.2, 4.3, 4.4",
          "reasoning": "The datasets' subjective ratings were collected by multiple non-expert listeners via crowdsourcing following ITU-T Rec P.808 framework. For instance, in Section 4.1, each clip was rated by more than 24 listeners who provided subjective scores, averaged to obtain MOS. Similarly, other datasets used crowdsourced listening tests. Crowdsourcing typically involves multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper describes AI models for prediction but does not mention AI models used for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or rule-based quality assurance for the annotations. The subjective ratings rely on human listeners."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "A quality assurance process is clearly described and implemented via multiple human non-expert annotators using crowdsourcing; therefore, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9149,
      "completion_tokens": 571,
      "total_tokens": 9720
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.4",
          "reasoning": "The Tencent Corpus and PSTN Corpus introduced by the authors include speech clips that were created by recording live speech or selecting clean speech from existing datasets and then subjected to various impairments and processing such as noise addition, reverberation, and packet loss concealment. The speech clips were rated by human listeners through crowdsourcing following ITU-T P.808, which involves human subjective evaluation, thus embodying newly created and scored data contributed by humans. The datasets include live recordings and controlled recordings combined with human subjective MOS ratings."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets do not contain entirely AI-generated speech or content; impairments are simulated but the speech signals originate from human speech recordings, not generated by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that any data was produced by translating content from another language via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication in the paper that machine translation was used to generate any data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1, 4.3, 4.4",
          "reasoning": "The Tencent Corpus includes speech clips sourced from publicly available datasets such as Magic Data, ST Mandarin, and AIshell 100h. The IU Bloomington Corpus includes speech clips extracted from COSINE and VOiCES datasets. The PSTN Corpus uses clean reference files derived from the public Librivox audiobook dataset. These data were collected and aggregated from existing sources without solely being created from scratch by the authors."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.4",
          "reasoning": "Several datasets, especially the Tencent and PSTN Corpora, are based on existing clean speech but modified through simulated degradations such as noise addition, reverberation, codec impairments, noise suppression, and packet loss concealment to simulate realistic impairments experienced in online conferencing. This demonstrates the data is derived from existing sources with modifications and transformations applied."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins and generation methods are clearly described in the paper."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9667,
      "completion_tokens": 495,
      "total_tokens": 10162
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any use of the new datasets for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 (Task Description) and Section 5 (Baseline System)",
          "reasoning": "The training datasets provided (Tencent Corpus, PSTN Corpus, NISQA Corpus, and IU Bloomington Corpus) are used to train models from scratch or to develop models for the non-intrusive speech quality assessment task, as described in the challenge. Baseline systems were trained on these datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not explicitly describe the use of the new datasets for supervised fine-tuning of pre-trained models."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of the datasets being used for reinforcement learning post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3 (Task Description), Section 4.5 (Dataset Division), Section 6 (Challenge Results)",
          "reasoning": "The datasets, including newly created test datasets such as TUB and portions of Tencent and PSTN Corpora, are explicitly used for evaluation, benchmarking, and performance measurement of submitted models in the challenge."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss using the new datasets primarily for trend or pattern analysis."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are not used as knowledge bases to augment models through retrieval or similar methods."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes practical usages for the new datasets in training and evaluation, so N/A does not apply."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10390,
      "completion_tokens": 499,
      "total_tokens": 10889
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 4.5 Dataset Division, last paragraph",
          "reasoning": "The challenge datasets include speech clips composed of Chinese, English, and German languages, which makes the dataset multilingual with more than two human languages represented."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than two languages (Chinese, English, German), so it is not limited to exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages including Chinese and German, not only English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains multiple languages including English, Chinese and German, so it is not monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech clips and subjective MOS scores without any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although some mathematical notation is used for evaluation metrics and polynomials, the dataset content itself does not include mathematical or logical expressions."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication data are included in the dataset; it only contains human speech."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fictional or artificially created languages in the datasets."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are explicitly specified as Chinese, English, and German."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets consist of human speech recordings, which inherently contain language."
        }
      }
    }
  },
  {
    "id": "yi22b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7608,
      "completion_tokens": 192,
      "total_tokens": 7800
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4 and elsewhere",
          "reasoning": "The paper does not provide any links or references to code repositories or any indication that the code used for constructing the new datasets (Tencent Corpus, IU Bloomington Corpus subset, PSTN Corpus subset) is made publicly available. There is no mention of code release or open-source repositories for the dataset construction or processing."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4 Data Description",
          "reasoning": "The paper documents the dataset creation processes in detail in Section 4, describing the sources, impairment types, processing steps, subjective testing methodologies (crowdsourcing with ITU-T P.808 or ITU-R BS.1534), and dataset division strategies. The documentation includes proportions of different impairments applied, sampling frequencies, number of subjective ratings, and references to related datasets and methodologies, providing transparency and completeness about dataset construction."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6940,
      "completion_tokens": 351,
      "total_tokens": 7291
    },
    "response": {
      "sources": [
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data Collection, Section 2.2 Data Annotation, and Section 6 Conclusion",
          "Reasoning": "The authors collected a novel dataset consisting of 11 televised political debate videos from the 2020 Democratic presidential primaries (Section 2.1). These are real-world videos recorded from TV broadcasts by human-operated cameras, making them human-generated data. The paper states this dataset was newly collected by the authors for their study (Section 2 and 6)."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3.1 Vocal Features and Section 2.1 Data Collection",
          "Reasoning": "The audio modality is extracted from the collected debate videos, which were recorded in real-world televised debates, a human-generated source. This data captures the vocal features from the original human speech audio tracks in the videos. The audio is not synthetically generated but directly extracted from human speech (Section 2.3.1)."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.3.2 Facial Features and Section 2.3.3 Gestural Features",
          "Reasoning": "The facial and gestural features are extracted from the frames of the collected debate videos, which are human-generated video recordings of the candidates in debates. These features originate from human recordings captured in the videos. Although features are computationally extracted, the base data is human-generated video data (Section 2.3.2 and 2.3.3)."
        }
      ]
    }
  },
  {
    "id": "yoon22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7740,
      "completion_tokens": 218,
      "total_tokens": 7958
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The paper states that eight college students were recruited to annotate the emotional intensity scores for video clips. College students are typically considered non-expert annotators, and multiple individuals performed the annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "Annotators were asked to rate emotional intensity on a scale of 1 to 5 for given 5-second video clips, which indicates some level of instruction was provided regarding the task and rating scale."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Annotation",
            "reasoning": "The rating scale from 1 (lowest) to 5 (highest) emotional intensity acts as a rubric guiding the scoring process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "None",
            "reasoning": "The paper does not mention providing examples of annotations or annotated clips as part of the annotation guidelines."
          }
        }
      ]
    }
  },
  {
    "id": "yoon22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8930,
      "completion_tokens": 310,
      "total_tokens": 9240
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that annotators were college students, with no indication they were subject matter experts or belonged to the target demographic, so this label does not apply."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The annotators are described as college students without subject matter expertise; thus, multiple human experts did not perform QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Annotation was conducted by multiple annotators (eight college students), not a single annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Annotation",
          "reasoning": "The dataset annotation was performed by eight college students, who are not described as experts or belonging to the target demographic. This indicates that multiple human non-expert annotators conducted the quality assurance process."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that an AI model was used for quality assurance of annotation in the paper."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification process used for quality assurance of the annotated dataset."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "Quality assurance was performed by multiple human non-expert annotators (eight college students), as described in Section 2.2; therefore, the N/A label does not apply."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8540,
      "completion_tokens": 431,
      "total_tokens": 8971
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2 Data Annotation",
          "reasoning": "The authors collected video clips from televised debates and recruited eight college students to annotate the perceived emotional intensity scores on a scale from 1 to 5, generating original emotional intensity labels for 920 short clips. This annotation is original content created entirely from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The authors train a model to predict emotional intensity, but they do not generate new data purely from models as a dataset."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that any data was generated by translating content from other languages by humans."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper of any machine translation being used to generate dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection",
          "reasoning": "The debate videos were collected from publicly available televised debate sources such as NBC, ABC, and CBS. These videos were aggregated and collated without significant modification for the purpose of analysis."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.3 Non-verbal Feature Extraction",
          "reasoning": "The authors extracted vocal, facial, and gestural features from the collected videos using tools such as Librosa, dlib, and OpenPose, thus deriving structured feature data from the raw videos to be used in the model."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origins and generation methods are clearly documented."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9058,
      "completion_tokens": 474,
      "total_tokens": 9532
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using the dataset for pre-training large models in an unsupervised or self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1 (Problem Statement), Section 4.1 (Experimental Settings)",
          "reasoning": "The novel debate dataset with annotated emotional intensity scores is used to train the proposed multimodal deep-learning model from randomly initialized weights to predict perceived emotional intensity."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fine-tuning a pre-trained model using the dataset; all model weights are randomly initialized and trained from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used for any reinforcement learning based post-training techniques such as RLHF."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.3 (Experimental Results)",
          "reasoning": "The dataset is split into training and test sets; the test portion is used for evaluating model performance and benchmarking against baselines."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Case Study: 2020 Democratic Party Presidential Primaries in the USA)",
          "reasoning": "The dataset is used to analyze correlations between the predicted emotional intensity and changes in candidates' favorability in public polls, thus for analyzing trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset is actively used for training, evaluation, and analysis, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9781,
      "completion_tokens": 606,
      "total_tokens": 10387
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of political debate videos from the 2020 Democratic presidential primaries in the USA, which are conducted in English only, as there is no mention of multiple languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are from televised debates in English only, with no indication of a second language being included."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data Collection; Abstract; Section 5",
          "reasoning": "The dataset consists of televised debate videos from the 2020 Democratic presidential primaries in the USA, all in English as the candidates speak English. The paper explicitly mentions using timestamped transcripts with speaker information, and all source debates are from US news channels (NBC, ABC, CBS) which feature English language content."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset entries contain only a non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes methods that use code (e.g., Librosa, dlib, OpenPose) for feature extraction, the dataset itself consists of video clips of political debates, not programming or code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of video clips and annotated emotional intensity scores. Mathematical notations appear only in the description of the prediction model and problem formulation, not as part of the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains human political debate videos and their annotations. There is no inclusion of biological sequences or non-human communication content."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of constructed or fictional languages such as Klingon or Esperanto is found in the dataset description."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper clearly specifies the content source and language (English), so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken human language (English) in video form, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "yoon22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6999,
      "completion_tokens": 183,
      "total_tokens": 7182
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 2 and 6",
          "reasoning": "The paper does not provide any link or mention that the code used for collecting, processing, or annotating the dataset is publicly available. While the paper describes the dataset collection and annotation procedures, there is no indication of a publicly accessible repository or code release."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2.1, 2.2, and 2.3",
          "reasoning": "The paper provides detailed documentation on dataset collection, annotation process, and feature extraction methods in Sections 2.1 (Data Collection), 2.2 (Data Annotation), and 2.3 (Non-verbal Feature Extraction). This includes specifics on video sources, annotator recruitment and rating scheme, and extraction tools and parameters for vocal, facial, and gestural features."
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8513,
      "completion_tokens": 129,
      "total_tokens": 8642
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1, 2.2",
          "Reasoning": "The dataset includes bonafide singing vocals sourced from open-source singing datasets, which are recordings involving human singers. This counts as human generated audio data. Additionally, it has deepfake singing vocals generated by various state-of-the-art singing voice synthesis (SVS) and singing voice conversion (SVC) models, indicating parts of the data are model generated audio. The whole dataset consists of audio clips only, validating the modality as audio."
        }
      ]
    }
  },
  {
    "id": "zang24_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9313,
      "completion_tokens": 283,
      "total_tokens": 9596
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2.2, 2.2.1, 2.2.2",
            "reasoning": "The deepfake vocals in the CtrSVDD dataset are generated using state-of-the-art singing voice synthesis (SVS) and singing voice conversion (SVC) systems. These generation methods involve trained neural network models and commercial software applied to bonafide vocals. The paper describes the entire synthesis pipeline and models used for data generation, indicating an automatic process rather than human manual annotations."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No direct mention in Sections 2 or 4",
            "reasoning": "The paper does not mention providing detailed manual annotation instructions for human annotators because the labels (bonafide or deepfake) are assigned automatically via the generation process, not by human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "No direct mention in Sections 2 or 4",
            "reasoning": "No scoring rubrics for annotation were described since the labels are based on systematic generation procedures, not subjective human labeling requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "No direct mention throughout the paper",
            "reasoning": "No annotation examples are provided since no manual annotation guidelines are present; the dataset labels come from controlled synthesis methods rather than manual labeling."
          }
        }
      ]
    }
  },
  {
    "id": "zang24_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10503,
      "completion_tokens": 364,
      "total_tokens": 10867
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description in the paper indicating quality assurance was performed by multiple human experts for the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human non-expert performing quality assurance on the dataset."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No evidence or mention of multiple human non-experts conducting quality assurance is present in the paper."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that an AI model was used as a judge or for quality assurance of the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2",
          "reasoning": "The dataset construction involved automatic segmentation for Japanese datasets based on musical scores or voice activity detection (Section 2.1). The deepfake singing voice clips were generated using automated synthesis methods following standardized training recipes (Section 2.2). There is no mention of manual verification or annotation of the dataset labels; the bonafide and deepfake labels derive from sourcing from open-source singing datasets and known synthesis methods. This implies quality assurance relies on automated processes intrinsic to the dataset design and synthesis pipelines."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents the dataset construction process clearly and implies quality assurance via automated methods such as segmentation and controlled generation pipelines, so it is not the case that no quality assurance is applied or documented."
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10113,
      "completion_tokens": 461,
      "total_tokens": 10574
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper states that the bonafide singing vocals are sourced from existing open singing datasets (Section 2.1). There is no indication that original singing vocals were newly recorded or created entirely from scratch by human contributors specifically for this dataset."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The deepfake singing voice clips are generated by applying 14 state-of-the-art singing voice synthesis (SVS) and singing voice conversion (SVC) systems, most of which are open-source machine learning models trained on publicly available datasets. This constitutes original data generated entirely by AI or machine learning models as detailed in Section 2.2."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention or indication of any translation of data through human translators in the dataset construction."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description or indication that machine translation was used to generate any of the dataset content."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The bonafide singing voice clips are collected from multiple existing public singing datasets without significant modification, as described in Section 2.1."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The deepfake data is derived from the bonafide singing clips by applying various singing voice synthesis and conversion models that transform the source recordings into deepfakes. This indicates transformations based on existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the sources and methods used to generate the dataset, so this category does not apply."
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10631,
      "completion_tokens": 292,
      "total_tokens": 10923
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 and 4",
          "reasoning": "The paper describes the use of the CtrSVDD dataset to train baseline deepfake detection models from scratch using various front-end features and a fixed back-end module, with experimental details in Sections 3 and 4 indicating training procedures on the dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The dataset is used for benchmarking and performance evaluation of baseline deepfake detection systems, with specific evaluations, metrics (Equal Error Rate), and ablation studies reported in Section 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.2 and 5",
          "reasoning": "The paper uses the dataset to analyze feature effectiveness, generalization challenges, and distribution characteristics of various deepfake methods, demonstrated through detailed discussions and t-SNE visualizations."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11354,
      "completion_tokens": 558,
      "total_tokens": 11912
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2.1 Details of bonafide vocals",
          "reasoning": "The CtrSVDD dataset's bonafide vocals are sourced from various open singing datasets across multiple languages, explicitly Mandarin Chinese and Japanese (e.g., Opencpop, M4Singer, Kising for Mandarin; Ofuton-P5, OnikuKurumi6, Kiritan, and JVS-MuSiC for Japanese). The presence of multiple distinct human languages (Mandarin and Japanese) clearly indicates multilingual coverage."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes more than two languages and thus does not qualify as only bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content; rather, it includes Mandarin and Japanese singing vocals."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset includes multiple languages (Mandarin and Japanese), so it is not monolingual."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains singing vocal audio data but does not include any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of audio clips and not mathematical or logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset focuses on human singing voice audio and deepfake singing voice; no biological sequences or non-human communication data is included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no presence of fictional or artificially created languages in the dataset."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper explicitly states the languages of the singing vocals in the dataset; thus the language is not unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains singing voice audio clips in known human languages, so language coverage is applicable."
        }
      }
    }
  },
  {
    "id": "zang24_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8572,
      "completion_tokens": 185,
      "total_tokens": 8757
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2.2, Section 5",
          "reasoning": "The abstract states the CtrSVDD dataset and baselines are publicly accessible. Section 2.2 explains that models used for deepfake generation were trained with open-source toolkits and public recipes, indicating code availability. Section 5 (Conclusion) reiterates the release of dataset, baseline implementations, and pretrained model weights, implying code for data generation and processing is made public."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (CtrSVDD dataset design)",
          "reasoning": "Section 2 provides detailed documentation on the dataset creation process including the source of bonafide singing data, detailed description of 14 deepfake generation methods, selection of datasets, segmentation methods, and dataset partitioning. This level of documentation ensures transparency and reproducibility."
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7363,
      "completion_tokens": 150,
      "total_tokens": 7513
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2 and Abstract",
          "Reasoning": "The CN-Spoof dataset introduced in this paper is created by applying five different vocoders (three neural vocoders and two DSP vocoders) to real mel-spectrograms extracted from the waveforms of the CN-Celeb1&2 datasets, which were collected via human speech recordings. Thus, the source real audio is human-generated, and the spoofed audio samples are model-generated due to vocoder synthesis. Therefore, the audio modality encompasses both human-generated (original) and model-generated (vocoder reconstructed) audio data explicitly described in the paper."
        }
      ]
    }
  },
  {
    "id": "zeng23_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8163,
      "completion_tokens": 277,
      "total_tokens": 8440
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2, 5 Conclusions",
            "reasoning": "The CN-Spoof dataset is created utilizing copy-synthesis methods and five different vocoders to generate spoofed audio from real data. This generation process is algorithmic and not performed by humans or standard AI models for annotation. Therefore, the annotation (labeling real/fake) is implicit and derived automatically based on the synthesis process, constituting an automatic process rather than manual human annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2, no mention of annotation instructions",
            "reasoning": "The paper does not provide any description or indication of human annotators receiving instructions for labeling data. The dataset's real/fake labels stem from the synthesis process rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2, no mention of annotation rubrics",
            "reasoning": "No scoring rubrics or detailed criteria for human annotation are discussed in the paper, consistent with the dataset being generated automatically rather than manually annotated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2, no appendix or section describing annotation examples",
            "reasoning": "No example annotations or labeling examples are provided in the paper that relate to how humans labeled data, reflecting the lack of manual annotation evidence."
          }
        }
      ]
    }
  },
  {
    "id": "zeng23_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9353,
      "completion_tokens": 373,
      "total_tokens": 9726
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance conducted by a single human expert on the CN-Spoof dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of multiple human experts performing quality assurance on the dataset in the paper."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document quality assurance performed by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that multiple non-expert human annotators conducted quality assurance on the dataset."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any AI model used as a judge for quality assurance of the dataset."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the dataset was created using vocoders (automated synthesis), the paper does not report explicit automated verification or algorithmic quality assurance processes on the dataset annotations or content."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process applied to validate or verify the annotations or content of the newly introduced CN-Spoof dataset or its augmentation of CN-Celeb1&2 data."
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8963,
      "completion_tokens": 412,
      "total_tokens": 9375
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset CN-Spoof is not described as original content created entirely from scratch by human contributors. Instead, it is produced by processing existing data using vocoders."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "Section 2 (CN-Spoof dataset and protocols)",
          "reasoning": "While the CN-Spoof dataset is generated using neural vocoders (models), the data is derived from existing real mel-spectrograms. Thus, it is not generated entirely by AI without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by machine translation from other languages."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not merely collected or aggregated without modification; it involves transformation using vocoders."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 (CN-Spoof dataset and protocols)",
          "reasoning": "The CN-Spoof dataset is created by applying multiple vocoders to reconstruct waveforms from mel-spectrograms extracted from the existing CN-Celeb1&2 datasets. This involves modifications and transformations applied to existing data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source and generation method for CN-Spoof is clearly documented in the paper."
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9481,
      "completion_tokens": 414,
      "total_tokens": 9895
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe the CN-Spoof dataset being used for pre-training large models on general patterns or in an unsupervised/self-supervised manner."
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.2",
          "reasoning": "The CN-Spoof dataset combined with CN-Celeb1&2 datasets is used to train multiple anti-spoofing models from scratch using supervised learning, as delineated in the experiments and training methodology sections."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not described as being used to fine-tune pre-trained models; the training appears to be from scratch."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication that the dataset is used for reinforcement learning or RL-based post-training techniques."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 and 4.3",
          "reasoning": "A subset of the combined datasets including CN-Spoof is used as a held-out evaluation dataset for benchmarking the performance of anti-spoofing models under cross-genre mismatch protocols."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 1 and Section 4.3",
          "reasoning": "The dataset is used to analyze genre mismatch effects on generalization ability by training models under different cross-genre protocols and visualizing embeddings, thus characterizing trends and patterns related to genre."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base to augment models; rather, it is used for training and evaluation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper extensively describes practical uses of the dataset in training, evaluation, and analysis, hence 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10204,
      "completion_tokens": 364,
      "total_tokens": 10568
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2",
          "reasoning": "The CN-Spoof dataset is based on the CN-Celeb1&2 datasets, which are Chinese speaker recognition corpora consisting of multi-genre speech in the Chinese language. No other human languages are mentioned as part of this dataset, thus it contains entries with exactly one non-English human language, i.e., Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zeng23_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7422,
      "completion_tokens": 156,
      "total_tokens": 7578
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No section or explicit mention",
          "reasoning": "The paper does not provide any links, URLs, or explicit references to source code repositories for constructing or generating the CN-Spoof dataset or related code."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2 (CN-Spoof dataset and protocols)",
          "reasoning": "The paper provides a detailed description of how the CN-Spoof dataset was created, including using five different vocoders on mel-spectrograms from CN-Celeb1&2 datasets, selection of numbers of utterances, total size of fake utterances, grouping of genres, and data splits for training and evaluation, constituting comprehensive documentation of dataset construction."
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8494,
      "completion_tokens": 201,
      "total_tokens": 8695
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.2 Synthetic text generation",
          "Reasoning": "The synthetic texts are generated by the authors using a data augmentation method based on a seq2seq model which replaces words with others from the same semantic field for agglutinative languages. This is a model-generated dataset specifically created by the authors as part of their new data augmentation approach."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 4.3 Synthetic audio generation",
          "Reasoning": "The synthetic audios are generated by the authors using a Tacotron 2 TTS model trained on 15 hours of single-speaker transcribed audio. This TTS model synthesizes 99 hours of audio from the synthetic texts created in the previous step, resulting in a new synthetic audio dataset generated entirely by the model without direct human recording."
        }
      ]
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 9294,
      "completion_tokens": 315,
      "total_tokens": 9609
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.2, Table 3, and Section 4.3",
            "reasoning": "The paper describes using a Quechua lexicon and semantic frames obtained from experts to label semantic frames in synthetic text generation, and mentions that the Tacotron 2 TTS model was trained on 15 hours of transcribed audio recorded by a single speaker. Additionally, the mean opinion score (MOS) to evaluate TTS quality was obtained by consulting Quechua speakers from southern Peru, indicating human expert involvement in annotation and evaluation."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.2",
            "reasoning": "Section 4.2 details a text data augmentation method involving tokenization, POS tagging, lemmatization, semantic frame tagging with the BIO standard, and use of a bilingual dictionary for frame labeling, implying instructions were provided to annotators or for the process to perform semantic frame annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly discussed in the paper",
            "reasoning": "The paper does not mention scoring rubrics or formal evaluation criteria as part of annotation guidelines for either the semantic frame annotation or the creation of synthetic data."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 3 (Section 4.2)",
            "reasoning": "Table 3 provides a concrete example of a delexicalised Quechua sentence with semantic frame codes and English translations, indicating that annotation examples were provided."
          }
        }
      ]
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10484,
      "completion_tokens": 325,
      "total_tokens": 10809
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any single human subject matter expert performing quality assurance on the dataset annotations or content."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit indication in the paper that multiple human experts validated or ensured the quality of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any quality assurance by a single human non-expert."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of quality assurance performed by multiple human non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by an AI model acting as a judge."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 4.1 and Section 5",
          "reasoning": "The paper describes that both corpora were preprocessed to improve data quality by eliminating noisy audio, non-Quechua audio, long audio segments, and converting data formats automatically. This indicates an automated verification and filtering process for data quality assurance. Additionally, the language model used singleton pruning via a rule-based process to ensure quality. However, there is no explicit human-in-the-loop verification reported for the datasets."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper documents an automated preprocessing and cleaning process of the datasets, so some form of quality assurance was performed and described."
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 10094,
      "completion_tokens": 710,
      "total_tokens": 10804
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 The Corpora, Section 1 Introduction, and Section 7 Conclusions",
          "reasoning": "The authors have created a new speech corpus for Southern Quechua consisting of 99 transcribed and cleaned hours of audio data, collected from multiple sources including dictionaries and a book. They preprocessed and cleaned this corpus themselves, improving audio quality and segmenting as needed. This dataset is original content created by human contributors specifically for this research, as stated in the Introduction (contribution 1) and detailed in Section 4.1. It is not a translation or adaptation but an original collection and preparation of Quechua speech data."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 4.2 Synthetic Text Generation, Section 4.3 Synthetic Audio Generation",
          "reasoning": "The paper describes synthetically generated text produced using a seq2seq model with a modified delexicalisation algorithm for agglutinative languages, generating 8,320 synthetic sentences (Section 4.2). These synthetic sentences were then used to generate 99 hours of synthetic audio using a Tacotron 2 text-to-speech model trained on 15 hours of single-speaker audio from the corpus (Section 4.3). This indicates that new data\u2014both synthetic text and synthetic audio\u2014was generated entirely by AI models based on the original data but representing novel generated content, not mere transformations. Thus, the synthetic text and synthetic audio data qualify as New Data from Model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using machine translation systems to generate data."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 4.1 The Corpora",
          "reasoning": "The initial corpora used include the Siminchik corpus and Lurin corpus, which contain audio and transcripts collected from existing multiple sources such as dictionaries and a book. These datasets were preprocessed and cleaned but essentially represent collated data aggregated from existing sources without significant fundamental modification. This collated data formed the base for further augmentation."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.2 Synthetic Text Generation, Section 4.3 Synthetic Audio Generation",
          "reasoning": "The synthetic texts were generated by applying a modified delexicalisation algorithm and seq2seq model on the original corpus, which is a transformation and adaptation of the existing textual data. Then, synthetic audio was derived by synthesizing these texts using a TTS model trained on a subset of original audio. Therefore, the synthetic audio is derived from the original data through a multi-step process involving model-based transformations. This indicates a derived data nature."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin and methods of generation are explicitly documented in the paper."
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10612,
      "completion_tokens": 234,
      "total_tokens": 10846
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.4 (Training of Southern Quechua ASR system) and Section 5 (Results)",
          "reasoning": "The authors used the introduced augmented dataset, consisting of 99 hours of original transcribed audio plus 99 hours of synthetic audio generated via their data augmentation method, to train an ASR model from scratch with the wav2letter++ architecture, as described in Section 4.4 and evaluated in Section 5."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 11335,
      "completion_tokens": 608,
      "total_tokens": 11943
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset introduced in the paper contains only one language, Southern Quechua, as discussed throughout the paper. Although English is used for explanation and in bilingual dictionaries for linguistic processing, the dataset entries themselves are exclusively in Quechua."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not bilingual. While the paper mentions the use of Quechua-English bilingual dictionaries for semantic frame labeling in synthetic text generation (Section 4.2), the dataset entries are not presented as bilingual text pairs but only as Southern Quechua sentences."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain only English content. English appears in translations and explanations but is not part of the dataset entries."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1, 'The Corpora'; Section 2, Quechua Language",
          "reasoning": "The new dataset introduced is a speech corpus in Southern Quechua, a non-English language. It contains 99 hours of transcribed audio solely in Southern Quechua, with synthetic text and audio also in the same language. There is no indication of presence of other human languages in the data entries themselves."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper discusses the use of machine learning toolkits and models, the dataset itself does not contain code or programming language entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical notations are present in the dataset entries."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include biological sequences or non-human communication data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain any constructed or fictional languages; it only contains Southern Quechua."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset is clearly specified as Southern Quechua."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains human language data (Southern Quechua), so the 'N/A' category does not apply."
        }
      }
    }
  },
  {
    "id": "zevallos22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8553,
      "completion_tokens": 163,
      "total_tokens": 8716
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 4.1 The Corpora",
          "reasoning": "The paper states that both the Siminchik corpus and the Lurin corpus can be found at https://github.com/Llamacha/quechua_resources, indicating that the associated data and code used for collection and preprocessing are publicly available in an accessible repository."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1 The Corpora and Section 4 Methodology",
          "reasoning": "The paper provides detailed descriptions of the dataset creation and preprocessing steps, including cleaning audio files, segmentation, normalization to Southern Quechua standard, and the usage of tools like the Quechua morphological parser and normalizer. This documentation comprehensively covers the dataset construction process."
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7250,
      "completion_tokens": 183,
      "total_tokens": 7433
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "The paper explicitly states that the authors collected speech recordings from 254 Chinese male patients who read a given Chinese text in sitting and lying positions, with audio recorded at 16kHz. This speech data is newly collected by the authors for their study, indicating the audio modality and human origin."
        },
        {
          "Modality": "tabular",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 4.1 Dataset",
          "Reasoning": "Patient characteristic features such as age, BMI, and neck circumference were collected alongside the speech data from the patients. These features are manually gathered clinical data about the patients, representing tabular data with human origin, introduced newly by the authors for their OSA detection study."
        }
      ]
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8050,
      "completion_tokens": 238,
      "total_tokens": 8288
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 4.1 Dataset",
            "reasoning": "The dataset was collected from 254 patients at Beijing Tongren Hospital, and patient characteristics along with speech recordings were obtained following ethics committee approval. This implies clinical and expert human involvement for data collection and possibly annotation of labels such as AHI severity."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 Dataset",
            "reasoning": "Patients were asked to read a specified Chinese text consisting of 10 sentences in two positions, indicating instructions were given to the patients for consistent data recording."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1 Dataset",
            "reasoning": "Labels were assigned based on the apnea-hypopnea index (AHI) threshold of 30 events/h as a gold standard for OSA diagnosis, which serves as a rubric for classification."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no explicit mention of examples of annotations or labeled data provided in the guidelines or appendix; the paper only describes the dataset and labeling criteria without examples."
          }
        }
      ]
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9240,
      "completion_tokens": 356,
      "total_tokens": 9596
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance conducted by a single human expert on the dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that multiple human experts performed quality assurance on the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper provides no evidence that a single non-expert was responsible for quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication is given that multiple non-expert humans performed quality assurance on the data."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Quality assurance using an AI model to validate the dataset annotations or content is not described."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated or algorithmic quality assurance processes applied to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not describe any quality assurance process for the in-house OSA speech dataset collected from 254 patients. No information about annotation validation, expert verification, or automated QA processes is provided, so it is considered that no QA process is documented."
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8850,
      "completion_tokens": 387,
      "total_tokens": 9237
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The paper states that the authors collaborated with Beijing Tongren Hospital to collect data from 254 patients who read a set piece of text in both sitting and lying positions. This data was recorded specifically for this study, thus it is original human-contributed speech data generated from scratch."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated entirely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by human translation from another language."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data produced by machine translation from another language."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset was not aggregated from existing sources; it was collected anew from patients and thus not collated."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Though the authors use pre-trained models, the actual dataset collected consists of original recordings and patient characteristics, not derived from existing datasets or transformed versions thereof."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data source is clearly documented as newly collected data from human patients in Section 4.1."
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9368,
      "completion_tokens": 471,
      "total_tokens": 9839
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The model uses a pre-trained XLSR encoder and fine-tunes it with the dataset; there is no mention of training a model from scratch with the dataset."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 3 (Method), Section 4.1 (Dataset), Section 5 (Results)",
          "reasoning": "The in-house dataset collected by the authors is used to fine-tune the pre-trained XLSR model in a supervised manner for OSA detection. Section 3 describes using supervised training with audio-AHI paired data. Section 4.1 describes the dataset collection. Section 5 shows results based on fine-tuning."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning or RL-based post-training methods are described or used with the dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4.1 (Dataset), Section 5 (Results)",
          "reasoning": "The dataset is split into training, validation, and test sets, and the test set is used to evaluate and benchmark model performance as described in Sections 4.1 and 5."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is used for training and evaluation, not primarily for analysis of trends or characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used as a knowledge base or for retrieval-augmented generation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset has clear and practical usage in both supervised fine-tuning and evaluation stages."
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10091,
      "completion_tokens": 356,
      "total_tokens": 10447
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 4.1 Dataset",
          "reasoning": "The in-house dataset consists of speech recordings from 254 patients who read a piece of Chinese text consisting of 10 sentences. The paper explicitly states that all patients were Chinese males and that the text read was in Chinese, indicating the dataset is monolingual and non-English."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang23b_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7309,
      "completion_tokens": 118,
      "total_tokens": 7427
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 4.1",
          "reasoning": "The paper describes the dataset collection procedure but does not include any mention of publicly available code repositories or links related to the dataset construction."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 4.1",
          "reasoning": "The paper provides detailed documentation of the dataset collection including patient demographics, data acquisition procedure (reading 10 sentences in sitting and lying positions), recording specifications (sample rate), and ethical approval. This constitutes thorough documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7256,
      "completion_tokens": 229,
      "total_tokens": 7485
    },
    "response": {
      "sources": [
        {
          "Modality": "signal/sensor",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data set",
          "Reasoning": "The newly built EEG dataset was collected from 12 healthy participants during EEG recording using a 64-channel electrode cap and NeuroScan Acquire 4.3 software, as described in Section 2.1. The EEG data comprises approximately 240 minutes of recordings evoked by speech emotional prosody stimuli, and was manually recorded in a controlled experimental protocol by human subjects."
        },
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 Data set",
          "Reasoning": "The emotional prosody speech stimuli used to evoke the EEG responses were selected from the Mandarin Chinese Speech Emotional Stimulation Database (MCAESD) built by natives with broadcasting experience. The stimuli consist of concatenated declarative Mandarin Chinese meaningless sentences spoken by human actors to evoke specific emotions. This dataset was manually recorded by human speakers and validated by human listeners, as explicitly stated in Section 2.1."
        }
      ]
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8056,
      "completion_tokens": 230,
      "total_tokens": 8286
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.1 Data set",
            "reasoning": "The EEG emotion data set was collected from 12 healthy participants (subjects/decoders) who responded to emotional prosody speech stimuli and provided emotion annotations via self-assessment after each clip, indicating multiple human subjects performed annotations."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Data set",
            "reasoning": "Participants were informed of the purpose, protocol, and precautions of the EEG experiment before annotation, indicating the presence of instructions provided for the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.1 Data set",
            "reasoning": "No explicit mention of scoring rubrics or annotation scoring guidelines provided to subjects is found; annotation was a self-assessment task choosing among four emotion categories with no detailed rubric described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.1 Data set",
            "reasoning": "The paper does not describe providing annotation examples to the subjects who performed the emotion identification during EEG experiment sessions."
          }
        }
      ]
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9246,
      "completion_tokens": 347,
      "total_tokens": 9593
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The speech stimuli were recorded by six students with broadcasting experience (emotion encoders), and then verified by a group of listeners who did not know the emotion labels of the speeches. The presence of multiple emotion encoders and multiple listeners for annotation and validation indicates quality assurance was performed by multiple human experts or members of the target demographic, as broadcasters and listeners are proficient in the auditory task and emotional perception."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information indicating that QA was performed by a single non-expert annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "Listeners who verified the recordings by providing annotation results were not given any indication of the emotion labels, suggesting they were multiple non-expert human annotators contributing to valence of emotional intensity of stimuli. This demonstrates QA by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of AI models being used for quality assurance."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No indication that automated verification or algorithmic QA was conducted on dataset annotation or content."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes human validation procedures for the speech stimuli, indicating that QA was applied and documented."
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8856,
      "completion_tokens": 516,
      "total_tokens": 9372
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The EEG dataset was newly built by collecting EEG data from 12 healthy participants responding to specific speech stimuli selected from the Mandarin Chinese Speech Emotional Stimulation Database (MCAESD). The speech stimuli were recorded by human speakers with broadcast experience, and the EEG data was collected through human-conducted experiments. Therefore, both the EEG data and the speech stimuli inducing the EEG responses are original human-generated data collected via experimental procedures described in the paper."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that any data was generated entirely by AI or machine learning models. All data discussed are collected from human participants or existing speech databases."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was produced by human translation from other languages."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention that any data was generated via machine translation methods."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The speech stimuli used to evoke EEG responses were selected from an existing speech database (MCAESD). The authors aggregated and selected specific speech stimuli based on evaluation criteria to build the EEG experiment materials. Hence, the speech stimuli component can be considered collated from existing sources without significant modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The authors concatenated multiple speech sentences from the existing MCAESD database to create longer, approximately one-minute clips to induce stable EEG responses, which is a modification and adaptation of the original speech stimuli. Thus, the final speech stimuli can be considered derived data based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and methods of generation for the data are clearly described in the paper, so 'N/A' does not apply."
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9374,
      "completion_tokens": 335,
      "total_tokens": 9709
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3.1, Experiments",
          "reasoning": "The paper describes training the EEG-Prosody CRNN model from scratch on the newly constructed EEG dataset and prosody features using supervised learning, as detailed in the experiments section."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of fine-tuning a pre-trained model using this dataset; the model is trained directly on the dataset."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No reinforcement learning based post-training (such as RLHF) is described or utilized on this dataset."
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 3.2, Results and Discussions",
          "reasoning": "The dataset is used as a benchmark for evaluating the performance of the proposed and baseline models, with classification accuracies reported using cross-validation."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 3.2, Results and Discussions and Section 4, Conclusion",
          "reasoning": "The dataset is used to analyze feature importances, frequency bands, and to assess the impact of emotional prosody features on EEG emotion classification, providing insights beyond just training and testing."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not used or described as a knowledge base to augment models."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10097,
      "completion_tokens": 542,
      "total_tokens": 10639
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset described in the paper uses only Mandarin Chinese in the speech stimuli, so it does not contain more than two human languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset contains only one language, Mandarin Chinese; therefore, it is not bilingual."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English speech stimuli; it is based on Mandarin Chinese meaningless sentences."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.1 Data set",
          "reasoning": "The paper states that the speech stimuli are selected from the Mandarin Chinese Speech Emotional Stimulation Database and consists of Chinese meaningless sentences. All subjects are native Mandarin Chinese speakers. Thus, the dataset entries contain exactly one non-English language: Mandarin Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not include programming or structured code content; it consists of speech stimuli and EEG recordings."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain mathematical or formal logical expressions or symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although EEG recordings are biological signals, the dataset entries are EEG data and speech stimuli, the stimuli being human language speech, not non-human communication systems or biological sequences like DNA."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The speech stimuli use Mandarin Chinese meaningless sentences, not constructed languages such as Klingon or Esperanto."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language of the dataset entries is clearly specified as Mandarin Chinese, so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language stimuli in Mandarin Chinese and EEG recordings; therefore, it cannot be considered as having no language."
        }
      }
    }
  },
  {
    "id": "zhang23f_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7315,
      "completion_tokens": 161,
      "total_tokens": 7476
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "No mention in the text",
          "reasoning": "The paper does not contain any mention or link to code repositories or supplementary materials related to code for constructing or processing the dataset. There is no indication that the code is publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.1 (Data set) and Section 2 (Material and Methods)",
          "reasoning": "The paper provides a detailed description of the dataset creation process, including the selection criteria for speech stimuli from MCAESD, description of the EEG recording experiment, participant demographics, experiment protocol, equipment and preprocessing methods. This extensive documentation covers how the EEG data and stimuli were collected and processed for the new EEG emotional dataset built by the authors."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7287,
      "completion_tokens": 129,
      "total_tokens": 7416
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 3.2 CVTE-poly",
          "Reasoning": "The CVTE-poly dataset is a new dataset introduced by the authors, consisting of sentences containing target polyphone words crawled from \u2018Baidu Baike\u2019, a human-generated encyclopedia website. The labels for pronunciations were manually confirmed and corrected for polyphones with multiple pronunciations. Therefore, the data are text modality and originate from human-generated sources (webpages authored by people and manual annotation). No indication of model-generated or synthetic data is present."
        }
      ]
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8087,
      "completion_tokens": 219,
      "total_tokens": 8306
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.2",
            "reasoning": "Section 3.2 states that after crawling sentences with polyphones, some labels were manually confirmed and checked to ensure correctness for cases with multiple pronunciations, implying that multiple human experts performed the manual confirmation and labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not mention any detailed annotation instructions provided to annotators for CVTE-poly manual confirmation. Only that manual confirmation was performed, without describing instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "There is no mention in the paper of scoring rubrics or detailed criteria to guide the annotation or manual confirmation in CVTE-poly dataset creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 3.2",
            "reasoning": "The paper does not provide or mention annotation examples or sample labels given as guidance during the manual confirming step of CVTE-poly."
          }
        }
      ]
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9277,
      "completion_tokens": 341,
      "total_tokens": 9618
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.2 (paragraph 3)",
          "reasoning": "The paper states that for polyphones with more than one pronunciation in a word, the authors check the sentences and labels manually. This implies that a manual review was done by human annotators with knowledge of Chinese pronunciation and linguistics to confirm some labels. Since it does not mention multiple experts, it indicates QA was likely done by single human experts during manual confirmation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no explicit mention in the paper that multiple human experts performed quality assurance or labeling. The manual confirmation is described without stating involvement of multiple experts."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the QA was conducted by a single non-expert or without expertise. The manual confirmation suggests knowledgeable annotators."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention that multiple non-expert annotators performed QA."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe using AI models to perform quality assurance of the dataset labeling."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of automated verification or rule-based algorithmic validation used as quality assurance for the dataset labels."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly describes a manual confirmation step for ambiguous pronunciations in Section 3.2, indicating that some form of quality assurance was indeed applied."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8887,
      "completion_tokens": 433,
      "total_tokens": 9320
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate that the dataset was created entirely from scratch by human contributors without relying on existing data sources. Instead, data was collected and then refined manually."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The data was not generated by AI or machine learning models. The dataset was created by crawling web sources and manual annotation."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of translating content from another language using human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that machine translation was used to produce the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 3.2 CVTE-poly",
          "reasoning": "The dataset was created by collecting polyphone words from the latest official dictionary (MCD-7) and then crawling sentences containing these words from Baidu Baike, an existing web encyclopedia. This indicates data collection from existing sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 3.1 Refinements to CPP and Section 3.2 CVTE-poly",
          "reasoning": "The authors refined the existing CPP dataset by correcting wrong labels and removing certain samples. Additionally, for CVTE-poly, after crawling sentences from Baidu Baike, some labels were manually confirmed or corrected to ensure quality. Thus, the dataset is not purely raw collection but contains modifications derived from existing sources."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper specifies clearly the collection and refinement process; data origin is documented."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9405,
      "completion_tokens": 480,
      "total_tokens": 9885
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention using CVTE-poly for pre-training large models in an unsupervised or self-supervised manner. Instead, it leverages pre-trained language models as backbones."
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper uses pre-trained language models (ERNIE) as the backbone and does not describe training models from randomly initialized parameters using CVTE-poly."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 4 and 5 (Baseline configuration, Experiments)",
          "reasoning": "The CVTE-poly dataset is used to fine-tune pre-trained ERNIE models with supervised learning by training a fully connected layer and softmax layer on top to predict polyphone pronunciations."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of using CVTE-poly for reinforcement learning post-training techniques such as RLHF is provided in the paper."
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The CVTE-poly dataset is not used exclusively for evaluation or benchmarking. Instead, the paper uses the original CPP test set for evaluation; CVTE-poly is primarily a training dataset."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While analysis is conducted regarding dataset balance and bias, CVTE-poly itself is not primarily used for analysis but for training."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe CVTE-poly serving as a knowledge base for retrieval-augmented generation or model augmentation."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly uses CVTE-poly in training and demonstrates its utility in the modeling pipeline."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10128,
      "completion_tokens": 427,
      "total_tokens": 10555
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset CVTE-poly focuses exclusively on Chinese text with polyphone disambiguation, and there is no mention of multiple human languages in the dataset entries."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset contains only Chinese language data; there is no indication of exactly two languages being present."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries are not in English; they are in Chinese characters with Pinyin annotations for pronunciation."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 3.2 (CVTE-poly description) and throughout the paper",
          "reasoning": "The dataset CVTE-poly is exclusively Chinese language data, consisting of Chinese polyphone characters and their correct Pinyin pronunciations based on context. There is no inclusion of other languages besides Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No programming or code language content is present within the dataset entries; the dataset consists of natural language text in Chinese."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset entries do not include mathematical or formal logical symbolic representations."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No biological or non-human communication data is part of the dataset, which is strictly Chinese textual data."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "No mention or inclusion of fictional or constructed languages such as Klingon or Esperanto is present."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "N/A",
          "reasoning": "The dataset explicitly documents its language as Chinese with detailed annotations; language is clearly specified."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset entries contain Chinese language text; therefore, language is present."
        }
      }
    }
  },
  {
    "id": "zhang23h_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7346,
      "completion_tokens": 160,
      "total_tokens": 7506
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract and footnote on page 1",
          "reasoning": "The paper states in the abstract and footnote that both codes and data are available to the public at a specific URL (https://github.com/NewZsh/polyphone). This indicates that the code related to data collection and processing is made publicly available."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 3, especially 3.2 CVTE-poly",
          "reasoning": "The paper provides detailed documentation on the dataset creation process in Section 3, describing the pipeline for collecting polyphones from the dictionary, crawling sentences from Baidu Baike, manual checking of ambiguous labels, and data statistics. This constitutes transparent and complete documentation of the dataset construction."
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7903,
      "completion_tokens": 172,
      "total_tokens": 8075
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 1 Introduction, Section 4 Method, Section 5 Experiments",
          "Reasoning": "The paper explicitly states that they build and publish a new personalised speech enhancement (PSE) dataset to reflect on-device close-talk microphone scenarios with numerous types of environmental noise. This dataset includes corrupted input utterances paired with enrolment utterances from the same speaker. The clean speech is from LibriSpeech, but the new dataset is constructed by the authors by adding silence, overlapping and non-overlapping noise, and noise can start/end anywhere in the clip to better represent on-device scenarios. This data consists of audio recordings (speech and noise mixtures) captured with human involvement (human speech recordings from LibriSpeech and ambient noises)."
        }
      ]
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8703,
      "completion_tokens": 273,
      "total_tokens": 8976
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 1 (Abstract), Section 4 (Method), Section 5 (Experiments)",
            "reasoning": "The paper describes the creation of a new personalized speech enhancement (PSE) dataset reflecting on-device close-talk microphone scenarios. The dataset is constructed by algorithmically mixing clean speech from LibriSpeech with environmental ambient noise and babble noise from datasets like FreeSoundDataset and LibriMix, applying silence periods and noise overlays. There is no mention of human annotators performing manual labeling; the dataset generation is via controlled data synthesis and mixing methods, thus it is an automatic process."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "The paper does not describe any annotation instructions provided to annotators for this dataset, as the dataset is generated automatically through data mixing and augmentation, not by human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No scoring rubrics or evaluation guidelines for manual annotation are discussed because the dataset is synthesized automatically rather than annotated by humans."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Not explicitly mentioned",
            "reasoning": "No examples of annotation guidelines or labeled data examples are provided because the data generation is automatic and not manual annotation."
          }
        }
      ]
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9893,
      "completion_tokens": 284,
      "total_tokens": 10177
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any human annotation or quality assurance performed by a single human expert for their newly introduced dataset."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that multiple human experts were involved in the quality assurance of the dataset."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No details about quality assurance by a single human non-expert are provided."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance by multiple non-expert annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The QA process is not described as being performed by an AI model."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not document any automated verification or rule-based quality assurance applied to the dataset."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper describes the creation of a new dataset that synthesizes corrupted and clean speech data to match on-device close-talk microphone scenarios. However, it does not provide any description of a quality assurance or validation process applied to the dataset, nor any human or automated verification methods. Therefore, no quality assurance process is documented."
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9503,
      "completion_tokens": 448,
      "total_tokens": 9951
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention the creation of entirely original datasets made from scratch by human contributors. Instead, it constructs its dataset using existing data sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that any data was generated purely by AI or machine learning models without reference to existing data."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not discuss translating data from another language, neither by humans nor by machines."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating data by machine translating from other languages."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section '4.1 Self-attention Encoder' and Abstract",
          "reasoning": "The paper states that the new dataset is built to reflect on-device close-talk microphone scenarios and uses speech samples from existing datasets like LibriSpeech and noise samples from FreeSoundDataset and LibriMix. Thus, the dataset is aggregated from pre-existing sources without creation of entirely new original recordings."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section '4.1 Self-attention Encoder', Abstract",
          "reasoning": "The authors generate a new PSE dataset by applying modifications such as adding noise, periods of silence, overlapping/non-overlapping noises, and varying noise placement to existing speech data from LibriSpeech. These transformations constitute derivation since the data is based on existing sources but adapted to reflect real-world on-device use cases."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The data origin is documented in the paper; therefore, N/A is not applicable."
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10021,
      "completion_tokens": 261,
      "total_tokens": 10282
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4, Section 5",
          "reasoning": "The newly built dataset reflects the on-device close-talk microphone scenario and is used to train the proposed fully Transformer-based personalized speech enhancement (PSE) models from scratch. The paper details training experiments on this dataset, indicating it is used for training the models from randomly initialized parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5, Table 1 and 2",
          "reasoning": "The dataset is used to evaluate model performance in terms of signal-to-distortion ratio (SDR) and word error rate (WER) for downstream ASR tasks, benchmarking the proposed models against baselines."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10744,
      "completion_tokens": 566,
      "total_tokens": 11310
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper's introduced dataset is based on LibriSpeech, which is an English speech corpus. There is no mention of including multiple languages; therefore, the dataset is not multilingual."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two human languages; it is based solely on English speech from LibriSpeech."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 4.1 and throughout the paper, e.g., 'For ground-truth clean speech, we use LibriSpeech [38]'",
          "reasoning": "The new dataset is constructed by using LibriSpeech as clean speech source, which contains only English language. The paper specifically states using LibriSpeech for training and testing, indicating dataset is monolingual English."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on LibriSpeech, an English dataset, so non-English monolingual does not apply."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech audio data combined with noise; there is no programming or code content included in dataset entries."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper describes mathematical expressions related to the model and methods, the dataset entries do not include mathematical or logical notation as content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset exclusively consists of human speech audio and environmental noise; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset contains any constructed or fictional languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset language is clearly documented as English from LibriSpeech, so the language is known."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of speech audio data in English and thus contains language."
        }
      }
    }
  },
  {
    "id": "zhang23r_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7962,
      "completion_tokens": 187,
      "total_tokens": 8149
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "Section 1 Introduction, Section 5 Evaluation",
          "reasoning": "The paper mentions that the authors design and publish a new PSE dataset for on-device close-talk microphone scenarios, but there is no explicit link, URL, or reference to any publicly available code repository for dataset construction, preprocessing, or generation provided in the paper."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 1 Introduction, Section 5 Evaluation",
          "reasoning": "The paper documents the dataset creation process in general terms: describing the use of LibriSpeech for clean speech, adding periods of silence, overlapping and non-overlapping noise, types of environment noise from FreeSoundDataset and LibriMix babble noise, and aligns the data splits for train, dev, and test sets. These details on dataset construction are sufficient to understand the dataset creation process even though not exhaustive."
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 6700,
      "completion_tokens": 158,
      "total_tokens": 6858
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.4 Data collection",
          "Reasoning": "The DysArinVox dataset includes speech recordings from 173 participants including healthy individuals and those with vocal disorders. Recordings are collected in professional studio settings using human speakers who read carefully designed Mandarin scripts covering phonemes and various speech styles."
        },
        {
          "Modality": "image",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 3.4 Data collection",
          "Reasoning": "The dataset provides laryngoscopic images of patients, which are medical images captured via human-operated laryngoscopic equipment, thus constituting human generated data in image modality."
        }
      ]
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7500,
      "completion_tokens": 208,
      "total_tokens": 7708
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 3.3",
            "reasoning": "Annotations include subjective (MOS and GRBAS scores) assessed by professional doctors, which implies multiple human experts were involved in the perceptual evaluation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "No explicit section describing annotation instructions",
            "reasoning": "The paper does not provide explicit details or mention detailed annotation instructions given to the experts for conducting MOS and GRBAS evaluations or laryngoscopic labeling."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3",
            "reasoning": "GRBAS and MOS scoring systems following established clinical evaluation scales are used, which implies the presence of scoring rubrics for consistent judgment across experts."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3, Table 4",
            "reasoning": "Table 4 provides examples of subjective evaluations and objective laryngoscopic indices for four sample speakers, demonstrating example annotations."
          }
        }
      ]
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 8690,
      "completion_tokens": 400,
      "total_tokens": 9090
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": true,
          "reference": "Section 3.3 (Labels in DysArinVox)",
          "reasoning": "The dataset's subjective evaluations (MOS and GRBAS scores) were provided by professional doctors, who are subject matter experts in speech pathology, as clearly described in Section 3.3. This indicates that at least a single human expert conducted quality assurance via perceptual evaluation."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention involvement of multiple human experts jointly validating annotations or labels. It only states evaluations by professional doctors, without specifying multiple independent experts performing QA."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that non-expert annotators conducted any quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information is provided about quality assurance by non-experts."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any use of AI models performing quality assurance or judging annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention is made of algorithmic or automatic verification techniques used as a quality assurance process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes multiple quality assurance steps, including expert ratings and detailed laryngoscopic annotations; therefore, it is incorrect to say no QA was performed."
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8300,
      "completion_tokens": 419,
      "total_tokens": 8719
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 3 (The construction of DysArinVox)",
          "reasoning": "The DysArinVox dataset consists of speech recordings and laryngoscopic images collected directly from 173 participants, including 146 pathological speakers and 27 healthy controls. The recordings were designed and recorded specifically for this corpus, with original, carefully crafted scripts covering all Mandarin phonemes and prosody variations. The data were recorded in professional settings with human participants and involved expert clinical diagnostic labels and evaluations by speech pathology professionals. Therefore, the data is original content generated entirely from scratch by human subjects and clinical annotations."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data generated by AI or machine learning models. All data are collected from human participants and clinical evaluations."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not indicate any data produced by translating content from another language through human translators. The corpus is composed of original Chinese speech data."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the data included machine translated content."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The corpus is newly recorded and does not aggregate or compile existing datasets."
        },
        "Derived": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No information shows that the data is based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly documents the origin of the data collection."
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 8818,
      "completion_tokens": 258,
      "total_tokens": 9076
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 1, 3.3, and 4",
          "reasoning": "The paper primarily introduces DysArinVox as a resource for pathological speech research and analysis. The corpus provides detailed subjective and objective evaluations (e.g., MOS, GRBAS scores, laryngoscopic indices) and rich labels for diverse disorders, aimed at enhancing the understanding of physical bases of speech disorders, patterns, and characteristics. The dataset is emphasized as a foundation for further research and diagnostic framework development, rather than being explicitly described as used for model training or evaluation in the paper."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9541,
      "completion_tokens": 578,
      "total_tokens": 10119
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset DysArinVox only contains entries in Mandarin Chinese. No mention is made of any other human languages being included."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain exactly two languages; it only contains Mandarin Chinese."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset does not contain English content; it is developed specifically for Mandarin Chinese pathological speech."
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Abstract, Section 3.1",
          "reasoning": "The DysArinVox corpus contains only Mandarin Chinese speech data. The paper explicitly states that the corpus is a 'Chinese pathological speech corpus' and that recording scripts were designed to cover phonemes in Mandarin Chinese, including modern and classical Chinese poetry (Section 3.1). Hence, it is considered monolingual with exactly one non-English language, Mandarin."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of speech and laryngoscopic image data related to Mandarin speech disorders. There is no indication of including programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mathematical or logical symbolic representations or formal notations are described as entries in the dataset."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the dataset includes laryngoscopic images showing the anatomical vocal folds, these visual data are medical images rather than biological sequences or non-human communication data such as DNA sequences or animal signals."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention of constructed or fictional languages in the dataset; all speech data is in natural Mandarin Chinese."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language composition of the dataset is explicitly documented as Mandarin Chinese; it is not unspecified or unknown."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains spoken language data in Mandarin Chinese, so it does contain language entries."
        }
      }
    }
  },
  {
    "id": "zhang24l_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 6759,
      "completion_tokens": 153,
      "total_tokens": 6912
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": false,
          "reference": "N/A",
          "reasoning": "The paper does not mention any publicly available code or provide links to code repositories related to the construction, preprocessing, or generation of the DysArinVox dataset."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 1, 2, and 3 (especially Section 3)",
          "reasoning": "The paper provides detailed documentation of the dataset creation process including recording script design (Section 3.1), diversity of disorders (Section 3.2), labeling methodology (Section 3.3), and data collection environment and procedures (Section 3.4). This comprehensive description of methods ensures transparency and allows understanding of dataset construction."
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 8091,
      "completion_tokens": 330,
      "total_tokens": 8421
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2, Database description: The MISP2021-A VWWS database is described as a collection of recorded audio-visual data from conversations in a home TV scenario, including far-, mid-, and near-field audio data. Further, Section 4.1 mentions audio data simulation using pyroomacoustic and noise addition with MISP2021 tools.",
          "Reasoning": "The audio data in the MISP2021-A VWWS database includes real recorded human speech collected during sessions from native speakers (human generated). However, Section 4.1 states that data simulation for audio data was done using room impulse response generation and noise addition tools, indicating that part of the audio data was also model generated (simulated). Therefore, both human and model generated labels are true for audio modality."
        },
        {
          "Modality": "video",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.2 and 3.2 describe video data as the lip-region video of speakers captured by cameras in the recording sessions. The video data is collected from actual speakers in natural conversation scenarios. Section 3.2 details using video frames as input for lip feature extraction.",
          "Reasoning": "The video data modality consists of real human-recorded video sequences of speakers' lip regions during data collection sessions, implying human involvement during data capture. There is no mention of video data being generated or simulated by models. Thus, only Human Generated is true and Model Generated is false for video modality."
        }
      ]
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8891,
      "completion_tokens": 213,
      "total_tokens": 9104
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Experts",
            "reference": "Section 2.2 The MISP2021-A VWWS database",
            "reasoning": "The database consists of 327 native Chinese Mandarin speakers recorded in multiple sessions with controlled scenarios and speakers. The paper mentions data checking to remove asynchronous samples, indicating human expert involvement for quality control and annotation."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not explicitly mention any annotation instructions provided to annotators or labelers for this newly introduced dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "There is no mention of scoring rubrics or detailed annotation criteria for human annotators in the database documentation in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2.2",
            "reasoning": "The paper does not describe providing annotation examples or guideline examples for data annotators for the MISP2021-A VWWS database."
          }
        }
      ]
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 10081,
      "completion_tokens": 378,
      "total_tokens": 10459
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information indicating that quality assurance was conducted by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by multiple human experts or annotators with subject matter expertise."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that quality assurance was done by a single non-expert human annotator."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any QA process involving multiple non-expert human annotators."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description of quality assurance performed by an AI model acting as a judge on the dataset annotations or content."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any automated verification or algorithmic/rule-based quality assurance processes for dataset validation."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "The paper does not explicitly document any quality assurance process applied to validate or verify the dataset annotations or content. Although the dataset was double-checked to remove asynchronous samples (Section 2.2), no description is given about the QA process or the annotators involved."
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9691,
      "completion_tokens": 539,
      "total_tokens": 10230
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The MISP2021-A VWWS database is described as a newly designed and publicly released audio-visual wake word spotting database. It includes about 125 hours of audio-visual data collected from 327 native Mandarin Chinese speakers in various realistic conversational home TV scenarios. The data involve multi-microphone audio and multi-camera video recordings at multiple distances (near-, mid-, and far-field). The authors explicitly state that this database was designed to overcome the limitations of existing corpora and the recordings were made with human participants, indicating the data is original content created from scratch by human contributors."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention generating any dataset content fully synthetically or automatically by models. While data augmentation is applied during training, the original dataset is recorded from human speakers in real environments."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication that the dataset involves translated content from other languages via human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any machine translation involvement in the dataset creation."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not collected or aggregated from existing sources. The data were newly recorded by the authors' teams specifically for this challenge and study."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 4.1",
          "reasoning": "In Section 4.1, the authors describe audio data simulation involving Room Impulse Responses (RIR) generated by pyroomacoustic toolkit according to real room sizes and microphone positions, and adding noise with different SNR levels using official MISP2021 tools. This indicates that part of the data (simulated audio variations) is derived from original recorded data by applying modifications and transformations including reverberation and noise addition to augment the training data."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper clearly describes the origin and methods of data generation for the introduced dataset; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 10209,
      "completion_tokens": 340,
      "total_tokens": 10549
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), especially 4.1 (Data simulation and implementation details) and 4.2 - 4.4 (Results)",
          "reasoning": "The MISP2021-A VWWS database is used to train audio-only, video-only, and audio-visual fusion wake word spotting models from scratch, as described in Section 4. Data augmentation and training details confirm usage for training models starting from initial parameters."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments) and Section 3.4 (Evaluation)",
          "reasoning": "The database is used for evaluating system performance on metrics such as false alarm rate, false reject rate, and combined Score on development and evaluation subsets, as detailed in Sections 3.4 and 4."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Sections 4.2 - 4.4 and the Conclusion",
          "reasoning": "The dataset is used for analyzing system behavior and the complementary benefit of visual information to acoustic data, including detailed result analysis and misclassification examination, indicating usage for analysis of trends and characteristics."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10932,
      "completion_tokens": 355,
      "total_tokens": 11287
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (Non-English)": {
          "is_applicable": true,
          "reference": "Section 2.2 - The MISP2021-A VWWS database",
          "reasoning": "The dataset consists of audio-visual data where the wake word is \u201cXiao T Xiao T\u201d. All speakers are native Chinese speaking Mandarin without strong accents, indicating the dataset contains exactly one non-English human language, Mandarin Chinese."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhou22g_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 8150,
      "completion_tokens": 155,
      "total_tokens": 8305
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2.2",
          "reasoning": "The paper explicitly states in the abstract and in Section 2.2 that the MISP2021 audio-visual wake word spotting database and the code are released publicly online. This indicates that the code related to data collection, preprocessing, and generation is made available to the community."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2.2 (Database)",
          "reasoning": "The paper provides a detailed description of the dataset creation process in Section 2.2, including dataset features, speaker distribution, recording details, data segmentation, and specifications of data fields and modalities. This constitutes clear documentation of the dataset creation process."
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 9441,
      "completion_tokens": 262,
      "total_tokens": 9703
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Section 2.1 and 4.1",
          "Reasoning": "The VCTK-Stutter dataset is created by extending the existing VCTK++ dataset via editing real acoustic waveforms based on forced alignment and word-level alignments acquired by WhisperX. It introduces dysfluencies such as word repetition and missing by editing audio to introduce silences or repetitions. Since it involves extensions on real human speech recordings from the VCTK corpus, the audio data are human generated recordings with manual or rule-based editing, not synthesized by a model."
        },
        {
          "Modality": "audio",
          "Human Generated": false,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2.2, especially 2.2.1 and 2.2.2",
          "Reasoning": "The VCTK-TTS dataset is a synthetic dysfluency dataset created by injecting dysfluencies in the text (phoneme) space using TTS rules and then generating corresponding speech via the VITS text-to-speech model. This process is fully automated where the dysfluent speech audio is synthesized by the VITS model, making the audio data model generated rather than human recorded."
        }
      ]
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 10241,
      "completion_tokens": 318,
      "total_tokens": 10559
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2, subsections 2.1 VCTK-Stutter and 2.2 VCTK-TTS",
            "reasoning": "The paper describes that VCTK-Stutter was created by extending VCTK++ using forced alignment and audio editing with WhisperX for word-level alignments to insert dysfluencies, and VCTK-TTS was synthesized using TTS rules applied to IPA phoneme sequences via a VITS model generating dysfluent speech automatically. Both datasets are generated through automated simulation pipelines rather than human labeling."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "The paper does not provide or mention any human annotation instructions or guidelines for the simulated datasets; the dysfluency insertion is done via automated rule-based or TTS-based simulation pipelines, not manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "-",
            "reasoning": "There is no mention in the paper of scoring rubrics or evaluation criteria being provided as part of annotation guidelines for these simulated datasets, since annotations are generated automatically through the simulation process."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2 and Figure 1",
            "reasoning": "Section 2 and Figure 1 provide specific examples and descriptions of the dysfluency types injected (e.g., word-repetition, word-missing), and the paper also provides a link to example audio samples illustrating the simulated speech, serving as examples of the annotation/simulation output."
          }
        }
      ]
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 11431,
      "completion_tokens": 475,
      "total_tokens": 11906
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention quality assurance being performed by a single human expert for the new datasets VCTK-Stutter and VCTK-TTS."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any information that multiple human experts performed quality assurance or validation of the dataset annotations or content for VCTK-Stutter or VCTK-TTS."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that a single non-expert human annotator conducted quality assurance for these datasets."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not report any quality assurance conducted by multiple non-expert human annotators for the new datasets."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper mentions usage of AI models such as VITS for speech synthesis and alignment but does not state that these AI models were used explicitly for quality assurance or validation of the datasets."
        },
        "Automatic Process": {
          "is_applicable": true,
          "reference": "Section 2.1 and 2.2 (Naturalistic Dysfluency Simulation)",
          "reasoning": "The datasets VCTK-Stutter and VCTK-TTS were created through automated processes involving forced alignment tools (WhisperX, MFA) and TTS synthesis pipelines with predefined dysfluency rules. The annotation of dysfluency types and time regions was performed automatically based on these tools and rule-based dysfluency injection methods, representing an automated verification approach rather than manual QA."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "There is documentation describing dataset creation and annotation methods that inherently include an automatic quality assurance process, so it is not applicable to state no QA process was performed."
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 11041,
      "completion_tokens": 480,
      "total_tokens": 11521
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "The VCTK-Stutter dataset is an extension of the VCTK++ dataset by incorporating word-level dysfluencies using word alignments generated by WhisperX to insert artificial dysfluencies at precise regions. This process involves manually designing dysfluency annotations and utilizing human-understandable alignments, indicating original content created with human effort, not automated generation."
        },
        "New Data from Model": {
          "is_applicable": true,
          "reference": "Section 2.2",
          "reasoning": "The VCTK-TTS dataset is created by injecting dysfluencies in the text space using TTS rules applied to IPA phoneme sequences, then generating naturalistic dysfluent speech using the VITS text-to-speech synthesis model. This process automatically generates synthesized speech with dysfluencies, constituting new data generated by a machine learning model."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data created by translating content from another language through human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention of data generated by machine translation systems is found in the paper."
        },
        "Collated": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "While the authors use existing datasets like VCTK, their new datasets involve modifications or generation processes rather than mere aggregation without modification."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2.1",
          "reasoning": "VCTK-Stutter is derived from the pre-existing VCTK++ dataset by modifying it with artificial dysfluencies introduced at word-level alignments, hence it is based on existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The origins and generation methods of the new datasets are explicitly stated; thus, N/A does not apply."
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 11559,
      "completion_tokens": 272,
      "total_tokens": 11831
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4.2 Training",
          "reasoning": "The VCTK-Stutter and VCTK-TTS datasets are used to train the YOLO-Stutter detector from scratch as described in the training procedures where the model is trained over multiple epochs on these datasets."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Sections 4.3 Evaluation Metrics, 4.4 Validation, 4.5 Results and Discussions",
          "reasoning": "The datasets VCTK-Stutter and VCTK-TTS are used as test sets for benchmarking the performance of the dysfluency detection model as shown in evaluation metrics and validation sections. Their performance results on these datasets are reported to measure detection accuracy and bound loss."
        },
        "Analysis": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 12282,
      "completion_tokens": 588,
      "total_tokens": 12870
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets introduced are based solely on English speech data derived from the VCTK corpus, which consists of English speakers with different accents but only one language."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No mention or indication of exactly two human languages are present in the datasets. The datasets focus exclusively on English speech."
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Section 2 (Naturalistic Dysfluency Simulation), Section 2.1 (VCTK-Stutter), Section 2.2 (VCTK-TTS)",
          "reasoning": "The two new datasets, VCTK-Stutter and VCTK-TTS, are created from the VCTK corpus which contains English speech from 109 native English speakers with various accents. The simulation rules and injections also operate on English text and phonemes (IPA derived from English sentences). All evaluations and experiments were conducted on English speech data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No datasets with non-English language speech data are introduced."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets consist of speech and aligned dysfluency annotations, without any programming or structured code-related content."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the paper uses mathematical notation to describe loss functions, the datasets themselves do not contain mathematical or logical symbolic representations as content."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets are human speech datasets focused on dysfluency; no biological sequences or non-human communication data are included."
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The datasets use natural (English) language and simulated dysfluencies but do not include any fictional or constructed languages."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The language in the datasets is clearly specified as English."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The datasets obviously contain human language data (English speech and transcriptions), so this category does not apply."
        }
      }
    }
  },
  {
    "id": "zhou24e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 9500,
      "completion_tokens": 234,
      "total_tokens": 9734
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 2.2, Footnote 1",
          "reasoning": "The paper states in the Abstract that \"Code and datasets are open-sourced at https://github.com/rorizzz/YOLO-Stutter,\" indicating public availability. The method pipeline and TTS rules for dataset creation are detailed in Section 2.2.1 and 2.2.2, and a footnote references https://github.com/timmahrt/pysle for tools related to data construction, supporting code availability."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 2, 2.1, 2.2, Tables 1 and 2",
          "reasoning": "The paper provides detailed documentation of the dataset creation process in Section 2 and its subsections (2.1 and 2.2), describing the methodology for simulating dysfluencies in both acoustic and text space, including dysfluency types, annotation procedures, and TTS rules. Tables 1 and 2 present dataset statistics and naturalness evaluation, demonstrating comprehensive documentation of the datasets."
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7740,
      "completion_tokens": 146,
      "total_tokens": 7886
    },
    "response": {
      "sources": [
        {
          "Modality": "text",
          "Human Generated": true,
          "Model Generated": true,
          "Unknown Origin": false,
          "Reference": "Section 2, 'Creating pronunciation dictionaries' and 'Collecting existing dictionaries'",
          "Reasoning": "The new dataset is a curated G2P dictionary dataset combining multiple publicly available pronunciation dictionaries (human generated text data from linguistic resources) and newly created dictionaries generated by applying the espeak-NG model (model generated) on filtered human-created wordlists from Leipzig corpora. Thus, the resulting dataset includes both human-generated text data and model-generated phonetic transcriptions as text. The data modality is 'text' since the dataset consists of word-to-phoneme mappings represented as textual entries."
        }
      ]
    }
  },
  {
    "id": "zhu22_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 8540,
      "completion_tokens": 230,
      "total_tokens": 8770
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Automatic Process",
            "reference": "Section 2: Multilingual Pronunciation Dictionaries",
            "reasoning": "The paper states that pronunciation dictionaries were aggregated from various publicly available G2P datasets and constructed via espeak-NG for some new languages, which is a rule-based tool rather than human annotation. Hence, the annotations come from automatic processes."
          },
          "Has Instructions": {
            "is_applicable": false,
            "reference": "Section 2: Multilingual Pronunciation Dictionaries",
            "reasoning": "There is no mention of detailed annotation instructions provided for human annotators since the data aggregation and dictionary creation were done from existing datasets and via espeak-NG automatically."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "Section 2: Multilingual Pronunciation Dictionaries",
            "reasoning": "The paper does not describe any scoring rubrics or evaluation criteria provided for the annotation process or data creation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "Section 2: Multilingual Pronunciation Dictionaries",
            "reasoning": "No example annotations or annotation guidelines including example mappings are provided or mentioned for the newly created or aggregated datasets."
          }
        }
      ]
    }
  },
  {
    "id": "zhu22_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9730,
      "completion_tokens": 403,
      "total_tokens": 10133
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any quality assurance performed by single human experts for the dataset annotations. There is no information about manual expert validation of the pronunciation dictionaries."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide any details indicating that multiple human experts performed quality assurance or validation on the dataset annotations. There is no mention of expert annotation or verification processes."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no information about quality assurance being conducted by a single human non-expert. The paper does not describe any human annotation or review processes at all."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not describe any human quality assurance by multiple non-experts. No manual annotation or verification by non-expert humans is mentioned."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of using an AI model as a judge or for quality assurance on the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "Section 2, especially \"Collecting existing dictionaries\" and \"Creating pronunciation dictionaries\"",
          "reasoning": "While the dataset is aggregated from multiple sources and some pronunciation dictionaries were created using espeak-NG (an automatic grapheme-to-phoneme system), there is no explicit description of an automated quality assurance process to verify or validate the correctness of the dataset annotations. The data is post-processed to remove duplicates and filter low frequency words, but this is data cleaning rather than QA of annotations."
        },
        "N/A": {
          "is_applicable": true,
          "reasoning": "No explicit quality assurance process for the dataset annotations is documented in the paper. The authors aggregated existing pronunciation dictionaries and generated some dictionaries automatically but do not describe any manual or automated validation or verification steps to ensure annotation quality."
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 9340,
      "completion_tokens": 460,
      "total_tokens": 9800
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that any entirely new datasets were created from scratch by human contributors. Instead, data was aggregated from existing public sources."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No part of the dataset is described as being generated entirely by AI or machine learning models. The data used for training was gathered from existing pronunciation dictionaries or derived through existing tools (e.g., espeak-NG), but no new data was synthesized by models."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any data being produced by human translators or by translating content from one language to another."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no mention in the paper of data being generated by machine translation or derived through automated translation systems."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2, especially 'Collecting existing dictionaries'",
          "reasoning": "The authors aggregated pronunciation dictionaries from various publicly available sources including online dictionaries, dictionary collections like ipa-dict and Wikipron. They merged and cleaned these sources without significant modification other than removing duplicates and selecting among competing datasets, constituting collated data."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2, 'Creating pronunciation dictionaries'",
          "reasoning": "For languages lacking existing dictionaries or with small Wikipron entries, the authors created pronunciation dictionaries by applying espeak-NG to wordlists obtained from the Leipzig corpora after filtering. This represents data derived by applying a pronunciation generation model to existing wordlists."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper explicitly specifies the data sources and their processing, so the origin of data is documented."
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9858,
      "completion_tokens": 378,
      "total_tokens": 10236
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 4 (Experiments), Section 5 (Evaluations)",
          "reasoning": "The curated multilingual G2P dataset of around 100 languages was used to train ByT5-based multilingual and monolingual models from randomly initialized weights as baselines, demonstrating the effect of dataset size and architecture on performance."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": true,
          "reference": "Section 6 (Low resource G2P)",
          "reasoning": "The pretrained multilingual G2P models were fine-tuned with supervised learning on low-resource languages using the respective low-resource dataset splits, leading to improved phone error rate and word error rate."
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 (Evaluations) and Section 6 (Low resource G2P)",
          "reasoning": "The dataset was partitioned into train, development, and test sets, then used to evaluate the G2P models' performance quantitatively by measuring phone error rate (PER) and word error rate (WER) across languages."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5 (Evaluations) and Section 6 (Low resource G2P)",
          "reasoning": "The dataset was analyzed for distribution of dictionary sizes per language, orthographic characteristics impact on model performance, correlation between dataset size and error rates, and differences in zero-shot and fine-tuned performance on low-resource languages."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 10581,
      "completion_tokens": 407,
      "total_tokens": 10988
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": true,
          "reference": "Section 2: Multilingual Pronunciation Dictionaries",
          "reasoning": "The authors curated a dataset from various sources covering around 99 languages with more than 3000 entries each, combining publicly available G2P dictionaries and new dictionaries, resulting in a massive multilingual G2P dataset spanning approximately 100 languages."
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset covers many languages (about 100), not exactly two languages."
        },
        "Monolingual (English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is not limited to English only; it includes data from about 100 languages."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset covers multiple languages rather than exactly one non-English language."
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset entries are grapheme-to-phoneme mappings for human languages and do not contain code or programming language data."
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset consists of grapheme-to-phoneme conversion pairs in human languages and does not include mathematical or logical notation."
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "No biological sequences or non-human communication systems are part of the dataset."
        },
        "Constructed Language": {
          "is_applicable": true,
          "reference": "Section 5: Evaluations",
          "reasoning": "The dataset includes Esperanto, a constructed language, as noted where Esperanto (epo) is mentioned as a language with phonetically regular spelling system that is easily handled by G2P models."
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The languages in the dataset are properly identified with codes and documented."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset contains linguistic entries with multiple languages."
        }
      }
    }
  },
  {
    "id": "zhu22_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7799,
      "completion_tokens": 185,
      "total_tokens": 7984
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Section 2 and Introduction",
          "reasoning": "The paper states in the Introduction and Section 2 that they have made their code publicly available at https://github.com/lingjzhu/CharsiuG2P. This repository includes the code and pretrained models related to dataset construction from various sources, preprocessing, and training. Therefore, code related to the dataset creation and processing is available for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Section 2",
          "reasoning": "Section 2 provides a detailed description of how the dataset was constructed from a variety of existing pronunciation dictionaries and other resources. It specifies the data sources, merging strategies, filtering criteria, the use of espeak-NG to generate dictionaries for some languages, and data partitioning. This provides transparent and sufficient documentation of the dataset creation process in the paper."
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-0",
    "token_usage": {
      "prompt_tokens": 7124,
      "completion_tokens": 147,
      "total_tokens": 7271
    },
    "response": {
      "sources": [
        {
          "Modality": "audio",
          "Human Generated": true,
          "Model Generated": false,
          "Unknown Origin": false,
          "Reference": "Abstract; Section 1 Introduction; Section 4.1 Data split and training; Section 6 Conclusion",
          "Reasoning": "The PodcastFillers dataset introduced in this work consists of 145 hours of speech recordings from over 350 speakers, sourced from 199 public podcast episodes. The data consists of annotated filler words and common podcast sounds. Since podcasts are recordings of human speech, the modality is audio and the origin is human-generated speech recordings. The paper explicitly states that the dataset was acquired from publicly available podcasts and annotated by humans through crowdsourcing, confirming human involvement in the data generation process."
        }
      ]
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-1",
    "token_usage": {
      "prompt_tokens": 7924,
      "completion_tokens": 234,
      "total_tokens": 8158
    },
    "response": {
      "data_annotations": [
        {
          "Subject Annotators": {
            "category": "Multiple Human Non-Experts",
            "reference": "Section 2 (Dataset details)",
            "reasoning": "The dataset PodcastFillers was annotated via crowdsourcing, with each candidate labeled by two or three people. Crowdsourcing suggests multiple non-expert human annotators were involved, as no expert or specialized annotators are mentioned."
          },
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 (Dataset details)",
            "reasoning": "Annotators were presented with clips with highlighted candidates and asked to determine if the candidate was a filler word or not, and select one of specified filler or non-filler labels, indicating that annotation instructions were given to guide their labeling process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "The paper does not describe or mention any scoring rubrics or specific grading criteria provided to annotators for label assignment or quality control."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "N/A",
            "reasoning": "No mention or inclusion of explicit annotation examples or samples provided to annotators as part of guidelines is found in the paper."
          }
        }
      ]
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-2",
    "token_usage": {
      "prompt_tokens": 9114,
      "completion_tokens": 436,
      "total_tokens": 9550
    },
    "response": {
      "quality_assurance": {
        "Single Human Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not provide information indicating that quality assurance was performed by a single human expert annotator."
        },
        "Multiple Human Experts": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no evidence in the paper that multiple human annotators with subject matter expertise were involved in the quality assurance process for annotations."
        },
        "Single Human Non-Expert": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not state that a single non-expert human annotator performed quality assurance."
        },
        "Multiple Human Non-Experts": {
          "is_applicable": true,
          "reference": "Section 2 Dataset: Annotation Process",
          "reasoning": "The paper describes that annotation of filler candidates was performed via crowdsourcing, with each candidate annotated by two people and a third annotator involved if there was disagreement. While the annotators are not explicitly described as experts, the process involved multiple human non-expert annotators providing labels, implying that quality assurance was conducted by multiple non-expert humans."
        },
        "AI Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no description indicating that an AI model was used as a judge for quality assurance of the dataset annotations."
        },
        "Automatic Process": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "Although the initial candidate selection uses automated VAD and ASR systems, the quality assurance process for annotation validation was not automated but done by human annotators; thus, QA is not considered an automatic process."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The paper describes a clear quality assurance process via crowdsourced multiple human annotations; therefore, it is not true that no QA was performed or documented."
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-3",
    "token_usage": {
      "prompt_tokens": 8724,
      "completion_tokens": 504,
      "total_tokens": 9228
    },
    "response": {
      "data_novelty": {
        "New Data from Human": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 2 Dataset",
          "reasoning": "The PodcastFillers dataset is a novel speech dataset created by the authors through annotation of public podcast episodes. Over 85K filler word candidates were annotated via crowdsourcing, resulting in 35K filler word annotations and 50K other event annotations. This indicates original creation of labeled data by human annotators, not derived from or translations of existing data."
        },
        "New Data from Model": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "There is no indication in the paper that the dataset content was generated entirely by AI or machine learning models without human involvement."
        },
        "Human Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The dataset is based on English podcasts with annotations; there is no mention of translation from another language by human translators."
        },
        "Machine Translation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": "The paper does not mention any use of machine translation to generate the dataset."
        },
        "Collated": {
          "is_applicable": true,
          "reference": "Section 2 Dataset; Section 6 Conclusion",
          "reasoning": "The dataset audio is collected from existing public podcast episodes (199 episodes), which are aggregated to create the dataset. While the authors perform annotation and preprocessing, the raw audio data is collected from these existing sources without significant modification, hence it is collated from external sources."
        },
        "Derived": {
          "is_applicable": true,
          "reference": "Section 2 Dataset; Section 3 Filler Detection Pipeline",
          "reasoning": "The authors used outputs from a Voice Activity Detection (VAD) model and Automatic Speech Recognition (ASR) system to generate filler candidates, which they then annotated. This process involves transformation and adaptation of existing audio data (podcast episodes) and model-generated candidate segments to produce the final annotated dataset, making the dataset derived from existing sources with modifications."
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The source and method of dataset creation are clearly documented as described above."
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-4",
    "token_usage": {
      "prompt_tokens": 9242,
      "completion_tokens": 275,
      "total_tokens": 9517
    },
    "response": {
      "task_utility": {
        "Pre-Training": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Training (From Scratch)": {
          "is_applicable": true,
          "reference": "Section 3 and Section 4.1",
          "reasoning": "The PodcastFillers dataset is used to train classifiers for filler word detection and classification from scratch, as described in sections about the filler detection pipeline and data split and training where models are trained on the train split of PodcastFillers."
        },
        "Post-Training (Supervised Fine-tuning)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Post-Training (RL-based Methods)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Evaluation": {
          "is_applicable": true,
          "reference": "Section 5 and Section 4.3",
          "reasoning": "PodcastFillers is used as a test set for evaluating the proposed filler detection and classification pipelines with detailed metrics provided in the results section."
        },
        "Analysis": {
          "is_applicable": true,
          "reference": "Section 5.1",
          "reasoning": "The paper conducts ablation studies and detailed analyses of the impact of pipeline components on performance using the PodcastFillers dataset, indicating its use for analysis purposes."
        },
        "Knowledge Base": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": ""
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-5",
    "token_usage": {
      "prompt_tokens": 9965,
      "completion_tokens": 399,
      "total_tokens": 10364
    },
    "response": {
      "lang": {
        "Multilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Bilingual": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Monolingual (English)": {
          "is_applicable": true,
          "reference": "Abstract; Section 1 Introduction; Section 4 Experimental Design",
          "reasoning": "The PodcastFillers dataset consists of speech from public podcast episodes primarily in English, as indicated throughout the paper. It leverages a commercial ASR trained on English (e.g., Fisher English corpus is referenced for baseline comparisons). There is no mention of other languages being included, and all examples and baseline comparisons center on English speech data."
        },
        "Monolingual (Non-English)": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Code / Programming Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Mathematical and Logical Notation": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Biological and Non-Human Communication Systems": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Constructed Language": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "Unknown": {
          "is_applicable": false,
          "reference": "",
          "reasoning": ""
        },
        "N/A": {
          "is_applicable": false,
          "reasoning": "The dataset consists of spoken podcast audio with annotated filler words and other speech events, which inherently involves human language (English)."
        }
      }
    }
  },
  {
    "id": "zhu22e_interspeech-rubric-6",
    "token_usage": {
      "prompt_tokens": 7183,
      "completion_tokens": 156,
      "total_tokens": 7339
    },
    "response": {
      "reproducibility": {
        "code": {
          "Has Code": true,
          "reference": "Abstract, Section 6 Conclusion",
          "reasoning": "The paper states in the abstract and conclusion that the PodcastFillers dataset is made publicly available online at podcastfillers.github.io, which implies the associated code used for dataset creation and annotation pipeline is also available to support the dataset release for reproducibility."
        },
        "documentation": {
          "Has Documentation": true,
          "reference": "Sections 3 Filler Detection Pipeline and 4 Experimental Design",
          "reasoning": "The paper provides clear, detailed documentation of the dataset creation process, including the pipeline involving voice activity detection and automatic speech recognition to generate filler candidates, the crowdsourced annotation procedure, dataset statistics, and data splits, all described mainly in Sections 3 and 4."
        }
      }
    }
  }
]