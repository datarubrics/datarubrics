[
  {
    "id": "01I55gys19-rubric-5",
    "token_usage": {
      "prompt_tokens": 13966,
      "completion_tokens": 378,
      "total_tokens": 14344
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that any new dataset was created from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data generated entirely by AI or machine learning models as new datasets."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not discuss any translation of data from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of machine-translated data is present in the paper."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2 Few-Class Dataset Preparation",
            "reasoning": "Few-Class Arena benchmarks subsets of existing datasets by light-weight configuration of subsets without duplicating the original dataset, indicating the use of existing datasets aggregated or collated and indexed for few-class subsets."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Few-Class Dataset Preparation; Section 3 introduction",
            "reasoning": "Few-Class Arena derives few-class subsets from many-class existing datasets by selecting subsets of classes and reorganizing data for few-class benchmarks, which is a modification or adaptation of existing datasets to form new benchmark subsets."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is described and documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "0SMhqvgHST-rubric-5",
    "token_usage": {
      "prompt_tokens": 31305,
      "completion_tokens": 450,
      "total_tokens": 31755
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the creation of new datasets from scratch by human contributors. It relies on existing datasets collected and curated in prior works."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets used are not generated by AI or machine learning models. The paper does not mention any newly generated synthetic datasets created by models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of datasets produced by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No datasets were indicated as generated through machine translation techniques."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Sections 1 Introduction; 10 Defining the GATE Benchmark; 12 Benchmark Pool Details; 13 Model Pool Details",
            "reasoning": "The paper constructs the GATE benchmark by aggregating and selecting subsets from a large set of existing, publicly available datasets spanning various domains, tasks, and modalities. The authors do not collect new raw data but assemble and reorganize previously published datasets into new benchmark tiers."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Sections 3 EEVEE Methodology; 10 Defining the GATE Benchmark",
            "reasoning": "The authors apply modifications such as task adapters, input layer replacements to enable modality shifting, and standardized fine-tuning recipes to existing datasets. These adaptations create a transformed evaluation pipeline based on existing data, constituting derived data and derived evaluation processes rather than raw new data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper sufficiently documents the data used and its sources; thus this category does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "2HzZIDo48o-rubric-5",
    "token_usage": {
      "prompt_tokens": 21790,
      "completion_tokens": 322,
      "total_tokens": 22112
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3 and Algorithm 5",
            "reasoning": "The authors introduce the Symbolic Behaviour Benchmark (S2B), which is instantiated via synthetically generated symbolic spaces using the Symbolic Continuous Stimulus (SCS) representation. These symbolic spaces and stimuli are algorithmically generated based on sampled semantic structures and Gaussian kernels defining partitions of continuous space. The datasets are thus derived from predefined symbolic structures and dynamically sampled according to meta-learning algorithms (Meta-Referential Games). This data is not human-generated or model-generated language data but artificially constructed from existing symbolic concepts and mathematical operations, i.e., derived from existing symbolic structures with adaptations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": ""
          }
        }
      }
    ]
  },
  {
    "id": "2myGfVgfva-rubric-5",
    "token_usage": {
      "prompt_tokens": 17658,
      "completion_tokens": 420,
      "total_tokens": 18078
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset MiraData is created from collecting existing videos from multiple sources, not original content created from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not generated by AI or machine learning models; rather, it is built from real video sources."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper of dataset captions being derived via human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No evidence is provided that dataset captions involved machine translation from other languages."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Collection",
            "reasoning": "MiraData is curated by collecting videos from diverse manually selected sources including YouTube, Videvo, Pixabay, and Pexels without significant modification. The videos are aggregated from these existing platforms."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Sections 3.2 Video Splitting and Stitching; 3.3 Video Selection; 3.4 Video Captioning",
            "reasoning": "Although the dataset videos are collected from existing sources, several processing steps including semantic consistent splitting, stitching clips together, filtering based on quality and motion, and enriching annotations using GPT-4V for dense and structured captions represent modifications and adaptations of the original data, yielding derived data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin and processing details are explicitly documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "4S8agvKjle-rubric-5",
    "token_usage": {
      "prompt_tokens": 36058,
      "completion_tokens": 599,
      "total_tokens": 36657
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 and Appendix J, K, L",
            "reasoning": "The paper states that the authors collected, adapted, and extensively annotated multiple environments and task instances with human effort. Specifically, subgoals for progress rates were manually annotated and verified through multiple rounds of human verification (Appendix J). They also adapted environments such as BabyAI to convert image-based observations to textual form and re-annotated subgoals to increase density (Section 3.1 and Appendix L). Additionally, several environment problems were edited by humans to ensure unique subgoal sequences (Appendix L.1). The comprehensive data verification process involving multiple humans and annotations indicates original human-created and adapted content."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any dataset or data was generated entirely by AI or machine learning models without reference or adaptation from existing data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention producing data by translating content from other languages via human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of any machine translation being used to produce data in the paper."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 and related environment descriptions",
            "reasoning": "The paper includes environments adapted from or based on existing datasets or benchmarks, such as AlfWorld, ScienceWorld, BabyAI, Jericho, PDDL, WebShop, WebArena, and tool environments from APIs like Todoist and Google Sheets (Section 3.1, Appendix K). This indicates data collated or aggregated from pre-existing sources."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 and Appendix L",
            "reasoning": "The paper describes adaptations and modifications to existing environments to tailor them for text-based interaction with LLM agents and for uniform multi-round, partially-observable settings. Examples include re-annotating subgoals, transforming observation and action formats, rewriting goals for shorter episodes (e.g., Jericho), and clarifying task instructions for unique subgoal sequences (Appendix L, Section 3.1). These represent derived data based on existing sources, with transformations and adaptations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the origins and adaptations of its datasets; hence, N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "5WFzk0H27p-rubric-5",
    "token_usage": {
      "prompt_tokens": 17780,
      "completion_tokens": 519,
      "total_tokens": 18299
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.1 and A.3 Collection process",
            "reasoning": "The dataset consists of over 204,000 pairwise comparisons made by over 20,000 human contributors on the Tournesol platform, providing original human judgments on YouTube videos. This data is created entirely from scratch by human contributors, collected through a web interface as described in Section 2.1, with contributors voluntarily making comparative judgments. This is further confirmed in the datasheet appendix where it states in A.3 that data was collected directly from individuals via the Tournesol platform."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset itself is comprised of human-generated comparative judgments. While processed data is derived using models (SOLIDAGO pipeline), the raw data is not generated by AI or machine learning models. Therefore, new data from models does not apply."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper or appendix that any data was produced by translating content from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and appendix do not mention any machine translation processes being applied to the data."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not a simple collection or aggregation of existing data without modification; it consists of newly made human assessments and votes collected via the platform."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.2 Processed Data",
            "reasoning": "The dataset includes processed data derived from the raw human judgments by applying several transformations and aggregation steps via the SOLIDAGO pipeline (Sections 2.2). These include computing trust scores, assigning voting rights, applying generalized Bradley-Terry models, scaling, and secure aggregation. This processing significantly transforms the original data to produce additional derived scores."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data source and method of generation are clearly documented throughout the paper and appendix."
          }
        }
      }
    ]
  },
  {
    "id": "5c1hh8AeHv-rubric-5",
    "token_usage": {
      "prompt_tokens": 102836,
      "completion_tokens": 624,
      "total_tokens": 103460
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.2 Practice in MultiTrust; Appendix C to G; Data Sheet H",
            "reasoning": "The benchmark includes 32 diverse tasks with over 15K image-text samples. The paper states that 20 datasets are constructed by adapting prompts, images, and annotations from existing datasets with both manual efforts and automatic methods, and 8 novel datasets are created from scratch by collecting images from the Internet or synthesizing images with Stable Diffusion and other algorithms specifically for the designed tasks. This indicates that a substantial portion of the data was newly created by human contributors, especially the manually collected images and annotations developed specifically for the benchmark's tasks."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2.2 Practice in MultiTrust; Data Sheet H",
            "reasoning": "The paper describes generating certain datasets and prompts using advanced AI models like GPT-4 and GPT-4V for tasks such as generating factual questions, prompt generation for visual preference selection, and dataset synthesis with Stable Diffusion. This shows that some data in the benchmark was generated entirely by AI models without direct use or adaptation of existing human-collected data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper that any data used or created was produced by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not document any data generated by machine translation systems from other languages."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.2 Practice in MultiTrust; Data Sheet H",
            "reasoning": "The benchmark uses several existing datasets (e.g., MME, RTVLM, VISPR, VizWiz, FACET) collected and aggregated from prior work. These datasets have been collated as part of the benchmark without significant modification, serving as the basis or components of several tasks."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.2 Practice in MultiTrust; Data Sheet H",
            "reasoning": "Many datasets are adapted from existing sources with modifications such as prompt reformulation, addition of new images, manual curation, or synthetic data generation augmenting existing samples. For example, adapting text-only tasks to multimodal scenarios, or extending datasets with GPT-4 generated prompts or annotated images indicates derivation from existing datasets with modifications."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides a detailed and transparent account of the dataset origins and construction methods; therefore, the data origin is well documented."
          }
        }
      }
    ]
  },
  {
    "id": "67N3FWoDtU-rubric-5",
    "token_usage": {
      "prompt_tokens": 17563,
      "completion_tokens": 616,
      "total_tokens": 18179
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3, 'The Chronicling Germany Dataset'; Abstract; Section 7, 'Conclusion and future work'",
            "reasoning": "The Chronicling Germany dataset consists of 581 manually and painstakingly annotated high-resolution newspaper pages from 1852 to 1924, with layout and text-line transcriptions corrected by expert human annotators over approximately 1,500 hours. The annotations were created entirely from scratch by human contributors, especially history students who manually annotated layout polygons and corrected text, following OCR-D guidelines. The data is not translated, adapted, or derived from pre-existing material but represents original human-generated annotations on scanned historical newspaper images."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset annotations are created and corrected by human annotators; while models are used in the processing pipeline for OCR and layout detection, the core dataset annotations are human-generated, not fully generated by AI or machine learning without human intervention."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention or indication that the dataset involves translations from other languages using human translators. The newspaper content is in German and the focus is on annotations and transcription of historical German newspapers, not translation tasks."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication of machine translation usage for generating or modifying the dataset content."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3, 'The Chronicling Germany Dataset'; Abstract",
            "reasoning": "The dataset is constructed by collecting scanned images of historical newspaper pages from existing archives (e.g., K\u00f6lnische Zeitung), which are publicly available digital scans of newspapers. These images are the source material, aggregated from existing sources without significant modification beyond the manual annotations. Thus, the base newspaper images themselves are collated from existing archives."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3, 'The Chronicling Germany Dataset'; Section B OCR",
            "reasoning": "The dataset includes text-line transcriptions that start from network-generated OCR output that is then corrected by expert annotators. This correction and post-processing can be understood as deriving data based on existing OCR outputs, involving transformation and adaptation. Similarly, polygon annotations for layout are derived from the original newspaper scans through manual annotation. Therefore, the dataset is derived from pre-existing scanned newspaper pages with added human-generated annotations and corrections to the OCR output."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the origin and methods used to create the dataset."
          }
        }
      }
    ]
  },
  {
    "id": "6cCFK69vJI-rubric-5",
    "token_usage": {
      "prompt_tokens": 22319,
      "completion_tokens": 455,
      "total_tokens": 22774
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 Collection Process, Section 3.2 Description, Section 1 Introduction",
            "reasoning": "The BTS dataset is described as a new, anonymized dataset collected from three real-world buildings over three years in Australia. The data originates from sensor and management systems in these buildings, collected via Building Management Systems and uploaded to CSIRO's Data Clearing House platform. This was not generated by models or collected from existing datasets but gathered directly from human-managed building infrastructure. Anonymization and semantic modeling with the Brick schema were performed by human experts. Therefore, the dataset represents original content created from scratch by human contributors, not derived from or modifications of pre-existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the dataset content was generated or synthesized by AI or machine learning models. The data consists of timeseries readings from actual building sensors."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not produced by translating content from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No machine translation was involved in the creation of the dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not aggregated from existing sources without modification; it is original sensor data collected specifically from the three buildings."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not derived from existing sources with modifications; it is a new data collection. The semantic labeling using Brick schema is metadata annotation rather than derivation of the data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data generation method is well documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "7TCK0aBL1C-rubric-5",
    "token_usage": {
      "prompt_tokens": 17326,
      "completion_tokens": 413,
      "total_tokens": 17739
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.2, Section 4 (Data collection and annotation process)",
            "reasoning": "The IaC-Eval dataset consists of 458 human-curated scenarios covering diverse AWS cloud infrastructure problems, created entirely from scratch by the authors and their team. The authors explicitly state that the dataset was built by experts with deep cloud and IaC expertise over 1720 hours, involving original problem descriptions, intent specifications crafted in Rego by human experts, and example correct configurations written in Terraform HCL. This content is not derived from or adapted from any existing datasets but is original human-created material."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not generated by any AI or machine learning models. There is no indication in the paper that the problem descriptions, specifications, or examples were created or generated by models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any translation from another language or human translation of data into the dataset."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any machine translation or use of translations in dataset creation."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the authors mention that they learned from official AWS and IaC documentation and public repositories, the dataset itself is not a collection or aggregation of existing data but consists of newly created scenarios, specifications, and example configurations. Therefore, it is not collated from existing sources."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not appear to be derived or adapted from existing datasets or other sources but is original content created by the authors. The paper emphasizes the dataset's originality and human curation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is clearly documented and described in detail in the paper, thus N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "9tVn4f8aJO-rubric-5",
    "token_usage": {
      "prompt_tokens": 42104,
      "completion_tokens": 413,
      "total_tokens": 42517
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper compiles and benchmarks existing datasets; it does not introduce any completely new datasets created entirely from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any datasets generated entirely by AI or machine learning models; it focuses on evaluation of multimodal models using existing datasets."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of datasets created by human translation from other languages in the paper."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that any datasets were created or processed via machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2 and Appendix A.1",
            "reasoning": "The HEMM benchmark is composed by collecting and aggregating 30 existing datasets from diverse sources such as VQA, DECIMER, MEMOTION, etc. The paper analyzes these pre-existing datasets without creating new raw data, indicating that the datasets are collated from previously established sources without significant modification."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that the datasets were derived with modifications, adaptations, or transformations from existing sources; rather, it uses the datasets in their original or standard formats."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The origins of the datasets are documented in the paper and Appendix A.1; thus, 'N/A' does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "AdpSHMOujG-rubric-5",
    "token_usage": {
      "prompt_tokens": 17809,
      "completion_tokens": 497,
      "total_tokens": 18306
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 4 - Material State Segmentation Benchmark",
            "reasoning": "The benchmark dataset consists of 1,220 real-world images manually taken from diverse real-world scenes, encompassing a wide range of material states and domains. The annotation involves humans manually sampling points that represent distributions of material states, grouping points by material state, and assigning relative similarities. This process is manually conducted and constitutes original data created from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any dataset was generated entirely by AI or machine learning models without reference or transformation of existing data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data produced by human translation of content from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any data generated by machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 4 - Material State Segmentation Benchmark",
            "reasoning": "The benchmark data collects and aggregates existing real-world images from diverse domains. The images themselves are collected from natural scenes, and the benchmark assembles these images and their annotations without extensive modification of the raw image sources."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3 - Data generation: patterns extraction and infusion; Sections 3.1 and 3.2",
            "reasoning": "The synthetic training datasets (2D and 3D MatSeg datasets) are generated by transforming existing real-world images: patterns and textures are extracted from natural images using unsupervised methods and then infused into synthetic scenes using physics-based rendering, procedural scene building, and texture mapping. This involves modifications and adaptations applied to existing images and materials to create novel synthetic data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly specifies the sources and generation methods of its datasets."
          }
        }
      }
    ]
  },
  {
    "id": "BZe6dmDk5K-rubric-5",
    "token_usage": {
      "prompt_tokens": 35069,
      "completion_tokens": 491,
      "total_tokens": 35560
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.3 Subjective Action Quality Assessment and Section 3 Dataset Acquisition",
            "reasoning": "The GAIA dataset includes 971,244 human annotations collected from 54 participants who evaluated the action quality of AI-generated videos from multiple perspectives. The human evaluation process involved detailed instructions, eligibility testing, continuous monitoring, and quality control procedures, indicating the subjective ratings were created entirely by human contributors specifically for this dataset."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3 Dataset Acquisition, especially 3.1 Prompt Sources and Text-to-Video Models",
            "reasoning": "The videos in the GAIA dataset were generated by 18 state-of-the-art text-to-video (T2V) generative models, both open-source laboratory studies and commercial platforms, producing 9,180 AI-generated videos from 510 distinct prompts. These videos are original AI-generated content created by generative models without direct reference to existing video data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of data produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe using machine translation to generate data from other languages."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data is not collected or aggregated from existing video datasets; rather, the videos are newly generated by models, and annotations are newly collected from humans."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the dataset is based on modifications or adaptations of pre-existing datasets or data; the videos are generated from prompts via generative models and the annotations collected from humans."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data source and method of generation are explicitly documented; thus, N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "D1MOK2t2t2-rubric-5",
    "token_usage": {
      "prompt_tokens": 12672,
      "completion_tokens": 370,
      "total_tokens": 13042
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 Demonstrations Dataset",
            "reasoning": "The Demonstrations Dataset consists of nearly 14,000 videos of human players completing the BASALT tasks from scratch, providing original content created by humans specifically for these tasks, as indicated by the collection of 361 hours of human gameplay and labeled trajectories. This data was created entirely by human contributors without translation or derivation from existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate that any of the introduced datasets consist of content generated purely by AI or machine learning models; rather, models are trained using the human demonstration data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that data was produced by translating content from another language by humans."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that data was produced by machine translation of another language."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data was collected from human gameplay videos specifically recorded for this benchmark, rather than aggregated from existing sources."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Evaluation Dataset",
            "reasoning": "The Evaluation Dataset is derived from the BASALT competition, containing over 3,000 human pairwise evaluations comparing human and algorithmic agent videos. This data is based on existing videos (some of which are from the Demonstrations Dataset and algorithmic agents) but involves new human evaluation annotations and natural language justifications, representing a transformed and adapted use of existing data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the source and generation process of the datasets."
          }
        }
      }
    ]
  },
  {
    "id": "DFr5hteojx-rubric-5",
    "token_usage": {
      "prompt_tokens": 93614,
      "completion_tokens": 519,
      "total_tokens": 94133
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2 (The PRISM Alignment Dataset), Abstract",
            "reasoning": "The PRISM dataset is comprised of original data collected through live interactions between 1,500 diverse human participants and 21 different LLMs, including both survey responses and conversation prompts and feedback. The participants created original prompts, feedback, and self-descriptions, all generated from scratch without indication of reuse or aggregation of pre-existing data. The paper repeatedly states this was a participatory data collection process with explicit human creation of content, e.g., \u00a72.1 Survey and \u00a72.2 Conversations describe participants generating survey answers, conversational prompts, and free-text feedback."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2.2 (The Conversations), Abstract",
            "reasoning": "The dataset includes model-generated responses from 21 different LLMs (including open access and commercial models), generated through API calls synchronized with human conversation turns. These responses are original outputs from the models to participant prompts, and thus newly generated by models in real-time during data collection. This is clearly described in \u00a72.2, where model responses to human prompts are recorded as part of the dataset."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that any data was produced by human translation."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any use of machine translation systems to generate or transform data."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset was not aggregated from existing sources without modification; it was collected from human participants and LLM-generated outputs during a live study."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the dataset includes model responses, these are directly generated rather than derived from existing textual sources with modifications. Human inputs are original, not derived from existing data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin and generation method is well documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "DJVyRhT8nP-rubric-5",
    "token_usage": {
      "prompt_tokens": 28453,
      "completion_tokens": 797,
      "total_tokens": 29250
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.1 (HAPS Dataset), Section 2.2 (HA-R2R Dataset), Appendix B.1 and C.1",
            "reasoning": "The Human Activity and Pose Simulation (HAPS) dataset was collected via human surveys and manual quality control to produce 145 human activity descriptions tailored for 29 indoor regions. Human annotators also manually selected and placed human poses in the interactive annotation tool to create detailed 3D human motion models based on existing descriptions. Furthermore, the Human-Aware Room-to-Room (HA-R2R) dataset extends the existing R2R dataset by incorporating human activity descriptions generated with the assistance of GPT-4 but then verified and validated by humans to create new navigation instructions that account for dynamic human environments. These datasets are original and created from scratch by human contributors, with careful curation and validation, making them new data from human."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2.1 (HAPS Dataset), Section 2.2 (HA-R2R Dataset)",
            "reasoning": "The 145 human activity descriptions collected from humans were converted into 435 detailed 3D human motion models using the Motion Diffusion Model (MDM), which is an AI generative model, to produce naturalistic 3D motion sequences represented by the SMPL model. Additionally, the new instructions in HA-R2R incorporate GPT-4 generated expansions that combine original R2R instructions with human activity descriptions and relative positional information, followed by human validation. Thus, parts of the dataset include data generated entirely by machine learning models as original content, particularly the 3D motion data and new natural language instructions generated by GPT-4 prior to human verification."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any dataset created via human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication or mention of machine translation from other languages is present in the paper."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.1 (Integration with Matterport3D), Section 2.2 (HA-R2R Dataset)",
            "reasoning": "The HA3D simulator combines the Matterport3D photorealistic indoor environments with the collected HAPS dataset human activity models that are integrated and annotated within the existing architectural spaces. The HA-R2R is built upon the original Room-to-Room dataset, expanding it with additional human activity annotations and instructions. Hence, parts of the dataset are aggregated from existing sources\u2014the Matterport3D and R2R datasets\u2014without substantial modification to the core scene geometry or navigation graph, constituting a collated dataset."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.2 (HA-R2R Dataset), Appendix C.1",
            "reasoning": "The HA-R2R dataset is derived from the R2R dataset, with modifications such as the addition of human activity annotations and synthesized new instructions combining original instructions with human activity descriptions and relative position info, generated via GPT-4 and validated by humans. This represents a data derivation process where new content is adapted and expanded from existing datasets."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides sufficient information on how new datasets were generated; hence, the origin of data is adequately specified."
          }
        }
      }
    ]
  },
  {
    "id": "DjCSjizgsH-rubric-5",
    "token_usage": {
      "prompt_tokens": 17273,
      "completion_tokens": 543,
      "total_tokens": 17816
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "Section 2.3 describes that 1K real-world wildfire scenarios are collected from satellite images and annotated by a group of 20 human annotators using specialized software for labeling wildfire regions. The annotations involved cross-checking among annotators to ensure quality, confirming the data is newly created by humans."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Section 2.2 explains that 1 million virtual wildfire scenarios are generated using wildfire simulators (FARSITE, WFDS, and WRF-SFIRE) based on empirical and physical models. The simulations produce multi-modal data including sequences of fire region masks and environmental data generated by models without human annotation, constituting original data from models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data created through human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that any data or content in the dataset was produced by machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.1, 2.3",
            "reasoning": "The dataset integrates multiple environmental modalities such as topography, vegetation, fuel maps, and weather data collected from existing sources like LANDFIRE and Remote Automatic Weather Station. The real-world satellite data are collected from public data sources (Landsat-8, Sentinel-2). These data are gathered and spatially/temporally aligned with annotations, representing aggregation from existing data sources without significant modification."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "Simulated fire region masks are derived by applying wildfire simulators to the collated environmental data with parameter jittering to produce diverse scenarios. The mask annotations also fuse fire region masks with satellite images, showing transformations and adaptations applied to base data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset's data sources and generation methods are explicitly described in the paper sections 2.1 to 2.3."
          }
        }
      }
    ]
  },
  {
    "id": "E18kRXTGmV-rubric-5",
    "token_usage": {
      "prompt_tokens": 21569,
      "completion_tokens": 443,
      "total_tokens": 22012
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The CVQA dataset consists of images and question-answer pairs created by fluent native speakers and cultural experts. The paper explicitly states that annotators selected or took cultural-specific images and created culturally-relevant questions and answers from scratch. The annotation guidelines and examples emphasize original question creation requiring image context, confirming that this data is newly created by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any portion of the CVQA dataset was generated by AI or machine learning models."
          },
          "Human Translation": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "The paper explains that each question-answer pair was created in the local language and manually translated into English by human annotators, indicating human translation was applied."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention the use of machine translation for any part of data generation or language translation."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although some images come from existing open sources, the paper emphasizes that the data is not merely aggregated but actively curated and annotated with new questions and answers; thus, it is not simply collated without significant modification."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data is not described as adapted or transformed from existing datasets. Instead, it is original and human-created, so it does not fall under derived data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data generation and origin are clearly documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "EADRzNJFn1-rubric-5",
    "token_usage": {
      "prompt_tokens": 29593,
      "completion_tokens": 467,
      "total_tokens": 30060
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets are constructed by aggregating existing data sources rather than being created entirely from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets are not generated by AI or machine learning models; rather, they come from pre-existing data collections."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the datasets involve human translation of content from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that machine translation was used in dataset creation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 4 Datasets; Appendix E Data Processing Details",
            "reasoning": "The paper states that the novel datasets are constructed by collecting and aggregating data from existing public sources such as Wikidata, POLECAT, ICEWS, GH Arxiv, Reddit, and Myket Android app market, with some filtering (e.g. removing nodes with low degree) and organizing, but no significant transformation indicating original human creation from scratch."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 4 Datasets; Appendix E Data Processing Details",
            "reasoning": "Datasets such as tkgl-wikidata and tkgl-smallpedia are extracted subsets of large existing knowledge graphs (e.g. Wikidata) with filtering and selection applied (e.g., filtering based on entity IDs, selecting temporal qualifiers). The thgl-myket dataset is anonymized interaction data provided by Myket, indicating some processing applied to original logs. These indicate the datasets are derived from existing data with modifications or transformations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly specifies the data sources and construction process for all datasets."
          }
        }
      }
    ]
  },
  {
    "id": "FXTeJvHE0k-rubric-5",
    "token_usage": {
      "prompt_tokens": 16347,
      "completion_tokens": 489,
      "total_tokens": 16836
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The paper introduces a new filtered dataset split (navtrain and navtest) based on the OpenScene dataset, which is itself a redistribution of nuPlan. The authors create a substantial number of challenging real-world driving scenarios (over 100k for training and 12k for testing) by employing filtering strategies that remove trivial and noisy scenes, thus curating new data subsets tailored for autonomous driving benchmarking. This data is derived from human driving data originally collected for nuPlan/OpenScene."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any dataset generated purely by AI or model-generated content; data is based on real-world driving logs."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention is made of data created via human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication or mention of machine translation being used in data preparation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The dataset used for NAVSIM is OpenScene, which is a redistribution of the nuPlan dataset, both publicly available real-world driving datasets. NAVSIM builds upon this pre-existing large dataset, aggregating data and metadata like HD maps and sensor streams."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The authors apply filtering methods to the existing OpenScene dataset to create curated standardized train and test splits (navtrain and navtest) focusing on challenging driving scenarios, thus deriving new dataset splits with modifications, transformations, and adaptations applied to the original data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper fully specifies the origin of the data used; it is based on existing real-world datasets with clear processing steps."
          }
        }
      }
    ]
  },
  {
    "id": "GHlJM45fWY-rubric-5",
    "token_usage": {
      "prompt_tokens": 21868,
      "completion_tokens": 476,
      "total_tokens": 22344
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not include new original content created entirely from scratch by human contributors; it is compiled from existing data sources."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not contain any data generated by AI or machine learning models; all data originates from existing observational and environmental sources."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of data produced by translating content from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that machine translation was used in generating the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Species Observation Data; Section 3.2 Environmental Predictor Data",
            "reasoning": "The GeoPlant dataset aggregates large volumes of species observation data from existing sources such as GBIF for Presence-Only data and the European Vegetation Archive (EVA) for Presence-Absence surveys. It also collects various environmental rasters, satellite imagery, and time series data from existing publicly available repositories. The dataset harmonizes and collates these heterogeneous data without significant modification to create a comprehensive resource."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Environmental Predictor Data; Section 3.3 Satellite Images",
            "reasoning": "Several environmental rasters and satellite data are processed and transformed from raw or existing data sources\u2014for example, reprojecting, resampling, cropping, and aggregating satellite images and climatic time series\u2014to align with the dataset's spatial and temporal resolution requirements. This constitutes derivation based on existing sources with modifications."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin and generation methods are well documented and specified in multiple sections of the paper."
          }
        }
      }
    ]
  },
  {
    "id": "HB5q6pC5eb-rubric-5",
    "token_usage": {
      "prompt_tokens": 24378,
      "completion_tokens": 459,
      "total_tokens": 24837
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not report creation of original content entirely from scratch by human contributors; rather, it operates on existing datasets."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.1 and 4 Experiments",
            "reasoning": "The PertEval toolkit generates new perturbed test samples on-the-fly from static benchmarks via human-like knowledge-invariant perturbations using LLM rewriters (i.e., large language models). This content-level paraphrasing and some format-level perturbations are generated by models and are original in formulation, as described explicitly in Section 3.1 and the experimental setup."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or indication of data being translated from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that machine translation was used to generate or modify data."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 4 Experiments - Datasets",
            "reasoning": "The base evaluation datasets used, such as subsets of the MMLU benchmark, are established public datasets collected and aggregated from existing expert-designed sources without modification; PertEval operates on these existing datasets."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Knowledge-invariant Perturbations",
            "reasoning": "PertEval creates new test samples derived from the original static benchmarks by applying transformations such as paraphrasing and format refactoring, which are modifications and adaptations of existing data, not completely new or purely model-generated content."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the origin and generation methods of the data used and generated."
          }
        }
      }
    ]
  },
  {
    "id": "IZtX4RNBeH-rubric-5",
    "token_usage": {
      "prompt_tokens": 14417,
      "completion_tokens": 611,
      "total_tokens": 15028
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.2, Building CVRR-ES Benchmark",
            "reasoning": "The benchmark includes 214 unique videos, with around 60% collected from public academic datasets and 40% from the internet. Human annotators generated captions for all videos, with personalized annotation guidelines for each video category. They then created 2,400 high-quality open-ended question-answer pairs for the benchmark. The QA pairs underwent manual filtering and verification by human annotators to ensure relevance, answerability, and quality. Thus, the QA pairs and captions are original content created entirely from scratch by human contributors, not translations or adaptations."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although GPT-3.5 was used to generate initial question candidates based on human-generated captions (Section 3.2, Stage 2), the final QA pairs included in the benchmark were verified, filtered, and refined by humans, and only high-quality pairs were kept. There is no indication that the benchmark data is composed solely or primarily of model-generated content without human curation."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention translating data from other languages or using human translators to produce dataset content."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no evidence of machine translation being used to generate or adapt the data."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2, Stage 1: Data collection and Annotation",
            "reasoning": "The benchmark includes videos collected from existing public academic datasets such as Something-Something-v2, CATER, Charades, ActivityNet, HMDB51, and YFCC100M, which constitute about 60% of the videos. These videos were gathered without significant modification, forming a collated core of the dataset."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2, Stage 2: Question-Answer Generation",
            "reasoning": "The QA pairs were generated by prompting GPT-3.5 with the human-generated captions, thus creating derivative data based on existing video captions. Additionally, humans filtered and refined these QA pairs, adapting model outputs into a high-quality benchmark dataset. Thus, the QA pairs are derived data based on existing sources with modifications."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides detailed documentation on the sources and generation methods of the benchmark dataset; therefore, the origin is specified."
          }
        }
      }
    ]
  },
  {
    "id": "JU0QvhhfVp-rubric-5",
    "token_usage": {
      "prompt_tokens": 14986,
      "completion_tokens": 449,
      "total_tokens": 15435
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not consist of original content created entirely from scratch by humans; instead, it primarily involves integration and processing of existing data."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the dataset was generated entirely by AI or machine learning models without reference to existing data. Although future work proposes generating in-silico Cell Painting readouts, this is not part of the current dataset."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data is not produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data is not generated by machine translation systems."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation collection",
            "reasoning": "The MOTIVE dataset aggregates compound-gene, compound-compound, and gene-gene annotations from seven publicly available databases (Table 1). These data were collected without significant modification, as stated in the paper."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Morphological profiles extraction and Section 3.3 Graph construction",
            "reasoning": "The dataset includes processed morphological profiles extracted from the JUMP Cell Painting dataset using software (CellProfiler) and various preprocessing steps to correct and normalize data. These processed feature vectors represent derived data based on existing imaging datasets. Additionally, the graph construction integrates this data with collated annotations, applying transformations and adaptations to build a composite graph dataset."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset's data origin is clearly described and documented, so N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "KZlJF8kguO-rubric-5",
    "token_usage": {
      "prompt_tokens": 24824,
      "completion_tokens": 594,
      "total_tokens": 25418
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Abstract and Section 3 (Data), Appendix A.1 (Audio transcription and alignment), A.2 (Feature annotation)",
            "reasoning": "The dataset is based on intracranial recordings collected from human subjects watching Hollywood movies, which are original electrophysiological data generated entirely through human participants. Furthermore, manual corrections and annotations (including speech transcription, word onset time alignment, part-of-speech tagging, dependency parsing, speaker identity annotation, and scene labeling) were performed by trained human annotators. These annotations are not derived or adapted from existing datasets but were created from scratch based on the original recordings and stimuli."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not include content generated entirely by AI or machine learning models as primary data. Although GPT-2 was used to compute surprisal as a feature, the underlying electrophysiological data and annotations originate from human experiments and manual annotation."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that the data was produced through translating content from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Machine translation was not applied for generating or adapting the dataset content."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3 (Data), Abstract",
            "reasoning": "The dataset includes aggregation of intracranial recordings across 10 subjects and 26 movies, with annotations collated from existing computer vision models (Places365 for scene labels), and speech recognition services (Rev.com, HappyScribe.com) for initial transcription, which were then manually corrected. This indicates parts of the annotations and features are aggregated or collected from existing sources and tools."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3 (Data), Appendix A.1, A.2",
            "reasoning": "Several dataset components are derived from existing sources or tools with modifications. For example, initial automatic speech recognition transcripts were corrected manually; automatic dependency parses were computed and manually corrected; scene labels were derived based on Places365 schema; and language features such as GPT-2 surprisal scores were computed from a language model. These represent data derived by transformation and adaptation of existing methods and resources."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin and generation methods are well documented in multiple sections of the paper and appendices."
          }
        }
      }
    ]
  },
  {
    "id": "L5aY1mWvXQ-rubric-5",
    "token_usage": {
      "prompt_tokens": 15071,
      "completion_tokens": 441,
      "total_tokens": 15512
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce any new dataset created completely from scratch by human contributors. Instead, it uses existing benchmark temporal graph datasets."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.1 (Temporal Distortion Techniques) and Section 4 (Experiment)",
            "reasoning": "The paper introduces techniques (INTENSE and SHUFFLE) to generate temporally distorted datasets by transforming existing test sets through algorithms that perturb or shuffle timestamps. These distorted datasets are generated via algorithmic transformations, effectively produced by model-based processes rather than being sourced originally from humans or collected as-is."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication of any data produced by human translation from other languages as per the paper content."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of machine translation or translation-based data generation."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets used (wikipedia, reddit, uci) are existing public temporal graph data; the paper does not collate or aggregate new raw data from diverse existing sources."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 (Temporal Distortion Techniques) and Section 4 (Experiment)",
            "reasoning": "The distorted datasets are derived from existing datasets by applying temporal distortion functions (INTENSE and SHUFFLE) that modify the original edge timestamps, creating modified versions of the original datasets for evaluation purposes."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The method of data generation for the distorted datasets is explicitly stated and documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "LdRZ9SFBku-rubric-5",
    "token_usage": {
      "prompt_tokens": 26396,
      "completion_tokens": 485,
      "total_tokens": 26881
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Collection",
            "reasoning": "The dataset is collected from public international news using a manual process where each news item is manually read, and original data such as title, content, time, images, image descriptions, event descriptions, and hierarchical event names are collected. This indicates that humans created this dataset from scratch rather than using pre-existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that the dataset content is generated entirely by AI or machine learning models from scratch."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data being produced by translation from another language via human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of machine translation being used to generate the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Collection",
            "reasoning": "The data is collected from publicly available international news sources and Wikipedia, and aggregated through a pipeline to construct the multimodal knowledge graph. The dataset uses existing news articles and images rather than creating new content, thus parts of the dataset are collated from existing sources."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Construction Pipeline for UKnow Protocol and Section 3.2 Dataset Collection",
            "reasoning": "The dataset is processed through a multi-phase pipeline that extracts content using pretrained models, symbolises information, and constructs a multimodal knowledge graph with five unit types of knowledge. The pre-processing and structuring steps transform the original collected news data into enriched graph nodes and edges, thus making the dataset derived with modifications and adaptations applied."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data source and method of generation are clearly documented in the paper and appendix."
          }
        }
      }
    ]
  },
  {
    "id": "LdxNWDNvC3-rubric-5",
    "token_usage": {
      "prompt_tokens": 21475,
      "completion_tokens": 530,
      "total_tokens": 22005
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 and Section 4.1",
            "reasoning": "The dataset AF-200K includes 2,150 new manually-designed supercritical airfoils contributed by engineers at Shanghai Aircraft Design and Research Institute according to Section 3. This data was designed entirely by human experts, providing original content created from scratch. The paper explicitly states inclusion of these manually designed airfoils alongside existing datasets."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.1 and Section 4.1",
            "reasoning": "The paper describes synthetic airfoil generation using unconditional generative models, specifically B\u00e9zierGAN and diffusion models, trained on selected data to produce 10,000 airfoils each, contributing original data generated entirely by AI models without direct derivation from existing airfoils. This newly generated data is part of the AF-200K dataset."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data being generated through translation from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of machine translation techniques applied to generate or transform the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3 and Section 4.1",
            "reasoning": "The AF-200K dataset includes collated existing open-source datasets UIUC and NACA, as described in Section 3 and 4.1, aggregated without significant modification beyond selection and format normalization, thereby qualifying as collated data."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 and Section 4.1",
            "reasoning": "The dataset includes airfoils generated by perturbing parameters of CST (Class Shape Transformation) physical models derived from manually-designed airfoils, as well as interpolations via B-spline methods. This process involves modifications and transformations of existing airfoils, indicating derived data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly specifies the provenance and generation methods for all parts of the dataset."
          }
        }
      }
    ]
  },
  {
    "id": "M32Ldpp4Oy-rubric-5",
    "token_usage": {
      "prompt_tokens": 33453,
      "completion_tokens": 477,
      "total_tokens": 33930
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 and Section 4",
            "reasoning": "The LogiCity simulator generates synthetic datasets based on customizable first-order logic rules and urban simulations designed by human authors. Concepts, rules, and agent sets are configured by humans from scratch to create urban-like environments and multi-agent interactions that are novel and not derived from existing datasets. The data generation involves human-defined symbolic abstractions and rule sets that produce diverse urban scenarios for the new tasks (SPF and VAP). This is original content created by the authors for their benchmark."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The visual rendering of the urban grid maps into diverse RGB images leverages generative models such as GPT-4 and diffusion-based generative models. These AI models generate varied iconographic representations from textual descriptions to create the visual data. Therefore, parts of the dataset that are RGB images are generated by AI models without directly transforming prior datasets, constituting new data generated by models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of datasets created by human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no statement indicating that any data was generated by machine translation."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets are not collected or aggregated from existing sources; they are generated via simulation and generative models."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the datasets as modified or transformed from existing data; rather, they are generated anew via simulation and AI generative models."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data generation process is well described and documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "MU2s9wwWLo-rubric-5",
    "token_usage": {
      "prompt_tokens": 27624,
      "completion_tokens": 499,
      "total_tokens": 28123
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the dataset as created entirely from scratch by human contributors. The prompts and evaluations utilize AI models extensively."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2.3 (Compositional Prompt Generation), Section 2.4 (Concept Evaluation)",
            "reasoning": "The ConceptMix benchmark data is generated by using GPT-4o to compose diverse and complex text prompts by combining randomly sampled visual concepts. These prompts are created by the model rather than manually authored. Furthermore, the evaluation involves GPT-4o generating questions and grading images based on those, thus the dataset of prompts, decomposition and evaluation questions are all generated by AI models, constituting data created by models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any data was produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or evidence that machine translation was used to create or modify the dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the dataset as collected or aggregated from existing sources without significant modification; instead, it is generated dynamically using AI."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.2 (Selecting Visual Concepts), Section 2.3 (Compositional Prompt Generation)",
            "reasoning": "While the prompts are generated by GPT-4o, the visual concepts are selected based on a combination of existing literature and GPT-4-supplemented concepts. The concept categories and their values are derived from prior work and supplemented by GPT-4 outputs, meaning the dataset is based on existing sources with modifications and adaptations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The origin of the dataset is well documented as being generated by models based on derived concept lists."
          }
        }
      }
    ]
  },
  {
    "id": "Mbd3QxXjq5-rubric-5",
    "token_usage": {
      "prompt_tokens": 28194,
      "completion_tokens": 482,
      "total_tokens": 28676
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not created entirely from scratch by human contributors; rather, it relies on synthetic solutions generated automatically."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2 and specifically Section 2.1 (Training Data Synthesis) and Section 2.2 (Prompting)",
            "reasoning": "The OpenMathInstruct-1 dataset is generated by synthesizing problem-solution pairs using the Mixtral open-source language model. Solutions are sampled via few-shot prompting strategies, including a novel masked text solution approach. The data is generated by the AI model producing code-interpreter style solutions, without human-written original solutions. The paper emphasizes extensive model-based synthetic data generation, providing detailed methodology and evidence."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data generated by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No machine translation from another language is used or mentioned in generating the dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not a mere aggregation or collation of existing data but generated synthetically with a language model."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.2.3 (Masked Text Solution Prompting) and Section 2.1 (Training Data Synthesis)",
            "reasoning": "The dataset uses existing mathematical benchmarks' problem statements and reference text solutions as the seed or context for synthetic generation. The reference text solutions are masked and used within prompts to guide the model in generating code solutions. Thus, the data is derived by transformation and adaptation of existing benchmark data, not wholly new or independent."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes the data generation pipeline, so the origin is well documented."
          }
        }
      }
    ]
  },
  {
    "id": "OL2JQoO0kq-rubric-5",
    "token_usage": {
      "prompt_tokens": 23261,
      "completion_tokens": 581,
      "total_tokens": 23842
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not created entirely from scratch by human contributors. Instead, it is curated from existing videos and other sources."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data is not generated entirely by AI or machine learning models from scratch; rather models are used to curate and extract data from existing videos."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention translating content from other languages through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the data was produced by machine translation systems."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3: 'We curate QUILT using 1,087 hours of valuable educational histopathology videos from expert pathologists on YouTube.'; Section 3.2: 'To create QUILT-1M, we expanded QUILT by adding other disparate histopathology image-text open-access sources: LAION, Twitter, and PubMed.'",
            "reasoning": "The data was collected or aggregated from existing public sources such as YouTube videos, PubMed articles, Twitter, and LAION dataset. The authors use multiple methods to extract, align, and clean this data, but fundamentally it is harvested from pre-existing sources without complete generation from scratch."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 and 3.3: 'QUILT was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition.'; 'We propose a four-step text de-noising and quality control pipeline...'; 'Image frame extraction and de-noising.'",
            "reasoning": "The data is derived by applying various transformations and post-processing steps: automatic speech recognition to obtain text, cleaning and correcting the ASR outputs using large language models and medical ontologies, extracting medical and region-of-interest text, selecting and denoising image frames, and aligning images with text based on temporal chunks. Hence, the data is a modified and transformed version of pre-existing video content."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides detailed explanations of the data sources and curation methodology; therefore, the source and generation method is documented."
          }
        }
      }
    ]
  },
  {
    "id": "OTjTKFk7gb-rubric-5",
    "token_usage": {
      "prompt_tokens": 22809,
      "completion_tokens": 488,
      "total_tokens": 23297
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset described is generated via model-based simulation and data generation rather than being created entirely from scratch by human contributors. No explicit indication of manually composed raw data by humans is provided."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Sections 3.1 and 4.1",
            "reasoning": "The dataset's core ad opportunity features are generated using deep generative networks specifically latent diffusion models, as described in Section 3.1. The generated data aim to reproduce real-world ad opportunities without exposing sensitive data. Section 4.1 verifies that this generated data resembles real data, confirming that the dataset is newly generated by AI models rather than collected from existing sources."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication or statement was made that the dataset was created by translating data from another language via human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no evidence or mention of machine translation being used to generate the dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although real-world data were used to train the generative models, the dataset itself is not a mere aggregation or collection of existing data, but rather generated data simulating auctions."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 4.2",
            "reasoning": "The dataset is derived from the interactions of baseline agents in a simulated environment that is based on real online data used to train the generative models. The dataset includes 10 million generated ad opportunities, auto-bidding agent behaviors, and auction results obtained by simulating these agents in the environment. Thus, it is based on modified and transformed data from the generative models and simulation modules."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin and generation methods are documented explicitly in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "PcbSZwVVc5-rubric-5",
    "token_usage": {
      "prompt_tokens": 21753,
      "completion_tokens": 463,
      "total_tokens": 22216
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Collection and Section 3.2 Annotation and Statistics",
            "reasoning": "The dataset, DreamCatcher, was collected from human participants sleeping in their homes wearing modified commercial earables. The data consists of multi-modal sensor recordings (dual-channel audio and motion) recorded during natural sleep in non-restrictive environments. Annotation was performed manually by human annotators reviewing the raw data to label sleep events, with consensus and voting to ensure accuracy. The paper clearly states the recruitment of participants and manual labeling without indication of deriving data from any pre-existing datasets or external sources, indicating that data was created entirely from scratch by human subjects and human annotation."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset was formed through direct human data collection and manual annotation; there is no indication in the paper that any data samples were generated by AI or machine learning models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication that the dataset was produced by translating data from another language by humans."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of machine translation being used to create or convert any data in the dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset was not collected or aggregated from existing sources; it was newly recorded from participants specifically for this study."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the data was processed (e.g., synchronized, annotated), it was not based on or adapted from existing source datasets; rather it was generated from original raw recordings."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin and collection method is clearly documented and specified."
          }
        }
      }
    ]
  },
  {
    "id": "QIJQ1qCGqV-rubric-5",
    "token_usage": {
      "prompt_tokens": 15053,
      "completion_tokens": 339,
      "total_tokens": 15392
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1, 3.2, 3.3",
            "reasoning": "The BlindWays dataset is collected directly from 11 human participants who are blind or have low vision, navigating carefully designed urban routes. The data includes 3D motion captured via wearable IMU sensors and detailed textual annotations created by human annotators based on synchronized third-person and egocentric videos. The paper clearly states that this dataset was created from scratch through human data collection and annotation, with no indication that it was translated or adapted from existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset consists of human motion data and human-annotated text; there is no indication that the motion or text data was generated by AI or machine learning models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of translation from other languages or use of human translators for the dataset annotations or data."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that machine translation was used in producing the dataset or annotations."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data was not collected by aggregating or compiling from existing datasets or sources; it was collected newly from participants."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data is original human motion and annotations, not derived or transformed from existing datasets."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset origin and generation method are clearly specified."
          }
        }
      }
    ]
  },
  {
    "id": "Qf8uzIT1OK-rubric-5",
    "token_usage": {
      "prompt_tokens": 38378,
      "completion_tokens": 395,
      "total_tokens": 38773
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce any new dataset created entirely from scratch by human contributors. Instead, it focuses on ethical considerations and recommendations for the curation of human-centric computer vision datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present any dataset generated by AI or machine learning models; its focus is on ethical guidelines and considerations rather than generation of new model-created data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of datasets produced via human translation from other languages in the paper."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not discuss or introduce datasets generated by machine translation systems."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper reviews and discusses existing HCCV datasets and their ethical issues but does not itself collate or aggregate data from existing sources to create a new dataset."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not derive data by applying modifications or transformations to existing datasets, but instead provides guidelines and recommendations for responsible data curation."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new dataset; it does not specify or document any new data source or method of data generation. Its contribution is a conceptual and ethical framework rather than new data."
          }
        }
      }
    ]
  },
  {
    "id": "QpF3DFP3Td-rubric-5",
    "token_usage": {
      "prompt_tokens": 20337,
      "completion_tokens": 507,
      "total_tokens": 20844
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 and Section 3.2",
            "reasoning": "The Archaeoscape dataset comprises Airborne Laser Scanning (ALS) data and orthophotography acquired from aerial surveys (Sections 3.1 and 3.2). The annotations were meticulously created by expert archaeologists through manual polygon tracing and field verification over decades of archaeological work, resulting in 31,411 annotated anthropogenic features (Section 3.1: Annotation). This indicates original data content created from scratch by human contributors, not translated, adapted, or derived from pre-existing annotated datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset data and annotations are not described as generated by AI or machine learning models; rather, the deep learning models were only used for benchmarking on the dataset (Section 4)."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the dataset or annotations involve human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of machine translation in the generation of dataset content."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data was acquired directly via ALS surveys and orthophotography, not collated from existing sources without modification. Although it builds upon a long history of archaeological surveys, the ALS data and annotations are newly acquired and created."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 (Preprocessing)",
            "reasoning": "The raw ALS point cloud data was processed to remove non-terrain points and then interpolated to produce normalized Digital Terrain Models (DTM) and orthophotos (Section 3.2). This constitutes deriving usable elevation and imagery data from raw ALS measurements, representing transformation and adaptation from original raw data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origins are clearly documented and described in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "R4rNYJ2slJ-rubric-5",
    "token_usage": {
      "prompt_tokens": 17818,
      "completion_tokens": 486,
      "total_tokens": 18304
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and A.1.3 Collection Process",
            "reasoning": "The authors employed a professional annotator team of approximately 50 human annotators who manually labeled fine-grained instance-level lane lines with rich attributes on high-resolution satellite images. This human labeling was done entirely from scratch on collected satellite imagery, as described in Sections 3.2 and the datasheet (A.1.3). Hence, the annotations are original content created entirely by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No part of the dataset was generated purely by AI or machine learning models. The annotations and data are collected and labeled by humans as explicitly described."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any translation of data or annotations from other languages by humans."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention use of machine translation to generate any part of the data."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Collection",
            "reasoning": "The raw satellite images were collected from Google Maps using the publicly available Maps Static API. The images themselves are existing data from Google Maps, thus collected/aggregated from an external existing source without significant modifications to the imagery content."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Section 3.1 Data Collection",
            "reasoning": "The dataset includes new fine-grained instance-level annotations derived from the collected Google Maps satellite images. These annotations are transformations and adaptations applied to the existing imagery to generate detailed vectorized polylines with attributes, representing derived data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The origin of the data and annotation processes are well specified and documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "ScPgzCZ6Lo-rubric-5",
    "token_usage": {
      "prompt_tokens": 36188,
      "completion_tokens": 586,
      "total_tokens": 36774
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The benchmark utilizes existing datasets such as Cora, Citeseer, ogbn-arxiv, Flickr, Reddit, ACM, DBLP, NCI1, DD, ogbg-molbace, ogbg-molbbbp, and ogbg-molhiv. These are publicly available and not newly created by human contributors specifically for this work."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2.2 and Appendix A.2",
            "reasoning": "The authors introduce condensed datasets generated by 12 representative graph condensation methods that synthesize smaller, informative graphs from original large-scale datasets. These condensed datasets are produced through AI models or algorithms as a core contribution of the paper. For example, methods such as GCond, DosCond, SGDD, SFGC, GEOM, GCDM, DM, KiDD, and Mirage generate new condensed graphs by applying learning-based or algorithmic modeling on existing graphs, creating novel datasets that did not previously exist."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate any dataset was produced by translation of content from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of datasets generated via machine translation from another language."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.1 and Appendix A.1",
            "reasoning": "The original large-scale datasets used in the benchmark (e.g. Cora, Citeseer, Reddit, OGB datasets) are collected and aggregated from existing sources, made publicly available, and used without significant modification other than standard preprocessing such as split. The benchmark collects and aggregates these existing datasets to facilitate evaluation."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.1 and 2.2",
            "reasoning": "For heterogeneous datasets like ACM and DBLP, the authors convert them to homogeneous graphs by summing adjacency matrices and performing zero-padding, effectively adapting existing datasets for the benchmark. Additionally, the condensed graphs generated by graph condensation algorithms are derived datasets based on original data with transformations applied to synthesize smaller graphs retaining key properties."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The origins and generation process of the datasets are well documented in multiple sections of the paper and appendix."
          }
        }
      }
    ]
  },
  {
    "id": "USUkwg5pW6-rubric-5",
    "token_usage": {
      "prompt_tokens": 24638,
      "completion_tokens": 359,
      "total_tokens": 24997
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets introduced are not created by direct human annotation from scratch, except for the existing ScribbleSup dataset which is pre-existing. The new datasets are generated algorithmically from existing dense labels, not freshly annotated by humans entirely."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The new scribble datasets are not generated by a trained AI or machine learning model producing novel labels from scratch, but by a deterministic algorithmic procedure processing existing dense segmentation masks."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication or statement in the paper that any datasets or labels were generated by human translators translating from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication or statement in the paper that any machine translation was used to generate the datasets."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets are not simply aggregated from existing sources without modification; rather, they are specifically generated by an algorithm creating scribble labels from dense label maps."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3 Automated Scribble Generation; Section 4 Automatic Scribble Datasets",
            "reasoning": "The new datasets (s4Pascal, s4Cityscapes, s4KITTI360, s4ADE20K) are generated by applying an automatic scribble generation algorithm to existing dense semantic segmentation datasets, thus the data is derived by transformation and adaptation of existing annotated data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the method of scribble generation from existing data, so this category is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "UYgE9IfQIV-rubric-5",
    "token_usage": {
      "prompt_tokens": 30683,
      "completion_tokens": 576,
      "total_tokens": 31259
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 4 and Appendix A",
            "reasoning": "The SustainDC environment and its datasets (workload traces, weather data, carbon intensity data) include elements created or synthesized by the authors. While the workload traces and external data sources are referenced from existing datasets, the authors created the environments and models to simulate data center operations, including custom physics-based models, configuration files, and reward structures. These modeling artifacts and the configuration files (e.g., dc_config.json) represent original human-created content specifically designed for this benchmark."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate that any new data were generated entirely by AI or machine learning models without reference to existing data sources. Instead, data such as workloads and environmental data are sourced or adapted from existing datasets, and the simulation environments are developed by human authors."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any translation of data from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any translation of data from another language by machine translation systems."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 4.1 (Workload Environment), Section E (External variables), and supplementary",
            "reasoning": "The workload traces are sourced from publicly available Alibaba and Google data center traces. Weather data files in .epw format are obtained from energyplus.net, and carbon intensity data are extracted from https://api.eia.gov. These data are collected or aggregated from existing sources without significant modification, constituting collated data."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 4.2 (Data Center Environment Models), Appendix A.2 (Data Center models)",
            "reasoning": "The authors derived simulation models based on existing knowledge and publications (e.g., Computational Fluid Dynamics results for temperature deltas, energy consumption models from prior work) and adapted them into a Python-based environment. This includes adaptation and modification of physical models and equations for IT power, cooling models, and battery models based on prior research, constituting derived data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data sources and generation methods are specified with references and documentation in the paper and supplementary materials."
          }
        }
      }
    ]
  },
  {
    "id": "VH1vxapUTs-rubric-5",
    "token_usage": {
      "prompt_tokens": 17833,
      "completion_tokens": 498,
      "total_tokens": 18331
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that any data was created entirely from scratch by human contributors without reference to existing sources. The dataset consists of harmonized and processed data from existing sources."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset introduced is not generated by AI or machine learning models; it is constructed from various existing observational data sources and satellite products. No mention is made of data generated entirely by models as original data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any data was produced via human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention machine translation being used to generate any part of the data."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3 'Mesogeos Datacube' data sources and datacube creation",
            "reasoning": "Mesogeos aggregates data collected from multiple existing sources, such as ERA5-Land, MODIS, EFFIS, and others, without presenting them as newly generated raw data. The dataset is constructed by collecting and storing these disparate existing datasets."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3 'Mesogeos Datacube' - 'Datacube Creation', 'Burned Areas Dataset', and 'Ignition Date Calculation'",
            "reasoning": "Mesogeos applies harmonization, interpolation, spatial and temporal resampling, coordinate transformations, semi-supervised processing, and refinement methods such as intersecting datasets (e.g., EFFIS burned areas refined using MODIS active fire hotspots for ignition date estimation). These steps indicate the data is derived through modifications and adaptations of existing data sources."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data sources and methods of generation are documented in detail; therefore, this category does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "VSJotgbPHF-rubric-5",
    "token_usage": {
      "prompt_tokens": 13438,
      "completion_tokens": 461,
      "total_tokens": 13899
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 Data Collection and Section 4 Dataset Composition",
            "reasoning": "The OpenAssistant Conversations dataset was collected through a crowd-sourcing effort involving over 13,000 human volunteers who created original prompts and replies entirely from scratch. The paper explicitly states that the data consists mainly of human-generated messages, totaling 152,867 human-submitted messages out of 161,443, with strict moderation and quality control ensuring originality and safety. Thus, the data is original content created by humans without translation or derivation from other sources."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 4 Dataset Composition",
            "reasoning": "The dataset includes a small subset of synthetic messages (8,576 messages) generated by models. These model-generated messages are part of the dataset composition, indicating that some content originates entirely from AI models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any part of the data being produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the use of machine translation systems to generate or translate dataset content."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not collected or aggregated from existing sources without modification; rather, it is created through new human and some model-generated conversations."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication that the dataset is derived from existing sources with modifications; the dataset is based on newly created conversation trees."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents how the dataset was created via human contributions and some synthetic messages; the origin is well specified."
          }
        }
      }
    ]
  },
  {
    "id": "VXohja0vrQ-rubric-5",
    "token_usage": {
      "prompt_tokens": 19467,
      "completion_tokens": 538,
      "total_tokens": 20005
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.2 Dataset Instance Collection",
            "reasoning": "The dataset contains patient notes that were either collected from publicly available patient notes or were synthesized manually by individuals with a medical background. For 21 calculators without eligible notes, 11 rule-based calculators' patient notes were handwritten by a medically trained individual, and answers along with explanations were generated by them. Thus, these instances are original content created from scratch by human contributors not derived directly from existing data."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper specifies that while GPT-4 was used to identify notes and extract attributes from Open-Patients, the final dataset instances, including answers and explanations, were verified or synthesized by human experts. There is no indication that entire data instances were generated solely by AI models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention is made of data being produced by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention is made of data being produced by machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.2 Dataset Instance Collection",
            "reasoning": "The dataset compiles over 180k publicly available patient notes from diverse sources (PMC-Patients, MedQA-USMLE, TREC Clinical Decision Support Tracks, and Clinical Trials Tracks) aggregated to form the Open-Patients collection. Eligible notes were identified, and attributes extracted to form the basis of many MEDCALC-BENCH instances, indicating collation of existing data."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.2 Dataset Instance Collection",
            "reasoning": "After collation of patient notes, GPT-4 extracted attribute values which were then verified and corrected by medical professionals. For the calculators without eligible notes, data was synthesized using templates and human input, representing modifications and transformations based on existing sources. Additionally, step-by-step explanations were generated using template-based explanation generators, derivatives based on rules."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is well documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "WUWHVN4gxk-rubric-5",
    "token_usage": {
      "prompt_tokens": 19979,
      "completion_tokens": 493,
      "total_tokens": 20472
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 and Section 4.1",
            "reasoning": "The paper describes a capture-the-flag (CTF) competition organized by the authors where teams created novel defenses and attacks against large language models. The resulting dataset consists of multi-turn chats generated by these human participants interacting with the models and defenses in the competition. This data\u2014comprising 137k multi-turn conversations\u2014is original human-generated content collected during the competition, as explicitly stated in Section 3 and detailed in Section 4.1."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset consists of interactions involving language models, but the dataset itself is not original data generated independently by models. Instead, model outputs occur as part of the human-model interaction during the competition and are captured together with the context, but the dataset is not purely model generated data detached from human input or control."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any part of the dataset was produced by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention machine translation being used to generate or adapt any data in this dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not described as being collected or aggregated from existing sources without modification. Rather, it was newly generated during the competition via human-model interactions."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the dataset includes model responses and filtered outputs, the core data consists of conversations and prompts generated during the competition, not derived or adapted from pre-existing datasets."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the origin and generation process of the dataset as human-human and human-model interactions during the competition, so the origin is specified and documented."
          }
        }
      }
    ]
  },
  {
    "id": "WVQ4Clw1VD-rubric-5",
    "token_usage": {
      "prompt_tokens": 12670,
      "completion_tokens": 565,
      "total_tokens": 13235
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset MedTrinity-25M is constructed by collecting and integrating existing medical images and annotations from over 90 online datasets and resources. There is no indication of original content created entirely from scratch by human contributors in the paper."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.2 (Data Construction Pipeline) and Section 3.1 (Data Triplet)",
            "reasoning": "The paper describes that the multigranular textual descriptions and visual annotations are generated by prompting multimodal large language models (MLLMs), such as GPT-4V and a specialized fine-tuned model LLaVA-Med++. These models generate detailed multimodal annotations including image-ROI-description triplets without relying on paired text, thus producing original content generated by AI models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any data was produced by translating content from another language by humans."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention the use of machine translation to generate any part of the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 (Data Triplet) and Section 3.2.1 (Data Processing)",
            "reasoning": "The dataset images and some annotations were collected and aggregated from over 90 existing medical datasets and online resources such as TCIA, Kaggle, Zenodo, Synapse, etc. This step involves data collation from diverse existing sources."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 (Data Construction Pipeline) and Section 3.1 (Data Triplet)",
            "reasoning": "The final dataset is derived by applying transformations and adaptations to the collated data: expert grounding models locate ROIs; coarse captions are generated by integrating metadata; retrieval-augmented generation with external medical knowledge is used to produce detailed textual descriptions; MLLMs generate multigranular annotations. This process transforms existing images and coarse annotations into enriched multimodal data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the data collection sources and the pipeline for generating annotations, so the data source is specified."
          }
        }
      }
    ]
  },
  {
    "id": "XBcStBjBIE-rubric-5",
    "token_usage": {
      "prompt_tokens": 31016,
      "completion_tokens": 502,
      "total_tokens": 31518
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate that the new datasets were manually created from scratch by humans. Instead, videos were collected from video platforms."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets are not generated entirely by AI or machine learning models, but curated from real-world video data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of translation of data from another language via human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of machine translation being applied to the data."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Benchmark Construction; Section 4 ChronoMagic-Pro; Appendix C.1 Data Preprocessing",
            "reasoning": "The ChronoMagic-Bench is constructed by collecting and crawling 1,649 high-quality time-lapse videos from existing video platforms (YouTube, Pexels, etc.) using carefully designed search terms. Similarly, ChronoMagic-Pro is created by crawling 66,226 original videos from video platforms and splitting them into 460k clips. Thus, the new datasets are fundamentally aggregated from existing sources without significant creation from scratch."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Benchmark Construction; Section 4 ChronoMagic-Pro; Appendix C.2 Time-Aware Annotation",
            "reasoning": "The datasets include detailed captions generated by large multimodal models (ShareGPT4Video) based on sampled frames and summarized for whole clips, transforming the raw video content into paired text-video data. Also, videos are split and recombined based on semantic consistency, and aesthetic filtering is applied. These modifications indicate the data is derived by applying transformations and processing to existing raw videos."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly details the dataset acquisition and annotation methods, so the data source is documented."
          }
        }
      }
    ]
  },
  {
    "id": "ZDvXY56DeP-rubric-5",
    "token_usage": {
      "prompt_tokens": 20059,
      "completion_tokens": 529,
      "total_tokens": 20588
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.1 Content, Section 3 Case Studies",
            "reasoning": "The paper introduces ORLB, a new benchmark dataset consisting of a large collection of reinforcement learning experimental runs that have been generated by human researchers running algorithms on various environments. These experiments are created from scratch by the authors and contributors using their own code and configurations, ensuring the data is original human-generated experimental data, not translated or adapted from pre-existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not include data generated entirely by AI or models without human experimental setup. Rather, the data originates from actual runs of reinforcement learning algorithms within specific environments and is not synthetic model-generated content."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no discussion or indication that the dataset involves human translation of data from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no evidence that machine translation was used to generate or transform the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.1 Content, Section 5 Discussion and Conclusion",
            "reasoning": "ORLB collates experimental runs from multiple RL libraries, environments and contributors, aggregating them into a unified repository. The data is collected from existing sources\u2014namely, runs by various researchers and libraries\u2014but the paper emphasizes that the data is stored and shared collectively without significant modification, serving as a repository or benchmark."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Case Study, Section 3.2 Case Study with Cleanba",
            "reasoning": "The authors derive new experimental data by modifying existing baseline runs (such as using Monte Carlo estimation instead of TD(\u03bb) in PPO) and by reproducing prior baselines with modifications or corrections, tracked and integrated into ORLB. Therefore, some dataset subsets are derived from existing experiments with transformations and adaptations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data source and method of generation are clearly documented and specified in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "aTXhTD44nF-rubric-5",
    "token_usage": {
      "prompt_tokens": 24679,
      "completion_tokens": 416,
      "total_tokens": 25095
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention that the collected Reddit conversations are newly created content by human contributors. Instead, the data are existing Reddit conversations crawled from the platform."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data content (Reddit conversations) is not generated by AI models. Instead, AI models were used for annotation; the underlying conversation data remain original human posts."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset involves English Reddit conversations only. There is no mention of translation by humans from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not include machine-translated data or mention such in its creation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Collection of Reddit Conversation Threads",
            "reasoning": "The dataset is curated by crawling existing Reddit conversations from 22 subreddits using the praw API. The authors filtered and selected subsets but mostly aggregated existing conversation data with no significant modification to original text content."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Obtaining LLM Annotations",
            "reasoning": "Annotations (stance and dogmatism labels) are generated by applying LLMs on the collected existing conversation data. The dataset includes these AI-generated labels and reasoning which are derived from the original crawled data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly documents data sources and annotation method."
          }
        }
      }
    ]
  },
  {
    "id": "aXeiCbMFFJ-rubric-5",
    "token_usage": {
      "prompt_tokens": 23280,
      "completion_tokens": 484,
      "total_tokens": 23764
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention that the dataset includes any content created entirely from scratch by human contributors; instead, it is largely based on existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data are not generated solely by AI or machine learning models without reference to existing data. Instead, AI models such as GPT-4 are used for annotations and generation of questions and reasoning steps based on existing images."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of human translation of data from one language to another."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention machine translation being used to generate or translate the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3, particularly 3 Visual CoT Dataset and 3.1 Data Generation",
            "reasoning": "The authors curate their Visual CoT dataset by selecting and aggregating data from twelve existing source datasets across five domains. They reuse images and existing annotations such as question-answer pairs and object relations from these datasets without generating original images or entirely new questions. This aggregation constitutes a collated dataset."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Generation",
            "reasoning": "Although the dataset is collated from existing sources, the authors apply modifications such as generating new questions and answers via GPT-4 where original question-answer pairs do not exist (e.g., TextCaps and Flickr30k), annotating intermediate chain-of-thought bounding boxes highlighting key regions, and generating detailed reasoning steps for about 98k pairs. These constitute transformations and adaptations that derive new data from existing sources."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data generation methodology is clearly documented; thus, N/A is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "abXaOcvujs-rubric-5",
    "token_usage": {
      "prompt_tokens": 16089,
      "completion_tokens": 472,
      "total_tokens": 16561
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not described as original human-created data from scratch. It is derived from an existing large public knowledge base, rather than authored anew by humans."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not generated purely by AI or machine learning models without reference to existing data. The authors do use GPT-4 to paraphrase table and column names, but the core data is derived from Wikidata."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication in the paper that the data was produced by translating data from another language by humans."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication in the paper that the data was produced by machine translation from another language."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3: Construction Methodology; Section 4: Dataset Characteristics",
            "reasoning": "The dataset WikiDBs is constructed by collecting and aggregating existing data items and properties from the Wikidata knowledge base, which is an existing public source. The authors aggregate these Wikidata items into relational databases without significant modification to the underlying data content."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.3: Improving Table and Column Names; Section 3: Construction Methodology",
            "reasoning": "The authors apply a paraphrasing step using GPT-4 to rename table and column names to increase diversity and realism. They also prune columns with high sparsity and build coherent databases by selecting related tables using semantic similarity measures. Thus the dataset is transformed and adapted from its original Wikidata form, making it derived data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The source and methodology of dataset generation are clearly specified in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "aiGN4UnNM7-rubric-5",
    "token_usage": {
      "prompt_tokens": 17343,
      "completion_tokens": 413,
      "total_tokens": 17756
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 4.1 Data Collection",
            "reasoning": "The dataset includes real-world driving data collected by the authors using commercial trucks equipped with multiple cameras, radars, and LiDAR sensors. The authors performed their own data collection and annotation process, generating over 200K sequences with ground-truth TTC values and bounding boxes, which represents original content created from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 4.1 Data Collection - NeRF Scenes",
            "reasoning": "To augment rare scenarios with small TTC values, the authors use Neural Radiance Fields (NeRF), a neural rendering method, to generate additional synthetic sequences. These scenes are artificially created by the model-based rendering system, thus the generated data is produced by AI models without direct transformation of existing data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any dataset creation involving human translation of content from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention utilizing machine translation to generate data."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the authors use existing data sources such as radar and LiDAR sensor outputs during annotation, the main dataset is not merely aggregated from existing datasets without modification. Instead, it involves new data collection and labeling."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 4.2 Annotation",
            "reasoning": "The dataset's TTC ground-truth annotations are derived from existing sensor data (LiDAR, radar, camera) through a process involving velocity estimation by RANSAC fitting and bounding box projection. Hence, part of the data (i.e., TTC labels) is produced by transforming and combining information from multiple existing measurement sources."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly specifies the data sources and methods for dataset generation and annotation."
          }
        }
      }
    ]
  },
  {
    "id": "b6IBmU1uzw-rubric-5",
    "token_usage": {
      "prompt_tokens": 31940,
      "completion_tokens": 475,
      "total_tokens": 32415
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The CARES benchmark data is not described as being newly created entirely from scratch by human contributors. Instead, existing open-source datasets are utilized as the data sources."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2 CARES Datasets, Construction of QA Pairs (specifically open-ended questions)",
            "reasoning": "Open-ended question-answer pairs in CARES are generated by using GPT-4 to transform medical reports or descriptions into open-ended QA pairs. This is a process where a model is used to generate new QA pairs based on existing reports, indicating model-generated new content."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or indication of data being produced by translating content from another language using human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that data was translated from another language using machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2 CARES Datasets, Data Source",
            "reasoning": "The CARES benchmark is constructed by collecting and aggregating data from seven open-source medical vision-language and image classification datasets covering multiple modalities and anatomical regions. This constitutes data collation from existing sources without significant modification."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2 CARES Datasets, Construction of QA Pairs",
            "reasoning": "Closed-ended question-answer pairs are created by transforming existing classification labels, reports, or questions from source datasets into QA formats, including the use of GPT-4 to generate multiple question templates. This represents adaptation and transformation of existing data into a derived format."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides clear descriptions of data sources and data generation methods, so the data origin is documented."
          }
        }
      }
    ]
  },
  {
    "id": "bAaM8cKoMl-rubric-5",
    "token_usage": {
      "prompt_tokens": 29916,
      "completion_tokens": 481,
      "total_tokens": 30397
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2 and Appendix C (multiple subsections)",
            "reasoning": "The paper states explicitly that most stimuli are created by the authors. These stimuli are original, programmatically generated image datasets designed to test DNNs on psychological phenomena. The stimuli are created systematically with parametrized scripts, reflecting human design choices and psychological experimental conditions."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of any data generated by AI or ML models without human design. The datasets were created by humans using procedural generation scripts."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The data do not involve translation from other languages through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication of machine translation used for dataset generation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2 (paragraph starting 'While most of the stimuli are created by us, in a few instances...')",
            "reasoning": "Some datasets integrate stimuli from external sources (e.g., line drawings and silhouette datasets from prior works) which are aggregated into the toolbox, with permission and citation. These external stimuli have been organized and integrated but without significant modification."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2, 2.1, 2.2, 2.3, and Appendix C (multiple subsections describing adaptations)",
            "reasoning": "Several datasets are adaptations, for example recreating or modifying stimuli from prior psychological experiments, with transformations and parameterization to fit the toolbox format. Examples include datasets based on prior line drawings, visual illusions, and shape stimuli adapted for this usage."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper describes clearly the methods for dataset creation or source of datasets, so data origin is documented."
          }
        }
      }
    ]
  },
  {
    "id": "cLga8GStdk-rubric-5",
    "token_usage": {
      "prompt_tokens": 16202,
      "completion_tokens": 557,
      "total_tokens": 16759
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3, Data Collection and Structure",
            "reasoning": "The LINGOLY benchmark data consists of puzzles originally created by human authors for the United Kingdom Linguistics Olympiad (UKLO). The puzzles are original content created by human contributors who researched low-resource and extinct languages and crafted linguistic reasoning problems specifically for the Olympiad. The paper states the puzzles are authored by a range of human authors who gave permission for their puzzles to be included, confirming the data is not derived, translated, or generated by machines or from existing datasets, but genuinely human-created."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that any data was generated by AI or machine learning models. The benchmark is compiled from existing human-created puzzles."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The benchmark excludes problems involving non-Latin scripts or transliterations, and the source puzzles are not described as translated from other languages by humans. The puzzles themselves include example translations but appear originally authored in English for the Olympiad context."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no evidence that any data in the benchmark was produced by machine translation systems."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2 Data Collection and Structure",
            "reasoning": "The puzzles were collected from the UKLO past paper archive, which consists of previously published Olympiad puzzles. The authors parsed these puzzles from pdf to text and formatted them for machine readability, aggregating the existing human-created puzzles without significant modification."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Format and Selection of the Linguistic Olympiad Puzzles and Section 3.2 Data Collection and Structure",
            "reasoning": "The authors made adaptations for machine readability and omitted puzzles with images or non-Latin scripts, as well as some questions requiring fuzzy answers. They corrected known errors and standardized formatting such as tabular data into text form. Therefore, the dataset is derived from existing puzzles with transformations and adaptations for their benchmark purposes."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The source and generation of the data is clearly specified in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "cR3T1ZYN8I-rubric-5",
    "token_usage": {
      "prompt_tokens": 36582,
      "completion_tokens": 385,
      "total_tokens": 36967
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce any new datasets created entirely from scratch by human contributors. Instead, it evaluates existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No new datasets generated by AI or machine learning models are introduced in this work. The paper uses existing datasets for benchmarking."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that data was produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data generated via machine translation systems."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not collect or aggregate existing data as a new dataset; it uses several existing datasets for evaluation."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not create new datasets derived from existing data with modifications or transformations; it converts the classification task into a VQA problem using existing datasets but does not derive new datasets."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets; all datasets used are pre-existing and publicly available as stated in Section 4.2 (Datasets) and elsewhere. The paper focuses on a new evaluation protocol and benchmarking approach rather than new data creation."
          }
        }
      }
    ]
  },
  {
    "id": "cu8FfaYriU-rubric-5",
    "token_usage": {
      "prompt_tokens": 29556,
      "completion_tokens": 398,
      "total_tokens": 29954
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce any new dataset created entirely from scratch by human contributors. Instead, it reports on qualitative research involving interviews with dataset curators."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or introduction of datasets generated entirely by AI or machine learning models in the paper."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does mention language barriers and use of translators in dataset curation (e.g., Section 3.3.3), but it does not detail producing any dataset via human translation."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that datasets produced via machine translation are introduced or discussed as new datasets."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper is a study about challenges in the curation of fair datasets based on interviews and does not describe collation of data into a new dataset."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce datasets derived from existing sources with modifications; it is focused on describing challenges and practices in dataset curation."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not present any new datasets. It instead provides a qualitative study of dataset curation practices and challenges. Thus, no dataset origin or method of generation is specified or documented."
          }
        }
      }
    ]
  },
  {
    "id": "cy8mq7QYae-rubric-5",
    "token_usage": {
      "prompt_tokens": 82163,
      "completion_tokens": 456,
      "total_tokens": 82619
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Sections 3.1 and 3.2",
            "reasoning": "The dataset CharXiv is composed of 2,323 real-world charts handpicked from arXiv papers and questions that are manually curated and verified by human experts. The questions include descriptive ones designed by templates and reasoning questions that are manually crafted by graduate students annotators. This indicates original content created from scratch by humans rather than adapted or derived from existing data."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the dataset as generated entirely by AI or machine learning models; instead, model-generated data is only used in comparison or stress tests."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that the data was produced by human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not involve any machine translation as per the paper's description."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1",
            "reasoning": "The charts are collected from existing sources, namely the figures extracted from arXiv preprints dated from 2020 to 2023. They are downloaded, re-rendered, and selected through a four-step pipeline, implying aggregation from existing data."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the charts are re-rendered and filtered, this process does not constitute substantial modification, transformation, or adaptation that would qualify as derived data according to the paper."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes the data collection and creation process, making the origin of the dataset documented."
          }
        }
      }
    ]
  },
  {
    "id": "d1Pup4gkWf-rubric-5",
    "token_usage": {
      "prompt_tokens": 14827,
      "completion_tokens": 434,
      "total_tokens": 15261
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the creation of any original datasets collected entirely from freshly created human contributions. Instead, it leverages existing human motion data sets and video data."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce any new datasets generated entirely by AI or machine learning models without reference to existing data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that data was produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention is made of data generated by machine translation systems."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper uses existing datasets such as AMASS and existing internet videos but does not present the dataset itself as merely collected or aggregated without modification; instead, data is processed and transformed, thus not simply collated."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 4.3 Acquiring Human Demonstration From Videos",
            "reasoning": "The paper derives new usable demonstration data by extracting 3D motion from internet videos using the TRAM method and further applies physics-based motion tracking (PHC) to correct and physically simulate these motions. This process transforms existing video data into physically plausible motion datasets compatible with their humanoid simulation. Additionally, they use pre-existing datasets like AMASS which are adapted and integrated for their purposes."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is specified and documented, hence N/A is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "dVaWCDMBof-rubric-5",
    "token_usage": {
      "prompt_tokens": 23772,
      "completion_tokens": 424,
      "total_tokens": 24196
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any original data created entirely from scratch by human contributors. The datasets discussed are sourced from the web or existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No data is described as generated directly by AI or models without reference to existing data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of data produced via human translation."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate any machine-translated data."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2 (COMMONPOOL generation)",
            "reasoning": "The COMMONPOOL dataset is constructed by collecting and aggregating image-text pairs from Common Crawl web data. It involves extracting URLs, downloading data, and applying filtering to create a large pool without significant modification of original content, thereby constituting collated data."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 (Safety preprocessing and face blurring); Section 4 (Filtering baselines and data filtering)",
            "reasoning": "COMMONPOOL is processed with steps such as NSFW detection, filtering, duplicate removal, and face blurring, which modify and transform the original collected data. Further subsets are created by filtering strategies based on image and text features, resulting in derived datasets from the original pool."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the data sources and processes, so the data origin is known."
          }
        }
      }
    ]
  },
  {
    "id": "g7OX2sOJtn-rubric-5",
    "token_usage": {
      "prompt_tokens": 47239,
      "completion_tokens": 608,
      "total_tokens": 47847
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 4: LeanDojo Benchmark",
            "reasoning": "The LeanDojo Benchmark dataset consists of 98,734 theorems and their proofs extracted from Lean's centralized math library, mathlib, which are originally human-written formal proofs contributed by mathematicians and programmers. The paper states these proofs are human-written and extracted from existing Lean repos (Section 4). Hence, the data is original human-created mathematical content, not translated or adapted from other sources but directly created by human contributors for Lean."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 6, Appendix C.4",
            "reasoning": "The paper reports that their model, ReProver, discovered new proofs for theorems that previously did not have proofs in Lean (e.g., 33 new proofs in MiniF2F and 39 in ProofNet). These proofs are generated entirely by the trained language model interacting with Lean without human intervention, constituting original data newly generated by a model. Therefore, this data is newly created by the model itself."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any data was produced by translating content from another language by humans."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data generated through machine translation from other languages in the paper."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 4: LeanDojo Benchmark",
            "reasoning": "The LeanDojo Benchmark data is extracted and aggregated from existing sources\u2014Lean's mathlib library and standard Lean library source code. The dataset includes collated premises, theorem statements, and proofs as they are found in these repositories, forming a comprehensive collected dataset for machine learning."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 4: LeanDojo's data extraction, Section 5: ReProver training",
            "reasoning": "The paper describes that LeanDojo instruments Lean to extract additional annotations not visible in raw code, such as proof trees with intermediate states and premises fully qualified names. Additionally, the model training involves generating new proofs (derived from the original theorems by applying model-inferred tactics). Hence, some data is derived from existing sources through modifications, transformations, or expansions."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides transparent documentation of the data origin and generation methods, so 'N/A' does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "gg3POFjqq8-rubric-5",
    "token_usage": {
      "prompt_tokens": 12954,
      "completion_tokens": 492,
      "total_tokens": 13446
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 4 (The Benchmarking Dataset)",
            "reasoning": "The authors manually annotate a dataset of generated images with filter labels to evaluate out-of-class (OOC) filtering strategies. These annotations involve human annotators labeling images as 'class', 'partial class properties', or 'not class'. This manual labeling is original content created entirely from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.1 (Continuous Nuisance Shifts for Benchmarking) and Section 4 (The Benchmarking Dataset)",
            "reasoning": "The paper generates a new dataset of images using diffusion models (Stable Diffusion 2.0) combined with LoRA adapters to create continuous nuisance shifts on ImageNet classes. These synthetic images are generated entirely by AI models to simulate various controlled shifts for robustness benchmarking."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper of any data produced by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or indication of data generated by machine translation from another language."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets introduced are not merely collected or aggregated from existing sources without significant modification; the generated data are synthetic and newly generated or human annotated."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 (Continuous Nuisance Shifts for Benchmarking)",
            "reasoning": "The synthetic images are derived by applying LoRA adapters to pre-trained diffusion models, which modifies the existing diffusion models outputs to generate images with continuous nuisance shifts. Thus, these datasets are based on existing sources (pretrained generative models) with modifications and adaptations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper documents the data generation and annotation process clearly for the new datasets."
          }
        }
      }
    ]
  },
  {
    "id": "h8LuywKj6N-rubric-5",
    "token_usage": {
      "prompt_tokens": 45999,
      "completion_tokens": 672,
      "total_tokens": 46671
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.2 and 2.3",
            "reasoning": "The GUI-WORLD dataset includes over 12,000 GUI videos collected from screen recordings performed by student workers executing specified tasks, as well as segments of tutorial videos from YouTube that were manually reviewed and curated. The keyframes in the videos were manually selected by human annotators who also provided detailed annotations including descriptions of operations, purposes between keyframes, and GUI element details. Additionally, human annotators reviewed and corrected the QA pairs generated by GPT-4V to ensure accuracy and remove hallucinations, thereby contributing original, human-verified data. This pipeline, described in Sections 2.2 and 2.3, evidences original content created from scratch by humans and not transformed or translated from pre-existing material."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2.3",
            "reasoning": "The dataset contains annotations, including captions and various types of QA pairs (e.g., multiple-choice, free-form, conversational) generated via a Human-MLLM collaborative approach, specifically utilizing GPT-4V. While the original videos and keyframe annotations are human-created, the textual data for instructions and question-answer generation were produced or refined by the model, with humans reviewing and editing to improve quality. Therefore, parts of the data (textual QA and captions) are original data generated by AI models, as described in Section 2.3."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not specify that any data in GUI-WORLD was produced via human translation of content from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication or mention of machine-translated data being used or included in the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.2",
            "reasoning": "A portion of the dataset was compiled by collecting publicly available tutorial videos from YouTube, which were segmented into clips representing GUI operations. This parts of the dataset are essentially aggregated from existing instructional video sources without major modification to raw video data itself, thereby representing collated data as described."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.2 and 2.3",
            "reasoning": "The dataset includes derived data such as keyframe annotations extracted manually from videos, and textual annotations refined with an LLM. Additionally, textual data initially generated by GPT-4V was reviewed and corrected by humans, representing a transformation and adaptation of existing data (videos and raw model outputs) to enhance quality and construct the final dataset."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides detailed explanations of the data sources and the generation processes, thus the data origin is documented."
          }
        }
      }
    ]
  },
  {
    "id": "hcOq2buakM-rubric-5",
    "token_usage": {
      "prompt_tokens": 34980,
      "completion_tokens": 293,
      "total_tokens": 35273
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": ""
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new dataset. It develops an assessment framework for evaluating existing AI benchmarks and assesses 24 existing benchmarks. The data used for assessment consists of publicly available information (e.g., papers, code repositories) about these benchmarks. Since the paper does not specify or document any origin of new data or datasets, and instead works on meta-assessment of existing datasets/benchmarks, the data source or generation method relevant for evaluation is not applicable or specified."
          }
        }
      }
    ]
  },
  {
    "id": "iSwK1YqO7v-rubric-5",
    "token_usage": {
      "prompt_tokens": 99200,
      "completion_tokens": 533,
      "total_tokens": 99733
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 Dataset Annotations and Benchmark Implementations; Appendix M",
            "reasoning": "The paper introduces new annotations for existing benchmarks VirtualHome and BEHAVIOR, including symbolic goal definitions, action trajectories, transition models, and natural language task instructions, which are created manually by the authors. These annotations are original content created from scratch by human experts (the authors) to augment existing datasets for systematic evaluation. This is explicitly described in Section 3 and Appendix M, where the authors detail their manual annotation process and quality verification."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets and annotations are manually curated or adapted from existing simulator data. There is no indication that the dataset itself was generated entirely by models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of translating data from another language via human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data generated by automatic machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3 Dataset Annotations and Benchmark Implementations; Section L Dataset Statistics and Analysis; Appendix M",
            "reasoning": "The benchmark extends existing datasets VirtualHome and BEHAVIOR by collecting and aggregating data including task instructions, symbolic goals, trajectories, and transition models, largely sourced from existing datasets and simulators. The paper explicitly states that they select and build upon existing simulations and datasets, aggregating and reformatting them with additional annotations to support their interface."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3 Dataset Annotations and Benchmark Implementations; Appendix M",
            "reasoning": "The authors generate derived data by adapting and transforming existing simulator data, for instance converting BDDL goals in BEHAVIOR into grounded LTL goal options, annotating PDDL transition models, and generating symbolic simulators on top of iGibson and VirtualHome simulators. This processing and adaptation constitutes derived data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is clearly documented and described in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "iTUlYblV0K-rubric-5",
    "token_usage": {
      "prompt_tokens": 21438,
      "completion_tokens": 443,
      "total_tokens": 21881
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.1, Section 4",
            "reasoning": "The dataset MQuAKE and its remastered version are constructed based on Wikidata RDF triples, where the authors manually selected relations and entities to build subgraphs and then created multi-hop questions via GPT-3.5-Turbo generation with human oversight. The remastering involved manual corrections such as rewriting incomplete natural language question instructions, removing duplicates, and correcting conflicting edits, indicating substantial human involvement in creation and curation."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.4",
            "reasoning": "The multi-hop question instructions in the dataset are generated by the gpt-3.5-turbo model from the raw triplet chains, repeated multiple times to ensure quality, hence part of the data (the natural language multi-hop questions) is model-generated content."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human translation of data from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any machine translation employed for the dataset creation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.1",
            "reasoning": "The dataset is fundamentally based on Wikidata RDF knowledge graph, which is an existing open-source dataset; the authors extracted and filtered a subset of this existing data including top entities and relations, aggregating it to build subgraphs that serve as the foundation of the dataset."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.1, Section 4",
            "reasoning": "The authors modified and fixed the existing MQuAKE dataset to create MQAKE-REMASTERED by correcting errors such as contamination, missing information, and conflicts. They also introduced dynamic masking and constructed contamination-free subsets. This constitutes derivation from existing data through transformation, correction, and adaptation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data source and generation method are explicitly described in several sections; therefore, N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "iwC19lVBoq-rubric-5",
    "token_usage": {
      "prompt_tokens": 14809,
      "completion_tokens": 431,
      "total_tokens": 15240
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset AVSET-10M is constructed from existing datasets (AudioSet and Panda-70M) rather than being created entirely from original human contributions from scratch."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not generated purely by AI or machine learning models; instead, it involves filtering and processing of existing data sources."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the data was produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the data was generated by machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction Pipeline",
            "reasoning": "The AVSET-10M dataset is constructed by collecting and aggregating samples from existing large-scale public datasets AudioSet and Panda-70M (both existing datasets sourced from YouTube videos). The authors collate data from these sources without creating new raw data."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction Pipeline; Sections describing filtering and sample recycling",
            "reasoning": "The dataset is filtered, processed, and transformed from existing datasets via stages including audio-visual correspondence filtering, voice-over filtering, and sample recycling with sound separation. This indicates adaptations and transformations applied to existing data to derive a new dataset with high audio-visual correspondence."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is clearly documented and described in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "j2wasUypqN-rubric-5",
    "token_usage": {
      "prompt_tokens": 21043,
      "completion_tokens": 422,
      "total_tokens": 21465
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate any dataset created entirely from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No data generated entirely by AI or machine learning models as original content is described."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of data produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication of data generated by machine translation of other language datasets."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2 Testsuites and Appendix A",
            "reasoning": "The MetaBox benchmark platform collates over 300 problem instances from existing sources, primarily from the COCO [28] platform and the Protein-Docking benchmark [29]. These datasets are incorporated with minimal changes (some adaptations for MetaBBO paradigms), which fits the definition of collated data collected or aggregated from existing sources without significant modification."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Testsuites",
            "reasoning": "The paper mentions that the Protein-Docking dataset is extracted and parameterized differently (e.g., dimensionality reduced to R^12) and the COCO functions are adapted with difficulty-based train-test splits and custom problem dimensions, indicating modifications and adaptations consistent with derived data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The source and method of generation for all datasets introduced are explicitly documented as collated or derived from existing datasets."
          }
        }
      }
    ]
  },
  {
    "id": "jSKtxmxc0M-rubric-5",
    "token_usage": {
      "prompt_tokens": 21306,
      "completion_tokens": 446,
      "total_tokens": 21752
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Construction",
            "reasoning": "The dataset VideoGUI is constructed by manually selecting instructional videos from YouTube and then inviting human participants to reproduce the instructional videos in a simulator environment. These participants manually perform and record user behaviors, break down tasks into subtasks with textual descriptions, and annotate GUI elements associated with actions. The data collection process involves human demonstration and manual multi-level annotation from procedural planning down to atomic actions, indicating that the dataset content is newly created by humans rather than being adapted or directly aggregated from existing data."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that any part of the dataset was generated by AI or machine learning models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the dataset was produced by human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention or evidence in the paper that machine translation was used to generate dataset content."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While instructional videos from YouTube are used as a reference source, the authors create a new dataset by human demonstration and detailed annotation in their simulator, rather than simply collecting or aggregating existing data without modification."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not derived by modifying or transforming existing datasets; instead, it is constructed from scratch by human reproduction of video instructions and detailed manual annotation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly documents the data source and generation methods, so this option does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "jbrMS0DNaD-rubric-5",
    "token_usage": {
      "prompt_tokens": 17008,
      "completion_tokens": 530,
      "total_tokens": 17538
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 (The INQUIRE Benchmark), especially 3.2 Query and Image Collection Process",
            "reasoning": "The paper describes 250 expert-level retrieval queries collected from interviews with ecological and environmental domain experts, including ornithologists, marine biologists, entomologists, and forestry specialists, as well as from reviews of academic literature. Additionally, image annotations labeling all relevant images for these queries over the five million image iNat24 dataset were performed by human annotators with domain familiarity. This constitutes original content created by human contributors specifically for this benchmark, not adapted or derived from pre-existing material."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset was not generated or synthesized by AI or machine learning models; rather, images are collected from a public platform and queries and annotations were human curated."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of the dataset or queries being produced by human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any use of machine translation for dataset or query creation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 The iNaturalist 2024 Dataset",
            "reasoning": "The iNat24 dataset consists of five million images collected and annotated by citizen scientists on the public iNaturalist platform from 2021\u20132024. The images are aggregated from this existing source without significant modification; thus, the image data is collated."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the iNat24 dataset shares classes with iNaturalist 2021, it contains different images; however, the dataset is a direct sample from iNaturalist and not a modification or transformation of existing datasets, so it is best described as collated rather than derived."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the origin and generation method of both the image dataset and the queries, so N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "kaHpo8OZw2-rubric-5",
    "token_usage": {
      "prompt_tokens": 105767,
      "completion_tokens": 526,
      "total_tokens": 106293
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section C.3 (Design of challenging user prompts)",
            "reasoning": "The paper explicitly states that they use GPT-3.5 and GPT-4 to generate challenging toxic task prompts, which are new prompts created by the model but curated and selected by human researchers. These prompts are intended to be more challenging than existing benchmarks. The initial toxic prompts are from REALTOXICITYPROMPTS, but the new dataset of 1.2K challenging toxic prompts is generated for evaluation purposes, indicating novel dataset creation with human oversight."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section C.3 (Design of challenging user prompts) and Appendix G (Robustness against adversarial demonstrations)",
            "reasoning": "The paper describes generating new adversarial inputs and challenging prompts using GPT-3.5 and GPT-4 models (e.g., challenging toxic user prompts generated by GPT models, adversarial texts for AdvGLUE++ generated against Alpaca, Vicuna and StableVicuna). These datasets are newly synthesized by AI models without directly copying or heavily modifying existing datasets, thus represent new data generated by models rather than human curation alone."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data created via human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data created via machine translation from other languages."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does discuss evaluation on existing benchmark datasets such as REALTOXICITYPROMPTS, AdvGLUE, ETHICS, Jiminy Cricket, RealtimeQA, etc., but does not present these as novel datasets introduced by the authors. Therefore, not applicable for new datasets in this paper."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Sections C.3, E.2 (AdvGLUE++), F (OOD robustness)",
            "reasoning": "Some evaluation datasets like AdvGLUE++ are generated by applying adversarial attack methods to existing datasets (e.g., GLUE tasks), thus are derived from existing sources with modifications. Also, OOD robustness datasets involve style transformations and modifications of existing benchmarks to create shifted distributions, which constitutes derived data based on existing sources with transformations applied."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper generally explains the origin and creation method of datasets and evaluation data, so the data origin is documented."
          }
        }
      }
    ]
  },
  {
    "id": "l985bXCatk-rubric-5",
    "token_usage": {
      "prompt_tokens": 16130,
      "completion_tokens": 464,
      "total_tokens": 16594
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the dataset as being created from scratch by human contributors. Instead, it is derived from existing online sources."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset images are collected from web sources (fashion retailers), not generated by AI or machine learning models. Only metadata annotations like captions were generated by models, but the images themselves are not newly generated data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that any data was translated by humans in the dataset construction."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No information about machine translation being used for dataset creation or annotations is provided."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Image Collection",
            "reasoning": "The dataset LRVS-Fashion is composed by collecting and aggregating images from LAION-5B dataset, which consists of web-crawled images from fashion retailers and brands. The authors extracted product identifiers from URLs and grouped existing images accordingly, making this a collated dataset from existing sources."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Annotations and Section 3.2 Composition",
            "reasoning": "The dataset includes synthetic labels such as image complexity, product categories, and generated captions created using pretrained and finetuned models (e.g., BLIP-2 for caption generation). These annotations are modifications or transformations of the existing collected images and metadata, constituting a derived dataset."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is clearly documented as collated from LAION-5B and derived with synthetic annotations; thus, N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "lnnNPiZtzR-rubric-5",
    "token_usage": {
      "prompt_tokens": 13237,
      "completion_tokens": 489,
      "total_tokens": 13726
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2 The FungiTastic Dataset",
            "reasoning": "The dataset is based on fungal records labeled and curated by experts and citizen scientists over a twenty-year span, thus created from scratch by human contributors. The observations include annotated photographs and metadata, indicating original content collected directly by humans, not derived or translated from other data."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state or imply that any part of the dataset was generated by AI or machine learning models. Segmentations were semi-automatically generated using a model, but these are annotations applied to original human-collected images, not original data generated by models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data being translated from another language via human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication is given in the paper that the dataset includes data produced via machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2 The FungiTastic Dataset",
            "reasoning": "The dataset is collected from the Atlas of Danish Fungi submissions made over many years, aggregating and organizing these existing fungal observations into a comprehensive multi-modal dataset without significant modification. Additional external data such as satellite images and meteorological data were also collated to enrich the observations."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.1 Additional observation data",
            "reasoning": "The dataset includes derived data such as satellite images processed via clipping, scaling, gamma correction, and rounding; meteorological data extracted from climatic rasters; and semi-automatic segmentation masks generated with the Segment Anything model and verified by humans, indicating modifications and derivations from existing sources."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The origin and generation of the data are well documented."
          }
        }
      }
    ]
  },
  {
    "id": "loJM1acwzf-rubric-5",
    "token_usage": {
      "prompt_tokens": 30469,
      "completion_tokens": 419,
      "total_tokens": 30888
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.2 Question and Answer Collection",
            "reasoning": "The paper states that ten expert-level human annotators created new questions (898 newly-annotated questions) from scratch on both existing and newly collected documents. These questions were manually composed to meet specific quality criteria, indicating original content generation by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any part of the data being generated directly by AI or machine learning models without human involvement."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of the dataset being produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not include any content generated by machine translation systems."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Document Collection",
            "reasoning": "The benchmark includes 76 documents collected from existing datasets (DUDE, SlideVQA, ChartQA, FinanceBench), which were aggregated without significant modification and reused as part of the dataset."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Question and Answer Collection",
            "reasoning": "Existing questions from these datasets were carefully edited and revised by human annotators to improve quality, remove errors, and balance distribution, indicating derivation and transformation from existing sources."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The origins and generation methods of the data are clearly described in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "mDRmX8IlBI-rubric-5",
    "token_usage": {
      "prompt_tokens": 17579,
      "completion_tokens": 536,
      "total_tokens": 18115
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 Manual Data Collection",
            "reasoning": "The paper explicitly states that the human-annotated dataset consists of 1,559 question-answer pairs and captions annotated and reviewed by humans. The data is collected from videos across various disciplines and annotated with human-generated questions and answers, indicating original content creation by humans rather than adaptation or translation."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.2 Automated Data Collection",
            "reasoning": "The synthetic dataset is generated via an automated pipeline that uses GPT-4V to generate questions, answers, and captions based on selected video frames and transcripts. This indicates that the QA pairs and captions in the synthetic dataset are generated entirely by a model without directly transforming existing data, characterizing them as new data from a model."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any translation of data from other languages performed by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any use of machine translation to generate the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Manual Data Collection and Section 3.2 Automated Data Collection",
            "reasoning": "The videos are collected from existing sources such as YouTube (with Creative Commons license), SportsQA, IKEA Assembly, RT-1, Ego4D, and YouTube-8M, meaning the video content itself is collated from existing repositories. However, minimal modification of these video sources is described beyond selection and summarization, indicating collated video data."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While question-answer pairs are created either manually or generated by models, the paper does not indicate that any data is based on existing sources with modifications or adaptations applied. The QA pairs from humans and models are newly created rather than derived by altering existing QA pairs. Therefore, derived data does not apply."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the data sources and generation methods for all datasets introduced."
          }
        }
      }
    ]
  },
  {
    "id": "mEJgnZZyfv-rubric-5",
    "token_usage": {
      "prompt_tokens": 17702,
      "completion_tokens": 412,
      "total_tokens": 18114
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present any newly created datasets constructed entirely from scratch by human contributors. Instead, it relies on existing public image datasets for evaluation."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets used are standard benchmarks rather than newly generated datasets produced entirely by machine learning models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of any datasets produced through human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state the use or creation of any machine-translated datasets."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2 Benchmarking Tasks, Table 2",
            "reasoning": "The paper uses 12 existing open-source visual classification datasets, including CIFAR-10/100, ImageNet-1K, CUB-200-2011, iNaturalist, and Places205, which are publicly available and collected from existing sources. The authors aggregate these datasets for comprehensive benchmarking without creating new datasets."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the paper applies various mixup augmentation strategies and transformations during training, it does not create new datasets derived or adapted from existing ones in a manner that constitutes a new dataset."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly specifies all datasets used and their origins; thus, the data origin is documented."
          }
        }
      }
    ]
  },
  {
    "id": "mlhFJE7PKo-rubric-5",
    "token_usage": {
      "prompt_tokens": 40680,
      "completion_tokens": 513,
      "total_tokens": 41193
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset HEST-1k is assembled from multiple existing public and internal cohorts and not primarily newly created original data by the authors. The paper describes collecting and unifying existing datasets rather than generating new primary data from human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication is given that any data in HEST-1k was generated entirely by AI or machine learning models. The dataset is derived from biological experiments and publicly available cohorts."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of any data created by translating content from another language via human translators in the dataset creation."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not discuss any data generation involving machine translation systems from other languages."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3 (HEST-1k Dataset), Abstract",
            "reasoning": "HEST-1k is described as a collection of 1,229 spatial transcriptomic profiles gathered from 153 public and internal cohorts across multiple repositories (10x Genomics, Mendeley, NCBI, etc.). The authors aggregated existing datasets without creating new original experimental data. They unified diverse legacy data with metadata and standardized formats for computational use."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Histology, Section 3.3 Nuclear segmentation and classification, Section 3.4 Gene expression",
            "reasoning": "The authors applied preprocessing steps such as converting images to pyramidal TIFF, performing automatic alignment and re-alignment of spots to images, nuclear segmentation and classification using CellViT, and compiling data into uniform ANNDATA formats. These transformations represent modifications and adaptations of existing data, making the dataset derived from collated sources with additional processing."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is documented and clearly specified as collected and processed from public and internal existing sources."
          }
        }
      }
    ]
  },
  {
    "id": "moMoWj7jLm-rubric-5",
    "token_usage": {
      "prompt_tokens": 24882,
      "completion_tokens": 584,
      "total_tokens": 25466
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 (Dataset Construction)",
            "reasoning": "The dataset FormulaReasoning was constructed from Chinese junior high school physics examination questions collected by the authors (Section 3). The questions are original exam questions from the past 14 years involving manual annotation by postgraduate volunteers for reasoning steps and formula normalization. The process involved manual annotation combined with LLM assistance but the original questions and annotations were created by humans and not derived from other datasets or translated."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the authors utilized LLMs to assist in annotation and generate additional data for augmentation (Section 4.2.4 and Appendix C.3.1), the main dataset of 5,420 questions was collected from existing exams and manually annotated. The augmented data is supplementary and not the primary dataset introduced."
          },
          "Human Translation": {
            "is_applicable": true,
            "reference": "Section 1 (Introduction) and Appendix A",
            "reasoning": "The dataset is originally in Chinese, and all examples and the dataset were translated into English by humans for the convenience of understanding (Section 1). This indicates human translation of the dataset content."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate using machine translation to produce dataset content. Although the authors mention LLM assistance and translation efforts, the translation is specified as human (Section 1 and Appendix A)."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 (Preprocessing) and Section 3 (Dataset Construction)",
            "reasoning": "The initial data was collected by crawling public sources of junior high school physics examination questions from 2015 onwards, thus collated from existing public exams without significant modification of the original questions themselves."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 (Formula Normalization) and Section 3.3 (Formula Database Construction)",
            "reasoning": "The authors normalized and annotated the original exam questions\u2019 reasoning steps, extracted and merged formulas and parameters, and constructed a formula database by merging formula variants and semantically similar formulas. This process involved significant transformation and derivation of data from the original sourced exam questions."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly details the data origin, collection, annotation, and transformation processes, so the origin is documented."
          }
        }
      }
    ]
  },
  {
    "id": "nrEqH502eC-rubric-5",
    "token_usage": {
      "prompt_tokens": 80256,
      "completion_tokens": 571,
      "total_tokens": 80827
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 Construction of BLEND, especially subsections 'Question Collection and Filtering', 'Answer Annotation', and 'Answer Aggregation'",
            "reasoning": "The dataset BLEnD was constructed through human efforts involving native speakers from various countries/regions who created 500 question templates per country, designed to capture daily life cultural knowledge. Annotators provided human-annotated answers in their native languages. The process explicitly involved human-written questions and answers, collected via crowdsourcing platforms like Prolific and direct recruitment, with guidelines to produce original culturally relevant questions and answers avoiding stereotypes. There is no indication the data was translated from pre-existing sources; rather, questions were composed by annotators from scratch and answers were collected directly from local people."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset was not generated by AI or machine learning models; rather, human annotators created questions and answers. Although LLMs were used for evaluation and multiple choice option selection assistance (such as GPT-4 for detecting similar terms), no original dataset content was generated entirely by models."
          },
          "Human Translation": {
            "is_applicable": true,
            "reference": "Section 3 Construction of BLEND, 'Question Collection and Filtering' and 'Answer Aggregation'",
            "reasoning": "The questions for most countries (except US and GB) were translated into local languages by native speakers. Moreover, answer annotations were translated into English by native speakers during aggregation, indicating the use of human translation in dataset construction."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that machine translation was used in the creation of the dataset. The paper explicitly mentions human translation but does not mention any machine translation systems."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not simply collected or aggregated from existing sources without modification. Instead, it was carefully created and annotated by humans to capture cultural knowledge not present in existing online sources."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not appear to be based on existing sources with modifications or adaptations. It is original content created by humans, rather than derived from prior datasets or texts."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is clearly documented and described."
          }
        }
      }
    ]
  },
  {
    "id": "p8eUitex7p-rubric-5",
    "token_usage": {
      "prompt_tokens": 21019,
      "completion_tokens": 616,
      "total_tokens": 21635
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction; Section A.2 Annotator Guidelines",
            "reasoning": "The paper states that 30 human annotators were recruited and trained to manually annotate 2D bounding boxes, 3D location, 3D viewpoint (6D pose), scene density, and object visual quality for over 86,000 objects across 200 categories. Detailed annotation guidelines, training sessions, and a web-based annotation tool were provided to ensure data quality. This indicates original data created entirely from scratch by human contributors rather than adaptation or translation."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.3 Natural Captions with 3D Information; Section A.3 Caption Generation",
            "reasoning": "The authors utilize GPT-4v, an AI model, to generate natural language image captions interleaved with 3D annotations. These captions are synthesized based on the 2D and 3D annotations, resulting in original text data generated entirely by a machine learning model without direct reference to pre-existing caption datasets."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data created by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data generated by machine translation from another language."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction (Overview and Object Categories)",
            "reasoning": "The dataset is based on images from the existing ImageNet21k dataset, and 100 categories and raw images from ObjectNet3D are adopted as unannotated data sources. Hence, part of the dataset consists of collected existing images from these established datasets without direct modification."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction (Data Collection); Section 3.2 Cross-Category 3D Alignment",
            "reasoning": "The 3D annotations (poses, locations) are created using existing 3D CAD models (from Objaverse) that were aligned as canonical poses for each category. The canonical poses were manually aligned across categories based on semantic parts, shapes, and common knowledge. Additionally, 3D annotations are linked to existing images. This implies the new data are derived from existing image and 3D model sources with annotations and transformations applied."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data sources and annotation methods are well documented throughout the paper and appendix."
          }
        }
      }
    ]
  },
  {
    "id": "pUcTrjRLOM-rubric-5",
    "token_usage": {
      "prompt_tokens": 35522,
      "completion_tokens": 580,
      "total_tokens": 36102
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 2.1 and 2.2",
            "reasoning": "The UltraMedical dataset includes manual instructions and data annotated and/or corrected by biomedical experts, indicating original content created by humans (manual curation and human annotation efforts as described in Sections 2.1 and 2.2). Specifically, human experts reviewed and corrected preference pairs (\u00a72.2.2), contributing to original human-generated annotations."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 2.1.1 and 2.2.1",
            "reasoning": "The dataset includes synthetic instructions generated by prompting GPT-4 and other AI models to create evolved and expanded biomedical instructions (e.g., MedQA-Evol, TextBookQA, WikiInstruct). Also, responses for completions and preference annotations were generated by GPT-4-turbo and other LLMs (\u00a72.2.1 and \u00a72.2.2). Therefore, portions of the dataset were newly created by models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human translation of data from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication of machine translation usage for data creation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2.1.1",
            "reasoning": "The UltraMedical dataset partially includes data collected or aggregated from multiple publicly available biomedical datasets such as MedQA, MedMCQA, PubMedQA, ChatDoctor, MedQuad, and others (\u00a72.1.1, Table 1). These datasets are integrated without indication of significant structural changes, which is consistent with collated data."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2.1.1 and 2.1.2",
            "reasoning": "Data such as MedQA-Evol and TextBookQA are synthetic datasets evolved or generated based on existing datasets or textbooks with modifications and complexity enhancements (e.g., self-evolution, filtering for complexity). The dataset also includes adaptation of these with prompts and filtering, indicating derived data based on existing sources with adaptations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset origin and creation process is well documented and described in the paper; hence N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "pYNl76onJL-rubric-5",
    "token_usage": {
      "prompt_tokens": 20779,
      "completion_tokens": 459,
      "total_tokens": 21238
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 Curating VidProM",
            "reasoning": "The dataset includes 1.67 million unique text-to-video prompts collected directly from real users on official Pika Discord channels between July 2023 and February 2024. These prompts are original user-generated textual inputs, created entirely from scratch by human contributors without translation or adaptation."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3 Curating VidProM",
            "reasoning": "The dataset contains 6.69 million videos generated by four state-of-the-art text-to-video diffusion models (Pika, Text2Video-Zero, VideoCraft2, and ModelScope). These videos are synthetically generated content created entirely by AI models based on the human prompts and are not derived from pre-existing video data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data being produced by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state any data being generated via machine translation systems."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3 Curating VidProM",
            "reasoning": "The text prompts and some original videos from Pika Discord channels are collected through web scraping and extraction methods without significant modification, aggregating existing user-generated inputs and their associated generated videos."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Though embeddings and NSFW scores are added, these are metadata augmentations and do not constitute substantial modifications or derivations of the original data sources."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data source and method of generation are clearly documented and described in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "qmvtDIfbmS-rubric-5",
    "token_usage": {
      "prompt_tokens": 28925,
      "completion_tokens": 555,
      "total_tokens": 29480
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 and Section 3.2",
            "reasoning": "The dataset includes 50 meticulously curated murder mystery scripts selected by seasoned murder mystery game experts (Section 3.1), as well as over 3,000 multiple-choice and open-ended questions that are newly annotated by experts based on these scripts (Section 3.2). The paper explicitly describes manual annotation, expert review, and quality control steps for question and reasoning chain creation, indicating that the data was created entirely from scratch by human contributors without being translated or adapted from pre-existing material."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although GPT-4 was used to create distractor options for multiple-choice questions, the primary data (scripts and questions) are human-created. There is no indication that the dataset itself or core content was generated entirely by AI models without human input."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that any of the data was created by human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate any use of machine translation for data creation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 and A.2.1",
            "reasoning": "The scripts were collected from existing publicly available murder mystery game scripts sourced from industry creative teams and platforms (Section 3.1, A.2.1). The authors curated and filtered these scripts based on selection criteria, indicating data was collected or aggregated from existing sources without significant modification."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 and A.2.1",
            "reasoning": "The multiple-choice and open-ended questions, including reasoning chains, were derived from the existing scripts by applying human annotation and GPT-4 assisted generation of distractors (Section 3.2, A.2.1). This demonstrates that the evaluation data is based on existing scripts with modifications and adaptations, thus derived."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides detailed documentation of the data sources and generation methods, so the origin is not unknown."
          }
        }
      }
    ]
  },
  {
    "id": "r8PnfcWQol-rubric-5",
    "token_usage": {
      "prompt_tokens": 32446,
      "completion_tokens": 475,
      "total_tokens": 32921
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 5.1, Section 4",
            "reasoning": "The paper introduces a newly constructed dataset, EFO_k-CQA, which includes 741 different abstract query graphs. The dataset is derived using a framework that systematically generates queries based on theoretical definitions and assumptions, with sampling procedures implemented by the authors. The process involves enumerating abstract query graphs, grounding them with sampled relations and constants from existing knowledge graphs, and filtering. This construction is original and created by the authors, not directly translated, adapted, or derived from pre-existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the dataset was generated automatically by models or AI systems without human involvement. The data is created via a structured enumeration and sampling framework authored by humans."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any translation of datasets from other languages or any use of human translation."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any machine translation techniques applied to produce the dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not merely collected or aggregated from existing data without modification; rather, it is generated through enumeration and grounding based on theoretical frameworks."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 4, Section 5.1",
            "reasoning": "The dataset is constructed by applying systematic enumeration and sampling methods on existing knowledge graphs (e.g., FB15k-237) to generate queries and their answers. This process involves modifications and adaptations of existing knowledge graph data to create the new dataset suited for complex query answering tasks."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is well documented and specified."
          }
        }
      }
    ]
  },
  {
    "id": "rdv2Fr6JTC-rubric-5",
    "token_usage": {
      "prompt_tokens": 22670,
      "completion_tokens": 413,
      "total_tokens": 23083
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention the creation of any new dataset generated entirely from scratch by humans."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate generating new datasets purely from AI or machine learning models without any reference to existing data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that any dataset was created by translating content from another language using human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of datasets created via machine translation is provided."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 4, Datasets and Settings",
            "reasoning": "The paper evaluates the proposed method using six publicly available benchmark graph datasets such as Cora, Citeseer, Pubmed, Amazon-Photo, Amazon-Computer, and Coauthor-Physics. These are well-known pre-existing datasets that have been collected and aggregated from prior works with no mention of significant novel modifications. Thus, they represent collated data from existing sources."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the paper performs transformations by creating computation trees and contribution trees and decomposing nodes into players for their methodology, these are conceptual and analytical derivations rather than new adapted datasets. No new dataset derived from modifying existing sources is introduced."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the datasets used, so this category does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "rovpCs3ZEO-rubric-5",
    "token_usage": {
      "prompt_tokens": 25624,
      "completion_tokens": 451,
      "total_tokens": 26075
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not report creating data entirely from scratch by human contributors; rather, it compiles existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the data introduced were generated by AI or machine learning models as original content."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data being produced via human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that machine translation was used to generate or transform the data."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 4, 'The FEDMEKI Dataset Suite'; Section E.7 Datasheet for Datasets",
            "reasoning": "The new dataset is curated by aggregating multiple publicly available existing medical datasets (RSNA, COVQU, PTB-XL, MIMIC-III, CheXpert, VQA-RAD, ECG-QA) without indication of significant modification, effectively collating them into a multi-site, multi-modal, multi-task dataset."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 4; Section E.4 Preprocessing/cleaning/labeling",
            "reasoning": "The curated dataset involves processing, preprocessing, and partitioning existing datasets to form new tasks and splits (e.g., selecting subsets, formatting for federated learning). This indicates the data are derived from existing sources with transformations applied such as selection and preprocessing."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly states the sources of data and their curation process, so the data origin is documented."
          }
        }
      }
    ]
  },
  {
    "id": "s1K5Z5QPog-rubric-5",
    "token_usage": {
      "prompt_tokens": 14228,
      "completion_tokens": 477,
      "total_tokens": 14705
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that any original content was created entirely from scratch by human contributors as data. The datasets consist of reanalysis and forecast data compiled from existing sources rather than newly generated raw data from human creation."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.2 Simulations",
            "reasoning": "The paper includes physics-based model forecasts from four national weather agencies (UKMO, NCEP, CMA, ECMWF) generated by numerical weather prediction models, which produce forecast data via simulations (models). These simulation outputs represent newly generated data by models, distinct from the collated observational reanalysis datasets."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any dataset involved human translation of content from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention machine translation of data from other languages."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Observations",
            "reasoning": "ChaosBench is comprised of multiple well-known reanalysis datasets (ERA5, ORAS5, LRA5) collected and processed from existing global observational and assimilated data products. These are aggregated from existing sources without significant modifications, representing collated data."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.1 Observations (ORAS5 monthly data replication for daily compatibility)",
            "reasoning": "Some processing steps such as replicating monthly ORAS5 sea-ice data to daily resolution for consistency are transformations applied to existing data, indicating derived data rather than purely collated."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents the sources and methods of generation for all datasets; therefore, this category does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "t9aThFL1lE-rubric-5",
    "token_usage": {
      "prompt_tokens": 31252,
      "completion_tokens": 597,
      "total_tokens": 31849
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 (Our Proposal: UNLEARNCANVAS Dataset), Appendix A (UNLEARNCANVAS Dataset Details), specifically A.3 Collection and Labeling Process",
            "reasoning": "The dataset UNLEARNCANVAS is constructed by the authors starting from seed images collected from Pexels, which are open-source photographs, and then transformed (stylized) into 60 predetermined artistic styles using the Fotor service. The process is a two-step human-curated construction: collecting specific seed images for 20 object classes, and then applying stylization systematically to generate the dataset. The labeling is explicit and manual, with each image annotated by object and style classes. The dataset is an original composition designed by the authors specifically for machine unlearning evaluation. Therefore, it qualifies as newly created data from human effort, not merely collated or transformed from existing datasets like WIKIART."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset images were generated by a stylization service (Fotor), not by an AI model trained for style transfer by the authors themselves or generated entirely by AI/ML models without existing data reference. Hence, the data is not considered newly generated from models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication in the paper that dataset content was acquired via human translation from other languages."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No indication in the paper that dataset content was machine translated from other languages."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although seed images were collected from Pexels, a public source, the final dataset consists of stylized variants of these images according to prescribed styles. This represents significant modification rather than mere collation."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3 (Our Proposal: UNLEARNCANVAS Dataset), Appendix A A.3 Collection and Labeling Process",
            "reasoning": "UNLEARNCANVAS images are derived by applying stylization transformations to seed images collected from an existing source (Pexels). These transformations apply specific artistic styles systematically, creating new images adapted from existing photos. This process constitutes data that is derived \u2014 based on existing data but transformed to a new style and structure for the dataset."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origins are clearly documented and described."
          }
        }
      }
    ]
  },
  {
    "id": "tPsw4NeLZx-rubric-5",
    "token_usage": {
      "prompt_tokens": 22629,
      "completion_tokens": 453,
      "total_tokens": 23082
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Sections 1 and 3.1",
            "reasoning": "The MM-WLAuslan dataset is explicitly recorded by the authors in a studio environment involving 73 human signers, including Auslan experts, deaf individuals, and volunteers. The data consists of videos of sign language signs performed by humans, captured using multiple RGB-D cameras. The paper states that the authors conducted over 2,500 hours of preparation and recording, capturing 282K+ sign videos covering 3,215 Auslan glosses. This indicates original content created entirely from scratch by human contributors and not translated, adapted, or derived from pre-existing material."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any portion of the dataset was generated by AI or machine learning models without reference to existing data."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not described as being produced by human translation from another language; rather, it is original sign language recordings."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of machine translation being used to generate the dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is newly recorded by the authors rather than collected or aggregated from existing sources without modification."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the glosses selected are from an existing dictionary (Auslan SignBank), the data recordings themselves are newly created rather than being derived or adapted from existing video data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is well documented and clearly specified in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "tWvVtOW0qg-rubric-5",
    "token_usage": {
      "prompt_tokens": 16917,
      "completion_tokens": 425,
      "total_tokens": 17342
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that any new dataset was created entirely from scratch by human contributors. Instead, it uses existing datasets from computer vision and medical imaging benchmarks."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No data is described as being generated entirely by AI or machine learning models without reference to existing data in the paper."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of any data produced by translating content from another language through human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any data generated by machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Appendix A 'Datasets' section",
            "reasoning": "The paper uses twenty existing computer vision and medical imaging datasets which are from established sources such as MNIST, ImageNet variants, and MedMNIST benchmark datasets. These datasets are aggregated and used for benchmarking data measurement methods without indication of new modification, indicating they are collated from existing sources."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the paper applies embedding and projection transformations to the data for measurement, these are analytic techniques applied to existing datasets and do not constitute creation of new derived datasets according to the rubric. There is no indication that a new dataset with modified or transformed samples is released as part of this work."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The origin and source of the datasets used in the paper are explicitly cited and documented."
          }
        }
      }
    ]
  },
  {
    "id": "vyraA7xt4c-rubric-5",
    "token_usage": {
      "prompt_tokens": 21734,
      "completion_tokens": 486,
      "total_tokens": 22220
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that the dataset consists of entirely original content manually authored from scratch by humans."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate that the dataset was entirely generated by AI or machine learning models as original content."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data being produced by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data being generated through machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 2, 'Mercury Datasets' and Section A.14 'Legal Compliance'",
            "reasoning": "The Mercury dataset is collected from existing public programming problems and solutions on Leetcode, a well-known online platform. The authors filtered, aggregated, and rigorously processed these existing solutions and tasks without creating them from scratch. This is an aggregation or collection of pre-existing code and tasks."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 2 'Mercury Datasets', Section 3 'Code Efficiency Metric', and Sections A.3 and A.14",
            "reasoning": "While the data is gathered from existing Leetcode tasks and solutions, the authors apply multiple modifications: filtering tasks by solution count and difficulty, removing tasks with non-unique outputs, including custom data types (TreeNode and ListNode), adding specially crafted test case generators generated and validated (sometimes revised manually) with the aid of GPT-4, and creating a runtime distribution to enable efficiency evaluation. These constitute modifications, transformations, and extensions to the original data, making the dataset derived from existing sources."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly documents the data source and generation methods, so N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "wOmtZ5FgMH-rubric-5",
    "token_usage": {
      "prompt_tokens": 34274,
      "completion_tokens": 595,
      "total_tokens": 34869
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Appendix A.3 Collection Process and Section 3.2 Data Collection and Construction",
            "reasoning": "The dataset includes 200 real-world famous people selected from 'The Most Famous All-time People Rank' and their corresponding probes, which were manually checked and verified by humans to ensure correctness and format. Human involvement is stated explicitly in data verification steps and in manual probe checking, particularly the correctness evaluation showing 99.1% accuracy in manual evaluation (Section 3.2 and Appendix A.3). Thus, the dataset contains new data created and verified by human contributors."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.2 Data Collection and Construction and Appendix A.3 Collection Process",
            "reasoning": "Many query-answer pairs (probes) and synthetic forget corpus examples are generated by GPT-4 using prompts based on Wikipedia passages about unlearning targets. This generation is autonomously produced by an AI model without direct human rewrite, constituting new data from models. The paper explicitly describes using GPT-4 API with temperature=1 to generate an excess of query-answer pairs, followed by filtering and manual checking."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data produced via human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any machine translation-based data generation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.2 Data Collection and Construction and Appendix A.3",
            "reasoning": "The unlearning targets are selected from an existing publicly available rankings list and Wikipedia pages. Neighboring knowledge probes are created based on hyperlinks and popularity measures derived from Wikipedia. This process involves aggregating existing information from public sources without significant modification, meeting the definition of collated data."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2 Data Collection and Construction and Appendix E.1 Prompt Templates",
            "reasoning": "Probes and forget corpora generated use existing Wikipedia texts as foundational data, but are transformed by GPT-4 to generate question-answer pairs and synthetic descriptions that are adapted and modified from the original source text. The dataset involves transforming and adapting existing data rather than pure direct copy."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is well specified throughout the paper with multiple generation and collection methods described."
          }
        }
      }
    ]
  },
  {
    "id": "wmO7z57wNK-rubric-5",
    "token_usage": {
      "prompt_tokens": 18332,
      "completion_tokens": 409,
      "total_tokens": 18741
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not state that new datasets are created entirely from scratch by human contributors. The datasets used (e.g., MMLU, ARC, Hellaswag, PIQA, WinoGrande, QNLI, MNLI, WikiText2) are existing public benchmarks."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention generating new datasets through AI or machine learning model generation. All evaluation datasets used are established benchmarks."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data produced via human translation of content from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not report any data created by machine translation from other languages."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper uses existing public datasets but does not indicate that the authors curated or aggregated new datasets by collecting and aggregating from multiple existing sources into a new dataset."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the paper creates new datasets derived from existing ones with modifications, transformations, or adaptations. The datasets used are standard evaluation benchmarks applied without stated alterations."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets. All evaluation datasets used are publicly available, existing datasets. Hence, no novel dataset construction or creation process is documented."
          }
        }
      }
    ]
  },
  {
    "id": "x8RgF2xQTj-rubric-5",
    "token_usage": {
      "prompt_tokens": 41943,
      "completion_tokens": 454,
      "total_tokens": 42397
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce any new datasets created from scratch by humans. Instead, it utilizes existing datasets such as ImageNet-1k, ImageNet-ReaL, CIFAR-10, and CIFAR-10H for benchmarking uncertainty estimators."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No data generated purely by models or AI is introduced in the paper. The focus is on evaluating uncertainty estimation methods using pre-existing datasets."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any data that is translated by humans from one language to another."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that any dataset used or introduced involves machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3, Section 5",
            "reasoning": "The datasets used, such as ImageNet-1k, ImageNet-ReaL, CIFAR-10, and CIFAR-10H, are standard, publicly available datasets collected by other researchers. The paper compiles and aggregates these existing datasets for its benchmarking experiments without significant modification."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the paper uses label distributions like human annotators' soft labels to represent aleatoric uncertainty, these are original to the datasets mentioned (e.g., ImageNet-ReaL, CIFAR-10H) and no new derived datasets are created or modified by the authors."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin is well-documented, involving widely known and cited datasets; thus, N/A is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "y09S5rdaWY-rubric-5",
    "token_usage": {
      "prompt_tokens": 21081,
      "completion_tokens": 400,
      "total_tokens": 21481
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset was collected using an expert model (Think2Drive) in a simulation environment rather than being directly created from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Collection Agent and Section 3.2 Expert Dataset",
            "reasoning": "The official training dataset consists of data collected from simulations driven by the expert model Think2Drive [31], which uses reinforcement learning and privileged information in the CARLA simulator to generate expert driving data across various scenarios, weathers, and towns."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the dataset involves any translation from another language by human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of machine translation usage in the dataset creation or annotation process."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not aggregated or collected from pre-existing datasets without modification; it was generated in simulation with a teacher model."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the data is collected using a trained expert model (Think2Drive), the dataset is not described as derived from existing datasets or pre-existing sources with modifications, but rather generated anew in simulation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data collection and source are clearly specified in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "y10DM6R2r3-rubric-5",
    "token_usage": {
      "prompt_tokens": 28779,
      "completion_tokens": 512,
      "total_tokens": 29291
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate that any completely new questions were created entirely from scratch by human contributors. The dataset integrates questions from existing sources such as the original MMLU, STEM Website, TheoremQA, and SciBench."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the paper uses GPT-4-Turbo for tasks like extracting short answers, generating distractors, and augmenting options from four to ten, it does not claim that the entire dataset was generated solely by AI. These model-generated aspects are modifications or enhancements to existing data rather than entirely new data from models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention translating data from other languages using human translators."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that machine translation was used to generate the dataset."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 and 3.2",
            "reasoning": "MMLU-Pro collects questions from multiple existing sources: the original MMLU dataset, STEM Website, TheoremQA, and SciBench. It aggregates these existing datasets to form a broader combined benchmark. The paper explicitly states that these sources are integrated to compose the dataset."
          },
          "Derived": {
            "is_applicable": true,
            "reference": "Section 3.2",
            "reasoning": "The dataset construction involves multiple stages where existing data are modified or transformed: trivial and erroneous questions from the original MMLU are removed; GPT-4-Turbo is used to extract short answers from solutions; GPT-4-Turbo generates additional distractors to augment options from 4 to 10; false negative options are identified and removed through expert review aided by models; thus, the original questions are adapted and derived to create the final MMLU-Pro dataset."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The data origin and generating methods are well documented in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "yS1dUkQFnu-rubric-5",
    "token_usage": {
      "prompt_tokens": 18594,
      "completion_tokens": 420,
      "total_tokens": 19014
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not indicate that any new dataset was created entirely from scratch by human contributors."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of data generated by AI or machine learning models as original datasets in the paper."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The datasets are not described as translations from other languages by humans."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that datasets were generated via machine translation."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3 (Tasks and Datasets), especially 3.1, 3.2, 3.3",
            "reasoning": "The benchmark consists of 30 diverse, challenging, and comprehensive datasets that are established public datasets in computer vision, such as ImageNet variants, Kinetics-400, MS COCO, ADE20K, and others. The paper explicitly lists these datasets and uses them as downstream tasks. There is no indication of new data collection or creation; instead, the benchmark collates existing datasets under unified evaluation for PETL."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe modifying, transforming, or adapting existing datasets to create new versions or derivatives. It relies on existing datasets as they are."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly specifies the datasets used, all of which are well-known public datasets with documented origins."
          }
        }
      }
    ]
  },
  {
    "id": "yUEBXN3cvX-rubric-5",
    "token_usage": {
      "prompt_tokens": 21655,
      "completion_tokens": 410,
      "total_tokens": 22065
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3.1 Data collection",
            "reasoning": "The ANDROIDCONTROL dataset was collected via crowdsourcing, where human annotators created new demonstrations of tasks on Android apps from scratch. Annotators generated both high-level and low-level task instructions and performed the UI actions on physical Android devices to record interaction traces. This indicates original content created entirely from human contributors, not derived or translated from pre-existing datasets."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset was collected through human demonstration and annotation. There is no indication that the data was generated entirely by AI or ML models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention that task instructions or interactions were generated by translating content from another language by humans."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of using machine translation to produce any part of the dataset."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not a collected aggregation of existing datasets or sources without modification; it was directly collected through new human demonstrations."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset does not appear to be derived from or adapted based on existing datasets or sources; the data is original human-collected demonstrations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset origin is clearly documented in the paper, so N/A is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "z64azPC6Nl-rubric-5",
    "token_usage": {
      "prompt_tokens": 19194,
      "completion_tokens": 552,
      "total_tokens": 19746
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 (Dataset Description), Appendix A.1 and A.2 (Details of Dataset and Annotation)",
            "reasoning": "The dataset GTSinger was entirely created from scratch by human contributors. The authors selected songs and professional singers across nine languages who performed and recorded singing using six specific techniques in professional studios. All recordings are new, original performances without using existing datasets or recordings. The annotation process was done manually by experts with musical and linguistic backgrounds. This is explicitly stated in Section 3 (Dataset Description) and detailed in Appendix A, where the authors describe hiring 20 professional singers who signed contracts and recorded original singing performances, with manual phoneme-level and style annotations. Hence, the dataset is original content created entirely by humans."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that any portion of the dataset was generated or synthesized entirely by AI or machine learning models. The singing data comes from human singers, and model-based tools were used only for annotations and score extractions, not data generation."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not report any data generated by translating pre-existing content from other languages using human translators. The singing data is original and multilingual, but not produced via human translation."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any use of machine translation to generate data. The multilingual data is obtained by recordings from singers proficient in each language rather than translation."
          },
          "Collated": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is not collected or aggregated from existing data sources without modification; rather, it is newly recorded and annotated data."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although some annotation tools (MFA, RMVPE, ROSVOT) and manual annotations were used to derive alignments and music scores, the core audio data is new original recordings, not a derivative of existing datasets with modifications."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset's origin and creation process is clearly documented in detail; thus, this category does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "zogaeVpbaE-rubric-5",
    "token_usage": {
      "prompt_tokens": 22479,
      "completion_tokens": 551,
      "total_tokens": 23030
    },
    "response": [
      {
        "data_novelty": {
          "New Data from Human": {
            "is_applicable": true,
            "reference": "Section 3 and Table 4",
            "reasoning": "The paper introduces DEVBENCH, a multimodal developmental benchmark comprising seven language evaluation tasks including data from children and adults. The human data are collected from original experimental studies involving human participants, including infants, children, and adults, using methods such as eye-tracking and forced-choice tasks. Although some datasets are combined from previous studies, the authors curated these datasets into a novel benchmark format and applied substitution of stimuli when necessary. The data reflect new human behavioral data in the context of this benchmark and are not translations or purely aggregated without modification."
          },
          "New Data from Model": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce any datasets generated solely by AI or machine learning models. The models are evaluated on human collected data, but no new data is generated by models."
          },
          "Human Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of data being produced by human translation from another language."
          },
          "Machine Translation": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not document use of machine translation to generate the data."
          },
          "Collated": {
            "is_applicable": true,
            "reference": "Section 3.1 Tasks and Section 3 (DEVBENCH description)",
            "reasoning": "DEVBENCH is constructed by aggregating multiple existing datasets collected in prior studies (e.g., Adams et al., Frank et al., Donnelly and Kidd for LWL; Long et al. for VV; Silverman and Yeatman for TROG; Thrush et al. for WG; others for semantics tasks). The authors collate these previously collected datasets into a unified benchmark format with some stimulus replacements due to licensing restrictions, but the core data are originally from existing human experiments."
          },
          "Derived": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the authors adapted some images due to licensing restrictions, the adaptations are limited replacements matched for semantic and visual properties rather than significant transformations or derivations resulting in a new dataset."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The origin of the data is clearly documented in the paper, referencing prior published and unpublished datasets."
          }
        }
      }
    ]
  }
]