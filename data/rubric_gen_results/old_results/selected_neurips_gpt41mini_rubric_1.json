[
  {
    "id": "01I55gys19-rubric-1",
    "token_usage": {
      "prompt_tokens": 12978,
      "completion_tokens": 249,
      "total_tokens": 13227
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation processes or instructions for labeling data. The focus is on benchmark datasets and evaluating vision models, without describing annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no description of any scoring rubrics or criteria for human annotators since no human annotation is described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of human annotation or labeling guidelines are provided in the paper."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces new datasets as subsets of existing datasets created for the benchmark, but does not describe any human annotation guidelines, instructions, rubrics, or examples for labeling. The datasets are prepared from existing labeled datasets such as ImageNet subsets, and the paper focuses on model benchmarking and dataset difficulty measurement rather than new annotation procedures."
          }
        }
      }
    ]
  },
  {
    "id": "0SMhqvgHST-rubric-1",
    "token_usage": {
      "prompt_tokens": 30317,
      "completion_tokens": 247,
      "total_tokens": 30564
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces new benchmark subsets (GATE tiers) but does not provide detailed human annotation instructions for data labeling. The benchmarks are mostly existing datasets with public annotations; the paper focuses on selecting subsets and creating an evaluation framework rather than creating or describing annotation processes."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that human annotators were involved or that rubrics for annotation were created or described. The paper discusses metrics and evaluation protocols but does not specify any annotation rubrics or scoring guidelines for humans."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples or sample annotation guidelines for human annotators are presented. The paper focuses on model evaluation methods and benchmark selection, not on data annotation or providing annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper is about selecting benchmark subsets from existing datasets and building a software framework for model evaluation. It does not introduce new human annotations or describe annotation guidelines, instructions, rubrics, or examples. Hence, no human annotation guidelines are provided for the new datasets introduced."
          }
        }
      }
    ]
  },
  {
    "id": "2HzZIDo48o-rubric-1",
    "token_usage": {
      "prompt_tokens": 20802,
      "completion_tokens": 176,
      "total_tokens": 20978
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces a new benchmark and datasets but does not mention any human annotation or manual labeling process or instructions for human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any rubrics or scoring guidelines intended for human evaluators or annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no examples of human annotation provided as the datasets are generated synthetically and evaluated automatically through agent interactions, with no indication of human labeling examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper presents a synthetic benchmark and meta-learning framework; no human annotation guidelines are provided or required, as all labeling and evaluation is performed by algorithms and automated metrics."
          }
        }
      }
    ]
  },
  {
    "id": "2myGfVgfva-rubric-1",
    "token_usage": {
      "prompt_tokens": 16670,
      "completion_tokens": 273,
      "total_tokens": 16943
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not explicitly describe providing detailed human annotation instructions for the MiraData dataset. Instead, it employs automated methods such as GPT-4V for captioning, and there is no mention of human annotators receiving guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention providing detailed rubrics or scoring systems to human annotators for labeling data. The annotations seem generated via automated systems without manual scoring schemes."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of human annotation guidelines, instructions, or examples are presented in the paper or appendix for the MiraData dataset."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "MiraData's captions and annotations are generated by state-of-the-art models like GPT-4V and Panda-70M and not by human annotators following provided annotation guidelines. The paper does not describe any human annotation processes with associated instructions, rubrics, or examples."
          }
        }
      }
    ]
  },
  {
    "id": "4S8agvKjle-rubric-1",
    "token_usage": {
      "prompt_tokens": 35070,
      "completion_tokens": 302,
      "total_tokens": 35372
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 and Appendix J",
            "reasoning": "Section 3.2 discusses manual subgoal annotation and task adaptations with thorough verification processes described in Appendix J, where multiple rounds of data verification and interactive interfaces were used to ensure quality and consistency of annotations. This demonstrates that detailed instructions were provided to annotators to carry out annotation and verification tasks carefully."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 and Appendix J",
            "reasoning": "Section 3.2 mentions that annotators labeled subgoals which were checked and adjusted through a rigorous process. Appendix J describes multiple rounds of verbal and sampled inspections with specific criteria for error checking and quality control, which indicates the presence of detailed rubrics and standards guiding the annotation process and ensuring consistency."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix J and Figures 6 and 8",
            "reasoning": "Appendix J references an interactive interface developed for annotators to check subgoals with examples shown in Figure 8 and the visualization panel in Figure 6 illustrating annotation examples and checking procedures. These concrete examples guided annotators in their labeling work, demonstrating exemplar annotation instances."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes human annotation guidelines including instruction, rubrics, examples, and multiple verification steps, so the absence of human annotation guidelines does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "5WFzk0H27p-rubric-1",
    "token_usage": {
      "prompt_tokens": 16792,
      "completion_tokens": 291,
      "total_tokens": 17083
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Raw data",
            "reasoning": "The paper describes that contributors are asked to select two videos and indicate which one should be recommended more largely using a slider from -10 to 10. It also introduces nine optional quality criteria with definitions provided on the Tournesol website, which act as instructions on how to evaluate various aspects of videos."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not specify detailed scoring rubrics or criteria guiding annotators on scoring beyond naming and brief description of quality criteria. It mentions that contributors likely use their own understanding of criteria without thorough reading of detailed instructions, suggesting lack of formal rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.1 Raw data, Figure 1",
            "reasoning": "The paper includes screenshots of the annotation interface (Figure 1) showing how comparisons and quality criteria ratings are provided, which serve as examples of annotations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are indeed provided, including instructions and example interface screenshots."
          }
        }
      }
    ]
  },
  {
    "id": "5c1hh8AeHv-rubric-1",
    "token_usage": {
      "prompt_tokens": 101848,
      "completion_tokens": 376,
      "total_tokens": 102224
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendices C to G, e.g., Section C.2.1, D.1.2, F.1.1",
            "reasoning": "The paper's appendices detail numerous tasks where data is either manually collected, adapted, or synthesized, accompanied by carefully designed prompts helping annotators or automatic evaluators understand how to label or score model responses; for instance, they describe specific prompt templates, instructions, and expected outputs for judgment tasks across truthfulness, safety, fairness, privacy, and robustness aspects."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendices C to G, e.g., Section C.2.1 (Text Misleading VQA), D.1.2 (Risk Identification), F.1.1 (Stereotypical Content Generation)",
            "reasoning": "The paper specifies evaluation metrics and clear decision criteria for annotation and scoring, such as accuracy, refuse-to-answer rates, stereotype containing rates, and specific criteria for toxicity and jailbreaking assessments; also, prompts are designed to guide annotators or automated evaluators to judge responses thoroughly and consistently."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Throughout Appendices C to G, e.g., Figures C.12, D.5, F.1, G.1",
            "reasoning": "The paper provides numerous concrete examples of image-text pairs, annotated samples, example prompts, and model responses illustrating the labeling process and clarifying how annotations and scoring should be conducted, facilitating consistent and interpretable human annotation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper, including the detailed appendices, provide human annotation guidelines with instructions, rubrics, and examples for many datasets; therefore, it is inaccurate to claim no guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "67N3FWoDtU-rubric-1",
    "token_usage": {
      "prompt_tokens": 16575,
      "completion_tokens": 371,
      "total_tokens": 16946
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Supplementary Section A.3 Annotation Guidelines",
            "reasoning": "The paper provides detailed annotation guidelines adapted from the OCR-D rules and extended with dataset-specific instructions. These include definitions for different region types, text subclasses, transcription rules, and handling of special cases such as drop capitals and archaic characters, ensuring consistency and clarity for annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Supplementary Section A.3 Annotation Guidelines and Section 3 The Chronicling Germany Dataset",
            "reasoning": "The paper defines clear labeling rubrics for region types (e.g., TextRegion, TableRegion, SeparatorRegion, GraphicRegion) and their subclasses (paragraph, heading, header, caption, inverted text), with explicit rules distinguishing these classes. The transcription follows OCR-D guidelines level 2, providing a standardized rubric for text transcription. This structured labeling schema acts as a rubric guiding annotation consistency."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures in main paper (e.g., Figure 2 and Figure 4) and Supplementary Section A.3",
            "reasoning": "The paper includes visual examples illustrating layout annotations (e.g., Figure 2 with polygon annotations and Figure 4 comparing ground truth and prediction). Supplementary Section A.3 discusses examples for transcription and describes handling of special characters and cases, supporting annotators with concrete instances."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Clear and thorough human annotation guidelines are provided, so no absence of guidelines applies."
          }
        }
      }
    ]
  },
  {
    "id": "6cCFK69vJI-rubric-1",
    "token_usage": {
      "prompt_tokens": 21331,
      "completion_tokens": 258,
      "total_tokens": 21589
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any detailed instructions provided to human annotators regarding labeling. There is no mention of human annotation procedures or guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No detailed scoring rubrics or criteria for human annotation are described in the paper. The labeling of timeseries data is derived from existing building metadata standardized by Brick schema, with no evidence of manual annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not show or discuss clear examples of human annotation guidelines, sample annotations, or examples used to train annotators."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset is collected from building management systems and annotated based on standardized Brick schemas with automated linkage of timeseries data to metadata. No human annotation guidelines, instructions, rubrics, or examples for labeling are provided or discussed in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "7TCK0aBL1C-rubric-1",
    "token_usage": {
      "prompt_tokens": 16338,
      "completion_tokens": 366,
      "total_tokens": 16704
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Infrastructure intent specifications; Section 2.2 Dataset characteristics; Section C3 of supplementary material",
            "reasoning": "The paper details that the creation of infrastructure intent specifications involves encoding user intent in the Rego language, crafted by human experts with deep domain knowledge. This process implies detailed instructions guiding how to specify valid resources, optional attributes, and required attributes, ensuring the dataset accurately reflects user intent and correct configurations. Furthermore, the data collection and annotation process mentions the team's expertise and systematic processes, indicating presence of detailed guidelines."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.3 Infrastructure intent specifications; Section 2.2 Dataset characteristics",
            "reasoning": "The intent specifications act as formalized validation criteria (rubrics) for determining correctness of generated IaC programs by specifying valid resources, optional and required attributes, and their allowed values. This structured approach serves as a rubric to consistently evaluate outputs against user intents, reflecting detailed scoring rubrics in the annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 3 (a) and (b); Section 2.3; Appendix B (prompt templates); Supplementary material Section C3",
            "reasoning": "The paper includes multiple examples illustrating both correct and incorrect Terraform configurations (Fig. 3) and validation blocks in Rego intent specifications, serving as clear examples for annotation guidelines. Additionally, the appendices include prompt exemplars and multi-turn interaction examples, enhancing clarity on annotation tasks and expected outputs."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly discusses the human annotation process, intent specification creation, and provides detailed explanations and examples, thus human annotation guidelines are clearly present."
          }
        }
      }
    ]
  },
  {
    "id": "9tVn4f8aJO-rubric-1",
    "token_usage": {
      "prompt_tokens": 41116,
      "completion_tokens": 279,
      "total_tokens": 41395
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section A.2.1 Human annotation of dimensions",
            "reasoning": "The paper describes a detailed multi-stage human annotation process where annotators from the machine learning research community answered specific questions about each dataset sample regarding knowledge, reasoning, information flow, fine-grained alignment, and interactions. This process implies that detailed instructions were provided to ensure consistent categorization."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section A.2.1 Human annotation of dimensions",
            "reasoning": "Annotators answered structured questions with categorical options (e.g., Yes/No, Less/More Reasoning), effectively implementing rubrics for scoring and categorizing datasets along multiple dimensions, which supports consistent labeling."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section A.2.1 Human annotation of dimensions and Appendix A.1 Dataset Details",
            "reasoning": "The annotation process used representative samples (five per dataset in stage 1, three per dataset in stage 2) to illustrate the dataset characteristics for annotation. Also, Appendix A.1 provides detailed dataset descriptions and prompts that serve as examples to clarify annotation and evaluation protocols."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are provided as described above, including instructions, rubrics, and examples for dataset characterization."
          }
        }
      }
    ]
  },
  {
    "id": "AdpSHMOujG-rubric-1",
    "token_usage": {
      "prompt_tokens": 16821,
      "completion_tokens": 348,
      "total_tokens": 17169
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4, Section H (Benchmark annotation strategy and points selection)",
            "reasoning": "The paper provides detailed annotation instructions for the benchmark data labeling, including sampling points that represent the distribution of each material state, grouping points by exact material state, assigning relative similarity between groups, and handling scattered, soft boundaries and gradual transitions. Section H explicitly describes the annotation strategy and guidelines."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4 (Benchmark annotation strategy), Section 4.1 (Evaluation metric)",
            "reasoning": "The annotation process includes grouping points by material state and defining partial similarity between groups, which serves as a rubric for annotators to encode relative similarity and transitions. The evaluation metric (triplet metric) is designed to utilize these annotations for consistency and scoring, implying a rubric-like structure for annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 3, Section H (Appendix H)",
            "reasoning": "The paper provides annotated images as examples showing points of different material groups and their similarity relations (e.g., Figure 3 and Figure 11 in Appendix H). These serve as clear examples to guide human annotators in the point selection and grouping process."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly describes the human annotation guidelines with instructions, rubrics, and examples for the new MatSeg benchmark dataset."
          }
        }
      }
    ]
  },
  {
    "id": "BZe6dmDk5K-rubric-1",
    "token_usage": {
      "prompt_tokens": 34081,
      "completion_tokens": 415,
      "total_tokens": 34496
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Subjective Action Quality Assessment",
            "reasoning": "The paper states that participants were instructed to have a clear and consistent understanding of all evaluated aspects before annotation, including a tutorial for each dimension where participants rated generated-real video pairs and compared answers to expert ground-truth to ensure comprehension. The rating was performed on a continuous scale with details about viewing conditions and rest intervals, demonstrating detailed instructions on how to perform the annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 Subjective Action Quality Assessment",
            "reasoning": "The annotation was based on three specific causal reasoning-based perspectives\u2014subject quality, action completeness, and action-scene interaction\u2014evaluated on a continuous [0, 100] rating scale. Multiple quality control mechanisms were applied, such as pre-labeling test with expert ground-truth ratings and inclusion of golden videos for in-process inspection. The detailed scoring scales and tri-dimensional framework act as a rubric guiding annotators on what and how to rate."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3 Subjective Action Quality Assessment and Figure 8 (Appendix)",
            "reasoning": "In the tutorial, participants were guided by rating 10 generated-real video pairs with the same caption and compared their ratings with expert scores. Additionally, the paper provides a screenshot of the rating interface in Figure 8 (Appendix) showing how annotation was presented to participants. These serve as clear examples illustrating the annotation process and evaluation scenarios."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly details the annotation procedure with instructions, controlled environment, quality control, and tutorial sessions, indicating that human annotation guidelines are indeed provided."
          }
        }
      }
    ]
  },
  {
    "id": "D1MOK2t2t2-rubric-1",
    "token_usage": {
      "prompt_tokens": 11684,
      "completion_tokens": 348,
      "total_tokens": 12032
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Evaluation; Appendix D",
            "reasoning": "The paper explicitly mentions providing methodological best practices and extensive details needed to understand and reproduce the human evaluations (Section 2.2 and Appendix D). It describes the evaluation process involving human judges who compare videos and answer multiple concrete and comparative questions, implying detailed instructions were provided to annotators to ensure consistency."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Evaluation Dataset; Appendix D",
            "reasoning": "The Evaluation Dataset includes responses to direct and comparative questions with specific answer options (e.g., Left, Right, Draw, N/A) along various factors and incorporates sentiment analysis of natural language justifications. The paper discusses multiple quantitative and qualitative evaluation criteria, suggesting the presence of detailed rubrics guiding the human annotators."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.3 Benchmarking Algorithms on BASALT; Appendix D",
            "reasoning": "The paper notes that examples of the form that human evaluators see are provided in Appendix D and describes a shared platform including example evaluation interfaces. This shows that concrete examples were provided as part of the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper describes comprehensive human evaluation procedures with instructions, rubrics, and examples, indicating that human annotation guidelines are indeed provided for the new Evaluation Dataset."
          }
        }
      }
    ]
  },
  {
    "id": "DFr5hteojx-rubric-1",
    "token_usage": {
      "prompt_tokens": 92626,
      "completion_tokens": 441,
      "total_tokens": 93067
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Conversations, Appendix V (Codebooks), and Appendix Q (Interface Screenshots)",
            "reasoning": "The paper provides detailed instructions for participants in Section 2.2, describing how they should engage in conversations of three defined types (unguided, values guided, controversy guided). The codebooks in Appendix V provide exact question texts and detailed variable descriptions, including instructions for rating model responses on multiple fine-grained attributes via visual analogue scales. Appendix Q contains screenshots of the interfaces shown to human raters, further indicating the presence of detailed instructions for human annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Conversations, Figure 2, Appendix V (Codebooks)",
            "reasoning": "Participants provide quantitative ratings on multiple fine-grained attributes (e.g., fluency, factuality, helpfulness) using Visual Analogue Scales (VAS) from labeled anchors such as 'Terrible' to 'Perfect' or 'Performed very poorly' to 'Performed very well'. These are detailed in Section 2.2 with a schematic (Figure 2) and further detailed in codebooks (Appendix V) that include exact question texts and rating scales, providing a clear rubric for annotation scoring."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix Q (Interface Screenshots), Appendices V and S (Codebooks and Case Studies)",
            "reasoning": "Appendix Q contains screenshots showing example interfaces and prompts presented to participants, illustrating how to provide feedback, rate model responses, and choose conversation types. Also, the case studies and extended methods include examples of prompt clusters, sample conversations, and feedback entries. These provide clear examples of annotation tasks and responses that participants produced, supporting the presence of example-based guidance for annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper describes extensive instructions, structured rubrics, and examples in the main text and appendices for human annotation in the PRISM dataset collection. Therefore, it is not the case that no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "DJVyRhT8nP-rubric-1",
    "token_usage": {
      "prompt_tokens": 27465,
      "completion_tokens": 280,
      "total_tokens": 27745
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix C.1 Instruction Generation, Listings 1 and 2",
            "reasoning": "The paper describes a detailed process for generating human-annotated instructions for the HA-R2R dataset using GPT-4 with a few-shot prompting method including a system prompt and specific few-shot examples. The instructions include guidelines such as delivering instructions in a single paragraph and concluding with relative position descriptions, indicating detailed instruction guidelines were provided to labelers or used to guide automatic generation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and appendices do not provide any explicit mention of scoring rubrics or detailed criteria for annotation beyond instruction generation and human survey for selecting activity descriptions. No specific rubric for annotation consistency or quality scoring is described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix C.1 Instruction Generation, Listing 2 and Figure 8",
            "reasoning": "The paper provides clear examples of human activity descriptions (145 curated human activity descriptions categorized by indoor region) and few-shot examples in the prompt for instruction generation. These serve as concrete examples guiding annotation generation and quality."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly provides human annotation guidelines for the HA-R2R dataset, so this label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "DjCSjizgsH-rubric-1",
    "token_usage": {
      "prompt_tokens": 16285,
      "completion_tokens": 256,
      "total_tokens": 16541
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Data Annotation",
            "reasoning": "The paper describes that human annotators label binary masks of wildfire regions using NV5 GEOSPATIAL software and ArcGIS. It mentions that 20 annotators were recruited, and three annotators cross-checked each binary mask to ensure quality. This implies that there were detailed instructions to manage the labeling task and quality control procedures to guide annotators in labeling fire areas accurately."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any information about detailed scoring rubrics or quantitative scoring criteria used for the annotation process. There is no mention of rubrics for label judgment or scoring metrics for annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not show any clear examples of annotation guidelines or example annotations illustrating how to label the fire regions. No instructional examples or visual guidance for annotators are provided."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes a human annotation process for the real-world wildfire scenarios, including the recruitment of annotators and the cross-checking process, indicating that annotation guidelines exist."
          }
        }
      }
    ]
  },
  {
    "id": "E18kRXTGmV-rubric-1",
    "token_usage": {
      "prompt_tokens": 20581,
      "completion_tokens": 378,
      "total_tokens": 20959
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Annotation Process and Appendix A",
            "reasoning": "The paper explicitly mentions that they developed concise annotation guidelines in English suitable for all Country-Language subsets (Section 2.2). Detailed instructions about image selection, question creation, and the annotation process are provided. Furthermore, Appendix A contains the full annotation guidelines document describing detailed instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Annotation Process and Appendix A",
            "reasoning": "The guidelines describe specific criteria that annotators and validators must follow, including instructions for ensuring cultural relevance, question difficulty, plausibility of distractors, ensuring questions are answerable only from images, and checking for typos and content relevance during validation, which constitute detailed rubrics for annotation quality control and consistency. This is discussed in Section 2.2 and in the detailed guidelines in Appendix A."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2 Annotation Process and Appendix A",
            "reasoning": "The paper states that the annotation guidelines provided multiple examples of well-formulated questions and answers to guide annotators, with examples shown in the appendix (Appendix A). These examples clarify expected specificity and cultural relevance. The extended examples in the appendix further demonstrate this."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The authors clearly provide extensive human annotation guidelines including instructions, rubrics, and examples, as described in the main text and appendices, so 'No human annotation guidelines' does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "EADRzNJFn1-rubric-1",
    "token_usage": {
      "prompt_tokens": 28605,
      "completion_tokens": 253,
      "total_tokens": 28858
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and its appendices do not describe any human annotation process or provide detailed instructions for manual data labeling. The datasets are constructed from large-scale existing knowledge graphs and interaction data sources, and the nature of the datasets is automatic extraction and processing rather than manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper of any human annotation involving scoring rubrics or guidelines for annotators. The task is formulated as a link prediction problem with automated evaluation metrics rather than subjective human ratings."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of human annotation or annotation guidelines are provided in the paper or appendices. The data is created from existing sources and automated processes, with no mention of example annotations."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces new large-scale temporal multi-relational graph datasets constructed from existing databases and interaction logs without any mention of human annotation or instructions for human annotators. The evaluation protocols are automated ranking tasks using model predictions against negative samples, not human-labeled ground truth data that require annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "FXTeJvHE0k-rubric-1",
    "token_usage": {
      "prompt_tokens": 15359,
      "completion_tokens": 312,
      "total_tokens": 15671
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper discusses the NAVSIM framework and mentions the use of an existing dataset, OpenScene, which itself is a redistribution of nuPlan. Although the paper mentions the data includes annotated HD maps, object bounding boxes, and sensor data, it does not describe any detailed human annotation guidelines, instructions, or protocols used for labeling the dataset. The focus of the paper is on simulation and benchmarking rather than data annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present or describe any scoring rubrics or detailed criteria for human annotation of data labels. Its evaluation metrics focus on automated simulation-based metrics (e.g., PDMS) and benchmarks for autonomous driving performance, not on labeling quality or annotation rubric."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no examples given in the paper related to human annotation guidelines or labeling tasks. The paper focuses on the use of existing annotated datasets and the development of simulation metrics and evaluation frameworks without describing example annotations or instructions for annotators."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The authors do not introduce any new human-annotated datasets with corresponding annotation guidelines. The dataset used (OpenScene) is pre-existing, and the paper does not provide any supplementary annotation protocols, detailed instructions, rubrics, or examples related to human labeling. Therefore, no human annotation guidelines are provided in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "GHlJM45fWY-rubric-1",
    "token_usage": {
      "prompt_tokens": 20880,
      "completion_tokens": 300,
      "total_tokens": 21180
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any detailed instructions provided to human annotators for data labeling. The dataset consists of presence-only and presence-absence records collected mainly from existing sources (citizen science platforms and expert botanist surveys), and the paper focuses on data collection, processing, and modeling, but does not mention manual annotation protocols or instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of scoring rubrics or detailed criteria for annotators to follow during the labeling process. The labels are derived from existing observational data and do not involve subjective annotation or scoring that would require rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any examples or excerpts of annotation guidelines or labeling examples. The plant species data are sourced from existing databases and surveys, and the paper does not describe or show example annotations or instructions for human labeling."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset is constructed from pre-existing biodiversity observations and survey data collected by experts and citizen scientists, not from new manual annotation efforts by the authors. Therefore, no specific human annotation guidelines, instructions, rubrics, or examples are provided or required by the authors for labeling. The paper focuses on dataset assembly, processing, and modeling, without describing human annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "HB5q6pC5eb-rubric-1",
    "token_usage": {
      "prompt_tokens": 23390,
      "completion_tokens": 380,
      "total_tokens": 23770
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix B.1 (Section B.1 Content-level Perturbation and knowledge invariance scoring details)",
            "reasoning": "The paper provides detailed instructions for human scorers who rate the knowledge invariance of perturbed questions, including standards such as Semantic Information Invariance, Reasoning Invariance, Answer Invariance, and Statement Clarity. These standards are described explicitly in Table 1 and the prompt templates in Figures 6 and 7 (Appendix B.1). Eight human volunteers are employed, grouped to independently score question pairs following these instructions, indicating detailed human annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix B.1 (Table 9: criteria of scores for knowledge invariance judgement, Figures 6 and 7: prompt templates with grading criteria)",
            "reasoning": "The annotation guidelines include explicit rubrics for scoring knowledge invariance from 1 to 5, with clear criteria defined for each score (1=fatal flaws, 5=perfect knowledge invariance). The templates provided instruct annotators to grade according to these detailed criteria, indicating the presence of detailed rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B.1 (Table 7: example of knowledge-invariant paraphrasing, Table 8: examples of format-level perturbations)",
            "reasoning": "The paper presents concrete examples of original and perturbed questions demonstrating the guidelines for annotation, showing expected paraphrasing and format changes, supporting annotators to better understand the perturbation types and scoring."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly provides human annotation guidelines with instructions, rubrics, and examples for evaluating knowledge-invariance of perturbations, thus the label N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "IZtX4RNBeH-rubric-1",
    "token_usage": {
      "prompt_tokens": 13429,
      "completion_tokens": 244,
      "total_tokens": 13673
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2, Stage 1: Data collection and Annotation",
            "reasoning": "The paper states that after collecting videos, two experienced human annotators were assigned to generate captions for each video. To ensure consistency and high quality, annotation instructions were provided to the annotators, and personalized annotation guidelines were used for each video category."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any detailed scoring rubrics or formal guidelines for annotation beyond providing instructions; the evaluation procedures use LLM-based judging rather than manual annotation scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2, and Appendix C (including Table 4)",
            "reasoning": "The paper mentions that example QA pairs are provided in Table 4 in Appendix C, demonstrating the nature and quality of annotations. This suggests that examples were included as part of the annotation guidelines or at least as references for annotation quality."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are provided in the paper; thus N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "JU0QvhhfVp-rubric-1",
    "token_usage": {
      "prompt_tokens": 13998,
      "completion_tokens": 268,
      "total_tokens": 14266
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the MOTIVE dataset as aggregated from existing databases and features extracted via bioinformatics pipelines, not involving human annotators performing data labeling tasks. No instructions for human annotation for labeling data are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of human annotators using scoring rubrics or criteria to label the data. The dataset is constructed from curated databases and precomputed features, so no annotation rubrics are described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper provides examples of dataset construction and model evaluation (tables and figures), but these are not examples illustrating human annotation guidelines or instructions. Thus, no examples for human annotation are provided."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The MOTIVE dataset is assembled from publicly available databases and computed image-based features without involving human annotation steps requiring guidelines. Hence, no human annotation guidelines are present or needed."
          }
        }
      }
    ]
  },
  {
    "id": "KZlJF8kguO-rubric-1",
    "token_usage": {
      "prompt_tokens": 23836,
      "completion_tokens": 280,
      "total_tokens": 24116
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix A.1 and A.2",
            "reasoning": "The paper details explicit instructions given to annotators for tasks such as manual correction of word onsets using spectrograms, literal transcription of speech, manual insertion of punctuation, segmentation of sentences, and speaker identification with guidelines for naming conventions and special cases. These instructions demonstrate detailed annotation guidelines to ensure consistency and accuracy in manual annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any formal scoring rubrics or detailed grading criteria being used during the annotation process. The focus is on correction and manual annotation rather than rubric-based scoring."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A.1 and A.2",
            "reasoning": "The paper includes specific examples and detailed descriptions for annotation cases, such as instructions on how to handle contractions, foreshortened words, overlapping speech, character naming conventions (e.g., superhero identities, characters pretending to be others), and marking lines requiring special attention. These reflect the presence of clear examples to guide annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are explicitly provided and described in detail in the appendices; thus, it is not the case that no guidelines were provided."
          }
        }
      }
    ]
  },
  {
    "id": "L5aY1mWvXQ-rubric-1",
    "token_usage": {
      "prompt_tokens": 14083,
      "completion_tokens": 247,
      "total_tokens": 14330
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation guidelines or instructions provided for labeling data. It focuses on temporal graph datasets and evaluation methods for temporal link prediction models without describing manual annotation procedures or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide detailed rubrics for human annotation. It does not discuss any criteria or scoring systems used by human annotators, as the datasets involved are pre-existing temporal graph datasets and the work focuses on temporal distortion and model evaluation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples related to human annotation processes or guidelines are provided in the paper. The paper discusses experiments with temporal graph datasets and distortion techniques but does not include example annotations or annotation guidelines."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets created via human annotation. Rather, it uses existing temporal graph datasets (wikipedia, reddit, uci) and applies algorithmic distortion techniques to generate new test splits. There is no indication of human labeling or annotation guidelines being involved in dataset creation."
          }
        }
      }
    ]
  },
  {
    "id": "LdRZ9SFBku-rubric-1",
    "token_usage": {
      "prompt_tokens": 25408,
      "completion_tokens": 417,
      "total_tokens": 25825
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Collection and Table 3",
            "reasoning": "The dataset includes hierarchical event annotations with two category sets of news events: Event-11 (coarse-grained) and Event-9185 (fine-grained). As described in Section 3.2 and shown in Table 3, the authors manually read each news item to collect original data and assign clean and structured labels such as title, content, time, image descriptions, event description, hierarchical event names, and event attributes (location, date, etc.). This manual process indicates detailed annotation instructions for human annotators to accurately label and categorize events."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not explicitly provide or describe any scoring rubrics or detailed evaluation criteria for annotation quality or label assignment for the hierarchical event categories. There is no mention of formal rubrics used by annotators or procedures to ensure consistent scoring."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Collection (Figure 4) and Table 3",
            "reasoning": "The paper provides clear examples of event annotations, including examples like 'Sports' for coarse-grained Event-11 and '2019 Daytona 500' for fine-grained Event-9185 categories. Figure 4 illustrates sample news events along with associated images, titles, and descriptions. Table 3 lists multiple example categories and counts of labeled nodes, demonstrating clear annotation examples."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are provided as evidenced by detailed description of manual data collection and annotation in Section 3.2, examples in Figure 4, and event category descriptions in Table 3."
          }
        }
      }
    ]
  },
  {
    "id": "LdxNWDNvC3-rubric-1",
    "token_usage": {
      "prompt_tokens": 20487,
      "completion_tokens": 282,
      "total_tokens": 20769
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the large-scale airfoil dataset (AF-200K) and the automatic data engine used to generate and annotate the data, including synthetic generation and CFD simulation for aerodynamic labels, but does not mention any human annotators or detailed instructions for human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No evidence of detailed rubrics or scoring criteria for human annotators is provided in the paper. All annotations are generated either automatically via physical simulation or computed from parametric models."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not contain examples or templates for human annotation, as the annotation process is automated and computational without mention of human labeling examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The annotations of geometric and aerodynamic parameters are generated via computational methods such as CFD simulation (Xfoil) and parametric calculations, not via human annotation. The paper does not provide or describe any human annotation guidelines, instructions, rubrics, or examples."
          }
        }
      }
    ]
  },
  {
    "id": "M32Ldpp4Oy-rubric-1",
    "token_usage": {
      "prompt_tokens": 32465,
      "completion_tokens": 378,
      "total_tokens": 32843
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 5.2.3, Section 5, Appendix B",
            "reasoning": "The paper explicitly describes instructions given to human participants involved in the Visual Action Prediction (VAP) task in Section 5.2.3, noting that human participants received training documents and instructions to learn LogiCity's formal verification rules. The full text of instructions given to participants and screenshots are reported as per the checklist, and appendix B contains detailed examples. This indicates detailed instruction guidelines were provided to human annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5.2.3, Section 6, Checklist part 5(a)",
            "reasoning": "The paper mentions scoring in tasks (e.g., recall rates, accuracy metrics) with human evaluations on rule correctness and decisions, suggesting the presence of rubrics or evaluation criteria. They note human participants' performance on logical rule learning was evaluated with accuracy scores in Table 4. The Checklist explicitly confirms inclusion of instructions and participant evaluation; thus detailed rubrics or scoring guidelines were provided to ensure consistency and measurement of human annotation quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B, Figures B, C, D",
            "reasoning": "The paper provides clear examples of annotation and reasoning, including natural language descriptions of scenes, predicate truth values, and action decisions. Appendix B and visualizations in Figures B, C, and D show concrete examples of the data and annotations used for human and model evaluation. These examples demonstrate that the annotation guidelines include clear examples to train and guide annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper describes human annotation involving expert human participants with instructions, evaluation, and examples provided, therefore 'N/A' does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "MU2s9wwWLo-rubric-1",
    "token_usage": {
      "prompt_tokens": 26636,
      "completion_tokens": 372,
      "total_tokens": 27008
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix A.1 Human Evaluation Instructions",
            "reasoning": "The paper provides detailed human evaluation instructions in Appendix A.1, specifying a two-step evaluation: first on image-prompt alignment and then on individual yes/no questions for each visual concept. It also includes general guidelines for interpreting subjective concepts such as size and style. This constitutes detailed instructions for human annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix A.1 Human Evaluation Instructions and Appendix A.2 Human Agreement Analysis",
            "reasoning": "The instructions include a clear rubric involving binary scoring (1 for yes, 0 for no) for both overall alignment and individual concept presence, which serves as a scoring rubric. Additionally, the paper discusses consistency and variability of human annotations, indicating structured scoring methods based on detailed rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A.1 Human Evaluation Instructions and Appendix A.5 Qualitative Analysis",
            "reasoning": "The paper includes concrete example prompts and example evaluations to illustrate how human annotators should perform their assessments. Appendix A.1 gives a specific example prompt used in instructions, and Appendix A.5 shows detailed examples of prompts, questions, and human vs GPT-4o grading results, demonstrating example cases of annotation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly provides detailed human annotation guidelines with instructions, scoring rubric, and examples, so it is not applicable to say that no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "Mbd3QxXjq5-rubric-1",
    "token_usage": {
      "prompt_tokens": 27206,
      "completion_tokens": 249,
      "total_tokens": 27455
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention or describe any human annotation guidelines or detailed instructions for labeling data. The dataset OpenMathInstruct-1 is synthetically generated by prompting LLMs and does not involve human annotation, hence no instructions are provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication of any human scoring rubrics or evaluation criteria provided for manual annotation or labeling. The dataset is generated synthetically from models, not via human annotators following rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples are provided as part of human annotation guidelines since no human annotation is involved for labeling the dataset. Examples given in the paper are illustrative solutions generated by models, not annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset OpenMathInstruct-1 is created fully by synthetic generation via open-source LLMs prompted with few-shot examples and masked text solutions; no manual annotation or human labeling is involved. Therefore, there are no human annotation guidelines, instructions, rubrics, or examples provided or required."
          }
        }
      }
    ]
  },
  {
    "id": "OL2JQoO0kq-rubric-1",
    "token_usage": {
      "prompt_tokens": 22273,
      "completion_tokens": 211,
      "total_tokens": 22484
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes automatic data curation using models, algorithms, and LLMs with no indication of human annotators or explicit instructions provided for manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of any scoring rubrics or criteria applied by human annotators for labeling or quality control; annotations are generated automatically without manual scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the paper provides examples of image-text pairs and illustrates use of LLM prompts, these are examples of data and prompts rather than human annotation guidelines or instructions; no human annotation examples are described."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset is curated automatically from YouTube videos and other online sources using a pipeline involving automatic speech recognition, LLMs, and classifiers without human annotation; therefore, no human annotation guidelines exist for the new datasets introduced."
          }
        }
      }
    ]
  },
  {
    "id": "OTjTKFk7gb-rubric-1",
    "token_usage": {
      "prompt_tokens": 21821,
      "completion_tokens": 214,
      "total_tokens": 22035
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation process or detailed instructions for annotators related to data labeling for the new dataset. The dataset is generated from simulation and baseline agent interactions rather than human labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no information or description in the paper about scoring rubrics or evaluation criteria provided to human annotators, as the data is generated programmatically and not annotated by humans."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of human annotation guidelines or annotated data from human annotators are provided or described in the paper."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The AuctionNet dataset is generated from simulated auto-bidding agents within an ad auction environment, with no indication of human annotation involved. The paper focuses entirely on simulation and algorithmic generation of data rather than manual labeling, and thus no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "PcbSZwVVc5-rubric-1",
    "token_usage": {
      "prompt_tokens": 20765,
      "completion_tokens": 346,
      "total_tokens": 21111
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Statistics; Appendix A.4 Manual Annotation",
            "reasoning": "The paper describes a hierarchical inspection process for annotation, where annotators reviewed audio and motion data to identify and label events. Annotators were instructed to inspect binaural audio and IMU signals using Audacity, select event intervals, and assign categories. In Appendix A.4, detailed annotation procedures and software interface are described indicating that the annotators had explicit instructions for the annotation task."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Statistics",
            "reasoning": "The paper states that eight distinct sleep events were annotated with fine-grained labels as shown in Table 2 with clear definitions of each event category. The annotation process included consensus from at least three annotators and voting when needed, indicating a rubric or scoring standard to ensure consistency and handling of disagreements."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Statistics; Figure 2",
            "reasoning": "The paper provides examples of each event in Figure 2 to assist annotators, demonstrating that clear examples were included in the annotation guidelines to support consistent labeling."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper contains detailed information about annotation instructions, labeling rubrics, and examples, therefore it does not lack human annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "QIJQ1qCGqV-rubric-1",
    "token_usage": {
      "prompt_tokens": 14065,
      "completion_tokens": 324,
      "total_tokens": 14389
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Data Annotation Pipeline",
            "reasoning": "The paper states that annotation was performed by 15 human annotators, including motion experts and novices, who were provided with detailed instructions. The instructions involved guidance on how to create high-level and low-level descriptions, such as focusing on actions, intent, use of mobility aids, step counts, and including subjective adjectives to capture behaviors expressively."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention the presence of detailed scoring rubrics or standard scales used for annotation. While feedback was provided to novice annotators after their first set of annotations to ensure quality, no formal rubric or scoring system is described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3 Data Annotation Pipeline and Figure 2",
            "reasoning": "The paper provides multiple clear examples of both high-level and low-level textual annotations describing blind motion and use of mobility aids. It explicitly states that novice annotators were provided demos of expert annotation samples as references, and Figure 2 presents qualitative examples illustrating annotated descriptions."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are clearly provided and described in the paper, thus this label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "Qf8uzIT1OK-rubric-1",
    "token_usage": {
      "prompt_tokens": 37390,
      "completion_tokens": 237,
      "total_tokens": 37627
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce a new dataset nor provide detailed human annotation guidelines or instructions for data labeling. It primarily discusses ethical considerations, recommendations, and a checklist for responsible data curation in human-centric computer vision. There is no specific mention of instructions for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no content in the paper describing detailed rubrics or scoring criteria for human annotations. The discussion is about ethical considerations and recommendations, not about annotation guidelines with rubrics for a dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of annotation guidelines or annotation examples. It mainly focuses on ethical considerations and recommendations rather than dataset creation or annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets or present specific human annotation guidelines for data labeling. It provides ethical guidelines, considerations, and a checklist for data curation but does not include annotation instructions, rubrics, or examples for a newly created dataset."
          }
        }
      }
    ]
  },
  {
    "id": "QpF3DFP3Td-rubric-1",
    "token_usage": {
      "prompt_tokens": 19349,
      "completion_tokens": 377,
      "total_tokens": 19726
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 (Annotation); Section 3.2 (Annotation)",
            "reasoning": "The paper describes a meticulous annotation process carried out by expert archaeologists involving individual tracing and field verification of archaeological features. They used Geographic Information System software (QGIS) for manual annotation, which implies the presence of detailed annotation instructions for identifying and delineating archaeological features across 12 feature types before merging into 5 classes. The long annotation process (from 1993 to 2024) involving both local and foreign archaeologists suggests detailed guidance was provided to ensure annotation consistency."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any formal scoring rubrics, rating scales, or detailed annotation criteria for scoring or quantifying the quality or certainty of labels. The annotation is described as manual polygon delineation and expert field verification without mention of specific rubric-based guidelines."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 3 (Section 3.1); Figure 1 (Abstract and Section 3.1)",
            "reasoning": "The paper provides clear annotated examples of the archaeological classes in figure form, including polygon annotations illustrated alongside in-situ images and hillshaded elevation maps (Figure 3). This serves as guidance and visual examples for annotation, helping annotators understand the features to be labeled."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The presence of explicit annotation description and visual examples provided in the paper indicates that human annotation guidelines were indeed provided."
          }
        }
      }
    ]
  },
  {
    "id": "R4rNYJ2slJ-rubric-1",
    "token_usage": {
      "prompt_tokens": 16830,
      "completion_tokens": 286,
      "total_tokens": 17116
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation; Appendix A.2 Dataset Annotation Documentation",
            "reasoning": "The paper and supplementary material detail comprehensive annotation procedures for the dataset collected by experienced annotators, including categories of lines, attributes, rules for instance definition, occlusion handling, overpass scenarios, and image-level tags, indicating detailed and clear instructions for human annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix A.2 Dataset Annotation Documentation",
            "reasoning": "The annotation guidelines specify attributes with clearly defined discrete classes and criteria, such as line color categories, line types, number of lines, function tags, occlusion levels based on percentage occluded, and clearness categories, acting as rubrics for consistent labeling."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures 3, 4 in main text and Figures 10-13 in Appendix A.2",
            "reasoning": "The paper and appendix provide multiple illustrative figures showing examples of categories, instance definitions, attribute variations, occlusion cases, overpass structures, and image-level tags, serving as visual examples in the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides extensive annotation guidelines including instructions, rubrics, and examples, hence it is not the case that no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "ScPgzCZ6Lo-rubric-1",
    "token_usage": {
      "prompt_tokens": 35200,
      "completion_tokens": 315,
      "total_tokens": 35515
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper primarily focuses on benchmarking graph condensation methods and datasets. It does not involve human-labeled data requiring human annotation instructions. All datasets used are standard graph datasets with predefined labels and splits. The condensation methods and evaluations are algorithmically based without a description of manual annotation processes requiring instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of detailed scoring rubrics or criteria for human annotators in the paper. The annotation or labeling for tasks such as node classification or graph classification relies on existing dataset labels rather than new human annotations that require rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples related to human annotation guidelines. The examples shown are experimental results and visualizations of condensed graphs, not instructions or examples for human annotators."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces a benchmark dataset for graph condensation by integrating existing publicly available datasets with their predefined annotations. No new human annotation guidelines are introduced, since the data labeling is already established in these datasets and the paper focuses on evaluating graph condensation methods rather than collecting or annotating new data via human annotators."
          }
        }
      }
    ]
  },
  {
    "id": "USUkwg5pW6-rubric-1",
    "token_usage": {
      "prompt_tokens": 23650,
      "completion_tokens": 315,
      "total_tokens": 23965
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper explicitly states that the scribbles for all datasets except ScribbleSup are generated automatically using an algorithm based on existing dense annotations. There is no description or provision of human annotation instructions for the generated scribbles. Only the ScribbleSup dataset contains hand-drawn scribbles, but that is pre-existing and not introduced by the authors."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or description of scoring rubrics or detailed criteria for the human annotators in the generation of any of the scribble datasets introduced by the authors. The scribble datasets introduced are all automatically generated, so rubrics for human annotation do not apply."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any annotation guideline examples for human annotators since the scribble labels for the new datasets are synthetically generated from the ground truth of existing dense datasets. There are example images showing the scribbles, but these represent data samples rather than examples of annotation instructions."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The newly introduced datasets (s4Pascal, s4Cityscapes, s4KITTI360, s4ADE20K) consist of automatically generated scribble labels from existing dense annotations, not hand-labeled by humans. Therefore, no human annotation guidelines, instructions, rubrics, or examples are provided for these new datasets in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "UYgE9IfQIV-rubric-1",
    "token_usage": {
      "prompt_tokens": 29695,
      "completion_tokens": 251,
      "total_tokens": 29946
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces SustainDC, a simulation environment for sustainable data center control, which is a benchmarking environment for reinforcement learning algorithms. It is a synthetic environment and simulation framework, not a dataset with human-annotated labels, and the paper does not mention any human annotation guidelines or instructions for labeling any data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no discussion or mention of human annotation tasks, labeling rubrics, or grading criteria within the newly introduced SustainDC environments or any related datasets. The environment focuses on simulations and reinforcement learning benchmarks, not on human-labeled data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present examples of human annotation guidelines or labeling tasks. All examples given pertain to simulation configurations and algorithm benchmarks, with no human annotation involved."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces a new simulated environment (SustainDC) for benchmarking multi-agent reinforcement learning in data center control. It does not introduce any dataset requiring human annotation. Hence, no human annotation guidelines are provided or relevant."
          }
        }
      }
    ]
  },
  {
    "id": "VH1vxapUTs-rubric-1",
    "token_usage": {
      "prompt_tokens": 16845,
      "completion_tokens": 280,
      "total_tokens": 17125
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation guidelines or instructions related to data labeling. The dataset is constructed from harmonized satellite and other ancillary data sources processed algorithmically, with no reported human labeling process requiring instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that human annotators were involved or that any rubrics or scoring guidelines were created to guide human labeling. The dataset creation is based on existing remote sensing datasets and automatic methods without manual annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of human annotation or labeling are provided since the dataset is formed from automated data integration and processing rather than human-annotated data."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset introduced (Mesogeos datacube and derived ML datasets) is compiled from satellite, meteorological, and other environmental data, with no human labeling or annotation procedures described. Therefore, no human annotation guidelines are provided or applicable."
          }
        }
      }
    ]
  },
  {
    "id": "VSJotgbPHF-rubric-1",
    "token_usage": {
      "prompt_tokens": 12450,
      "completion_tokens": 364,
      "total_tokens": 12814
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Contributor Guidelines and Section 3.1 Single-Step Collection",
            "reasoning": "The paper explicitly states in Section 3.3 that clear and detailed human annotation guidelines were issued to contributors to ensure quality and consistency, specifying goals such as clarifying meanings, scales, and criteria for labeling and ranking tasks. Section 3.1 further elaborates on labeling tasks where users categorize messages across multiple dimensions using defined criteria."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 Single-Step Collection and Section 3.3 Contributor Guidelines",
            "reasoning": "In Section 3.1, labels on messages include categorization tasks such as spam detection, guideline adherence, and quality rated on a five-point Likert scale across multiple dimensions. These constitute rubrics to guide annotation consistency. Section 3.3 confirms that the guidelines clarify these scales and criteria for labeling and ranking tasks, implying structured scoring rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A (mentioned in Section 3.3 Contributor Guidelines) and Appendix C (user interface examples)",
            "reasoning": "Section 3.3 refers to a full copy of detailed contributor guidelines in Appendix A, which presumably contains examples to clarify annotation tasks. Section 3.1 and 3.4 mention UI displays (Appendix C) that provide example interfaces for tasks such as replying and ranking, indicating that the guidelines include clear examples to guide contributors."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly describes the presence of comprehensive human annotation guidelines including instructions, rubrics, and examples for labeling and data collection tasks, making 'N/A' not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "VXohja0vrQ-rubric-1",
    "token_usage": {
      "prompt_tokens": 18479,
      "completion_tokens": 375,
      "total_tokens": 18854
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Dataset Instance Collection and Section 4.3 Limitations and Future Work",
            "reasoning": "The paper describes a clear three-step pipeline for dataset instance collection including note collection and attribute extraction, data verification and enrichment, and answer and explanation generation (Section 2.2). It specifies that GPT-4 extracted attributes were verified and corrected by individuals with medical background, indicating there were instructions guiding these processes. Additionally, Section 4.3 mentions that only a limited number of annotators verified the extraction, which implies some instruction or guideline was used to maintain consistency."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide or describe any detailed scoring rubrics or formalized criteria for human annotation beyond the verification and correction step. There is no mention of rubric sheets or scoring guidelines for annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.1 Calculation Task Curation and Figure 1",
            "reasoning": "The paper states that template-based explanation generators were implemented for each of the 55 calculators and that these templates produce step-by-step explanations (Section 2.1). Figure 1 shows example instances from the dataset with patient notes, questions, explanations, and final answers, revealing clear examples of how annotations (explanations and answers) are structured."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset clearly involves human verification and generation steps with instructions and templates, so human annotation guidelines are present."
          }
        }
      }
    ]
  },
  {
    "id": "WUWHVN4gxk-rubric-1",
    "token_usage": {
      "prompt_tokens": 18991,
      "completion_tokens": 268,
      "total_tokens": 19259
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any human annotation process or detailed instructions provided to human annotators for labeling data. The dataset consists of logs of multi-turn conversations from automated interactions in a capture-the-flag competition, not human-labeled annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that human annotators were provided with scoring rubrics or detailed rubrics for labeling. The labeling of success in chats is based on automated criteria (whether the secret was correctly extracted), not human judgment with rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper provides example data points in the appendix, but these examples illustrate dataset entries rather than human annotation guidelines or examples for annotators. The examples are of system-generated chat logs."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The released dataset comprises automatically recorded chat logs and defense details from an LLM security competition, with labels such as 'was_successful_secret_extraction' derived from system checks. There is no mention of any human annotation procedure, guidelines, instructions, rubrics, or example annotations. Therefore, no human annotation guidelines are provided for this dataset."
          }
        }
      }
    ]
  },
  {
    "id": "WVQ4Clw1VD-rubric-1",
    "token_usage": {
      "prompt_tokens": 11682,
      "completion_tokens": 313,
      "total_tokens": 11995
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any detailed human annotation instructions provided for the data labeling. The dataset was generated through an automated pipeline leveraging expert models and multimodal large language models (MLLMs), rather than manual human annotations guided by human instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no description or mention of detailed scoring rubrics or criteria for human annotators in the paper. Since the annotations are generated by automated methods without human annotators, rubrics are not provided."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1 and Figure 6",
            "reasoning": "The paper provides qualitative examples of the dataset's annotations, such as detailed textual descriptions and region of interest annotations (bounding boxes or segmentation masks). For instance, Figure 6 shows example ROIs and their corresponding textual descriptions, illustrating the annotation format and content."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Although there are no explicit human annotation guidelines like instructions or rubrics, the paper does provide example annotations and detailed descriptions of their generation process using automated methods. Therefore, it is not the case that no annotation guidelines or examples are provided."
          }
        }
      }
    ]
  },
  {
    "id": "XBcStBjBIE-rubric-1",
    "token_usage": {
      "prompt_tokens": 30028,
      "completion_tokens": 429,
      "total_tokens": 30457
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section D.6 Additional Details of Human Evaluation, and Appendix B.1",
            "reasoning": "Section D.6 describes the human evaluation process where human annotators were given a five-point rating scale for four criteria: Visual Quality, Text Relevance, Metamorphic Amplitude, and Coherence. It explicitly states that scoring guidelines were provided to ensure consistent user selections and minimize bias, indicating detailed instructions for annotation. Additionally, Appendix B.1 provides detailed scoring criteria for the GPT4o-MTScore, a related evaluation involving human-like scoring, further supporting the presence of detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Table 6 in Appendix B.1 and Section D.6 Additional Details of Human Evaluation",
            "reasoning": "Table 6 presents a detailed 5-point scoring rubric for the GPT4o-MTScore metric, describing what each score from 1 to 5 represents in terms of metamorphic amplitude, which illustrates the provision of detailed rubrics for evaluation. Furthermore, Section D.6 mentions using a five-point rating scale with scoring guidelines, which implies the presence of defined rubrics for human evaluators to follow in assessing outputs across multiple dimensions."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B.1 Table 6 and Section D.6 Additional Details, Figure 17 (implied)",
            "reasoning": "The paper provides explicit examples of rating criteria in Table 6 for GPT4o-MTScore, giving illustrative reasoning statements for each score level, which serves as clear annotation examples. Section D.6 also references user guidelines and demonstrates multiple criteria with five-point scales, which typically include example descriptions to help annotators understand the scoring procedure, indicating the presence of examples within the guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes detailed instructions, rubrics, and examples provided to human annotators during their evaluation of the datasets and models; thus, it is incorrect to say no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "ZDvXY56DeP-rubric-1",
    "token_usage": {
      "prompt_tokens": 19071,
      "completion_tokens": 205,
      "total_tokens": 19276
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes a benchmark dataset consisting of tracked reinforcement learning experiments with extensive metrics but does not involve human annotation tasks or guidelines provided for labeling data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of scoring rubrics or any form of human annotation scoring system as the dataset consists of logged experimental data from RL runs, not human-labeled subjective annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset and benchmark focus on collecting and sharing RL experimental runs and metrics; there are no examples of human-labeled annotations or instructions for annotators."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "ORLB is a collection of recorded raw experimental data from reinforcement learning runs with no indication of any human annotation or labeling process requiring guidelines, instructions, rubrics, or examples. Thus, human annotation guidelines are not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "aTXhTD44nF-rubric-1",
    "token_usage": {
      "prompt_tokens": 23691,
      "completion_tokens": 342,
      "total_tokens": 24033
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Obtaining LLM Annotations; Appendix B",
            "reasoning": "The paper provides detailed annotation guidelines as part of the system prompt given to LLMs, describing the definitions of stance and dogmatism and the tasks expected. There are explicit instructions for labeling stance and dogmatism at the user and post level, including how to handle multiple posts."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Obtaining LLM Annotations; Appendix B",
            "reasoning": "The guidelines specify distinct categories for stance (five-point scale) and dogmatism (four-point scale) with clear definitions of each class, effectively serving as rubrics for consistent annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 Obtaining LLM Annotations; Appendices E.1 to E.6; Appendix B",
            "reasoning": "The paper provides multiple examples of JSON-formatted annotated outputs from LLMs in different settings (zero-shot, one-shot, few-shot) demonstrating the use of the guidelines in practice. Appendix B also contains detailed instructions that include example formats."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly provides detailed human annotation guidelines in the system prompt and appendix; hence, it is not the case that no guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "aXeiCbMFFJ-rubric-1",
    "token_usage": {
      "prompt_tokens": 22292,
      "completion_tokens": 406,
      "total_tokens": 22698
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Generation, Appendix E.1, E.2, E.3",
            "reasoning": "The paper describes a detailed annotation process involving both linguistic and visual annotators, specifying tasks such as creating question-answer pairs, providing intermediate bounding boxes, and generating detailed reasoning steps. For example, prompts to GPT-4 are designed to generate reasoning steps (Appendix E.3), and linguistic annotators create questions and answers with guidelines to ensure diversity and precision (Appendix E.2). Filtering pipelines and OCR tools are also applied to improve annotation quality (Section 3.1). These details indicate that clear and detailed instructions were provided to annotators and the language models generating data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any explicit scoring rubrics or guidelines that define quality metrics or scoring criteria for human annotations. There is no description of annotation quality rating scales or rubric-based evaluation in the dataset construction or evaluation sections."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 1, Figure 1, Appendix E.3",
            "reasoning": "The paper provides concrete examples of annotated data with detailed reasoning steps (Table 1), including formatted step-by-step chain-of-thought explanations and bounding box coordinates. Figure 1 shows example images with corresponding questions, answers, and visual CoT bounding boxes. Appendix E contains prompt examples illustrating how annotations were generated. These constitute clear examples of annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Since the paper clearly describes annotation instructions and provides examples, and does not omit guidelines entirely, this label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "abXaOcvujs-rubric-1",
    "token_usage": {
      "prompt_tokens": 15101,
      "completion_tokens": 325,
      "total_tokens": 15426
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Improving Table and Column Names, Figures 4 and 8",
            "reasoning": "The paper details explicit instructions used to guide paraphrasing of table and column names using prompts provided to a Large Language Model (GPT-4o). The prompts include step-by-step tasks for renaming databases, tables, and columns to provide more realistic and diverse names. These constitute detailed human annotation instructions embedded in the prompts that direct annotation style and objectives."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any explicit rubrics or scoring guidelines for annotation or labeling tasks related to dataset creation or naming paraphrasing."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures 4 and 8",
            "reasoning": "The paper presents the exact prompt templates used which include examples of tables with sample rows and expected output formats (JSON objects) for renaming tables and columns. These prompts function as clear examples for the annotation/paraphrasing process."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "There are clear human annotation guidelines in the form of prompts for paraphrasing table and column names described in Section 3.3, so it is not correct to say no guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "aiGN4UnNM7-rubric-1",
    "token_usage": {
      "prompt_tokens": 16355,
      "completion_tokens": 345,
      "total_tokens": 16700
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.2 Annotation",
            "reasoning": "The paper describes in Section 4.2 in detail the procedure used for annotating the TTC ground truth. It explains how 2D and 3D detections are combined, how depth is computed from the closest 3D bounding box corner, how velocity is estimated with RANSAC over past frames with different frame window sizes, and how the TTC ground truth is computed and validated. This constitutes detailed instructions for annotators to generate the annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any scoring rubrics or grading criteria for human annotation quality or consistency checks. It only states that the annotations are manually checked by an annotation team to ensure quality but does not provide detailed rubrics or guidelines on how to score or evaluate annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No explicit examples illustrating annotation guidelines or step-by-step examples for the annotators are provided in the paper or appendix. Although Figure 2 illustrates the annotation pipeline, it does not serve as a clear example for annotators on how to label or annotate the data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are indeed provided as detailed annotation instructions are described in Section 4.2 for generating TTC ground-truth."
          }
        }
      }
    ]
  },
  {
    "id": "b6IBmU1uzw-rubric-1",
    "token_usage": {
      "prompt_tokens": 30952,
      "completion_tokens": 378,
      "total_tokens": 31330
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2 and Appendix C",
            "reasoning": "The paper specifies detailed processes for constructing QA pairs, including converting classification datasets into question-answer formats, designing multiple question templates (10-30 per question type), and using GPT-4 to generate open-ended questions and answers based on medical reports. The construction methods are described with examples of question templates (see Tables 9, 10, 11 in Appendix C) and explicit instructions to GPT-4 (Table 12). This amounts to detailed instructions for human or model-based annotation to ensure consistent data generation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2 'Types of Questions and Metrics' and Appendix D.1",
            "reasoning": "The paper describes specific metrics for evaluating answers, including accuracy for closed-ended QA and normalization of GPT-4-based scoring (ranging from 1 to 10) of open-ended answers based on helpfulness, relevance, accuracy, and detail. These evaluation criteria effectively serve as rubrics guiding annotation and scoring."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix C 'Construction Process of QA Pairs' including Tables 9, 10, 11, and 12",
            "reasoning": "The paper provides clear examples of the question templates used for annotating various datasets (e.g., HAM10000, OL3I), example instructions given to GPT-4 for generating QA pairs, and examples of how to convert medical image labels into QA format. This indicates the presence of clear examples in the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly details the data construction and annotation procedures, with examples, instructions, and scoring rubrics, indicating that human annotation guidelines are indeed provided and are not absent."
          }
        }
      }
    ]
  },
  {
    "id": "bAaM8cKoMl-rubric-1",
    "token_usage": {
      "prompt_tokens": 28928,
      "completion_tokens": 324,
      "total_tokens": 29252
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the creation and use of multiple image datasets designed for testing deep neural networks against psychological findings. However, it does not provide any human annotation guidelines, instructions, or protocols for how human annotators should label or score the data. The focus is on the generation of image datasets and automated testing methods for DNNs rather than human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present any detailed scoring rubrics for human annotators. The datasets are designed for automated testing of neural networks using computational methods such as similarity judgments, decoder training, and out-of-distribution classification. No rubrics or scoring criteria for human annotators are described for labeling the data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the paper provides many examples of images from the datasets and demonstrations of how to use the datasets for testing DNNs, these examples are not human annotation examples but rather sample data and use-case demonstrations. There are no examples relating to human annotation guidelines or how humans should label the data."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The MindSet: Vision toolbox datasets are programmatically generated and intended for evaluating deep neural network behavior. The paper and appendix include detailed descriptions of datasets, parameters, and testing scripts for automated evaluation, but do not specify or provide human annotation guidelines, instructions, rubrics, or examples. Hence, no human annotation guidelines are provided for these datasets."
          }
        }
      }
    ]
  },
  {
    "id": "cLga8GStdk-rubric-1",
    "token_usage": {
      "prompt_tokens": 15214,
      "completion_tokens": 316,
      "total_tokens": 15530
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.1 Evaluation - Metrics",
            "reasoning": "The paper describes the evaluation protocol where puzzles are designed to be solvable based on the context alone, outlining the strict exact match scoring criteria and the use of a no-context baseline to account for memorisation, indicating detailed instructions for human annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1 Evaluation - Metrics",
            "reasoning": "The paper notes that UKLO puzzles are assessed manually by expert linguists who can award partial credit if some phrase parts or rules are correct, demonstrating that human annotation uses detailed rubrics for partial credit scoring, though the benchmark automates with exact match metrics for reproducibility."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.4 Example Puzzle and Figure 2",
            "reasoning": "The paper provides an explicit example puzzle (Figure 2) with colored sections showing preamble, context, questions, and the correct answer with explanation, serving as a clear example for human annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents human annotation processes including instructions, scoring rubrics, and examples, so no lack of human annotation guidelines is present."
          }
        }
      }
    ]
  },
  {
    "id": "cR3T1ZYN8I-rubric-1",
    "token_usage": {
      "prompt_tokens": 35594,
      "completion_tokens": 276,
      "total_tokens": 35870
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix E, Figure 6(a)",
            "reasoning": "The paper provides a detailed annotator briefing in Appendix E, including clear instructions for human evaluators on how to score model predictions based on how closely their responses capture ground truth information. This indicates that detailed instruction guidelines were provided for human annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix E, Figure 6(c) and associated text",
            "reasoning": "The human evaluation includes annotation forms with a 1 to 5 rating scale from 'completely wrong' to 'completely right'. This structured rating scale constitutes a rubric, used to assess the quality of generated responses with respect to manipulated image areas."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix E, Figure 7 and associated examples",
            "reasoning": "The appendix includes clear examples of images, ground truth manipulations, generated responses, and the annotation form. These examples demonstrate to annotators the expected scoring approach and how to interpret the responses, indicating the presence of clear examples in the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents human annotation guidelines with instructions, rubrics, and examples for the human evaluation of fine-grained deepfake detection responses."
          }
        }
      }
    ]
  },
  {
    "id": "cu8FfaYriU-rubric-1",
    "token_usage": {
      "prompt_tokens": 28568,
      "completion_tokens": 287,
      "total_tokens": 28855
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide or describe any detailed instructions for human annotation guidelines for datasets. Rather, it is a qualitative study about challenges and recommendations for fair dataset curation, based on interviews with dataset curators. There is no mention of issuing or presenting annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no description or presentation of detailed scoring rubrics or structured evaluation criteria for human annotations in the paper. Although the paper discusses challenges related to subjective annotations and disagreement, it does not provide or reference any rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include or provide annotation examples or exemplar annotated data to guide annotators. Examples focus on interview quotes or conceptual challenges, not on annotation guidelines."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "This paper does not introduce or release any new dataset with accompanying human annotation guidelines. It primarily reports on empirical findings from interviews and provides a taxonomy of challenges. As such, no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "cy8mq7QYae-rubric-1",
    "token_usage": {
      "prompt_tokens": 81175,
      "completion_tokens": 401,
      "total_tokens": 81576
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 and Section R",
            "reasoning": "Section 3.2 describes detailed question construction for two question types (descriptive and reasoning), with clear instructions for annotators on selecting charts and formulating questions. The paper mentions annotators were graduate students who were presented with candidate charts, questions, and detailed instructions on how to create, select, or modify questions. Additionally, Section R includes screenshots of annotation interfaces indicating guidelines are provided during the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 and Section P.2",
            "reasoning": "Section 3.3 details the evaluation metrics including the use of GPT-4o as an automated judge with a clear scoring rubric to assign binary correctness scores based on answer equivalency. Section P.2 explicitly describes grading instructions given to annotators and GPT-4o judges including in-context learning examples and detailed rules for scoring correctness of model-generated answers."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendices O.1, P.1, R.2, R.3",
            "reasoning": "The paper provides multiple examples and templates for both question formulation and grading, shown in the appendix sections for descriptive (O.1) and reasoning (P.1) question generation, as well as screenshots of annotation interfaces (R.2, R.3) where example questions and answers are displayed for annotators. This indicates that clear examples accompany the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset includes extensive detailed human annotation guidelines with instructions, rubrics, and examples as described above."
          }
        }
      }
    ]
  },
  {
    "id": "d1Pup4gkWf-rubric-1",
    "token_usage": {
      "prompt_tokens": 13839,
      "completion_tokens": 264,
      "total_tokens": 14103
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation guidelines or detailed instructions for data labeling related to the new datasets. The dataset focuses on simulated sports environments and use of human motion data extracted from videos, but no annotation instruction protocols are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that detailed scoring rubrics for human annotations are provided for any annotations or labeling related to the new datasets. The paper focuses on reinforcement learning environments and extracting human motion data but does not provide rubrics for human annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not show or discuss examples illustrating human annotation guidelines or provide examples for labeling. The human demonstrations are obtained via motion capture or pose estimation methods from videos without manual human labeling processes requiring example guidelines."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "All new datasets introduced (the simulated sports environments and the video-based human motion demonstrations) do not involve human annotation requiring guidelines. The paper does not provide nor discuss any human annotation guidelines, instructions, rubrics, or examples. Therefore, it is appropriate to conclude that no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "dVaWCDMBof-rubric-1",
    "token_usage": {
      "prompt_tokens": 22784,
      "completion_tokens": 274,
      "total_tokens": 23058
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces new datasets such as COMMONPOOL and DATACOMP-1B but does not describe any human annotation guidelines or detailed instructions for labeling data. The dataset creation involves automated collection and filtering methods from web data, not manual human annotation procedures."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of any detailed scoring rubrics for human annotators in the construction or curation of the new datasets. The data is primarily filtered using automated techniques like CLIP score filtering, language filtering, image size filtering, and deduplication, without involving subjective human labeling with rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include any examples of human annotations or examples in guidelines because the datasets are constructed via automated filtering from web-sourced image-text pairs. There is no indication of manual labeling or example-driven annotation instructions."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper's new datasets (COMMONPOOL and DATACOMP-1B) are generated through automated extraction, filtering, and deduplication of web data and do not involve human annotations or labelers requiring guidelines. Therefore, no human annotation guidelines are provided or applicable."
          }
        }
      }
    ]
  },
  {
    "id": "g7OX2sOJtn-rubric-1",
    "token_usage": {
      "prompt_tokens": 46251,
      "completion_tokens": 251,
      "total_tokens": 46502
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any human annotation process or provide detailed instructions for human annotators on how to label data. The dataset consists of automatically extracted theorems, proofs, tactics, and premises from the Lean proof assistant's math library, requiring no manual labeling or subjective human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of scoring rubrics or criteria for human annotators to evaluate or rate data labels. The data is extracted programmatically from formal proofs without human judgment or scoring involved."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples related to human annotation guidelines or annotation tasks. It only shows examples of theorems and proofs in Lean code used as data, but these are not examples for annotators, rather examples of data instances."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset introduced (LeanDojo Benchmark) is generated automatically from formal proofs without any human annotations or labeling effort described. Thus, no human annotation guidelines, instructions, rubrics, or examples are relevant or provided."
          }
        }
      }
    ]
  },
  {
    "id": "gg3POFjqq8-rubric-1",
    "token_usage": {
      "prompt_tokens": 11966,
      "completion_tokens": 316,
      "total_tokens": 12282
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4, specifically subsection 'Dataset for Evaluating OOC Filtering Strategies'",
            "reasoning": "The paper describes a detailed multi-stage process for collecting and labeling images, including how images were selected for labeling, criteria for 'hard' and 'easy' samples, and labeling options such as 'class', 'partial class properties', and 'not class'. This indicates the annotators were given clear instructions on how to label the data."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4, subsection 'Dataset for Evaluating OOC Filtering Strategies'",
            "reasoning": "The annotation includes a rubric by allowing annotators to assign one of three labels\u2014'class', 'partial class properties', and 'not class'\u2014and defines out-of-class samples based on annotations. This graded labeling scheme constitutes a rubric for human annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not explicitly provide or reference any examples illustrating the human annotation guidelines or labeling interface. No indication of concrete examples or illustrative samples for annotation is given."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are provided in the paper as detailed instructions and rubrics as described above."
          }
        }
      }
    ]
  },
  {
    "id": "h8LuywKj6N-rubric-1",
    "token_usage": {
      "prompt_tokens": 45011,
      "completion_tokens": 222,
      "total_tokens": 45233
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section B.3 Human Keyframes Annotation Process",
            "reasoning": "Section B.3 details comprehensive instructions and guidelines for annotators including annotator information, tutorial provision, recording procedures, keyframe selection, sub-action description, and annotation software usage, indicating detailed human annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not explicitly mention or describe detailed scoring rubrics or criteria that annotators follow to score or judge annotations; thus, no clear rubric information is available."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Table 3 and Section B.3",
            "reasoning": "The paper provides concrete examples of question and answer types generated (Table 3) and presents example annotation excerpts and case studies, showing that annotators have access to clear examples for guideline adherence."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are explicitly described and supplied with both instructions and examples, so the lack of guidelines does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "hcOq2buakM-rubric-1",
    "token_usage": {
      "prompt_tokens": 33992,
      "completion_tokens": 188,
      "total_tokens": 34180
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper discusses assessing existing AI benchmarks rather than introducing new datasets; it evaluates benchmarks' quality but does not release new datasets with human annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe providing any detailed rubrics for human annotations for new datasets; it primarily focuses on meta-assessment of existing benchmarks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include examples of human annotation guidelines or instructions tied to newly introduced datasets."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets with human annotations or associated annotation guidelines. Instead, it assesses the quality of existing AI benchmarks and proposes a framework for benchmark assessment, so no human annotation guidelines are provided for new data labeling."
          }
        }
      }
    ]
  },
  {
    "id": "iSwK1YqO7v-rubric-1",
    "token_usage": {
      "prompt_tokens": 98212,
      "completion_tokens": 318,
      "total_tokens": 98530
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix M (Annotation Details)",
            "reasoning": "Appendix M describes detailed manual annotation procedures for goal definitions, action sequences, and transition models on both VirtualHome and BEHAVIOR datasets. The authors explain how they annotated complex goals including nested quantifiers and decomposed goals into grounded goals and alternatives. The process is described as careful and manual, indicating detailed instructions to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix M.4.1 (Annotation Quality Evaluation)",
            "reasoning": "Section M.4.1 provides quantitative evaluation metrics for annotation quality including action accuracy, coverage, and human preference. They mention measuring Weighted Mean Squared Error (MSE) to assess variability among annotators. Such detailed metrics reflect the presence of rubrics and formal evaluation criteria for annotation quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix I (Prompt and Analysis), Appendix M (Annotation Examples)",
            "reasoning": "The paper includes example prompts for generating annotations (Appendix I) and shows example annotated data structures (Appendix L), as well as examples of task definitions and PDDL action definitions (Appendix I.4). These constitute clear examples that guide annotators or users through the annotation format and process."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly reports detailed annotation instructions, rubrics, and examples for dataset curation and quality assurance in multiple appendices, thus guidelines are present."
          }
        }
      }
    ]
  },
  {
    "id": "iTUlYblV0K-rubric-1",
    "token_usage": {
      "prompt_tokens": 20450,
      "completion_tokens": 276,
      "total_tokens": 20726
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any process involving human annotators or explicit human annotation guidelines. The dataset MQuAKE and its remastered version are derived via automatic processes from Wikidata knowledge graphs and are audited and fixed algorithmically or through manual corrections by the authors, not through crowdsourced or human annotation requiring explicit instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no description in the paper of any scoring rubrics or formal guidelines provided for human annotators to label or judge dataset instances. The evaluation is based on comparing model outputs to ground truth answers, rather than human annotation involving rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any examples of human annotation instructions or examples intended to guide human labelers. The examples shown are to illustrate dataset errors or contaminations discovered through audits, not as annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset MQuAKE-Remastered is constructed from automated transformations of knowledge graph data and GPT-generated questions, followed by manual correction by authors but there is no indication of human annotation tasks involving external annotators or human labeling requiring guidelines. Thus, no human annotation guidelines are provided or necessary."
          }
        }
      }
    ]
  },
  {
    "id": "iwC19lVBoq-rubric-1",
    "token_usage": {
      "prompt_tokens": 13821,
      "completion_tokens": 309,
      "total_tokens": 14130
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1, Stage 1: Data Collection",
            "reasoning": "The AVSET-700K subset is constructed by selecting samples from AudioSet, where the audio category labels are manually annotated by human annotators. The paper states that during annotation, annotators were allowed to view accompanying videos to ensure accurate audio category identification, implying the existence of detailed human annotation instructions to guide this manual labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any detailed scoring rubrics for human annotation. While it mentions that audio categories are manually labeled, no explicit rubric or scoring scale is provided to indicate how to assign labels or assess the quality of annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any explicit examples or illustrative samples from annotation guidelines. Although Figures and qualitative samples are shown, these are outputs of the dataset, not guideline examples to instruct human annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "There are human annotation guidelines applicable at least for the AVSET-700K subset derived from AudioSet, which included human manual annotation for audio labels."
          }
        }
      }
    ]
  },
  {
    "id": "j2wasUypqN-rubric-1",
    "token_usage": {
      "prompt_tokens": 20055,
      "completion_tokens": 241,
      "total_tokens": 20296
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any human annotation guidelines involving detailed instructions for labeling data. The new datasets introduced are benchmark problem suites for optimization tasks, which do not require manual human annotation or labeling instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no indications of detailed scoring rubrics for human annotation tasks provided for the datasets. The datasets are algorithmic benchmarks rather than datasets labeled by humans, hence no rubrics are involved or needed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of human annotations or labeling for any dataset, since the introduced datasets are collections of optimization problems intended for algorithmic evaluation rather than data annotated by humans."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The newly introduced datasets in MetaBox are standard optimization problem suites (Synthetic, Noisy-Synthetic, Protein-Docking) used for benchmarking meta-optimization algorithms. These are not data labeled via human annotation and thus no human annotation guidelines, instructions, rubrics, or examples are provided or relevant."
          }
        }
      }
    ]
  },
  {
    "id": "jSKtxmxc0M-rubric-1",
    "token_usage": {
      "prompt_tokens": 20318,
      "completion_tokens": 523,
      "total_tokens": 20841
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Construction; Appendix A.1 Data Collection Settings; Figure 12",
            "reasoning": "The paper details a careful annotation process involving participants reproducing instructional videos and providing hierarchical annotations, including multi-level task planning and identifying UI elements. Section 3.1 describes the pipeline for data collection, instructing annotators to watch videos and reproduce operations, provide task and subtask descriptions, and identify active elements. Appendix A.1 further elaborates on the annotation interface and guidelines, such as defining action narrations and principles for selecting instruction videos, indicating the presence of detailed annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Evaluation and Metrics; Appendix A.3 Evaluation Settings; Table 28",
            "reasoning": "The paper provides detailed evaluation rubrics for multiple annotation levels, including scoring on a scale from 0 to 5 for high-level and middle-level planning using an LLM as critic, distance and recall metrics for atomic actions like click and drag, and accuracy for scroll actions. Appendix A.3 specifies how these metrics are calculated and offers prompt templates to guide evaluation. Table 28 explicitly shows a rubric prompt template for evaluating procedural plans against ground truth. These details demonstrate the presence of well-defined rubrics for annotation quality and consistency."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Construction (Table 2); Appendices C and D (Tables 10-17, Figures 6 and 12); Section A.3 Evaluation Settings",
            "reasoning": "The paper includes extensive examples illustrating various levels of annotations: sample hierarchical task decompositions, mid-level action narrations, and low-level action executions with coordinates. Figures and tables provide concrete annotated examples from multiple software, demonstrating how the annotations look and the expected format. Figure 12 illustrates the annotation tool interface, and Figures 6 and 9 show qualitative results. Appendix D provides prompt templates that serve as examples for annotation and evaluation. Such comprehensive examples support annotator understanding and guideline clarity."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly describes detailed instructions, rubrics, and examples for human annotation, so it is not applicable that no guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "jbrMS0DNaD-rubric-1",
    "token_usage": {
      "prompt_tokens": 16020,
      "completion_tokens": 226,
      "total_tokens": 16246
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 'Image Annotation'",
            "reasoning": "The paper states that annotators were instructed to label images as relevant or not relevant to each query and to mark images as not relevant if there was reasonable doubt. This indicates presence of detailed instructions guiding human annotators on labeling criteria and process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any detailed scoring rubrics or tiered labeling schemes for human annotation. The annotation is binary relevance labeling without explicit mention of rubrics or grading criteria."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples illustrating how to perform annotations are provided in the main text or appendices. The paper does not mention providing example annotations or detailed exemplars for annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes the annotation process and instructions given to annotators, indicating annotation guidelines exist; therefore, it is not applicable that no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "kaHpo8OZw2-rubric-1",
    "token_usage": {
      "prompt_tokens": 104779,
      "completion_tokens": 392,
      "total_tokens": 105171
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendices D.1 and I.5",
            "reasoning": "For the stereotype bias dataset, the paper describes specific design of stereotype templates, demographic groups, and instructions that guide GPT models to respond with agreement or disagreement, as in Appendix D.1 and examples with prompts are shown in Figure 14. For the machine ethics evaluation, Appendix I.5 details the prompt designs including instructions for moral recognition tasks. These demonstrate that detailed instructions were provided for human annotation or data generation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3 Stereotypes Evaluation Setup, Section 8 Machine Ethics, Appendix D.2, Appendix I.1",
            "reasoning": "The stereotype bias evaluation uses the agreementIndex metric, which quantifies frequency of agreement with stereotypes, as a rubric to assess bias (Section 3 and Appendix D.2). In machine ethics, the classification accuracy and false positive rate metrics serve as evaluation rubrics guiding annotation and measurement (Section 8 and Appendix I.1). The experimental descriptions and metrics show that scoring rubrics were used in annotation and evaluation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix D.2 Figure 14, Appendix I.5 Figures 30 and 35",
            "reasoning": "Clear examples of human annotation or prompt design are shown, such as example stereotype user prompts and model outputs in Figure 14 (Appendix D.2), and prompt templates with example moral evaluation prompts in Appendix I.5 (Figures 30 and 35). These examples help annotators or evaluators understand expectations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes detailed human annotation guidelines, rubrics, and examples for multiple new datasets introduced by the authors, especially in stereotype bias and machine ethics evaluation, thus the label N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "l985bXCatk-rubric-1",
    "token_usage": {
      "prompt_tokens": 15142,
      "completion_tokens": 318,
      "total_tokens": 15460
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the construction and annotation process of the dataset LRVS-Fashion, primarily involving the extraction of product identifiers, generation of synthetic labels via active learning classifiers for complexity and category, and synthetic captions generated by BLIP-2. However, there is no mention of human annotators being given detailed instructions for manual labeling tasks; rather, the labeling appears mostly automated or synthetically generated."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that annotators used any rubric or scoring system for labeling the data. The labels (complexity, product category) were generated using classifiers trained with active learning, and captions generated automatically. No mention is made of manual scoring rubrics or detailed evaluation criteria for human annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any examples of human annotation guidelines, nor examples intended to train or guide human annotators. The dataset annotations were primarily synthetically generated or programmatically extracted, thus no examples for annotation guidelines are described."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset LRVS-Fashion was constructed by automatic extraction and synthetic annotation methods such as active learning classifiers for labeling complexity and category and synthetic caption generation. There is no evidence of manual data labeling with corresponding human annotation guidelines, instructions, rubrics, or examples provided. Therefore, no human annotation guidelines are provided for the new dataset."
          }
        }
      }
    ]
  },
  {
    "id": "lnnNPiZtzR-rubric-1",
    "token_usage": {
      "prompt_tokens": 12249,
      "completion_tokens": 249,
      "total_tokens": 12498
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any explicit description of detailed instructions given to human annotators regarding the fungal species classification or other annotation tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of detailed scoring rubrics, criteria, or protocols used by annotators for labeling or quality control of annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include any example annotations, illustrative examples, or step-by-step guide for annotation tasks within the dataset creation."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset is derived from expert-labeled and curated fungal records collected over many years by experts and citizen scientists, but the paper does not present or describe any annotation guidelines, instructions, rubrics, or examples explicitly for human annotation as part of the dataset preparation."
          }
        }
      }
    ]
  },
  {
    "id": "loJM1acwzf-rubric-1",
    "token_usage": {
      "prompt_tokens": 29481,
      "completion_tokens": 335,
      "total_tokens": 29816
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Question and Answer Collection; Appendix A.4 Existing Question Editing; Appendix A.5 New Question Annotation",
            "reasoning": "The paper details a comprehensive annotation pipeline where 10 expert-level annotators undergo training, pre-annotation practice, and receive iterative feedback until meeting quality standards. Annotators are instructed on standards for question difficulty, diversity, relevance, and distribution. They review and revise existing questions, classify problems, and either revise or remove problematic annotations, indicating clear detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Question and Answer Collection; Appendix A.4 Existing Question Editing",
            "reasoning": "The annotation process involves classifying existing questions into categories such as Wrong Answers, Ambiguous Question, Potential Shortcut, etc., and making decisions such as retain, revise, or remove. This classification forms a rubric to maintain annotation quality and consistency during editing of existing questions."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A.4 Existing Question Editing (Figures 15-20)",
            "reasoning": "The appendix provides multiple explicit annotated examples illustrating problem types (e.g., Wrong Answer, Ambiguous Question, Potential Shortcut), including before and after revision samples, serving as clear examples for annotators on how to identify and handle annotation issues."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes a three-stage annotation pipeline with training, detailed standards, revision procedures, quality control, and provides illustrative examples. Thus, human annotation guidelines are clearly provided and documented."
          }
        }
      }
    ]
  },
  {
    "id": "mDRmX8IlBI-rubric-1",
    "token_usage": {
      "prompt_tokens": 16591,
      "completion_tokens": 323,
      "total_tokens": 16914
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Manual Data Collection",
            "reasoning": "The paper describes in Section 3.1 that the team conducted a two-stage manual data collection process including detailed examination and selection of videos across disciplines, followed by question annotation specifically designed to test seven aspects of multimodal video understanding. This indicates that human annotators were provided with detailed instructions for selecting relevant videos and crafting questions to comprehensively evaluate multi-faceted reasoning."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any specific scoring rubrics or detailed criteria used during human annotation or quality control for the question-answer pairs or other annotations. There is no explicit description of rubrics for annotators to score or rate the annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section F in the Appendix",
            "reasoning": "The paper refers in Section 3.1 to designed 'temporal understanding' question types with examples found in Section F of the Appendix, indicating that the annotation guidelines included clear examples to help annotators generate or understand question types and ensure consistency."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly describes human annotation processes and guidelines for the manual dataset, implying the presence of human annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "mEJgnZZyfv-rubric-1",
    "token_usage": {
      "prompt_tokens": 16714,
      "completion_tokens": 233,
      "total_tokens": 16947
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation guidelines or instructions for data labeling. The datasets used are standard image classification datasets, which are pre-existing and externally sourced, and no new human annotation task or labeling instructions are introduced by the authors."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no description or provision of scoring rubrics or criteria for human annotations in the paper. The evaluation metrics focus on model performance such as accuracy, robustness, and calibration, rather than any human labeling processes with rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper provides examples of model results and visualization of class activation maps, but does not provide examples related to human annotation or labeling guidelines. No examples of annotation tasks or instructions are given."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets that require human annotation or labeling by the authors. All datasets used are publicly available standard benchmarks, and no human annotation guidelines are provided or discussed."
          }
        }
      }
    ]
  },
  {
    "id": "mlhFJE7PKo-rubric-1",
    "token_usage": {
      "prompt_tokens": 39692,
      "completion_tokens": 259,
      "total_tokens": 39951
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any human annotation process for labeling data in the introduced datasets; it mainly involves collecting, curating, and processing spatial transcriptomics data and matched histology images from public and internal sources, but does not state that human annotators labeled the data for specific tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or detailed evaluation grading criteria for human annotators are provided or described in the paper for any data labeling or annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present any examples of human annotation guidelines or example labels to instruct human annotators, nor does it mention human annotation as part of dataset creation."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The datasets introduced (HEST-1k, HEST-Library, HEST-Benchmark) are derived from existing spatial transcriptomics and histology data and computational pipelines that unify and process these data. There is no mention of human annotators receiving instructions, rubrics, or examples for labeling or annotating data. Therefore, no human annotation guidelines are provided for these datasets."
          }
        }
      }
    ]
  },
  {
    "id": "moMoWj7jLm-rubric-1",
    "token_usage": {
      "prompt_tokens": 23894,
      "completion_tokens": 384,
      "total_tokens": 24278
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 (Dataset Construction) and Appendix C.1 (Dataset Construction details, especially C.1.1 and C.1.3)",
            "reasoning": "The paper details a multi-step dataset construction process involving manual annotation by 5 postgraduate volunteers with a science background, assisted by LLMs. Section 3 describes the formula normalization procedure which involves detailed instructions for annotators and LLMs to generate normalized explanations, extract and correct formulas, parameters, units, and verify computational correctness. The prompts used for normalization and semantic-based merging (Appendix C.1.1 and C.1.3) show explicit, structured instructions given to guide the annotation process, indicating the presence of detailed annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide or mention any explicit scoring rubrics or grading criteria used during the human annotation process. The annotation is focused on formula extraction, normalization, and verification rather than subjective scoring, and no rubric or scoring guidelines are described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3 and Appendix C.1.1 and C.1.2",
            "reasoning": "The paper provides concrete examples demonstrating the annotation process including original explanations, normalized explanations with formula annotations and parameter tables (Table 2, Figure 1). Additionally, prompts with examples for formula normalization and error correction are detailed in Appendix C.1.1. Examples of deleted poor-quality questions are also given in Appendix C.1.2. These examples serve as clear exemplars within the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Clear and detailed human annotation guidelines with instructions and examples are explicitly provided in multiple sections; hence, it is not accurate to state that no guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "nrEqH502eC-rubric-1",
    "token_usage": {
      "prompt_tokens": 79268,
      "completion_tokens": 327,
      "total_tokens": 79595
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 Construction of BLEND, specifically in 'Answer Annotation' and Appendices B.4 and B.5",
            "reasoning": "The paper describes detailed guidelines provided to annotators, including criteria for answer length, cultural relevance, and options if annotators do not know the answer. It explicitly states that annotators received written instructions in their local languages (Appendix B.5) and followed a specific protocol for answering and re-annotation processes to ensure quality."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5 Human Evaluation and Appendix D.3.1",
            "reasoning": "The paper includes detailed annotation schemas for human evaluation categories such as 'Applicability', 'Unnatural Language', 'Stereotypical', 'Partially Correct', etc., with clear definitions and scoring rules (weights assigned to applicability categories). These rubrics guide consistent labeling and evaluation of LLM responses."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B.5 Figure 11 and A.3 Annotation Examples (Figures 5-10)",
            "reasoning": "The paper provides clear examples of annotation guidelines and sample annotated questions with responses in both local languages and English translations to assist annotators in understanding expected answer forms. The answer annotation guidelines (Figure 11) include acceptable and unacceptable examples to clarify instructions."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Human annotation guidelines are explicitly provided, including instructions, rubrics, and examples, so the N/A label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "p8eUitex7p-rubric-1",
    "token_usage": {
      "prompt_tokens": 20031,
      "completion_tokens": 366,
      "total_tokens": 20397
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction, Section A.2 Annotator Guidelines, and Section C.3 Annotator guidelines",
            "reasoning": "The paper explicitly mentions detailed instructions and tutorials provided to annotators before annotation, including an onboarding stage with training sessions explaining annotation parameters and how to handle boundary cases. Section 3.1 describes annotator recruitment with requirements for annotator qualification, and Sections A.2 and C.3 provide further details with links to tutorial slides and guidelines, indicating presence of detailed instructions for human annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction and Section A.2 Annotator Guidelines",
            "reasoning": "The paper notes that annotators labeled categorical attributes such as scene density (binary label) and visual quality of objects (four categorical options) which implies existence of rubrics for consistent labeling. They also mention that annotators must meet accuracy thresholds on sample questions during onboarding, suggesting some form of scoring rubrics or evaluation criteria to ensure annotation quality and consistency."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section A.2 Annotator Guidelines and Section C.3 Annotator guidelines",
            "reasoning": "The paper states that detailed guidelines and tutorials with instructions and handling of boundary cases were provided to annotators, which reasonably includes clear examples to illustrate annotation tasks. Although explicit examples are not copied in the main paper, the references to tutorials and guidelines in appendices imply that examples accompany these instructions to aid annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides clear evidence that human annotation guidelines were developed and used, including instructions, rubrics, and examples, so the label of no guidelines is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "pUcTrjRLOM-rubric-1",
    "token_usage": {
      "prompt_tokens": 34534,
      "completion_tokens": 299,
      "total_tokens": 34833
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix C.4 Details of Human Annotation",
            "reasoning": "Section C.4 describes a detailed annotation process where human experts reviewed and annotated preference pairs, with specific voting criteria such as honesty, helpfulness, harmlessness, and length bias. Annotators were guided through a WebUI with instructions to evaluate responses carefully, indicating detailed instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix C.4 Details of Human Annotation",
            "reasoning": "Annotation criteria are explicitly defined as a 5-point scale covering aspects such as completeness, relevance, scientific accuracy, and helpfulness, along with specific definitions for each score from 1 (Inadequate) to 5 (Excellent). This constitutes a detailed rubric for scoring human annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix C.4 Details of Human Annotation and Section 2.2.2",
            "reasoning": "The annotation tasks provide clear examples of questions with candidate responses and guidelines on how to choose the best response. Additionally, Section 2.2.2 and Appendix C provide sample annotations and explanation of the process, demonstrating examples of the annotation guidelines in practice."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly describes the presence of detailed human annotation guidelines, rubrics, and examples, so the 'N/A' label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "pYNl76onJL-rubric-1",
    "token_usage": {
      "prompt_tokens": 19791,
      "completion_tokens": 270,
      "total_tokens": 20061
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any process of human annotation that requires instructions. The dataset is built by scraping user prompts and generated videos from the Pika Discord channels and other diffusion models without mention of human labeling or annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no discussion or indication of human annotation involving rubrics or scoring systems in the paper. The dataset does not involve human labeling tasks that need rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of human annotation guidelines, nor does it describe human annotation activities that would require examples. The examples shown are user prompts and generated videos, not annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "VidProM is created by collecting real user prompts and generated videos automatically without involving human annotation processes. No human annotation guidelines, instructions, rubrics, or examples are provided or discussed in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "qmvtDIfbmS-rubric-1",
    "token_usage": {
      "prompt_tokens": 27937,
      "completion_tokens": 527,
      "total_tokens": 28464
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2.3 Question Annotation; Section A.2.1 Dataset Collection and Access; Section A.3 Dataset Documentation",
            "reasoning": "The paper describes a detailed annotation process for constructing reasoning chains and forming multiple-choice questions (Section 3.2.3). It mentions the involvement of ten experienced experts for data annotation with specific compensations, indicating organized instructions given to annotators (Section A.2.1). Also, a three-stage review process including preliminary, logic consistency, and final confirmation reviews is stated, demonstrating detailed guidance and procedural instructions to maintain quality (Section A.2.1). The dataset documentation further details the composition and collection process, implying clear human annotator directives."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5.1 Assessment Details; Section 3.2.4 Data Quality Control; Section A.4.1 Prompts",
            "reasoning": "The evaluation metrics for various capabilities are clearly defined with scoring formulas, e.g., perception scores as ratios of correct answers, role play indexed on a 10-point scale with criteria, and final reasoning scored by GPT-4 (Section 5.1). The data quality control involves specific standards for logical consistency and correctness (Section 3.2.4). The evaluation also uses specialized prompts designed for scoring different aspects, indicating rubric-based assessments (Appendix A.4.1). This shows detailed scoring rubrics guide the human annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures 3, 9, 10; Section A.2.2 Additional Quantitative Examples; Appendix A.4.2 Additional Examples of Dialogue Content",
            "reasoning": "The paper provides multiple explicit examples of annotated reasoning chains, multiple-choice questions with ground truths and distractors (Figures 3, 9, 10), as well as example dialogue contents generated by agents across game phases (Appendix A.4.2). These instances function as clear examples in the annotation guidelines, serving as references for annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides thorough descriptions of annotation instructions, rubrics, and examples, including reviewer processes and detailed metrics, so the dataset is not without human annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "r8PnfcWQol-rubric-1",
    "token_usage": {
      "prompt_tokens": 31458,
      "completion_tokens": 229,
      "total_tokens": 31687
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation process or instructions provided to annotators for labeling data; the dataset is algorithmically generated based on formal logical definitions and constraints rather than human labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of annotation rubrics or scoring guidelines for human annotators is found in the paper; the dataset creation is based on formal enumeration and grounding of logical queries, not dependent on subjective human judgments."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper provides examples illustrating query types and abstract query graphs for understanding but does not include examples of human annotation guidelines or examples aimed at human annotators, since the dataset is generated programmatically."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset EFO_k-CQA introduced in the paper is constructed automatically by enumerating logical queries, sampling groundings, and computing answers algorithmically; there is no human annotation involved or mentioned, thus no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "rdv2Fr6JTC-rubric-1",
    "token_usage": {
      "prompt_tokens": 21682,
      "completion_tokens": 239,
      "total_tokens": 21921
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation guidelines, instructions, or protocols for data labeling. The datasets used are existing benchmark graph datasets, and the paper focuses on developing a graph data valuation method rather than collecting or annotating new data via human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no description of rubrics or scoring criteria for human annotation present in the paper. The work does not involve subjective human labeling or tasks requiring annotation quality controls."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper includes no examples illustrating how to perform data annotation or how to apply annotation guidelines, since no human annotation is involved."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces a new method (PC-Winter) for graph data valuation on existing graph datasets and includes extensive computational experiments. There is no indication of any human annotation process or guidelines. The datasets are standard and pre-existing, with no new human labeling introduced. Hence, no human annotation guidelines are relevant or provided."
          }
        }
      }
    ]
  },
  {
    "id": "rovpCs3ZEO-rubric-1",
    "token_usage": {
      "prompt_tokens": 24636,
      "completion_tokens": 300,
      "total_tokens": 24936
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe detailed instructions provided to human annotators for labeling the data in the new curated datasets. The datasets are compiled from publicly available datasets where annotations were originally provided; the authors rely on these existing labels without further describing new annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No detailed scoring rubrics or criteria for human annotation are described in the paper for the curated datasets. The paper uses existing labeled datasets and does not discuss any new rubric development to ensure consistent annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper provides data samples visualizations for several datasets (e.g., Figures 2, 3, 4, 6, 7, 8, 9), but these are generally example data instances, not explicit examples illustrating annotation guidelines or instructions for human annotators. Thus, clear examples for annotation guidance are not provided."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper curates a composite benchmark dataset from multiple publicly available medical datasets that already have annotations, and the paper does not provide or discuss new human annotation guidelines, instructions, rubrics, or examples. The annotations utilized come from original dataset sources, with no new annotation effort described by the authors. Therefore, no human annotation guidelines are provided in this work for the new benchmark dataset."
          }
        }
      }
    ]
  },
  {
    "id": "s1K5Z5QPog-rubric-1",
    "token_usage": {
      "prompt_tokens": 13240,
      "completion_tokens": 206,
      "total_tokens": 13446
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any human annotation guidelines or instructions related to labeling of data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no discussion or reference to any scoring rubrics for human annotation in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of human annotation guidelines or annotations are provided in the paper."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "ChaosBench is a benchmark based on physics-based and reanalysis datasets for subseasonal-to-seasonal climate prediction without involving any human annotation or labeling process, hence there are no human annotation guidelines provided."
          }
        }
      }
    ]
  },
  {
    "id": "t9aThFL1lE-rubric-1",
    "token_usage": {
      "prompt_tokens": 30264,
      "completion_tokens": 303,
      "total_tokens": 30567
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix A.3",
            "reasoning": "The dataset construction involves a clear two-step process of seed image collection from an open-source photography website and subsequent image stylization using a commercial service. Each image is labeled with both style and object classes, and annotated with specific text prompts, indicating detailed instructions for labeling."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3 and Appendix B.7",
            "reasoning": "The paper defines specific, detailed quantitative evaluation metrics such as Unlearning Accuracy, In-domain Retain Accuracy, Cross-domain Retain Accuracy, and additional metrics for style-object combination and sequential unlearning scenarios. These metrics function as scoring rubrics to assess annotation quality and unlearning effectiveness precisely."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix E (E.1, E.2, E.3) and Figures A9-A12",
            "reasoning": "The paper provides numerous visual examples illustrating the styles and objects in the dataset, as well as examples of generated images before and after unlearning, and images illustrating the effects of adversarial prompts. These visuals serve as clear examples within the annotation guidelines to help human annotators and evaluators understand the labeling and evaluation process."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset provides explicit instructions, rubrics, and examples as part of the annotation guidelines, thus N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "tPsw4NeLZx-rubric-1",
    "token_usage": {
      "prompt_tokens": 21641,
      "completion_tokens": 224,
      "total_tokens": 21865
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Recording Setup and Workflow",
            "reasoning": "The dataset recording process involved signers supervised by at least one Auslan expert to ensure precision of the sign language expression. This supervision implies that detailed instructions and guidelines were provided to signers to perform signs accurately during data collection."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any specific scoring rubrics or detailed criteria used during annotation or labeling of the dataset. It focuses mainly on expert supervision to ensure data quality rather than explicit rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include or refer to any examples illustrating annotation guidelines or labeling instructions. There are no example annotations or labeling demonstrations provided in the text or appendix."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Because there is mention of expert supervision ensuring precision of sign expression, some form of annotation guideline or instruction is present, so 'N/A' is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "tWvVtOW0qg-rubric-1",
    "token_usage": {
      "prompt_tokens": 15929,
      "completion_tokens": 220,
      "total_tokens": 16149
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces federated data measurements and benchmarks on existing datasets but does not mention any human annotation or detailed instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication or description of rubrics for human annotation, scoring, or grading in any section of the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples or illustrative samples of human annotation guidelines or annotation tasks are provided."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new dataset with human annotations requiring guidelines; the datasets used are existing computer vision and medical imaging datasets with no mention of annotator instructions or human labeling procedures."
          }
        }
      }
    ]
  },
  {
    "id": "vyraA7xt4c-rubric-1",
    "token_usage": {
      "prompt_tokens": 20746,
      "completion_tokens": 317,
      "total_tokens": 21063
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any human annotation procedures or guidelines for data labeling. The dataset, Mercury, is collected by scraping and filtering LeetCode tasks and their historical solutions, and evaluation is performed via automated test case generators and sandbox execution rather than human annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of scoring rubrics or detailed guidelines for human annotators to evaluate or label data. The evaluation metrics (Pass and Beyond) are automated metrics based on functional correctness and efficiency, not human judgment."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide human annotation examples or instructions for human annotators. While examples of tasks and solutions are shown to illustrate the dataset, these are part of the data itself and not annotation guideline examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The Mercury dataset is constructed from publicly available programming problems and their historical solutions from LeetCode, filtered and processed automatically. The evaluation relies on automatically generated test cases and code execution in a sandbox environment, without involving human annotators or manual labeling processes. Therefore, no human annotation guidelines are provided or applicable."
          }
        }
      }
    ]
  },
  {
    "id": "wOmtZ5FgMH-rubric-1",
    "token_usage": {
      "prompt_tokens": 33286,
      "completion_tokens": 342,
      "total_tokens": 33628
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Data Collection and Construction, Appendix E.1 Prompt Templates",
            "reasoning": "The dataset collection process describes detailed steps for generating probes, including using GPT-4 to generate query-answer pairs with specific prompt templates as detailed in Appendix E.1. Manual verification of probe format and type correctness is performed, indicating the presence of instructions for annotators or curators involved in dataset preparation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section E.3 Probe Quality Assessment",
            "reasoning": "The paper includes a section analyzing the quality of probes in terms of diversity and correctness, including manual clustering and accuracy evaluation both manually and via GPT-4, reflecting clear criteria (rubrics) used for assessing annotation quality and filtering."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix E.2 Data Examples; Section E.1 Prompt Templates",
            "reasoning": "The paper provides multiple example data entries for annotated probes and example prompt templates used for generating questions and answers, as found in Appendix E.2 and Section E.1, offering concrete guidance and exemplars for annotators or dataset curators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes instructions, evaluation rubrics, and data examples for the human annotation involved in probe construction, so it does not lack human annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "wmO7z57wNK-rubric-1",
    "token_usage": {
      "prompt_tokens": 17344,
      "completion_tokens": 228,
      "total_tokens": 17572
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper LLMCBench introduces a benchmark for evaluating large language model compression algorithms; it does not create new datasets requiring human annotation. Evaluation involves applying existing compression methods on existing LLMs and datasets, using automated metrics and benchmarks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of any human annotation or manual labeling process or scoring rubrics in the construction of datasets or annotations. The benchmark evaluation is fully automated or uses existing benchmark datasets with fixed scoring metrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present any examples or instructions for human annotators as the benchmark relies on existing datasets for evaluation, not newly annotated human data."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "LLMCBench does not introduce any new datasets that require human annotation guidelines. The work evaluates model compression methods using standard existing datasets and metrics, without involving human annotation or labeling. Therefore, no human annotation guidelines are provided or needed."
          }
        }
      }
    ]
  },
  {
    "id": "x8RgF2xQTj-rubric-1",
    "token_usage": {
      "prompt_tokens": 40955,
      "completion_tokens": 284,
      "total_tokens": 41239
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces benchmarking and reimplementations of existing uncertainty quantification methods on established datasets (ImageNet-1k and CIFAR-10) but does not introduce new datasets that require human annotation guidelines. The paper uses existing datasets with their own original annotations and label uncertainty (e.g., ImageNet-ReaL and CIFAR-10H) without providing or describing new annotation instructions for human annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that the authors provide detailed scoring rubrics for human annotators. The paper uses existing datasets with human annotation distributions but does not create or document new annotation rubrics or scoring systems for data labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of annotation guidelines or instructions given to human annotators. The datasets used are pre-existing with publicly available annotations, and no new annotation examples or guidelines are introduced or described."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new dataset requiring human annotations, nor does it provide any new human annotation guidelines. The datasets utilized are pre-existing and publicly available, and the work focuses on benchmarking uncertainty estimation methods rather than annotation collection or guideline provisioning."
          }
        }
      }
    ]
  },
  {
    "id": "y09S5rdaWY-rubric-1",
    "token_usage": {
      "prompt_tokens": 20093,
      "completion_tokens": 326,
      "total_tokens": 20419
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe or mention detailed human annotation guidelines or instructions for labeling the data. The data is automatically collected by an expert model (Think2Drive) and annotations are obtained via CARLA simulator APIs, with some manual corrections to API bugs. No human annotators or instructions to them are discussed."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any evidence of rubrics or scoring guidelines given to human annotators for labeling the data. The annotations come from simulation environment APIs and are not based on subjective human judgments requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of human annotation processes or label examples. While many annotations are shown (e.g., sensor settings, scenarios), these are generated from simulator APIs or expert policy, not via human labeling examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The Bench2Drive dataset is collected automatically using a reinforcement learning expert model in CARLA simulator and annotations extracted via simulator APIs. There is no mention of human annotators or human annotation guidelines, rubrics, or examples. Annotation is programmatic and deterministic from the simulation, thus no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "y10DM6R2r3-rubric-1",
    "token_usage": {
      "prompt_tokens": 27791,
      "completion_tokens": 399,
      "total_tokens": 28190
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Construction Pipeline; Appendix A.1 Dataset Construction Details; Table 6",
            "reasoning": "The paper describes a detailed multi-phase pipeline for constructing the MMLU-Pro dataset involving initial filtering based on model answers to remove easy questions, integration and adaptation of questions from various sources using GPT-4-Turbo with explicit prompt instructions for converting problem statements and solutions into multiple-choice questions, generating plausible distractors, followed by expert reviews that verify answers and distractors' validity. These steps indicate detailed human annotation instructions to ensure question quality and correctness."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Construction Pipeline, Expert Review subsection; Table 1; Appendix A.6 Error Analysis Cases",
            "reasoning": "The annotation process includes a two-phase expert review designed to identify and remove incorrect answers, false negative options (incorrectly marked distractors), and bad questions, with systematic categorization of issues and detailed criteria to evaluate question and answer quality. This implies the presence of explicit rubrics or scoring guidelines for annotators to maintain dataset quality and consistency."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A.2 5-shot CoT Prompt example; Appendix A.1 Table 6; main text Section 3.2 Dataset Construction Pipeline",
            "reasoning": "The paper provides clear prompt instructions with examples for transforming questions and answers into multiple-choice format, expanding options, and recalls false negative options. Additionally, Appendix A.2 provides an extended 5-shot Chain-of-Thought prompt example demonstrating step-by-step reasoning and final answer selection. These constitute explicit examples illustrating annotation and evaluation procedures."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper includes detailed descriptions of dataset construction, annotation instructions, expert review processes with issue categorization, and example prompts, indicating that human annotation guidelines are present."
          }
        }
      }
    ]
  },
  {
    "id": "yS1dUkQFnu-rubric-1",
    "token_usage": {
      "prompt_tokens": 17606,
      "completion_tokens": 243,
      "total_tokens": 17849
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any detailed human annotation instructions for the datasets used. It focuses on benchmarking parameter-efficient transfer learning algorithms on existing CV datasets, not on annotation processes."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of scoring rubrics or detailed annotation guidelines for labeling is provided in the paper for any newly created dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide or reference examples of human annotations or guidelines that accompany the datasets."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper uses 30 datasets selected from existing, publicly available datasets for benchmarking. No new datasets are introduced by the authors, and no annotation guidelines are mentioned or provided. The focus is on benchmarking existing datasets with various PETL algorithms rather than data annotation."
          }
        }
      }
    ]
  },
  {
    "id": "yUEBXN3cvX-rubric-1",
    "token_usage": {
      "prompt_tokens": 20667,
      "completion_tokens": 341,
      "total_tokens": 21008
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Data collection and Appendix B.1 Dataset details",
            "reasoning": "The paper describes a detailed annotation process where annotators were provided with generic feature descriptions, asked to instantiate tasks, and given instructions to provide detailed high-level task descriptions including app names. Annotators were also asked to type short natural language descriptions of each UI action before performing it, ensuring detailed low-level instructions. Furthermore, the annotation process involved a training phase of several weeks with personalized feedback, and annotators were given instructional documents and video tutorials (Appendix B.1). This indicates the presence of detailed human annotation guidelines and instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and appendix do not mention any specific scoring rubrics, detailed criteria, or formal evaluation metrics provided to annotators for labeling or evaluating annotations. The annotation process focuses on task and action descriptions, but does not include explicit rubrics or grading guidelines for labeling consistency or quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B.1 and Figure 6 in the main paper",
            "reasoning": "Examples of episodes from the ANDROIDCONTROL dataset are provided in Figure 6, illustrating how tasks and UI interactions are structured. The appendix further describes data format and shows example UI elements and instructions. This suggests that the annotation guidelines include clear examples to help annotators understand how to generate the required annotations."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents annotation guidelines and provides examples, so it is not true that no human annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "z64azPC6Nl-rubric-1",
    "token_usage": {
      "prompt_tokens": 18206,
      "completion_tokens": 248,
      "total_tokens": 18454
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Annotation; Appendix A.2 Details of Annotation",
            "reasoning": "The paper details a clear annotation process where annotators with musical background use Praat to correct forced alignments and annotate phoneme-level singing techniques and global style labels. Instructions include specific tasks such as boundary correction, word and phoneme correction, and unvoiced region labeling. These steps denote detailed annotation instructions provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe explicit scoring rubrics or detailed criteria for annotation decisions. While annotation tasks and labels are described, no formal rubric or grading scale is presented for human annotators."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and appendix do not provide concrete annotation examples or illustrative cases to guide annotators. No sample annotated instances or visual examples are given within the text or appendices."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Comprehensive annotation guidelines are described including instructions, but no mention of rubrics or examples. Therefore, N/A does not apply as guidelines exist."
          }
        }
      }
    ]
  },
  {
    "id": "zogaeVpbaE-rubric-1",
    "token_usage": {
      "prompt_tokens": 21491,
      "completion_tokens": 223,
      "total_tokens": 21714
    },
    "response": [
      {
        "human_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any annotation guidelines, especially instructions for human annotators, as the human data used were secondary and collected from existing datasets rather than annotated specifically for this benchmark."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of scoring rubrics or detailed annotation schemes for human annotators in the context of data labeling in the paper. The human data are from pre-existing studies without explicit mention of rubric details included by the authors."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of annotation guidelines or annotator instructions, nor does it present annotation examples since the human data used are from prior studies and secondary data sources."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "DEVBENCH uses existing human behavioral data from previously conducted studies; the paper explicitly states no primary human data collection or new annotation was done by the authors, and no annotation guidelines are provided or described."
          }
        }
      }
    ]
  }
]