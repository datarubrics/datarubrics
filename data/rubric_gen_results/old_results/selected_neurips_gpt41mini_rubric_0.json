[
  {
    "id": "01I55gys19-rubric-0",
    "token_usage": {
      "prompt_tokens": 12268,
      "completion_tokens": 218,
      "total_tokens": 12486
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Few-Class Dataset Preparation; Section 4 Experimental Results; Table 1; Section 4.1; Section 4.2",
            "Reasoning": "The paper introduces the Few-Class Arena (FCA) benchmark, which creates few-class subsets from existing full image classification datasets by randomly selecting classes without duplicating data points, enabling efficient few-class subset benchmarking. Specifically, the FCA tool prepares subsets by extracting classes from full real-world image datasets such as ImageNet1K and others listed in Table 1. This process uses the original image data, which is human-generated and manually captured for those public vision datasets, as the raw data foundation. The few-class subsets are derived directly from these image datasets; hence, the modality is image and the data origin is human-generated, not model-generated or unknown. The paper explicitly emphasizes that FCA does not create new image data but subsets existing human-captured image datasets for few-class benchmarking."
          }
        ]
      }
    ]
  },
  {
    "id": "0SMhqvgHST-rubric-0",
    "token_usage": {
      "prompt_tokens": 29607,
      "completion_tokens": 306,
      "total_tokens": 29913
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 11 Benchmark Pool Selection Details and Section 12 Benchmark Pool Details",
            "Reasoning": "The paper introduces the GATE benchmark pool as a collection of multiple datasets across domains, including medical imaging datasets such as Chexpert (chest X-rays), Diabetic Retinopathy Classification (retinal images), HAM10000 (dermatoscopic images), and medical segmentation with ACDC (cardiac MRI). These datasets are images and are described as real-world challenge datasets generated by humans (e.g., medical imaging)."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 12 Benchmark Pool Details and 13 Model Pool Details",
            "Reasoning": "The benchmark includes zero-shot text-image classification tasks such as Flickr30K and New Yorker Caption Context that involve textual data as part of the benchmarks. Text data is human generated; these datasets consist of human-written captions/text paired with images."
          },
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 12 Benchmark Pool Details and Section 11 Benchmark Pool Selection Details",
            "Reasoning": "GATE benchmark pool includes Video Classification tasks (HMDB51, UCF-101, Kinetics400) and Video Regression task (iWildcam). These datasets consist of videos recorded by humans or devices, considered human generated data."
          }
        ]
      }
    ]
  },
  {
    "id": "2HzZIDo48o-rubric-0",
    "token_usage": {
      "prompt_tokens": 20092,
      "completion_tokens": 189,
      "total_tokens": 20281
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 and 3.2",
            "Reasoning": "The Symbolic Continuous Stimulus (SCS) representation is introduced as a novel dataset of stimuli characterized as continuous vectors sampled from Gaussian kernels over latent factor dimensions. The stimuli represent differently semantically structured symbolic spaces while maintaining a consistent shape. This dataset is synthetically generated following a principled procedure to instantiate a domain-agnostic binding problem. It is human-designed and programmatically generated per episode with varied semantic structures, representing the core inputs for the newly introduced Symbolic Behaviour Benchmark (S2B). Hence, the data is best classified as human generated tabular data (continuous-valued vectors) created by the authors."
          }
        ]
      }
    ]
  },
  {
    "id": "2myGfVgfva-rubric-0",
    "token_usage": {
      "prompt_tokens": 15960,
      "completion_tokens": 251,
      "total_tokens": 16211
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data Collection",
            "Reasoning": "The MiraData dataset is a new dataset introduced by the authors, consisting of videos collected from manually selected high-quality sources including YouTube channels, Videvo, Pixabay, and Pexels. These are real, human-captured videos, not generated by models, and the sources are explicitly specified, thus origin is known and human-generated video content."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.4 Video Captioning",
            "Reasoning": "The captions in MiraData are generated by models: initially short captions are generated using the state-of-the-art captioner Panda-70M, then are enriched by GPT-4V (a vision-language large model) to obtain dense and structured captions. The caption generation is done automatically by these models, thus the textual captions are model-generated."
          }
        ]
      }
    ]
  },
  {
    "id": "4S8agvKjle-rubric-0",
    "token_usage": {
      "prompt_tokens": 34360,
      "completion_tokens": 226,
      "total_tokens": 34586
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Environments; Section 3.2 Annotation Verification and Metric Justification; Appendix K and L",
            "Reasoning": "The paper introduces AGENTBOARD, a new benchmark consisting of nine diverse environments/tasks, which are textual environments requiring multi-turn interactions. These environments include AlfWorld, ScienceWorld, BabyAI, Jericho, PDDL, WebShop, WebArena, Tool-Query, and Tool-Operation. The authors adapt and re-annotate these environments as necessary, adding human-annotated subgoals and instructions to ensure multi-round, partially observable environment characteristics. The data (task environments, goals, subgoals, annotated progress rates) are explicitly stated as human-annotated and carefully verified (section 3.2, Appendix J) to create unified progress rate metrics and evaluation samples representative of the new benchmark. Thus, the data is textual, human-generated through environment definition, task goal setting, and annotation, as per the paper content."
          }
        ]
      }
    ]
  },
  {
    "id": "5WFzk0H27p-rubric-0",
    "token_usage": {
      "prompt_tokens": 16082,
      "completion_tokens": 124,
      "total_tokens": 16206
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract; Section 2.1 Raw data; Appendix A.2 Composition",
            "Reasoning": "The Tournesol public dataset contains structured tabular data recording over 204,000 pairwise comparisons by about 20,000 human contributors on YouTube videos' recommendability and other quality criteria. It also includes metadata like pretrust statuses and user vouches. These data are explicitly collected from human users through the Tournesol platform, making it human-generated tabular data."
          }
        ]
      }
    ]
  },
  {
    "id": "5c1hh8AeHv-rubric-0",
    "token_usage": {
      "prompt_tokens": 101138,
      "completion_tokens": 279,
      "total_tokens": 101417
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Practice in MultiTrust, and Appendix C to G describe the datasets.",
            "Reasoning": "The paper states that they constructed 20 datasets by adapting existing datasets with manual efforts and automatic methods, and proposed 8 novel datasets from scratch by collecting images from the Internet or synthesizing with models like Stable Diffusion and GPT-4V, specifically for 32 new tasks designed for evaluating trustworthiness. Collecting images from the Internet and manually labeled datasets is human generated, whereas synthesis via models is model generated. Thus, the image data includes both human generated (collected, annotated) and model generated (synthetic)."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Practice in MultiTrust and Appendices detail text data generation and prompts synthesis.",
            "Reasoning": "The text data includes manually collected or adapted questions and instructions (human generated) and text generated or rephrased by models such as GPT-4 and GPT-4V for prompt generation and query synthesis (model generated). For example, prompts are generated by GPT-4 for certain tasks, and questions are manually designed for others. Hence, text data origins span human and model-generated sources."
          }
        ]
      }
    ]
  },
  {
    "id": "67N3FWoDtU-rubric-0",
    "token_usage": {
      "prompt_tokens": 15865,
      "completion_tokens": 213,
      "total_tokens": 16078
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3 The Chronicling Germany Dataset; Abstract; Introduction",
            "Reasoning": "The dataset consists of 581 manually annotated high-resolution scanned newspaper pages from historic German newspapers dating between 1852 and 1924. The annotations are polygon regions capturing the layout on image scans of the pages. The annotations were performed over 1,500 hours by expert human annotators, confirming human-generated origin for the image modality of the scanned pages."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3 The Chronicling Germany Dataset; Abstract; Introduction",
            "Reasoning": "The dataset includes ground truth text annotations in the form of transcribed text lines. These text lines include polygons and baselines and have been corrected by expert annotators after automatic OCR generation. The correction involved manual verification and adjustment, indicating human-generated text data based on manual transcription corrections for ground truth."
          }
        ]
      }
    ]
  },
  {
    "id": "6cCFK69vJI-rubric-0",
    "token_usage": {
      "prompt_tokens": 20621,
      "completion_tokens": 146,
      "total_tokens": 20767
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "time series",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1, Section 3.2, Abstract",
            "Reasoning": "The Building TimeSeries (BTS) dataset is introduced as a new dataset consisting of over ten thousand timeseries data points collected from sensors and other instrumentation in three real-world buildings over a three-year period. The data is collected via the Building Management Systems using CSIRO\u2019s Data Clearing House platform and Message Queuing Telemetry Transport Secured (MQTTS) protocol. Therefore, it is captured manually by data acquisition hardware, not generated by models, and its origin is explicitly detailed and not unknown."
          }
        ]
      }
    ]
  },
  {
    "id": "7TCK0aBL1C-rubric-0",
    "token_usage": {
      "prompt_tokens": 15628,
      "completion_tokens": 150,
      "total_tokens": 15778
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.2 and 2.3",
            "Reasoning": "The paper explicitly states that the IaC-Eval dataset introduced includes 458 human-curated scenarios, each consisting of natural language problem descriptions ('prompts'), user intent specifications written in the Rego language, and example correct Terraform HCL configurations. These are manually created by domain experts familiar with IaC and AWS cloud services, as explained in Sections 2.2 and 2.3, and reinforced in Section 4, emphasizing that dataset creation requires in-depth cloud and infrastructure knowledge and is not derived from existing repositories or automated means."
          }
        ]
      }
    ]
  },
  {
    "id": "9tVn4f8aJO-rubric-0",
    "token_usage": {
      "prompt_tokens": 40406,
      "completion_tokens": 7,
      "total_tokens": 40413
    },
    "response": [
      {
        "sources": []
      }
    ]
  },
  {
    "id": "AdpSHMOujG-rubric-0",
    "token_usage": {
      "prompt_tokens": 16111,
      "completion_tokens": 591,
      "total_tokens": 16702
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4, 'Image collection and annotation', and Appendix A",
            "Reasoning": "The MatSeg benchmark dataset consists of 1,220 real-world images manually collected by the authors from diverse real-world scenes covering multiple domains and materials states, as explicitly stated in Section 4. The images are raw visual data captured in the wild and used for evaluation and annotation, thus are human-generated real images."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 and 3.2, Figures 1 and 2, and Appendix A",
            "Reasoning": "The synthetic dataset introduced by the authors is composed of 2D and 3D synthetic scenes created by infusing patterns automatically extracted from real-world images into synthetic scenes, then rendered using a physics-based renderer (Blender Cycles). These scenes include materials mapped onto objects with realistic SVBRDF/PBR materials, produced at scale without manual annotation. The 3D scenes are fabricated using procedural methods, Blender rendering, and mapped textures/materials. Thus, the synthetic images are model-generated."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": true,
            "Reference": "Section 3.1 and 3.2, Appendix A",
            "Reasoning": "The authors use images from the Open Images v7 dataset [35] and Segment Anything dataset [34] as the source corpus from which patterns and textures (including SVBRDF/PBR materials) are automatically extracted in an unsupervised manner. The origin of these source images is public datasets with web-crawled images of unclear provenance. These source images themselves are not introduced as new datasets but serve as the basis for pattern extraction. Hence, the extracted pattern maps and materials derived are considered unknown origin in terms of direct origin."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": true,
            "Reference": "Section 3.2 and Appendix A",
            "Reasoning": "The paper introduces a repository of over 300,000 extracted textures and SVBRDF/PBR materials derived by automated unsupervised texture extraction and material property map generation from real-world images (from existing datasets), not manually created or model synthesized. As these are derived from images of unknown provenance, the overall data origin is unknown origin. This repository is introduced as a new resource by the authors."
          }
        ]
      }
    ]
  },
  {
    "id": "BZe6dmDk5K-rubric-0",
    "token_usage": {
      "prompt_tokens": 33371,
      "completion_tokens": 175,
      "total_tokens": 33546
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Abstract; Section 3 Dataset Acquisition; Table 1; Section 3.1 Prompt Sources and Text-to-Video Models; Section 3.3 Subjective Action Quality Assessment",
            "Reasoning": "The GAIA dataset contains 9,180 AI-generated videos created by 18 text-to-video generative models, including both open-source and commercial applications, as described primarily in the Abstract, Section 3, and Table 1. These videos are generated by models, not recorded by humans, hence the modality is video, and origin is Model Generated (true). The dataset is newly introduced by the authors specifically for AI-generated video action quality assessment. There is no indication that videos were captured from real human performance or of unknown origin."
          }
        ]
      }
    ]
  },
  {
    "id": "D1MOK2t2t2-rubric-0",
    "token_usage": {
      "prompt_tokens": 10974,
      "completion_tokens": 335,
      "total_tokens": 11309
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Demonstrations Dataset and Table 1",
            "Reasoning": "The Demonstrations Dataset contains 13,928 videos of human players completing BASALT tasks, captured as high-resolution image observations along with keyboard and mouse actions for each frame. These videos were manually created by human data collectors playing Minecraft, making them human generated video data."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Demonstrations Dataset",
            "Reasoning": "Each demonstration trajectory consists of sequences of state-action pairs, where each state is a 640x360 RGB frame from the player's perspective in Minecraft. These frames are images generated by humans playing the game and recorded during gameplay sessions."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Demonstrations Dataset",
            "Reasoning": "The action data consists of keyboard and mouse inputs (K and M) reflecting the human player's interactions. These input signals are recorded directly from human interactions during gameplay, thus they are human generated sensor-like data."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Evaluation Dataset, Table 2",
            "Reasoning": "The Evaluation Dataset includes human evaluations with natural language justifications for selecting better performing agents in pairwise comparisons. These textual justifications are manually written by human evaluators (MTurk workers), thus human generated text."
          }
        ]
      }
    ]
  },
  {
    "id": "DFr5hteojx-rubric-0",
    "token_usage": {
      "prompt_tokens": 91916,
      "completion_tokens": 218,
      "total_tokens": 92134
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Sections 2.1 and 2.2 (Survey and Conversations), Appendix A (Data Access and Format), and Appendix B (Data Statement)",
            "Reasoning": "The PRISM dataset introduces new data collected from 1,500 diverse human participants who completed detailed surveys about their demographics and preferences and engaged in 8,011 live multi-turn conversations with 21 different large language models. The dataset consists of multiple types of textual data: participant self-written profiles (self_description), participant-written system strings (system_string), participant prompts (user_prompt), model responses (model_response), and open-ended participant feedback (open_feedback). This data is text modality, with participant-generated text such as prompts, survey responses, and feedback being human-generated, and the LLM responses in conversation being model-generated text explicitly produced by AI models. This is all clearly stated in Sections 2.1 and 2.2, and detailed in Appendix A and B."
          }
        ]
      }
    ]
  },
  {
    "id": "DJVyRhT8nP-rubric-0",
    "token_usage": {
      "prompt_tokens": 26755,
      "completion_tokens": 485,
      "total_tokens": 27240
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.2 HA-R2R Dataset and Appendix C.1",
            "Reasoning": "The Human-Aware Room-to-Room (HA-R2R) dataset extends the original Room-to-Room dataset by incorporating human activity descriptions. Instructions were generated by combining original instructions, human activity descriptions, and relative position information using GPT-4 via a prompt engineering approach, followed by human validation. Hence, the dataset's instructions are both human-generated (validation and original instructions) and model-generated (GPT-4 generated instructions)."
          },
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.1 HAPS Dataset",
            "Reasoning": "The Human Activity and Pose Simulation (HAPS) dataset consists of 145 human activity descriptions which were transformed by a Motion Diffusion Model into 435 detailed 3D human motion sequences represented by SMPL models over time (time series of 3D mesh vertices with shape and pose parameters). The motions were generated by the diffusion model, a learned model, thus model-generated rather than human-captured."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.1 Realistic Human Rendering",
            "Reasoning": "The images used in the Human-Aware 3D (HA3D) simulator are photorealistic renderings derived from integrating model-generated dynamic human 3D mesh sequences with indoor environment scenes from Matterport3D. Since the images are rendered by the simulator using 3D models and rendering software, they are computer-generated but not by an AI model per se, so neither human-generated nor model-generated; rather, they are synthetic simulation-generated data from the environment and models."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.5 Evaluation on Real-World Robots",
            "Reasoning": "The real-world robot experiments capture RGB images via stereo fisheye cameras and distance via ultrasonic sensors and inertia measurements via IMU; these are sensor signals captured from the real environment, so the signal data are human-captured through sensors, i.e., human-generated modality as data directly captured from physical devices."
          }
        ]
      }
    ]
  },
  {
    "id": "DjCSjizgsH-rubric-0",
    "token_usage": {
      "prompt_tokens": 15575,
      "completion_tokens": 631,
      "total_tokens": 16206
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Simulation Data",
            "Reasoning": "The Sim2Real-Fire dataset contains 1 million simulated wildfire scenarios. The satellite images with annotated fire regions are generated by wildfire simulators (FARSITE, WFDS, WRF-SFIRE) producing simulated satellite image sequences with fire regions and smoke, replicating the satellite view of fire spread."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.3 Real-world Data",
            "Reasoning": "The dataset includes 1,000 real-world wildfire scenarios with satellite image sequences captured by Landsat-8 and Sentinel-2 satellites. Human annotators manually label the fire regions on the satellite images to produce binary masks of wildfire regions."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Simulation Data; Section 2.1 Data Modalities of Environmental Information",
            "Reasoning": "Weather data for the simulation scenarios are generated by simulators. These include temperature, relative humidity, precipitation, wind speed/direction, and cloud cover collected or simulated as hourly sequences forming tabular time series data."
          },
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.3 Real-world Data; Section 2.1 Data Modalities of Environmental Information",
            "Reasoning": "Real-world weather data are collected from Remote Automatic Weather Stations and included as tabular hourly data in the dataset for real wildfire scenarios."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Simulation Data; Section 2.1 Data Modalities of Environmental Information",
            "Reasoning": "Topography, vegetation, and fuel maps for simulated scenarios are generated or derived by combining real-world sources (e.g. LANDFIRE) with simulation parameters, thus treated as simulated/generated data associated with simulated wildfire scenarios."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.3 Real-world Data; Section 2.1 Data Modalities of Environmental Information",
            "Reasoning": "For real wildfire scenarios, topography, vegetation, and fuel maps are collected from data sources such as LANDFIRE, representing real-world geographic and environmental data included in the dataset."
          }
        ]
      }
    ]
  },
  {
    "id": "E18kRXTGmV-rubric-0",
    "token_usage": {
      "prompt_tokens": 19871,
      "completion_tokens": 194,
      "total_tokens": 20065
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Annotation Process; Section 2.1 Dataset Collection Design",
            "Reasoning": "The CVQA dataset includes culturally diverse images selected by native annotators; images are mainly collected from annotators' personal photos or open-license sources. Annotators were instructed to use self-made or culturally relevant images for each Country-Language pair, ensuring the images represent local culture."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Annotation Process; Appendix A Annotation Guidelines",
            "Reasoning": "For each image, annotators created 1-3 culturally relevant questions and multiple-choice answers written in the local language and manually translated to English. These questions and answers are human-generated by fluent native speakers familiar with the culture, as per the detailed annotation guidelines."
          }
        ]
      }
    ]
  },
  {
    "id": "EADRzNJFn1-rubric-0",
    "token_usage": {
      "prompt_tokens": 27895,
      "completion_tokens": 247,
      "total_tokens": 28142
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "graph",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4, Datasets; Section C Dataset Documentation and Intended Use",
            "Reasoning": "The paper introduces eight novel datasets within TGB 2.0, four Temporal Knowledge Graphs (TKG) and four Temporal Heterogeneous Graphs (THG), all representing multi-relational temporal graphs, i.e. graph data with temporal and relational structure. These datasets are constructed from various external sources such as Wikidata, POLECAT, ICEWS, GitHub data, Reddit interactions, and Myket app market data. This clearly classifies the data modality as 'graph'. The data is not model generated; it is collected from real-world sources or databases, hence 'Human Generated' is false for manual creation, but the data is originally generated by humans or human activity recorded digitally, so we consider 'Human Generated' as true in the sense that data reflects human activity. 'Model Generated' is false since no synthetic or AI-model generated data is introduced. The provenance is specified and comes from external well-defined sources rather than unknown web crawl, so 'Unknown Origin' is false."
          }
        ]
      }
    ]
  },
  {
    "id": "FXTeJvHE0k-rubric-0",
    "token_usage": {
      "prompt_tokens": 14649,
      "completion_tokens": 156,
      "total_tokens": 14805
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset and Filtering for Challenging Scenes",
            "Reasoning": "The NAVSIM framework uses the OpenScene dataset, which is a redistribution of nuPlan, the largest publicly available driving dataset containing 120 hours of real-world driving data recorded by multiple sensors including eight cameras and LiDAR. This sensor data is human-generated as it was captured by real sensor rigs during vehicle operation. NAVSIM authors introduce new standardized and challenging training and testing splits (navtrain and navtest) of this dataset by filtering for non-trivial driving scenarios, thus constituting a new dataset split curated and introduced by the authors for benchmarking purposes."
          }
        ]
      }
    ]
  },
  {
    "id": "GHlJM45fWY-rubric-0",
    "token_usage": {
      "prompt_tokens": 20170,
      "completion_tokens": 558,
      "total_tokens": 20728
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Species Observation Data",
            "Reasoning": "The dataset includes Presence-Absence (PA) survey records representing exhaustive botanical surveys conducted by experienced botanists in small spatial plots, where plant species presence is manually observed and recorded."
          },
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Species Observation Data",
            "Reasoning": "Presence-Only (PO) occurrence records derive primarily from citizen science platforms, representing geolocated species observations reported manually by human observers without controlled sampling protocols."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3 Satellite Images",
            "Reasoning": "Sentinel-2 RGB and Near-Infrared (NIR) satellite images at 10m resolution are included in the dataset, acquired from remote sensing satellites and preprocessed for the dataset."
          },
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3.1 Satellite Time-series",
            "Reasoning": "The dataset provides time series data of satellite-derived spectral bands (from Landsat) over 20 years, representing quarterly median point values, acquired and processed from satellite sensors."
          },
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3.2 Climatic Variables",
            "Reasoning": "Monthly climatic rasters over 20 years containing temperature and precipitation data are included as time series scalar values for each observation point, derived from preexisting climatic datasets such as CHELSA."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2.1 Environmental Rasters",
            "Reasoning": "Environmental predictor data such as elevation, soil properties, land cover, and human footprint rasters are included, derived from various publicly available geospatial datasets and processed to form a set of spatially aligned environmental covariates."
          }
        ]
      }
    ]
  },
  {
    "id": "HB5q6pC5eb-rubric-0",
    "token_usage": {
      "prompt_tokens": 22680,
      "completion_tokens": 341,
      "total_tokens": 23021
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 and 4 Experiments, specifically 4.1 Knowledge Invariance Verification for Perturbations and 4.2 Real Knowledge Capacity Evaluation",
            "Reasoning": "The paper introduces PertEval, a new evaluation toolkit that generates knowledge-invariant perturbations on existing static multiple-choice test datasets (from MMLU subsets: College Mathematics, World History, Professional Psychology, and Professional Medicine). These perturbations are generated using a content-level paraphrasing algorithm that uses a Large Language Model (LLM) rewriter (model generated) to restate original human-generated multiple-choice questions (human generated text) sentence-by-sentence to maintain the knowledge while altering irrelevant details, thus creating new test samples per original question. The resulting data consists of paraphrased question texts and variations with format-level perturbations (e.g., option permutations) applied algorithmically. Therefore, the new dataset is composed of text data that is both human generated originally and model generated during paraphrasing to create new knowledge-invariant perturbed question sets derived from original human-authored questions in the MMLU benchmark. These are introduced and described in Section 3.1 under 'Content-level perturbation: knowledge-invariant paraphrasing' and experimentally evaluated in Section 4. The data leveraged (MMLU subsets) are pre-existing, but the perturbed data samples introduced by PertEval are new dataset entries created by the authors. Hence, the new dataset introduced is text modality and has mixed human (original) and model-generated (rewritten perturbations) origin."
          }
        ]
      }
    ]
  },
  {
    "id": "IZtX4RNBeH-rubric-0",
    "token_usage": {
      "prompt_tokens": 12719,
      "completion_tokens": 327,
      "total_tokens": 13046
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": true,
            "Reference": "Section 3.2 Building CVRR-ES Benchmark",
            "Reasoning": "The benchmark uses 214 unique videos collected from multiple sources: around 60% from public academic datasets (e.g., Something-Something-v2, CATER, Charades, ActivityNet, HMDB51, YFCC100M) and the remaining 40% from internet sources. These videos are original videos, not generated by models, but since about 40% are collected from the internet, the exact provenance is less explicitly specified, leading to classification as Unknown Origin."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Building CVRR-ES Benchmark, Stage 2 and Stage 3",
            "Reasoning": "The dataset includes 2,400 open-ended question-answer (QA) pairs curated for video understanding evaluation. Captions for videos collected from the internet are human generated, while captions for videos from academic datasets are sometimes generated by annotators based on provided metadata. The QA pairs are generated by GPT-3.5 LLM based on ground-truth captions (model generated), followed by manual human filtration to remove noise and ensure quality (human generated). Hence, text data contains both model-generated and human-generated components."
          }
        ]
      }
    ]
  },
  {
    "id": "JU0QvhhfVp-rubric-0",
    "token_usage": {
      "prompt_tokens": 13288,
      "completion_tokens": 330,
      "total_tokens": 13618
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Morphological profiles extraction",
            "Reasoning": "The MOTIVE dataset includes image-based morphological profiles extracted from the JUMP Cell Painting dataset, which comprises images of cells after chemical or genetic perturbations. These images were generated by experimental assays, i.e., humans performed the chemical and genetic perturbations and the imaging captures real cell morphologies."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Morphological profiles extraction and Section 3.3 Graph construction",
            "Reasoning": "The morphological profiles are processed feature vectors extracted from the Cell Painting images using CellProfiler software and subsequent preprocessing pipelines. These feature vectors are numerical tabular data representing cellular morphology for each compound and gene perturbation. As such, the dataset includes tabular data modality representing the morphological features for nodes."
          },
          {
            "Modality": "graph",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Annotation collection and Section 3.3 Graph construction",
            "Reasoning": "The MOTIVE dataset aggregates compound-gene, compound-compound and gene-gene interactions from seven public databases and constructs a graph where compounds and genes are nodes and their known interactions are edges. The graph structure is created by unifying these curated relationships sourced from public databases. Although these relationships are from external databases, the aggregation and curation into the MOTIVE graph is a novel contribution by the authors as part of the new dataset."
          }
        ]
      }
    ]
  },
  {
    "id": "KZlJF8kguO-rubric-0",
    "token_usage": {
      "prompt_tokens": 23126,
      "completion_tokens": 348,
      "total_tokens": 23474
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract, Section 3 Data, and Section A.4 Data acquisition and signal processing",
            "Reasoning": "The dataset includes intracranial stereoelectroencephalographic (sEEG) neural recordings obtained invasively from 10 human subjects implanted with electrodes while they watched Hollywood movies. This signal data is directly captured from human brain electrical activity using implanted sensors, not generated by models nor from unknown sources."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract, Section 3 Data, Section A.1 Audio transcription and alignment, and Section A.2 Feature annotation",
            "Reasoning": "The dataset contains movie audio transcripts that were first generated by commercial automatic speech recognition services and then extensively manually corrected by trained human annotators to ensure accuracy and word-level timing alignment. Additionally, manual annotations of universal dependencies parses, part of speech tags, speaker identities, and sentence segmentation were performed by human experts. Hence, this text data is human generated or corrected."
          },
          {
            "Modality": "video",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": true,
            "Reference": "Abstract and Section 3 Data (Task and stimuli)",
            "Reasoning": "The raw movie stimuli are commercially produced Hollywood movies and are not included in the released dataset due to copyright; only the derived annotations and neural data are released. The paper states that these movies were naturalistic stimuli shown to subjects. The original video data is not created or altered by the authors and is thus not human generated by them, and its origin is commercial with natural video content."
          }
        ]
      }
    ]
  },
  {
    "id": "L5aY1mWvXQ-rubric-0",
    "token_usage": {
      "prompt_tokens": 13373,
      "completion_tokens": 325,
      "total_tokens": 13698
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "graph",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4 Datasets and throughout the paper",
            "Reasoning": "The paper does not introduce any new dataset; instead, it uses well-known existing temporal graph datasets (wikipedia, reddit, uci) to perform experiments. These datasets represent interaction graphs with edges as events between nodes at specific times. Since the authors explicitly mention that they do not introduce new datasets but create temporally distorted versions of existing datasets for evaluation, the distorted datasets are generated from existing real data and not new datasets per se. The distorted datasets are generated using algorithmic temporal distortion techniques (INTENSE and SHUFFLE) applied to test splits of existing datasets, thus they are algorithmically generated (model generated) from human generated datasets."
          },
          {
            "Modality": "graph",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Temporal Distortion Techniques and Section 4 Experiment",
            "Reasoning": "The temporally distorted test splits of existing datasets are created by applying the proposed temporal distortion algorithms (INTENSE and SHUFFLE) to existing real datasets, thus these distorted datasets are model generated (algorithmic transformations) rather than human generated or unknown origin. The distorted data have the same node and edge information but altered temporal patterns. This transformation is explicitly described as a contribution of the paper and forms a dataset used for evaluations (sanity checks). Thus, the temporally distorted datasets qualify as new data sources of graph modality generated by the authors' algorithms (model generated)."
          }
        ]
      }
    ]
  },
  {
    "id": "LdRZ9SFBku-rubric-0",
    "token_usage": {
      "prompt_tokens": 24698,
      "completion_tokens": 397,
      "total_tokens": 25095
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Dataset Collection; Section A.1 Dataset Documentation",
            "Reasoning": "The dataset consists of international news text data including titles, content, event descriptions, and hierarchical event annotations, which are manually collected and read from public international news sources. This is human-generated text data as it originates from original news articles manually gathered and curated. The dataset includes clean text annotations such as titles, contents, and event names as stated in Sec. 3.2 and Appendix A.1."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Dataset Collection; Section A.1 Dataset Documentation",
            "Reasoning": "The dataset contains news images related to the collected international news events. These images are human-generated in the sense they are sourced from original news organizations and are not created or synthesized by AI models. The images and visual objects form a significant portion of the nodes in the multimodal knowledge graph, explicitly mentioned as \"571,791 vision-related\" nodes. Images are collected directly from public news sources."
          },
          {
            "Modality": "graph",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Construction Pipeline; Section 3.3 Dataset Statistics and Visualizations",
            "Reasoning": "The multimodal knowledge graph dataset is constructed by the authors according to the UKnow protocol using a three-phase pipeline consisting of content extraction, information symbolization, and knowledge construction. The graph data structure is generated by linking the extracted text and image nodes using associative knowledge such as semantic and visual similarity and event annotations. Hence, the graph modality is model-generated following algorithmic construction steps from the raw human-generated text and image data. This is described clearly in Section 3.1 and in dataset statistics in Section 3.3."
          }
        ]
      }
    ]
  },
  {
    "id": "LdxNWDNvC3-rubric-0",
    "token_usage": {
      "prompt_tokens": 19777,
      "completion_tokens": 370,
      "total_tokens": 20147
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 and 4.1",
            "Reasoning": "The AF-200K dataset includes 2,150 newly manually-designed supercritical airfoils from Shanghai Aircraft Design and Research Institute, which are human-created. This data is part of the new large-scale dataset combining manually designed and synthetic airfoils."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Synthetic Airfoil Generation Stage and 4.1",
            "Reasoning": "In AF-200K, the authors generated synthetic airfoils using physical parameter perturbation methods (CST-assisted generation with Latin hypercube sampling) and unconditional generative models (B\u00e9zierGAN and diffusion models). These synthetic data entries are model-generated."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": true,
            "Reference": "Section 3 and 4.1",
            "Reasoning": "The dataset includes existing datasets UIUC and NACA, which are publicly known datasets of airfoils. Although included in AF-200K, these datasets are pre-existing and represent data with unclear provenance in this paper, thus 'Unknown Origin' is true. However, per instruction, only the new dataset introduced by authors is to be listed, so actual entries with human or model origin are only those generated or manually designed by authors."
          }
        ]
      }
    ]
  },
  {
    "id": "M32Ldpp4Oy-rubric-0",
    "token_usage": {
      "prompt_tokens": 31755,
      "completion_tokens": 549,
      "total_tokens": 32304
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.2, Abstract, Introduction, Algorithm 1",
            "Reasoning": "The LogiCity dataset includes diverse RGB images rendered from simulated urban grid maps by composing icons generated by GPT-4 and a diffusion-based generative model (Stable-Diffusion XL) as detailed in Section 3.2 and Algorithm 1. These images are generated by models based on semantic concepts and agent configurations, hence model generated. They are not manually created by humans nor from unknown origins."
          },
          {
            "Modality": "graph",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.2 Visual Action Prediction, Appendix A.2",
            "Reasoning": "The dataset involves scene graphs formed from unary and binary predicate groundings of agents' attributes and relations derived from simulated environment states and images. These graphs represent the structured knowledge about entities and their relations within each scene. The graphs are programmatically constructed from the simulator and grounding models, thus not human generated or model generated per se, but derived directly from simulator state, making their origin known and controlled."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1, 4.2, Appendix A, Table A",
            "Reasoning": "Agent attributes such as locations, sizes, directions, priorities, and related parameters are provided as numerical vectors (tabular-like data) for each agent as part of the dataset, e.g., vector h_n containing location, scale, and other features. These data are generated by the simulation environment deterministically, reflecting agent states, thus not human generated or model generated but synthesized by simulator with known origin."
          },
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2, 4.1, Algorithm 1",
            "Reasoning": "The dataset includes sequences of agent states and actions over time steps forming time series data representing the dynamics of the urban environment. These time series are generated by the simulator based on user-defined concepts, rules and agents. They are simulated and rendered programmatically, not manually created or generated by a model without traceable source."
          }
        ]
      }
    ]
  },
  {
    "id": "MU2s9wwWLo-rubric-0",
    "token_usage": {
      "prompt_tokens": 25926,
      "completion_tokens": 192,
      "total_tokens": 26118
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.3, 2.4",
            "Reasoning": "The paper introduces ConceptMix as a new benchmark dataset consisting of automatically generated text prompts combining visual concepts. These text prompts are generated by sampling concepts and using GPT-4o to generate and validate coherent prompt descriptions. Hence the text data modality is generated by a model (GPT-4o), not manually by humans or unknown origin."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.3, 2.4, Figure 2",
            "Reasoning": "Images in ConceptMix are generated by text-to-image models given the generated text prompts. These images form the data to be evaluated in the benchmark, and are therefore generated by models, not humans or unknown origin datasets."
          }
        ]
      }
    ]
  },
  {
    "id": "Mbd3QxXjq5-rubric-0",
    "token_usage": {
      "prompt_tokens": 26496,
      "completion_tokens": 179,
      "total_tokens": 26675
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Abstract, Section 2, Table 2, and Section 2.2.3",
            "Reasoning": "The new dataset introduced is OpenMathInstruct-1, comprising 1.8 million math problem-solution pairs where problems come from existing math benchmarks (GSM8K and MATH), but solutions are newly synthesized in a code-interpreter format using the Mixtral-base open-source model. The solutions are generated via few-shot prompting with innovations such as masked text solutions to avoid trivial copying and ensure solution quality. This dataset is text modality, consisting of problem statements and their corresponding solution texts with embedded code blocks. It is model generated because the solutions were synthesized by the Mixtral language model, not manually created by humans nor from unknown sources."
          }
        ]
      }
    ]
  },
  {
    "id": "OL2JQoO0kq-rubric-0",
    "token_usage": {
      "prompt_tokens": 21563,
      "completion_tokens": 266,
      "total_tokens": 21829
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1, 3.4",
            "Reasoning": "The QUILT dataset images are derived from narrative-style educational histopathology videos on YouTube featuring expert pathologists who manually captured and presented these images. The images extracted correspond to histopathology visual content from videos, which are recorded by humans and represent real histopathology tissue samples."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1, 3.4",
            "Reasoning": "The text data consists of transcriptions of expert clinicians' spoken vocal descriptions (narrations) in the YouTube videos. Although transcribed by the automatic speech recognition (ASR) system and corrected with LLMs, the original spoken content is human-generated expert descriptions explaining histopathology images."
          },
          {
            "Modality": "audio",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1",
            "Reasoning": "The source videos on YouTube include expert clinicians' narrated audio tracks, captured during educational presentations. These audio signals are human-generated, as they are the speech recordings of expert narrators."
          }
        ]
      }
    ]
  },
  {
    "id": "OTjTKFk7gb-rubric-0",
    "token_usage": {
      "prompt_tokens": 21111,
      "completion_tokens": 202,
      "total_tokens": 21313
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 4 Pre-Generated Dataset Based on the Environment; Appendix B Datasheet for AuctionNet Dataset; Appendix C Data Format of AuctionNet Dataset",
            "Reasoning": "The paper introduces a new large-scale dataset composed of 10 million ad opportunities with associated features, values, bids, and auction results generated from the AuctionNet environment. This dataset is explicitly described as pre-generated based on their environment module, which uses a deep generative network for generating ad opportunity features (Section 3.1) and value predictions. The dataset entries are structured with multiple tabular fields (as detailed in Appendix C), including numeric, categorical, and one-hot encoded fields representing advertiser features and auction outcomes. The data is not manually created by humans but generated by a trained deep generative model and simulation environment. Therefore, the modality is tabular, and its origin is model generated."
          }
        ]
      }
    ]
  },
  {
    "id": "PcbSZwVVc5-rubric-0",
    "token_usage": {
      "prompt_tokens": 20055,
      "completion_tokens": 256,
      "total_tokens": 20311
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "audio",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset Collection, Section 3.2 Annotation and Statistics",
            "Reasoning": "The DreamCatcher dataset introduced in the paper is collected by the authors using modified Bose QC 20 earbuds with dual microphones (feedback and feedforward) recording audio signals at 24 kHz. The recordings encompass real-world sleeping environments with 12 pairs of participants sleeping in the same room, capturing synchronous dual-channel audio data. This is a clear manual data collection process involving real human subjects, not generated or derived from models or other unknown sources."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset Collection, Section 3.2 Annotation and Statistics, Table 6 in Appendix A.1",
            "Reasoning": "The dataset includes motion data captured by an MPU6050 IMU sensor integrated into the earbuds, sampling accelerometer and gyroscope signals at approximately 94 Hz. These sensor data are collected directly from human participants during sleep, recorded manually via the hardware designed by the authors. The data is not model generated or from unknown origins."
          }
        ]
      }
    ]
  },
  {
    "id": "QIJQ1qCGqV-rubric-0",
    "token_usage": {
      "prompt_tokens": 13355,
      "completion_tokens": 339,
      "total_tokens": 13694
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "time series",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Overview, Section 3.2 Data Collection Procedure",
            "Reasoning": "The BlindWays dataset includes 3D motion data captured using wearable inertial measurement units (IMUs) from 11 blind participants navigating eight urban routes. This is continuous sensor data forming time series reflecting human motion, collected directly from human participants."
          },
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Overview, Section 3.2 Data Collection Procedure",
            "Reasoning": "The dataset includes synchronized third-person RGB videos and egocentric videos (from a chest-mounted GoPro and researcher chest-mounted smartphone) captured during the navigation routes. These videos are captured by human-operated cameras during the study."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3 Data Annotation Pipeline",
            "Reasoning": "The dataset contains detailed paired high-level and low-level textual descriptions annotated by human annotators based on review of synchronized videos and motion data, describing pedestrian motion, intent, and interactions. These annotations are manually created by motion experts and novices."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Data Collection Procedure",
            "Reasoning": "Data from wearable Xsens IMU sensors (18 IMUs on body joints and mobility aid) is collected as raw sensor signal data, representing human motion capture signals gathered during the study."
          }
        ]
      }
    ]
  },
  {
    "id": "Qf8uzIT1OK-rubric-0",
    "token_usage": {
      "prompt_tokens": 36680,
      "completion_tokens": 150,
      "total_tokens": 36830
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.2 Practical Recommendations",
            "Reasoning": "The paper explicitly recommends that datasets should be collected with explicit informed consent from each person depicted, allowing sharing of their facial, body, and biometric information for assessing fairness and robustness in human-centric computer vision. This implies creation of new datasets collected with human participation and consent, i.e., human-generated image data. The paper presents recommendations for such consent-driven datasets (e.g., Casual Conversations v2 [254]) as example of new datasets collected consensually. Hence, the modality is image, and the data is human-generated as collected directly from humans with consent."
          }
        ]
      }
    ]
  },
  {
    "id": "QpF3DFP3Td-rubric-0",
    "token_usage": {
      "prompt_tokens": 18639,
      "completion_tokens": 402,
      "total_tokens": 19041
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset characteristics; Section 3.2 Acquisition and processing",
            "Reasoning": "The Archaeoscape dataset includes orthophotos which are images obtained from contemporary aerial photography acquired manually using aircraft-mounted cameras during the 2012 and 2015 campaigns. These orthophotos are orthorectified and resampled to 0.5 m resolution, as described in Sections 3.1 and 3.2. The data acquisition was performed by human-operated equipment and the images are therefore human generated."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset characteristics; Section 3.2 Acquisition and processing",
            "Reasoning": "The dataset contains Airborne Laser Scanning (ALS) data representing ground elevation, processed into normalized Digital Terrain Models (nDTM). This data is acquired via airborne LiDAR sensors mounted on helicopter platforms during acquisition campaigns in Cambodia. The 3D point clouds and derived elevation grids are from sensor measurements collected by human-directed data acquisition efforts, hence human generated."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset characteristics; Annotation descriptions",
            "Reasoning": "The dataset includes polygon annotations of archaeological features which are drawn and field-verified manually by expert archaeologists. These annotations are provided as polygon geometries in GeoPackage format and represent semantic labels for the pixels of the images. Such annotations are human-generated data and not produced by a model."
          }
        ]
      }
    ]
  },
  {
    "id": "R4rNYJ2slJ-rubric-0",
    "token_usage": {
      "prompt_tokens": 16120,
      "completion_tokens": 328,
      "total_tokens": 16448
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data Collection; Section 6 Availability and Maintenance; Abstract; Table 1; A.1.3 Collection Process",
            "Reasoning": "The dataset consists of high-resolution satellite images collected directly from Google Maps using their public Maps Static API. This is explicitly stated in Section 3.1 and Section 6, and further explained in the Appendix A.1.3. The images were not generated by a model; they are original satellite images obtained from a publicly available mapping service. The data source is clearly known and specified as Google Maps."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Annotation; Abstract; A.1.2 Composition; A.1.4 Preprocessing/Labeling; A.2 Dataset Annotation Documentation",
            "Reasoning": "The annotations for the dataset are fine-grained instance-level labels on lane lines and their attributes, performed by a professional remote sensing imagery labeling team of approximately 50 annotators. This manual annotation process is human-generated data derived from the original satellite images and is explicitly described in Section 3.2 and the supplementary materials A.1.2 and A.2. These annotations include vectorized polylines with multiple categories and attributes."
          }
        ]
      }
    ]
  },
  {
    "id": "ScPgzCZ6Lo-rubric-0",
    "token_usage": {
      "prompt_tokens": 34490,
      "completion_tokens": 159,
      "total_tokens": 34649
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "graph",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.1 Benchmark Datasets and Appendix A.1 Datasets",
            "Reasoning": "The paper introduces the GC-Bench benchmark which uses 12 widely-used existing graph datasets for evaluation. These datasets (e.g., Cora, Citeseer, ogbn-arxiv, ACM, DBLP, NCI1, DD, ogbg-molbace, ogbg-molbbbp, ogbg-molhiv) are publicly available datasets adapted for use in the benchmark; however, these are pre-existing datasets, not new datasets introduced by the authors. Therefore, they do not qualify as new dataset sources introduced in this paper."
          }
        ]
      }
    ]
  },
  {
    "id": "USUkwg5pW6-rubric-0",
    "token_usage": {
      "prompt_tokens": 22940,
      "completion_tokens": 140,
      "total_tokens": 23080
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 4, 'Automatic Scribble Datasets' and Section 3.2 Scribble Generation Algorithm",
            "Reasoning": "The paper introduces new scribble-labeled datasets (s4Pascal, s4Cityscapes, s4KITTI360, s4ADE20K) generated by an automatic algorithm taking images and their dense semantic annotations from existing datasets and producing scribble labels. These are image data (segmentation label images and scribble coordinate sets) generated algorithmically, not drawn manually, and are based on the existing dense annotation datasets."
          }
        ]
      }
    ]
  },
  {
    "id": "UYgE9IfQIV-rubric-0",
    "token_usage": {
      "prompt_tokens": 28985,
      "completion_tokens": 321,
      "total_tokens": 29306
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "time series",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Workload Environment and Appendix E.1 Workload",
            "Reasoning": "The paper introduces new workload traces as part of the SustainDC environment, including traces from Alibaba and Google data centers configured as time series representing CPU loads over time. These traces are provided by the authors as part of their benchmark environment, hence they are human generated via measurements and introduced as new datasets."
          },
          {
            "Modality": "time series",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.2 Data Center Environment and Appendix E.2 Weather",
            "Reasoning": "The paper includes weather data files in .epw format for multiple US locations, sourced from https://energyplus.net/weather. This weather data, consisting of ambient temperature and related variables over time, is incorporated as part of the SustainDC environment by the authors, thus representing new dataset introduced with human generated time series observations."
          },
          {
            "Modality": "time series",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Workload Environment and Appendix E.3 Carbon Intensity",
            "Reasoning": "SustainDC includes carbon intensity (CI) data files for various locations, downloaded from eia.gov APIs. These tabular time series data of carbon intensity by energy source are included in SustainDC for different states as new datasets introduced by the authors; they are human generated measurements from official sources and included explicitly for benchmarking."
          }
        ]
      }
    ]
  },
  {
    "id": "VH1vxapUTs-rubric-0",
    "token_usage": {
      "prompt_tokens": 16135,
      "completion_tokens": 255,
      "total_tokens": 16390
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Sections 3 and 7",
            "Reasoning": "Mesogeos dataset is a large-scale spatio-temporal datacube with dimensions of longitude, latitude, and time, providing daily temporal resolution and 1km spatial resolution for variables related to wildfire modeling. The data are harmonized from various publicly available datasets (e.g., ERA5-Land, MODIS, EDO, Worldpop, Copernicus DEM, and EFFIS burned area) into a unified format by the authors, thus constituting a new combined dataset introduced by the paper."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3 (Data Sources and Datacube Creation)",
            "Reasoning": "The dataset includes satellite-derived raster data such as land surface temperature, vegetation indices (NDVI, LAI), burned area maps, and ignition points, all harmonized into the spatio-temporal datacube. These are geospatial raster data resembling imagery modality, and were collected from established Earth observation products and processed by the authors to create the new Mesogeos dataset."
          }
        ]
      }
    ]
  },
  {
    "id": "VSJotgbPHF-rubric-0",
    "token_usage": {
      "prompt_tokens": 11740,
      "completion_tokens": 161,
      "total_tokens": 11901
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract; Section 3 Data Collection",
            "Reasoning": "The OpenAssistant Conversations dataset introduced by the authors consists of assistant-style conversation messages that are explicitly described as human-generated and human-annotated. These messages were collected through a worldwide crowd-sourcing effort involving over 13,500 volunteers via a web app interface, where users created prompts, replies, labels, and rankings, all being text data. The dataset comprises 161,443 messages, predominantly human-submitted, in 35 languages, making text the sole modality and confirming a human-generated origin."
          }
        ]
      }
    ]
  },
  {
    "id": "VXohja0vrQ-rubric-0",
    "token_usage": {
      "prompt_tokens": 17769,
      "completion_tokens": 285,
      "total_tokens": 18054
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Dataset Instance Collection",
            "Reasoning": "The MEDCALC-BENCH dataset introduced in the paper consists of 1047 instances containing patient notes, questions, step-by-step explanations, and answers. The patient notes are either sourced from publicly available anonymized real case reports and vignettes (Open-Patients collection) or manually synthesized by individuals with medical backgrounds for calculators without eligible notes. The explanations and answers are generated using implemented templates by the authors. Therefore, the data is human-generated (real patient notes and manual synthesis) and model-generated elements are used in attribute extraction and explanation template application, but not the core data instances (patient notes and ground truth answers)."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.2 Dataset Instance Collection and 2.1 Calculation Task Curation",
            "Reasoning": "While the patient notes are human generated (from reports or manual templates), the attribute extraction from these notes was performed automatically via GPT-4 to identify necessary parameters, and explanations with step-by-step rationales are generated by implemented templates (automated generation). Thus, explanation texts and attribute extraction steps involve model-generated content, but the core instances are primarily human-generated or manually synthesized."
          }
        ]
      }
    ]
  },
  {
    "id": "WUWHVN4gxk-rubric-0",
    "token_usage": {
      "prompt_tokens": 18281,
      "completion_tokens": 224,
      "total_tokens": 18505
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3, Section 4.1, Section 4.2",
            "Reasoning": "The paper explicitly introduces a new dataset compiled from the attack phase of the capture-the-flag competition, consisting of more than 137k multi-turn conversations (chats) and details of 44 accepted defenses. These datasets are composed of textual chat interactions between human participants (attackers and defenders) and the models. The chat contents and defense prompts are generated through interactions with LLMs and filters during the competition, so the textual data includes model-generated responses, but it also includes annotations and labels from human organizers. The dataset is presented as multi-turn text conversations with metadata, confirming its text modality and that its origin includes model-generated content in a controlled competition setting rather than being human-authored immediately or scraped from the web. The dataset is explicitly described as new and released by the authors for future research (see Sections 3, 4.1, and 4.2)."
          }
        ]
      }
    ]
  },
  {
    "id": "WVQ4Clw1VD-rubric-0",
    "token_usage": {
      "prompt_tokens": 10972,
      "completion_tokens": 286,
      "total_tokens": 11258
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 MedTrinity-25M Dataset, Paragraph 1 and 3",
            "Reasoning": "The dataset consists of medical images collected from over 90 different online resources such as TCIA, Kaggle, Zenodo, etc., indicating that the image data are captured or derived manually by humans as standard medical imaging datasets. The paper states these images span 10 modalities and over 65 diseases, and includes raw medical images originally stored in DICOM or NIfTI formats converted to PNG, confirming the human origin of these images."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 MedTrinity-25M Dataset, Paragraphs 1-3 and Section 3.2 Data Construction Pipeline",
            "Reasoning": "The multigranular textual descriptions associated with each image-ROI triplet are generated by prompting medical large language models (MLLMs) such as GPT-4V and LLaVA-Med++. These descriptions include global and local disease and lesion information, ROI details, and inter-regional relationships, generated using expert grounding models, retrieval-augmented generation, and multimodal LLM prompting. Therefore, the text annotations are generated by models rather than humans."
          }
        ]
      }
    ]
  },
  {
    "id": "XBcStBjBIE-rubric-0",
    "token_usage": {
      "prompt_tokens": 29318,
      "completion_tokens": 391,
      "total_tokens": 29709
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4 (ChronoMagic-Pro), Section 3.1 (Benchmark Construction), Table 2, and Appendix C",
            "Reasoning": "The ChronoMagic-Pro dataset is a newly introduced large-scale time-lapse video-text dataset curated by the authors. It consists of 460k high-quality time-lapse videos sourced from video platforms like YouTube under CC BY 4.0 license, filtered and processed manually and automatically to ensure quality and semantic consistency. The videos represent natural and human-made processes with significant metamorphic amplitude over time. This is confirmed by the detailed description of data collection (manual search term design, crawling videos, filtering, splitting semantically consistent clips), and time-aware annotations obtained via large multimodal models. Thus, the videos are human captured/generated (natural videos), not model generated, and their origin is explicitly stated, not unknown."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 (Benchmark Construction), Section 4 (ChronoMagic-Pro), Appendix C.2, and related captioning methodology",
            "Reasoning": "The textual prompts and captions associated with the benchmark and the ChronoMagic-Pro dataset are generated by large multimodal language models (e.g., GPT-4o and ShareGPT4Video). The authors use large language models to automatically caption the videos to produce detailed physical content descriptions with large metamorphic amplitude for evaluation prompts (ChronoMagic-Bench) and dataset captions (ChronoMagic-Pro). Hence, the text data is model generated, not human manually written or unknown."
          }
        ]
      }
    ]
  },
  {
    "id": "ZDvXY56DeP-rubric-0",
    "token_usage": {
      "prompt_tokens": 18361,
      "completion_tokens": 148,
      "total_tokens": 18509
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.1 Content (multiple paragraphs), especially where ORLB dataset description and tracked experiment runs are detailed.",
            "Reasoning": "ORLB is described as a large dataset of more than 25,000 RL experimental runs, each tracked over time including episodic return and various metrics, stored on Weights and Biases. The data consists of recorded metrics through the learning process, thus time series data of training runs. This data is generated automatically by running RL experiments and logging the metrics, not manually generated or model generated. The authors explicitly introduce ORLB as their new dataset in the paper."
          }
        ]
      }
    ]
  },
  {
    "id": "aTXhTD44nF-rubric-0",
    "token_usage": {
      "prompt_tokens": 22981,
      "completion_tokens": 216,
      "total_tokens": 23197
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Collection of Reddit Conversation Threads",
            "Reasoning": "The USDC dataset consists of user conversations crawled from Reddit (22 subreddits) collected via the praw API, which are human-generated text data from user posts and comments in online forums."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Obtaining LLM Annotations",
            "Reasoning": "The user stance and dogmatism labels in USDC are generated by large language models (GPT-4 and Mistral Large) using zero-shot, one-shot, and few-shot prompting, thus these annotations are model-generated text labels and reasoning outputs over the human-generated Reddit conversations."
          }
        ]
      }
    ]
  },
  {
    "id": "aXeiCbMFFJ-rubric-0",
    "token_usage": {
      "prompt_tokens": 21582,
      "completion_tokens": 232,
      "total_tokens": 21814
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3, Table 2, and Section 3.1 Data Generation",
            "Reasoning": "The Visual CoT dataset introduced in the paper includes images from multiple existing source datasets, but the bounding box annotations highlighting key image regions and detailed reasoning steps are generated by the authors' annotation process, involving human and GPT-4 annotations. Thus, the image data is human generated as it is sourced from existing datasets, reused with human curation and annotation for bounding boxes."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data Generation and Table 2",
            "Reasoning": "The Visual CoT dataset contains question-answer pairs, and importantly, about 98k of them have detailed reasoning steps. These reasoning step annotations are generated using GPT-4 as linguistic annotator, indicating that the textual reasoning steps are model generated. The questions and answers partly come from source datasets, some are human-provided, others augmented by GPT-4."
          }
        ]
      }
    ]
  },
  {
    "id": "abXaOcvujs-rubric-0",
    "token_usage": {
      "prompt_tokens": 14391,
      "completion_tokens": 288,
      "total_tokens": 14679
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Sections 1, 3, and 4 (e.g., Abstract, Introduction, Construction Methodology, Dataset Characteristics)",
            "Reasoning": "The new dataset WikiDBs introduced consists of 100,000 multi-table relational databases extracted directly from Wikidata, a collaboratively edited knowledge base. The data is primarily structured as tables with multiple columns and rows linked via foreign keys, clearly indicating tabular modality. The data is not human manually curated tables but rather derived automatically from an openly available dump of Wikidata items by the authors' extraction and processing pipeline described in Section 3. The dataset is therefore not human-generated at the table level but curated by an automatic process from Wikidata. It is likewise not model-generated since the cell values are from Wikidata facts, not produced by AI models. The origin is not unknown because it is explicitly stated to be sourced from Wikidata, a known open database. Thus, the data modality is tabular, generated automatically by the authors' method from human-curated Wikidata content, but the final dataset tables are not themselves human or model generated, but derived with clear provenance from Wikidata. The dataset is a new asset introduced by the authors."
          }
        ]
      }
    ]
  },
  {
    "id": "aiGN4UnNM7-rubric-0",
    "token_usage": {
      "prompt_tokens": 15645,
      "completion_tokens": 267,
      "total_tokens": 15912
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Data Collection, Real Scenes",
            "Reasoning": "The authors collected raw image data in real driving scenes using multiple cameras mounted on commercial trucks, capturing frames at 10Hz. This constitutes a new dataset with sequences of images from real-world urban and highway driving scenarios."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Data Collection, Real Scenes; Section 4.2 Annotation",
            "Reasoning": "The dataset includes depth and velocity information derived from LiDAR and Radar sensors mounted on the trucks. These sensor data are collected alongside the image data and used for ground truth annotation of 3D bounding boxes and the estimation of TTC, hence part of the new dataset."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Data Collection, NeRF Scenes",
            "Reasoning": "To augment rare small TTC cases, the authors generate artificial image sequences using Neural Radiance Fields (NeRF) rendering methods. These synthetic image sequences are model-generated data included as part of the new dataset to supplement training data."
          }
        ]
      }
    ]
  },
  {
    "id": "b6IBmU1uzw-rubric-0",
    "token_usage": {
      "prompt_tokens": 30242,
      "completion_tokens": 287,
      "total_tokens": 30529
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2 CARES Datasets, Table 13 Dataset statistics",
            "Reasoning": "The CARES benchmark is newly curated by the authors from multiple existing open-source datasets, including 18K images with 41K question-answer pairs. These images are captured medical images across 16 medical imaging modalities and 27 anatomical regions, hence 'image' is the modality. The images themselves come from existing open-source datasets; the authors combined and curated these datasets for the benchmark but did not generate the images via models. Thus, the data is human-generated (medical images captured from patients), not model-generated, and the origin is known (selected from identified datasets)."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2 CARES Datasets - Construction of QA Pairs, Appendix C",
            "Reasoning": "The benchmark contains about 41K question-answer pairs, some of which are open-ended questions constructed from medical reports and descriptions by the authors using GPT-4. They used GPT-4 to transform reports into QA pairs and also to self-check and refine QA pairs, resulting in a substantial portion of QA text data being generated by the model (GPT-4). Thus, the 'text' modality QA data is model-generated."
          }
        ]
      }
    ]
  },
  {
    "id": "bAaM8cKoMl-rubric-0",
    "token_usage": {
      "prompt_tokens": 28218,
      "completion_tokens": 174,
      "total_tokens": 28392
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Abstract; Section 2; Section 4",
            "Reasoning": "The MindSet: Vision toolbox consists of a collection of 30 image datasets systematically manipulated to test psychological hypotheses. The stimuli are pre-generated images or generated programmatically using provided scripts. The datasets are procedurally generated images parameterized for different experimental conditions, not captured or created by humans manually, nor produced by a model, but generated via code we can classify as 'model generated' for datasets as produced by authors' scripts rather than human artwork. The paper specifies that stimuli were created by the authors (with some exceptions noted) for the purpose of testing DNNs on psychological experiments, thereby qualifying as new datasets introduced in the paper, all in image modality."
          }
        ]
      }
    ]
  },
  {
    "id": "cLga8GStdk-rubric-0",
    "token_usage": {
      "prompt_tokens": 14504,
      "completion_tokens": 169,
      "total_tokens": 14673
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3, especially 3.1 Format and Selection of the Linguistic Olympiad Puzzles; Section 3.2 Data Collection and Structure; and Abstract",
            "Reasoning": "The LINGOLY benchmark introduced is composed of linguistic puzzles (texts) originally authored by human experts (UKLO puzzle authors). The data consists entirely of textual questions and contexts, including preambles, contexts, and questions with their answers. These were manually parsed from PDF files into text-based JSON format for machine readability. The benchmark explicitly excludes images, diagrams, and non-Latin scripts to maintain a unimodal text dataset. The puzzles are original human-generated content, adapted for the benchmark without model generation or unknown sources."
          }
        ]
      }
    ]
  },
  {
    "id": "cR3T1ZYN8I-rubric-0",
    "token_usage": {
      "prompt_tokens": 34884,
      "completion_tokens": 103,
      "total_tokens": 34987
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.2 Datasets; throughout the paper describing evaluation on SeqDeepFake and R-Splicer datasets",
            "Reasoning": "The paper evaluates on existing datasets, none of which are introduced as new datasets by the authors; SeqDeepFake and R-Splicer are published datasets used for benchmarking in the study, furthermore the authors do not introduce any new datasets in the paper."
          }
        ]
      }
    ]
  },
  {
    "id": "cu8FfaYriU-rubric-0",
    "token_usage": {
      "prompt_tokens": 27858,
      "completion_tokens": 332,
      "total_tokens": 28190
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2 Method, Appendix A Participant Recruitment and Table 2",
            "Reasoning": "The paper reports interviews with 30 authors who have previously curated at least one fair ML dataset, including datasets focused on language, vision, multi-modal, and tabular data. This demonstrates that the authors are engaging with real new datasets they have collected, involving human-generated data sources such as text and images collected and annotated by humans."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2 Method and Table 2; Section 3.3 Implementation (especially 3.3.1 and 3.3.2)",
            "Reasoning": "Participants curated vision datasets consisting of images, where data collection involved manually gathering images (e.g., from the web with manual curation or human annotators) and annotating them with human effort. The paper describes challenges involved in collecting images from underrepresented regions, diverse annotators labeling images, and mentions that synthetic data was not widely trusted, implying the reliance on human-generated images."
          },
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2 Method and Table 2",
            "Reasoning": "Some participants worked on datasets in the 'Other' category which includes tabular datasets. Given that tabular data likely comes from human-generated records or collected data, and since the participants curated their own datasets, this data is human generated and not model generated."
          }
        ]
      }
    ]
  },
  {
    "id": "cy8mq7QYae-rubric-0",
    "token_usage": {
      "prompt_tokens": 80465,
      "completion_tokens": 234,
      "total_tokens": 80699
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 and throughout Section 3",
            "Reasoning": "The CharXiv dataset contains 2,323 real-world charts sourced directly from arXiv preprints between 2020 and 2023. The charts are re-rendered from vector-based source files into high-resolution JPEG images. This data is explicitly collected by the authors and manually curated to include diverse and natural charts, as detailed in Section 3.1."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 and 3.3",
            "Reasoning": "CharXiv includes human-curated questions and answers for each chart, where each chart is paired with four descriptive questions and one reasoning question. These questions and validated ground-truth answers are manually annotated by graduate students to ensure quality and diversity without advanced domain-specific knowledge. Thus, the text data (questions and answers) is human generated, as described in Section 3.2 and 3.3."
          }
        ]
      }
    ]
  },
  {
    "id": "d1Pup4gkWf-rubric-0",
    "token_usage": {
      "prompt_tokens": 13129,
      "completion_tokens": 89,
      "total_tokens": 13218
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.3 Acquiring Human Demonstration From Videos",
            "Reasoning": "The paper describes acquiring human demonstration data via motion reconstruction from Internet videos using TRAM method (Section 4.3). These videos are manually created and captured by humans and are leveraged for motion prior learning in simulated humanoids."
          }
        ]
      }
    ]
  },
  {
    "id": "dVaWCDMBof-rubric-0",
    "token_usage": {
      "prompt_tokens": 22074,
      "completion_tokens": 252,
      "total_tokens": 22326
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2, 3.3, and 5.1",
            "Reasoning": "The new dataset COMMONPOOL is constructed by the authors from Common Crawl, a public web crawl, and consists of image-url and alt-text pairs, where images exist on the web (human generated). The dataset is curated by downloading images from URLs extracted, applying safety filtering, deduplication with evaluation test sets, and face blurring to protect privacy, resulting in a large pool of image-text pairs. This is a new dataset introduced by the authors specifically for the filtering track of DATACOMP."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2, 3.3, and 5.1",
            "Reasoning": "The accompanying text in COMMONPOOL consists of alt-text extracted from the web pages in Common Crawl; these captions are human generated as they are original web text descriptions from web pages, not generated by models. This text forms part of the image-text pairs that comprise the COMMONPOOL dataset, newly introduced by the authors."
          }
        ]
      }
    ]
  },
  {
    "id": "g7OX2sOJtn-rubric-0",
    "token_usage": {
      "prompt_tokens": 45541,
      "completion_tokens": 134,
      "total_tokens": 45675
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4, Data Extraction; Section 4, LeanDojo Benchmark; Appendix B.1 Dataset Format",
            "Reasoning": "LeanDojo Benchmark is a new dataset introduced in this paper, consisting of 98,734 theorems and their proofs extracted from Lean's math library (mathlib). The data includes textual code representations of definitions, theorems, proofs, and premises. This dataset is human-written formal mathematical proofs in Lean, extracted from the mathlib repository, thus is human generated and textual in modality."
          }
        ]
      }
    ]
  },
  {
    "id": "gg3POFjqq8-rubric-0",
    "token_usage": {
      "prompt_tokens": 11256,
      "completion_tokens": 136,
      "total_tokens": 11392
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 4, 'The Benchmarking Dataset' and throughout the paper describing the generation process",
            "Reasoning": "The paper introduces a new benchmark dataset generated by applying LoRA adapters to diffusion models to produce images of ImageNet classes under 14 different nuisance shifts at varying severity levels. The dataset consists of synthetic images generated explicitly for this work to allow continuous and controlled nuisance shifts. The data is generated using Stable Diffusion 2.0 with LoRA adapters learned for nuisance shifts, thus is model generated synthetic image data used for robustness benchmarking."
          }
        ]
      }
    ]
  },
  {
    "id": "h8LuywKj6N-rubric-0",
    "token_usage": {
      "prompt_tokens": 44301,
      "completion_tokens": 247,
      "total_tokens": 44548
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.2 - GUI Video Collection and Keyframe Annotation Process",
            "Reasoning": "The GUI-WORLD dataset includes over 12,000 GUI videos collected via screen recordings performed by student workers and curated instructional videos from YouTube, as manually recorded and carefully segmented into clips representing GUI tasks."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.2 - GUI Video Collection and Keyframe Annotation Process",
            "Reasoning": "Human annotators manually extract keyframes from GUI videos with detailed annotations describing GUI elements and user actions to ensure high-quality data for model training and evaluation."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.3 - GUI Tasks Generation from Human-MLLM Collaboration",
            "Reasoning": "Text data including captions, questions (free-form, multiple choice, conversational), and answers are produced through a collaborative pipeline where human annotators and GPT-4V collaboratively generate and refine textual annotations to create diverse task formats for GUI understanding."
          }
        ]
      }
    ]
  },
  {
    "id": "hcOq2buakM-rubric-0",
    "token_usage": {
      "prompt_tokens": 33282,
      "completion_tokens": 172,
      "total_tokens": 33454
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Sections 1, 3, 4, and Appendix J.2",
            "Reasoning": "The paper introduces BetterBench, a benchmark assessment framework and a living repository for AI benchmarks. The data involved are structured assessment criteria, checklist templates, scoring results, and related documentation, all in textual form. The authors manually reviewed and scored 24 benchmarks using publicly available information (papers, websites, GitHub repositories), and the results are provided as text. There is no indication of new datasets with other modalities such as image, audio, or tabular data introduced by the authors. The data generated and released (assessment scores, criteria) are human-generated textual data, as explicitly described in the methodology and Appendix J.2."
          }
        ]
      }
    ]
  },
  {
    "id": "iSwK1YqO7v-rubric-0",
    "token_usage": {
      "prompt_tokens": 97502,
      "completion_tokens": 355,
      "total_tokens": 97857
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3 Dataset Annotations and Benchmark Implementations, Appendix M.2 and M.3",
            "Reasoning": "The dataset used in this paper consists of newly annotated data for two simulators: BEHAVIOR and VirtualHome. The paper details that the authors manually annotated task plans, goals, action trajectories, and transition models based on existing simulator data, supplemented with human curation and validation through simulator execution (Section 3, Appendix M.2 and M.3). Thus, this data is text modality describing symbolic goals, trajectories, and PDDL models, and is human-generated, not model-generated or of unknown origin."
          },
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3 Dataset Annotations and Benchmark Implementations, Appendix M.2.1",
            "Reasoning": "The BEHAVIOR dataset includes accompanying demonstration videos of human executions of tasks in VR (Section 3, Appendix M.2.1). These videos are human-generated recordings of humans performing the tasks and are used as reference for annotations."
          },
          {
            "Modality": "graph",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section K Further Discussion on Visual Information in Our Benchmark",
            "Reasoning": "The paper mentions the use of scene graphs as an abstraction of the environment input to the LLMs. These scene graphs represent the semantic relationships of objects and states in the environment and are used as inputs or visual relational representations. While scene graphs are derived from simulator state or perception models, they are automatically generated and thus considered as model-generated graphs rather than human-generated."
          }
        ]
      }
    ]
  },
  {
    "id": "iTUlYblV0K-rubric-0",
    "token_usage": {
      "prompt_tokens": 19740,
      "completion_tokens": 165,
      "total_tokens": 19905
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 4 - Remastering MQuAKE",
            "Reasoning": "The paper introduces MQuAKE-REMASTERED, a fixed and improved version of the original MQuAKE dataset. The dataset consists of multi-hop question-answering pairs where questions are generated by prompting GPT-3.5-turbo model multiple times to produce natural language instructions from chains of triples extracted from a filtered Wikidata RDF knowledge graph. Hence, the dataset includes both human knowledge (Wikidata curated knowledge triples) and AI-generated text (multi-hop question instructions generated by GPT-3.5-turbo). The dataset is newly introduced by the authors as a corrected version and extension of MQuAKE."
          }
        ]
      }
    ]
  },
  {
    "id": "iwC19lVBoq-rubric-0",
    "token_usage": {
      "prompt_tokens": 13111,
      "completion_tokens": 188,
      "total_tokens": 13299
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset Construction Pipeline",
            "Reasoning": "The AVSET-10M dataset is constructed by filtering and processing video clips from existing large-scale datasets AudioSet and Panda-70M. Both datasets provide videos from open-domain YouTube content originally captured and uploaded by humans, thus the video modality is human generated and not model generated or of unknown origin."
          },
          {
            "Modality": "audio",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset Construction Pipeline",
            "Reasoning": "The dataset contains audio extracted from the videos sourced from AudioSet and Panda-70M, which are from real-world videos uploaded by humans. The audio is therefore human generated, captured naturally with the videos, and not generated by any model nor unknown in origin."
          }
        ]
      }
    ]
  },
  {
    "id": "j2wasUypqN-rubric-0",
    "token_usage": {
      "prompt_tokens": 19345,
      "completion_tokens": 383,
      "total_tokens": 19728
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Testsuites",
            "Reasoning": "The paper introduces the 'MetaBox testsuite' as a new dataset consisting of over 300 problem instances for benchmarking MetaBBO-RL methods. These include 24 synthetic functions taken from the COCO platform (with modifications), 30 noisy synthetic functions from COCO with noise models, and 280 protein-docking problem instances from the Protein-Docking benchmark 4.0 with adaptations. The data consists of problem instances and their evaluation functions, which are parameterized datasets represented in tabular (numerical) form rather than raw text or images. The datasets are not human-generated manually but derived from synthetic continuous function definitions and computational protein-docking instances\u2014thus they are algorithmically generated datasets (not from human manual generation, nor model-generated data, nor uncertain provenance)."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3 Baseline library",
            "Reasoning": "The paper presents a Baseline Library included in MetaBox comprising implementations of 19 optimization algorithms, including classic optimizers, MetaBBO-RL methods, and one MetaBBO-SL method. These consist of algorithm code implementations and benchmark algorithm specifications for numerical optimization tasks. Although these baselines are code and algorithmic in nature, the data associated with each baseline's internal logic and configuration are structured algorithmic parameters and procedural data (akin to tabular or structured data). This baseline collection is explicitly created by the authors to enable comparison and evaluation in MetaBox, thus considered a new dataset of algorithmic baseline configurations for benchmarking. It is not human-generated data per se, but not AI-generated; the data is of known source and coded by authors explicitly."
          }
        ]
      }
    ]
  },
  {
    "id": "jSKtxmxc0M-rubric-0",
    "token_usage": {
      "prompt_tokens": 19608,
      "completion_tokens": 254,
      "total_tokens": 19862
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data Construction and Pipeline",
            "Reasoning": "The VideoGUI dataset is derived from high-quality web instructional videos manually selected by the authors to teach practical and novel GUI tasks. The videos are sourced from YouTube and are accompanied by transcripts. The human participants then reproduce the instructional videos in a simulator to generate manual action trajectories and annotations."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data Construction and Pipeline",
            "Reasoning": "During data collection, participants' screenshots are captured manually as they reproduce the instructional videos. These screenshots are used as visual states for annotation of UI elements and action targets."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data Construction and Pipeline",
            "Reasoning": "Annotations including hierarchical manual planning descriptions (task goals, subtasks, action narrations) and textual descriptions of subtasks are manually created by participants to describe the GUI tasks and actions, which are not generated by models or automatically from web-crawled data."
          }
        ]
      }
    ]
  },
  {
    "id": "jbrMS0DNaD-rubric-0",
    "token_usage": {
      "prompt_tokens": 15310,
      "completion_tokens": 236,
      "total_tokens": 15546
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 The iNaturalist 2024 Dataset",
            "Reasoning": "The new dataset iNaturalist 2024 (iNat24) introduced in the paper consists of five million natural world images spanning 10,000 species classes. These images were collected and annotated by community scientists (citizen scientists) from 2021\u20132024 on the iNaturalist platform, thus manually captured and labeled by humans."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Query and Image Collection Process",
            "Reasoning": "The benchmark includes 250 expert-level text queries which are brief textual descriptions of the concepts of interest. The queries were collected from interviews and discussions with ecological and environmental experts and academic literature, thus these are human-generated textual queries."
          }
        ]
      }
    ]
  },
  {
    "id": "kaHpo8OZw2-rubric-0",
    "token_usage": {
      "prompt_tokens": 104069,
      "completion_tokens": 128,
      "total_tokens": 104197
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section C.3 Design of challenging user prompts (pages 11-12)",
            "Reasoning": "The paper introduces a new dataset of 1.2K challenging toxic user prompts generated by GPT-3.5 and GPT-4 themselves. They generate continuations from toxic prompts in REALTOXICITYPROMPTS and select the most toxic ones as new challenging prompts. These prompts are thus text data generated by GPT models but curated by humans to serve as a new testing set for toxicity evaluation."
          }
        ]
      }
    ]
  },
  {
    "id": "l985bXCatk-rubric-0",
    "token_usage": {
      "prompt_tokens": 14432,
      "completion_tokens": 368,
      "total_tokens": 14800
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": true,
            "Reference": "Section 3 Dataset and Section 3.1 Construction",
            "Reasoning": "The LRVS-Fashion dataset images are extracted from LAION-5B, a large web-crawled dataset, focusing on images from known fashion retailers identified by URL patterns. The images include 'simple' isolated product images and 'complex' images showing products in scenes. Being from web-crawled sources, the image origin is not human-generated or model-generated, but of unknown origin due to the dataset's nature."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Construction - Annotations",
            "Reasoning": "The textual data such as captions replacing noisy LAION alt-texts are generated using the BLIP-2 model, a pretrained and finetuned image captioning model. Therefore, these captions are model-generated. The original noisy captions from LAION are not used as primary referring information."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Construction - Annotations",
            "Reasoning": "The product categories used as conditioning information are generated using an active learning classification approach that aggregates fine-grained apparel items into 10 coarse groupings. This indicates the categories are produced by a machine learning method, i.e., model-generated, not human labeled or unknown."
          }
        ]
      }
    ]
  },
  {
    "id": "lnnNPiZtzR-rubric-0",
    "token_usage": {
      "prompt_tokens": 11539,
      "completion_tokens": 378,
      "total_tokens": 11917
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract; Section 2; Figure 1; Table 2",
            "Reasoning": "The dataset includes photographs of fungi specimens, collected and curated by experts over 20 years, indicating human generation. These images form the core visual data for classification."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.1 Additional observation data",
            "Reasoning": "Satellite images (RGB, 128\u00d7128 pixels) are included as part of each observation, extracted from public Sentinel2 raster data, processed and standardized, thus not human generated nor model generated but derived from satellite sensors."
          },
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.1 Additional observation data",
            "Reasoning": "Meteorological data comprising 20 years of historical temperature and precipitation time-series were extracted from public climatic rasters (Chelsa dataset), representing sensor-derived, non-human generated data."
          },
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2; Table 3",
            "Reasoning": "Textual metadata such as timestamps, camera settings, geographic location, substrate, habitat, and taxonomy labels are manually provided by citizen scientists and experts (human generated) accompanying each observation."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2.1; Figure 4",
            "Reasoning": "Fruiting body part segmentation masks for fungi are human-verified instance segmentation masks (cap, gills, pores, rings, stems) semi-automatically created and verified by human annotators, hence human generated."
          }
        ]
      }
    ]
  },
  {
    "id": "loJM1acwzf-rubric-0",
    "token_usage": {
      "prompt_tokens": 28771,
      "completion_tokens": 408,
      "total_tokens": 29179
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Document Collection, Section 3.2 Question and Answer Collection, Table 2 Dataset Statistics",
            "Reasoning": "The MMLONGBENCH-DOC benchmark introduces 898 newly-annotated human-generated questions based on 59 newly collected documents. The documents are PDF-formatted and contain extensive text with an average of 21,214 textual tokens per document. The text data is generated and compiled by human annotators who collected and curated documents from sources such as Arxiv, ManualsLib, and Google Search, as described explicitly in Sections 3.1 and 3.2."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.2 Experimental Setup, Table 2 Dataset Statistics, Section 3.1 Document Collection",
            "Reasoning": "The benchmark uses PNG-formatted screenshot images of each page of the documents to evaluate LVLMs, with an average of approximately 47.5 pages per document. These images are directly derived from the documents and are not generated by models; they represent the original visual page contents, so they are human-produced document scans or digital renderings but not manually created by humans beyond original documents. Hence, the modality is image and origin is not model generated or unknown, but the images derive from human-produced documents."
          },
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract, Table 2 Dataset Statistics, Section 3.2 Question and Answer Collection",
            "Reasoning": "The documents include extensive tables serving as data evidence sources for questions. These tables originate from the collected human-generated long documents (e.g., reports, papers). The tabular data is human-produced content from original source documents and manually annotated for questions. Not generated by any model and clearly specified as original documents' data."
          }
        ]
      }
    ]
  },
  {
    "id": "mDRmX8IlBI-rubric-0",
    "token_usage": {
      "prompt_tokens": 15881,
      "completion_tokens": 532,
      "total_tokens": 16413
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Manual Data Collection; Abstract; Figure 1",
            "Reasoning": "The MMWorld's main subset is a human-annotated dataset with 1,910 videos manually collected from YouTube under Creative Commons licenses and other datasets such as SportsQA and IKEA Assembly. The videos cover multiple broad disciplines and subdisciplines and are carefully selected for multi-discipline coverage. The questions and answers are also manually annotated by humans, indicating human-generated video content."
          },
          {
            "Modality": "video",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Automated Data Collection",
            "Reasoning": "The synthetic subset of MMWorld consists of videos collected automatically from YouTube and YouTube-8M dataset sources. The question-answer pairs and captions associated with these videos are generated by GPT-4V (an AI model). This indicates that although the videos themselves are from existing sources (web videos), the annotations and questions are generated by a model. The dataset focuses on single modality perception (audio or video). Hence, the synthetic dataset includes video modality with model-generated annotations."
          },
          {
            "Modality": "audio",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Automated Data Collection; Table 2 Synthetic Subset I",
            "Reasoning": "The synthetic dataset's Subset I corresponds to QA pairs generated based solely on audio content, with videos collected via an automated pipeline and GPT-4V-generated questions and answers. Therefore, the audio modality is present with model-generated annotations. The underlying audio is from videos sourced automatically, so the audio itself is human recorded, but the QA annotations are model generated. Considering the dataset is introduced by authors with an automatic data collection pipeline, the overall data source entry is treated as model-generated annotation over human-recorded audio."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Automated Data Collection; Figure 3; Table 2 Synthetic Subset II",
            "Reasoning": "The synthetic dataset's Subset II is focused on evaluating models with visual content (images extracted as frames from videos). The QA pairs and captions are generated by GPT-4V based on a set of frames representing video content. The visual modality here is represented as images (video frames). The original frames come from automatically collected videos, but the QA data is model-generated, so the data source is model generated concerning the image modality."
          }
        ]
      }
    ]
  },
  {
    "id": "mEJgnZZyfv-rubric-0",
    "token_usage": {
      "prompt_tokens": 16004,
      "completion_tokens": 196,
      "total_tokens": 16200
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Benchmarking Tasks, Table 2",
            "Reasoning": "The paper introduces a benchmark called OpenMixup using 12 diverse open-source image classification datasets, e.g., CIFAR-10, CIFAR-100, FashionMNIST, STL-10, Tiny-ImageNet, ImageNet-1K, CUB-200-2011, FGVC-Aircraft, iNaturalist2017, iNaturalist2018, Places205. These datasets are mentioned as sourced from existing public datasets (e.g., references [35] to [43]). No indication suggests these images were newly generated by the authors, so natural images are human-generated data captured in real world and curated for research use."
          }
        ]
      }
    ]
  },
  {
    "id": "mlhFJE7PKo-rubric-0",
    "token_usage": {
      "prompt_tokens": 38982,
      "completion_tokens": 592,
      "total_tokens": 39574
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Histology; Abstract; Section 3 HEST-1k Dataset",
            "Reasoning": "The dataset includes H&E-stained whole-slide images (WSIs) of tissue sections, which are generated and prepared manually by humans through histological staining and imaging of tissue samples. These images are normalized and converted into a generic TIFF format for computational use."
          },
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.4 Gene expression; Section 3 HEST-1k Dataset; Abstract",
            "Reasoning": "The gene expression profiles in the dataset are raw transcript counts measured spatially from tissue sections using spatial transcriptomics technologies such as Visium and Xenium. This count data is acquired through experimental protocols and represented as matrix/tabular data (genes by spots) in the dataset."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.3 Nuclear segmentation and classification; Section 3 HEST-1k Dataset",
            "Reasoning": "The nuclear segmentation and classification maps are generated computationally using the CellViT model for instance segmentation and classification of nuclei in the WSIs. These derivative images are produced by a trained AI model and provided as additional annotations in the dataset."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.3 Nuclear segmentation and classification; Section 3 HEST-1k Dataset",
            "Reasoning": "The classification of each nucleus into five classes (neoplastic epithelial, non-neoplastic epithelial, inflammatory, stromal, necrotic) is generated by the CellViT model, representing a model-generated data modality stored as tabular annotations per nucleus."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.4 Gene expression (Xenium); Section 3.4; Section 3 HEST-1k Dataset",
            "Reasoning": "For Xenium samples, the dataset includes transcript-level spatial coordinates (2D position in tissue) derived from experimental data but computationally aligned using image registration pipelines (VALIS). The alignment files mapping gene expression spots to image coordinates are computationally generated."
          }
        ]
      }
    ]
  },
  {
    "id": "moMoWj7jLm-rubric-0",
    "token_usage": {
      "prompt_tokens": 23184,
      "completion_tokens": 191,
      "total_tokens": 23375
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3 Dataset Construction; Appendix A Dataset Card",
            "Reasoning": "The new dataset introduced by the authors is FormulaReasoning, which consists of 5,420 numerical reasoning questions from Chinese junior high school physics examinations. The data is in textual form containing questions, explanations with normalized formulas, parameter names, symbols, numerical values, units, and answers. The data was collected and manually annotated by human experts (five postgraduate volunteers with science and engineering background), with assistance from LLMs to improve annotation efficiency. The dataset is explicitly stated as formula-based with every question requiring formulas. This confirms human generation of the text data. There is no indication of the dataset being generated by a model or unclear provenance."
          }
        ]
      }
    ]
  },
  {
    "id": "nrEqH502eC-rubric-0",
    "token_usage": {
      "prompt_tokens": 78558,
      "completion_tokens": 152,
      "total_tokens": 78710
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Main text Sections 1 and 3, Appendix A.1",
            "Reasoning": "The BLEnD dataset is a newly introduced benchmark consisting of 52.6k question-answer pairs in 13 different languages and 16 countries/regions. The data is entirely hand-crafted and manually annotated by native speakers from each culture, as described in the dataset construction process including question collection, filtering, answer annotation, and answer aggregation. The dataset content is textual in nature, comprising questions and short answers or multiple-choice formats, all generated and curated manually by human annotators rather than AI models or crawled from unknown web sources."
          }
        ]
      }
    ]
  },
  {
    "id": "p8eUitex7p-rubric-0",
    "token_usage": {
      "prompt_tokens": 19321,
      "completion_tokens": 477,
      "total_tokens": 19798
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset Construction",
            "Reasoning": "The ImageNet3D dataset consists of images sourced from the ImageNet21k dataset, which are real-world images curated and labeled by humans. The authors extend these images with new 2D bounding box and 6D pose annotations, which are manually annotated by 30 human annotators using a web-based tool. Hence, the dataset images are human-generated data."
          },
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Dataset Construction",
            "Reasoning": "The image annotations including 2D bounding boxes, 3D poses (6D pose represented by rotation and location), scene density labels, and object visual quality labels are manually annotated by human annotators. These annotations are structured data (numbers and categorical labels), hence tabular modality. These tabular annotations are human-generated."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.3 Natural Captions with 3D Information; Appendix A.3 Caption Generation",
            "Reasoning": "Image captions interleaved with 3D information are generated via a GPT-assisted approach, specifically using GPT-4v. The textual captions describe the image and encode 3D pose information. Since these captions are generated by an AI model (GPT-4v) based on annotations and image metadata, this data is model-generated text."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Appendix A.3 Caption Generation",
            "Reasoning": "Separate category-level captions describing object shape, part structure, and human interaction for all categories are manually annotated by humans. These are textual descriptions created by human annotators to provide detailed category-level information."
          }
        ]
      }
    ]
  },
  {
    "id": "pUcTrjRLOM-rubric-0",
    "token_usage": {
      "prompt_tokens": 33824,
      "completion_tokens": 325,
      "total_tokens": 34149
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2 The UltraMedical Dataset and Table 1: Instructions Statistics",
            "Reasoning": "The UltraMedical dataset is a large collection (~410K) of medical instructions comprising mainly text prompts and questions. Many of these instructions are manual, created by humans from medical exams, literature, and clinical questions, providing a human-generated textual dataset."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.1.1 Instruction Composition and Section 2.1.2 Principle of Complexity",
            "Reasoning": "The dataset includes synthetic instructions generated by models such as GPT-4 and evolved instructions using self-evolution techniques, resulting in synthetic textual data generated by models designed to augment and increase dataset complexity and diversity."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.2.1 Completions Annotation",
            "Reasoning": "Completions (answers) to the instructions are annotated using GPT-4 Turbo, thus the textual answers are model-generated to optimize for supervised fine-tuning."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 2.2.2 Preference Annotation",
            "Reasoning": "The preference annotations over multiple completions are generated and scored primarily by GPT-4 (an AI model), providing model-generated preference data in text format for supervised preference learning and reward modeling."
          }
        ]
      }
    ]
  },
  {
    "id": "pYNl76onJL-rubric-0",
    "token_usage": {
      "prompt_tokens": 19081,
      "completion_tokens": 200,
      "total_tokens": 19281
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract, Section 3, Section E (Data Sheet)",
            "Reasoning": "The paper explicitly states that the 1.67 million unique text-to-video prompts are collected from real users via official Pika Discord channels (Section 3: Collecting Source HTML Files). Thus, the text prompts are human-generated data."
          },
          {
            "Modality": "video",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Abstract, Section 3 (Scraping and Generating Videos), Section E (Data Sheet)",
            "Reasoning": "The dataset includes 6.69 million videos that are generated by four state-of-the-art text-to-video diffusion models: Pika, Text2Video-Zero, VideoCraft2, and ModelScope, as described in Section 3. These videos are model-generated, not real-world videos captured by humans."
          }
        ]
      }
    ]
  },
  {
    "id": "qmvtDIfbmS-rubric-0",
    "token_usage": {
      "prompt_tokens": 27227,
      "completion_tokens": 338,
      "total_tokens": 27565
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1, Section 3.2, Appendix A.2.1, A.3 Dataset Documentation",
            "Reasoning": "The new dataset includes 50 meticulously curated murder mystery game scripts comprising extensive textual information such as character scripts, background stories, dialogues, and clue descriptions. These scripts were collected manually by experts from publicly available sources and then reviewed and refined thoroughly, as described in Section 3.1, 3.2 and Appendices A.2.1 and A.3. The text modality encompasses the game scripts and the evaluation questions (multiple-choice and open-ended) which were manually annotated by human experts."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 (Data Collection), Section 3.2.1 (Perception QA), Appendix A.2.1, A.3 Dataset Documentation",
            "Reasoning": "The dataset includes visual clues in the form of images (e.g., diagrams, maps, textual clues embedded as images) that are part of the murder mystery scripts. These images were processed by humans through manual curation, including cropping and OCR extraction. The perception evaluation contains questions explicitly about text-rich and media-rich images. Therefore, the image modality is clearly present and originates from human-generated game materials and annotations."
          }
        ]
      }
    ]
  },
  {
    "id": "r8PnfcWQol-rubric-0",
    "token_usage": {
      "prompt_tokens": 30748,
      "completion_tokens": 148,
      "total_tokens": 30896
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "graph",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 5.1 The EFO_k-CQA dataset",
            "Reasoning": "The EFO_k-CQA dataset is a newly introduced dataset constructed by the authors as described in Section 5.1. It is a collection of complex queries on knowledge graphs, represented as combinatorial query graphs with multiple free and existential variables. The data modality corresponds to graphs since queries and knowledge graphs are represented as graphs and abstract query graphs. The dataset is manually curated and mechanically generated according to systematic algorithms developed by the authors, hence human generated. There is no indication it was generated by a model or lacks provenance."
          }
        ]
      }
    ]
  },
  {
    "id": "rdv2Fr6JTC-rubric-0",
    "token_usage": {
      "prompt_tokens": 20972,
      "completion_tokens": 170,
      "total_tokens": 21142
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "graph",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4 (Datasets and Settings), Appendix G.2 (Datasets)",
            "Reasoning": "The paper introduces the new use of existing real-world graph datasets such as Cora, Citeseer, Pubmed, Amazon-Photo, Amazon-Computer, and Coauthor-Physics as benchmark datasets for evaluation of the proposed PC-Winter method. However, these are not new datasets introduced by the authors, but standard benchmark datasets. Importantly, the paper explicitly states no new datasets are introduced (Checklist 4c states no new assets or datasets are included). Therefore, the PC-Winter method is evaluated on pre-existing datasets only, and no new dataset is introduced by the authors in the paper."
          }
        ]
      }
    ]
  },
  {
    "id": "rovpCs3ZEO-rubric-0",
    "token_usage": {
      "prompt_tokens": 23926,
      "completion_tokens": 306,
      "total_tokens": 24232
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Training Tasks, Task (1) Lung Opacity Detection",
            "Reasoning": "The RSNA Pneumonia Detection dataset consists of chest X-ray images annotated by medical practitioners, indicating it is human-generated image data."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Training Tasks, Task (2) COVID-19 Detection",
            "Reasoning": "The COVID-19 detection task uses chest X-ray images from the COVQU dataset, which are medical images collected and annotated by medical professionals, thus human-generated images."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Training Tasks, Task (3) ECG Abnormal Detection",
            "Reasoning": "The ECG abnormal detection data are electrocardiogram signals from the PTB-XL database, which are human-captured sensor signals recorded from patients."
          },
          {
            "Modality": "tabular",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4.1 Training Tasks, Task (4) Mortality Prediction",
            "Reasoning": "Mortality prediction utilizes clinical features such as vital signs and lab test results extracted from MIMIC-III, which is tabular patient data collected from hospital records, human generated."
          }
        ]
      }
    ]
  },
  {
    "id": "s1K5Z5QPog-rubric-0",
    "token_usage": {
      "prompt_tokens": 12530,
      "completion_tokens": 204,
      "total_tokens": 12734
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Observations",
            "Reasoning": "ChaosBench includes multi-decadal time series data from global reanalysis products ERA5 (surface-atmosphere), ORAS5 (sea-ice), and LRA5 (land), spanning over 45 years. These are physical measurements and reconstructions compiled by meteorological organisations, not directly generated by models or humans, but from assimilated observational data reprocessed into consistent time series."
          },
          {
            "Modality": "time series",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Simulations",
            "Reasoning": "ChaosBench also includes simulated forecasts from physics-based Numerical Weather Prediction models operated by national weather agencies (UKMO, NCEP, CMA, ECMWF). These are forecasts generated by complex physical simulation models (numerical models), thus are model generated time series data."
          }
        ]
      }
    ]
  },
  {
    "id": "t9aThFL1lE-rubric-0",
    "token_usage": {
      "prompt_tokens": 29554,
      "completion_tokens": 162,
      "total_tokens": 29716
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3: Our Proposal: UNLEARNCANVAS Dataset; Appendix A.3 Collection and Labeling Process",
            "Reasoning": "The UNLEARNCANVAS dataset is newly introduced by the authors, consisting of high-resolution stylized images created via a two-step process: first, seed images are collected from Pexels, an open-source photography website (human-generated photographs), and second, these images are transformed into 60 predetermined artistic styles using stylization services provided by Fotor. Therefore, the dataset images originate from human-generated photographs that were subsequently processed systematically using existing stylization tools, not generated by models within this work, nor are they of unknown origin."
          }
        ]
      }
    ]
  },
  {
    "id": "tPsw4NeLZx-rubric-0",
    "token_usage": {
      "prompt_tokens": 20931,
      "completion_tokens": 134,
      "total_tokens": 21065
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1, Section 3.2, Abstract",
            "Reasoning": "The MM-WLAuslan dataset is a newly introduced dataset by the authors consisting of over 282K+ high-quality isolated Auslan gloss videos recorded using a multi-view setup with Kinect-V2 and RealSense RGB-D cameras in a controlled studio environment. The videos are human-captured sign language performances by 73 signers, supervised by Auslan experts to ensure precision. This clearly establishes the modality as video and the origin as human-generated."
          }
        ]
      }
    ]
  },
  {
    "id": "tWvVtOW0qg-rubric-0",
    "token_usage": {
      "prompt_tokens": 15219,
      "completion_tokens": 267,
      "total_tokens": 15486
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 4 Experiments: Correlation with Downstream Classifier Performance; Appendix A Datasets",
            "Reasoning": "The paper introduces and uses four medical imaging datasets from the MedMNIST benchmark: BloodMNIST, OrganAMNIST, PathMNIST, and TissueMNIST. These are explicitly mentioned as part of the benchmark experiments evaluating federated data measurements. MedMNIST datasets consist of biomedical images (2D and 3D) captured from human medical imaging modalities. The authors refer to these datasets by name and use them as new datasets for evaluation, differentiating them from standard computer vision datasets like ImageNet or CIFAR10 which are pre-existing. Since medical imaging data are derived from human patients, the data are human-generated images captured via medical imaging devices. There is no indication that these datasets are model generated or of unknown origin. Hence, the modality is 'image', with origins as human generated based on their collection from medical imaging data of human subjects. This is supported by the detailed description in Section 4 and Appendix A listing these datasets as used for experiments."
          }
        ]
      }
    ]
  },
  {
    "id": "vyraA7xt4c-rubric-0",
    "token_usage": {
      "prompt_tokens": 20036,
      "completion_tokens": 115,
      "total_tokens": 20151
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 2 Mercury Datasets",
            "Reasoning": "The Mercury dataset is introduced as a new code generation benchmark comprising 1,889 Python programming tasks collected and filtered from Leetcode, a public online platform. These tasks include task descriptions, test case generators, and historical code solutions. The data is therefore text-based (programming problem descriptions and code) and human generated, as it is curated from human-created Leetcode tasks and solutions."
          }
        ]
      }
    ]
  },
  {
    "id": "wOmtZ5FgMH-rubric-0",
    "token_usage": {
      "prompt_tokens": 32576,
      "completion_tokens": 498,
      "total_tokens": 33074
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Data Collection and Construction; Appendix A.3 Collection Process",
            "Reasoning": "The RWKU benchmark introduces new text datasets consisting of probes and unlearning targets. The unlearning targets are 200 famous people selected based on a public ranking and Wikipedia popularity data. The probes including forget set probes (fill-in-the-blank, question-answer, and adversarial attack style) and neighbor set probes are generated by GPT-4 model guided by Wikipedia texts, then filtered and manually checked by humans to ensure correctness. The initial Wikipedia texts for targets and neighbors are human-generated from Wikipedia, an open human-curated source. The queries and answers are model-generated (GPT-4 generated) but then manually verified, thus the dataset is a combination of human and model generated data. However, since the probes are the main introduced dataset entries and are synthesized by a model and then corrected/curated by humans, the overall dataset entries are considered model generated text with human curation. Wikipedia source texts are human generated."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Data Collection and Construction; Appendix G.2 Original Wikipedia Descriptions",
            "Reasoning": "As part of the benchmark, original Wikipedia descriptions of the 200 famous people are collected as text data for the unlearning targets. Wikipedia content is human generated and publicly accessible, thus the dataset includes human generated text from Wikipedia as a component."
          },
          {
            "Modality": "text",
            "Human Generated": false,
            "Model Generated": true,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Data Collection and Construction; Appendix G.3 Generated Data Examples",
            "Reasoning": "The synthetic forget corpus used for unlearning methods is generated by prompting the original LLM (like LLaMA3-Instruct) to generate short biographies of the unlearning targets following specific prompts. This is model generated text designed to simulate a forget corpus. It is explicitly stated that the synthetic corpus is generated by the model itself given the unlearning targets."
          }
        ]
      }
    ]
  },
  {
    "id": "wmO7z57wNK-rubric-0",
    "token_usage": {
      "prompt_tokens": 16634,
      "completion_tokens": 106,
      "total_tokens": 16740
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 and 4",
            "Reasoning": "The benchmark LLMCBench uses datasets such as MMLU, ARC-easy, ARC-challenge, Hellaswag, PIQA, WinoGrande, QNLI, MNLI, WikiText2 which are standard NLP datasets providing text data. These datasets are human-created benchmarks used to evaluate model knowledge and inference ability."
          }
        ]
      }
    ]
  },
  {
    "id": "x8RgF2xQTj-rubric-0",
    "token_usage": {
      "prompt_tokens": 40245,
      "completion_tokens": 296,
      "total_tokens": 40541
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3, Section 3.3, Section 3.7, Appendix J, Appendix G",
            "Reasoning": "The paper uses ImageNet-ReaL [2] as a source of ground truth aleatoric uncertainty labels, which are collected via multiple human annotators per image, indicating human-generated uncertainty annotations on an image classification task. This dataset is new in the sense that it provides multiple human labels per ImageNet image to quantify aleatoric uncertainty. The authors employ this dataset in their experiments directly, but do not introduce it themselves. Since the instruction restricts to only new datasets introduced by the authors, ImageNet-ReaL is not introduced in this paper, so this entry is not considered in the final list."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3, Section 3.7, Appendix J, Appendix G",
            "Reasoning": "The paper uses CIFAR-10H dataset [41] which includes human uncertainty annotations collected from multiple annotators on the CIFAR-10 dataset. Like ImageNet-ReaL, this dataset provides human-generated soft labels indicating aleatoric uncertainty. However, this dataset is not introduced by the authors themselves but is pre-existing and used in benchmarking the methods. The instruction excludes such datasets."
          }
        ]
      }
    ]
  },
  {
    "id": "y09S5rdaWY-rubric-0",
    "token_usage": {
      "prompt_tokens": 19383,
      "completion_tokens": 457,
      "total_tokens": 19840
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract and Section 3.2 Expert Dataset",
            "Reasoning": "The new dataset introduced, termed as Bench2Drive official training data, consists of camera images captured from CARLA simulator. The images are part of the sensor suite similar to nuScenes with 6 surround cameras. The dataset includes 2 million fully annotated frames derived from 13638 short clips under diverse scenarios and weathers. The data is generated by the simulation environment and collected automatically by the expert model Think2Drive, thus not human-generated, not model-generated as image data itself, and its origin is specified as from CARLA simulator."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Abstract and Section 3.2 Expert Dataset",
            "Reasoning": "The dataset includes LiDAR point clouds, radar data, IMU and GNSS signals collected from CARLA simulation. This sensor data is automatically generated by the simulation environment and collected by the expert agent. Since these signals are physical sensor measurements (albeit simulated), they are not human-generated or model generated; the origin is clearly stated as from simulated sensors in CARLA."
          },
          {
            "Modality": "image",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Expert Dataset and Figure 3",
            "Reasoning": "The dataset provides semantic and instance segmentation images as annotations. These are generated programmatically using CARLA's sensors and APIs during data collection, thus not human generated or model generated. The origin is explicitly stated to be obtained from the simulation environment."
          },
          {
            "Modality": "tabular",
            "Human Generated": false,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2 Expert Dataset and Figure 3",
            "Reasoning": "Annotations include 3D bounding boxes, HD-Map data (lanes, centerlines, topology, signals), and precise coordinates, which are stored in structured formats (tabular). These annotations are generated by the CARLA simulator and the expert model during collection, thus are not manually generated by humans nor model generated; origin is clearly simulation-based generation."
          }
        ]
      }
    ]
  },
  {
    "id": "y10DM6R2r3-rubric-0",
    "token_usage": {
      "prompt_tokens": 27081,
      "completion_tokens": 242,
      "total_tokens": 27323
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 and Appendix A.1 Dataset Construction Details",
            "Reasoning": "The MMLU-Pro dataset introduced in this paper comprises over 12,000 multiple-choice questions sourced from four main origins: (1) a filtered subset of original MMLU questions (with trivial and noisy questions removed), (2) STEM Website problems, (3) TheoremQA questions, and (4) SciBench advanced science questions. All these questions are in textual format (multiple-choice questions with options). The dataset construction involved human expert reviews to verify correctness, ensure appropriate distractors, and remove unsuitable questions. Human experts curated and verified the questions, and although GPT-4-Turbo was used to generate plausible distractors and extract short answers from solutions, the core questions themselves are human-generated, not purely model-generated. Thus, the dataset modality is 'text' (multiple-choice questions), originating from human-generated data, with some options augmented by model-generated distractors under human supervision. The sources are explicitly detailed in Section 3.1 and Appendix A.1."
          }
        ]
      }
    ]
  },
  {
    "id": "yS1dUkQFnu-rubric-0",
    "token_usage": {
      "prompt_tokens": 16896,
      "completion_tokens": 355,
      "total_tokens": 17251
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1, Table 2",
            "Reasoning": "The paper includes new usage of 30 diverse computer vision datasets covering image recognition, video action recognition, and dense prediction tasks. All datasets listed in Table 2 and described in Section 3 are existing datasets assembled for benchmarking. The datasets contain images captured by humans or devices. However, the paper does not indicate any new datasets introduced by the authors themselves. They constructed a unified benchmark using publicly available datasets but did not introduce any new dataset created or collected by themselves."
          },
          {
            "Modality": "video",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2, Table 2",
            "Reasoning": "The video action recognition datasets (Kinetics-400, SSv2, HMDB51) are existing widely used datasets selected for the benchmark. The paper uses these standard video datasets for evaluation but does not introduce or create any new video dataset."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3, Table 2",
            "Reasoning": "Dense prediction datasets such as MS COCO, ADE20K, and PASCAL VOC are standard public datasets included in the benchmark. The authors use them but do not introduce any new dense prediction dataset."
          }
        ]
      }
    ]
  },
  {
    "id": "yUEBXN3cvX-rubric-0",
    "token_usage": {
      "prompt_tokens": 19957,
      "completion_tokens": 285,
      "total_tokens": 20242
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data collection; Abstract; Section 3 The ANDROIDCONTROL dataset",
            "Reasoning": "ANDROIDCONTROL is a new dataset introduced by the authors consisting of human-generated natural language task descriptions at both high and low-level instructions. The dataset was collected via crowdsourcing where human annotators wrote detailed task instructions and step-by-step action descriptions for controlling Android apps."
          },
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data collection; Appendix B.2 Dataset format",
            "Reasoning": "The dataset includes human-generated screenshots of the Android device display taken during the execution of the tasks. These screenshots are captured and stored as PNG byte strings in the dataset, representing images of the UI screen at each step."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Data collection; Appendix B.3 Accessibility node metadata",
            "Reasoning": "The dataset includes Android accessibility trees capturing the UI screen state as structured data, derived from the device's accessibility APIs and thus human-collected from actual Android devices. This information contains metadata about UI elements, their properties, and layout, representing structured sensor-like information about the UI."
          }
        ]
      }
    ]
  },
  {
    "id": "z64azPC6Nl-rubric-0",
    "token_usage": {
      "prompt_tokens": 17496,
      "completion_tokens": 603,
      "total_tokens": 18099
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "audio",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2, Appendix A.1",
            "Reasoning": "GTSinger is introduced as a newly recorded singing corpus consisting of 80.59 hours of high-quality singing voices recorded by 20 professional singers in a professional studio setting. The singing voices are manually performed by these human singers. This is explicitly stated in the abstract, Section 3.2 (Recording), and Appendix A.1."
          },
          {
            "Modality": "audio",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.2, Appendix A.1",
            "Reasoning": "The dataset includes 16.16 hours of paired speech audio recordings spoken by the same professional singers, recorded specifically to provide paired speech for speech-to-singing tasks. These speech recordings are human produced and simultaneously recorded with singing voices. This is confirmed in the Abstract, Section 3.2, and Appendix A.1."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3, Appendix A.2",
            "Reasoning": "Phoneme-level annotations and manual phoneme-to-audio alignments for six singing techniques are performed manually by experts based on human auditory perception and spectrogram analysis. These annotations are human-generated textual data used to label audio samples and are explicitly described in Section 3.3 Annotation and Appendix A.2."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3, Appendix A.2",
            "Reasoning": "Global style labels (singing method, emotion, pace, and range) for each group are manually labeled by human annotators with music expertise, as described in Section 3.3 and Appendix A.2."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.3, Section 3.5, Appendix A.2",
            "Reasoning": "Realistic music scores in musicxml format are composed by music experts who listen to the songs, reference original accompaniments, and manually adjust tempo, clef, key, pitch, and note duration to make realistic music scores reflecting the recorded singing voices. This process is human-generated and described in Section 3.3, Figure 3, and Appendix A.2."
          }
        ]
      }
    ]
  },
  {
    "id": "zogaeVpbaE-rubric-0",
    "token_usage": {
      "prompt_tokens": 20781,
      "completion_tokens": 380,
      "total_tokens": 21161
    },
    "response": [
      {
        "sources": [
          {
            "Modality": "image",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Tasks; Table 5 overview",
            "Reasoning": "DEVBENCH includes tasks such as Looking-while-listening (LWL), Visual Vocabulary (VV), Test of Receptive Grammar (TROG), Winoground-NoTag (WG), Visual Object Categorization (VOC), and THINGS similarity ratings, all involving visual stimuli presented as images. The images were collected or originally designed in human psychological experiments and involve human participants' viewing behavior, thus the image data is human-generated, not model-generated, and provenance is explicitly reported."
          },
          {
            "Modality": "text",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Tasks; Table 5 overview",
            "Reasoning": "The verbal cues, labels, or words used in tasks such as LWL, VV, TROG, WG, and WAT (Word Association Task) are human-generated stimuli employed in psychological research. The linguistic data correspond to labels and verbal prompts used in experiments, collected during controlled studies involving children and adults. Therefore, the text data are human-generated, not model-generated, and have specified provenance."
          },
          {
            "Modality": "signal/sensor",
            "Human Generated": true,
            "Model Generated": false,
            "Unknown Origin": false,
            "Reference": "Section 3.1 Tasks - for LWL and VOC mention eyetracking data; Table 5",
            "Reasoning": "Looking-while-listening (LWL) and Visual Object Categorization (VOC) tasks involve collecting eye-tracking measurements of participants' gaze and looking time proportions toward stimuli images. These are time-resolved physiological signals captured from human participants during experiments, explicitly collected by human researchers; thus, this data modality is human-generated and with clearly specified origin."
          }
        ]
      }
    ]
  }
]