[
  {
    "id": "01I55gys19-rubric-10",
    "token_usage": {
      "prompt_tokens": 11882,
      "completion_tokens": 118,
      "total_tokens": 12000
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.2 and footnote 2",
        "reasoning": "The paper states that the Few-Class Arena is implemented using the MMPreTrain framework and that data subsets are generated through lightweight configurations without redundant duplicates, indicating code infrastructure for dataset loading. Moreover, Footnote 2 provides a direct link to the publicly available repository at https://github.com/fewclassarena/fca that includes code with detailed documentation. This repository covers the benchmark, dataset preparation, and loading procedures, ensuring reproducibility of the few-class datasets introduced by the authors."
      }
    ]
  },
  {
    "id": "0SMhqvgHST-rubric-10",
    "token_usage": {
      "prompt_tokens": 29221,
      "completion_tokens": 110,
      "total_tokens": 29331
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 5 and 6.1",
        "reasoning": "The paper states that they developed the GATE engine, a unified benchmark suite and software framework that automates dataset downloading, preprocessing, and pipelining for fine tuning and evaluation. They also mention that the full code and data are available and shared on GitHub and Huggingface with sufficient instructions to reproduce the main experimental results. This indicates that the code related to dataset handling, preprocessing, and benchmark evaluation is publicly available, facilitating reproducibility."
      }
    ]
  },
  {
    "id": "2HzZIDo48o-rubric-10",
    "token_usage": {
      "prompt_tokens": 19706,
      "completion_tokens": 133,
      "total_tokens": 19839
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3 and Appendix B",
        "reasoning": "The paper explicitly states in the reproducibility checklist (Section E) that the code, data, and instructions needed to reproduce the main experimental results are included, either in the supplemental material or as a URL. Additionally, Appendix B contains detailed descriptions of the agent architectures and hyperparameters, indicating careful documentation intended for reproducibility. While the exact URL or repository name for the dataset generation code is anonymized (HIDDEN_FOR_REVIEW_PURPOSE), the indication in the checklist and detailed algorithmic descriptions strongly support that the dataset construction code is publicly available or provided for reproducibility."
      }
    ]
  },
  {
    "id": "2myGfVgfva-rubric-10",
    "token_usage": {
      "prompt_tokens": 15574,
      "completion_tokens": 121,
      "total_tokens": 15695
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 5.1; also mentioned in the Checklist (Item 3a) and project URL below the title",
        "reasoning": "The paper explicitly states in the abstract and Section 5.1 that the code and data are provided in the project URL https://github.com/mira-space/MiraData. The checklist confirms that the authors included the code, data, and instructions needed to reproduce the main experimental results. This indicates that the code related to data collection, preprocessing, and generation is publicly available in an accessible repository, supporting reproducibility."
      }
    ]
  },
  {
    "id": "4S8agvKjle-rubric-10",
    "token_usage": {
      "prompt_tokens": 33974,
      "completion_tokens": 98,
      "total_tokens": 34072
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract footnote and Section 2",
        "reasoning": "The paper explicitly states in the Abstract footnote that the code and data are available at https://github.com/hkust-nlp/AgentBoard, which is an accessible GitHub repository. Furthermore, Section 2 mentions that AGENTBOARD is an open-source benchmark and evaluation framework. This indicates that all code related to data collection, preprocessing, and generation is publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "5WFzk0H27p-rubric-10",
    "token_usage": {
      "prompt_tokens": 15696,
      "completion_tokens": 160,
      "total_tokens": 15856
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 2.2 and A.4 Preprocessing/cleaning/labeling",
        "reasoning": "The paper explicitly states that the Tournesol dataset is publicly available at https://api.tournesol.app/exports/all, and that the code used to preprocess and analyze the data is open-source and available via the Solidago Python package. Section 2.2 describes the SOLIDAGO pipeline for processing data, and Appendix A.4 confirms that the software used for preprocessing/cleaning/labeling is open-source and publicly accessible. Additionally, the authors note in Section 5 that the code base is available at https://github.com/tournesol-app/tournesol/, providing sufficient access for reproducing dataset construction and processing."
      }
    ]
  },
  {
    "id": "5c1hh8AeHv-rubric-10",
    "token_usage": {
      "prompt_tokens": 100752,
      "completion_tokens": 157,
      "total_tokens": 100909
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 2.5",
        "reasoning": "The paper states in the Abstract that the code and resources are publicly available at https://multi-trust.github.io/. In Section 2.5, it further elaborates that they have developed a scalable toolbox integrating different MLLMs and tasks with a unified interface and modular design. The codebase provides a universal infrastructure for evaluating MLLM trustworthiness and is intended for public and community use. Also, in the Data Sheet (Appendix H), it is mentioned that the codebase will be open-sourced on GitHub with instructions for dataset download. This indicates that the code related to data collection, preprocessing, and generation for the new datasets is publicly available, enabling reproducibility."
      }
    ]
  },
  {
    "id": "67N3FWoDtU-rubric-10",
    "token_usage": {
      "prompt_tokens": 15479,
      "completion_tokens": 89,
      "total_tokens": 15568
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 3",
        "reasoning": "The paper explicitly states in the abstract that both the dataset and corresponding baseline code are freely available online. Furthermore, in Section 3, the dataset and code are referenced with URLs, including GitHub and GitLab repositories where the code and data can be accessed. Therefore, code related to data collection, preprocessing, and generation is publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "6cCFK69vJI-rubric-10",
    "token_usage": {
      "prompt_tokens": 20235,
      "completion_tokens": 110,
      "total_tokens": 20345
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract; Section 4; Appendix C and D",
        "reasoning": "The paper states in the abstract that access to the dataset and the code used for benchmarking are available at https://github.com/cruiseresearchgroup/DIEF_BTS. Section 4 and Appendices C and D provide details about the benchmarking and mention that all implementations and hardware setups are available on the GitHub repository. This indicates that the code related to the dataset construction, preprocessing, and benchmarking is publicly available, supporting reproducibility."
      }
    ]
  },
  {
    "id": "7TCK0aBL1C-rubric-10",
    "token_usage": {
      "prompt_tokens": 15242,
      "completion_tokens": 104,
      "total_tokens": 15346
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Appendix A.1",
        "reasoning": "The paper explicitly states in the abstract and Appendix A.1 that the IaC-Eval dataset and evaluation code are open-sourced and publicly available at https://github.com/autoiac-project/iac-eval, including detailed descriptions, instructions to reproduce experimental results, setup details, and licensing information. This demonstrates that all code related to dataset creation, preprocessing, and evaluation is accessible in a GitHub repository for reproducibility."
      }
    ]
  },
  {
    "id": "9tVn4f8aJO-rubric-10",
    "token_usage": {
      "prompt_tokens": 40020,
      "completion_tokens": 125,
      "total_tokens": 40145
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Appendix A.1 and related dataset descriptions",
        "reasoning": "The paper extensively uses a suite of 30 existing datasets, listing sources and data licenses for each, but does not indicate introducing any new dataset created by the authors themselves, nor is there mention of code repositories for data collection, preprocessing, or generation related to new datasets. The datasets used are all pre-existing, with their own access URLs and licenses, and the paper provides evaluation code but not data construction code. Therefore, there is no code for new dataset creation made publicly available in this paper."
      }
    ]
  },
  {
    "id": "AdpSHMOujG-rubric-10",
    "token_usage": {
      "prompt_tokens": 15725,
      "completion_tokens": 156,
      "total_tokens": 15881
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Appendix A and sections 3 and 6",
        "reasoning": "The paper explicitly states in Appendix A that the MatSeg dataset, benchmark, full documentation, python readers, evaluation scripts, croissant metadata, and generation scripts used to create the synthetic data are permanently available at provided URLs. It also mentions that over 300,000 extracted textures and SVBRDF/PBR materials, as well as the documented code used for extraction, are permanently available at these URLs. Additionally, the training code and scripts for the nets are made available. Sections 3 and 6 describe the data generation process and training details, supporting that the code for data collection, preprocessing, and generation is publicly accessible, enabling reproducibility of the datasets introduced."
      }
    ]
  },
  {
    "id": "BZe6dmDk5K-rubric-10",
    "token_usage": {
      "prompt_tokens": 32985,
      "completion_tokens": 122,
      "total_tokens": 33107
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3 Dataset Acquisition, Appendix C Implementation Details, and checklist section 3(a)",
        "reasoning": "The paper clearly states that all assets are accessible at https://github.com/zijianchen98/GAIA, including code and data for dataset construction and experiments. The checklist confirms inclusion of code, data and instructions needed to reproduce main experimental results. Appendix C provides detailed implementation details and URLs for text-to-video models. This evidence strongly indicates that all code related to data collection, preprocessing, and generation is publicly available in an accessible repository (GitHub)."
      }
    ]
  },
  {
    "id": "D1MOK2t2t2-rubric-10",
    "token_usage": {
      "prompt_tokens": 10588,
      "completion_tokens": 120,
      "total_tokens": 10708
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Sections 2.3 and 3",
        "reasoning": "The paper explicitly states that the authors provide a streamlined codebase for training new models from the Demonstrations Dataset and for evaluating them against the leaderboard from the Evaluation Dataset. In Section 2.3, they describe a Python-installable library including tools for behavior cloning training and conducting human evaluations, with code released publicly at https://github.com/minerllabs/basalt-benchmark. This confirms the availability of all relevant code to utilize, preprocess, and generate dataset components, facilitating reproducibility."
      }
    ]
  },
  {
    "id": "DFr5hteojx-rubric-10",
    "token_usage": {
      "prompt_tokens": 91530,
      "completion_tokens": 107,
      "total_tokens": 91637
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 6 (Codebooks)",
        "reasoning": "The abstract provides direct links to the dataset and code repository on GitHub (https://github.com/HannahKirk/prism-alignment) and HuggingFace (https://huggingface.co/datasets/HannahRoseKirk/prism-alignment), indicating the code is publicly available. Additionally, the supplementary material includes detailed codebooks for the dataset, which further supports availability and transparency of data processing code."
      }
    ]
  },
  {
    "id": "DJVyRhT8nP-rubric-10",
    "token_usage": {
      "prompt_tokens": 26369,
      "completion_tokens": 129,
      "total_tokens": 26498
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section B Simulator Details and B.1 HAPS Dataset",
        "reasoning": "The paper explicitly states that the HA3D simulator's source code is available in their GitHub repository at https://github.com/lpercc/HA3D_simulator. Additionally, they provide a detailed description of the Human Activity and Pose Simulation (HAPS) dataset collection and annotation process, including the use of GPT-4 and human surveys, facilitating reproducibility. The availability of the interactive annotation tool and detailed prompts for instruction generation in the HA-R2R dataset further supports reproducibility of data generation and preprocessing steps."
      }
    ]
  },
  {
    "id": "DjCSjizgsH-rubric-10",
    "token_usage": {
      "prompt_tokens": 15189,
      "completion_tokens": 91,
      "total_tokens": 15280
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 5 Conclusion and Supplementary Material",
        "reasoning": "The paper clearly states in the Conclusion section that the dataset, model code, and documentation are publicly accessible via the provided GitHub link: https://github.com/TJU-IDVLab/Sim2Real-Fire. This indicates that all code related to data acquisition, preprocessing, simulation generation, and model training/testing is publicly available, enabling reproducibility."
      }
    ]
  },
  {
    "id": "E18kRXTGmV-rubric-10",
    "token_usage": {
      "prompt_tokens": 19485,
      "completion_tokens": 125,
      "total_tokens": 19610
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist and Section 3 Experimental Setup",
        "reasoning": "In the checklist, the authors answer 'Yes' to including the code, data, and instructions needed to reproduce main experimental results (3a). Though the details are brief in the main text, the link to the dataset is provided (https://huggingface.co/datasets/afaji/cvqa), and the experimental setup section mentions prompts and evaluation, indicating reproducibility support. Therefore, code related to data collection, preprocessing, and generation is publicly available or at least provided to reproduce the benchmark."
      }
    ]
  },
  {
    "id": "EADRzNJFn1-rubric-10",
    "token_usage": {
      "prompt_tokens": 27509,
      "completion_tokens": 128,
      "total_tokens": 27637
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4 (Datasets), Appendix E (Data Processing Details), Appendix C (Dataset Documentation and Intended Use)",
        "reasoning": "The paper explicitly states that the code and datasets are publicly available with download links provided in Appendix C and that mining scripts for dataset collection are available on GitHub as detailed in Appendix E. The 'Reproducibility' section in the introduction also confirms this. Furthermore, Appendix C includes links to the TGB2 Github repository containing code to reproduce dataset downloading, processing, and experimental results, confirming that all code related to data collection and preprocessing is available publicly."
      }
    ]
  },
  {
    "id": "FXTeJvHE0k-rubric-10",
    "token_usage": {
      "prompt_tokens": 14263,
      "completion_tokens": 126,
      "total_tokens": 14389
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and supplementary pdf; see Abstract and supplementary pdf section.",
        "reasoning": "The paper explicitly states in the checklist under section 3(a) that 'Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Abstract & supplementary pdf.' Additionally, the abstract provides a GitHub URL (https://github.com/autonomousvision/navsim) for accessing the code. This indicates that the code related to the dataset construction, data curation, and evaluation framework is publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "GHlJM45fWY-rubric-10",
    "token_usage": {
      "prompt_tokens": 19784,
      "completion_tokens": 111,
      "total_tokens": 19895
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 5, Section C (Training Strategy and Hyperparameters), and Datasheet - Preprocessing/cleaning/labeling",
        "reasoning": "The paper explicitly states that all preprocessing and cleaning code is available through Kaggle and GitHub. Moreover, in Section 5 and Appendix C, it details reproducible training strategies and hyperparameters, providing code for baseline models. The datasheet confirms that software used to preprocess/clean/label the data is publicly accessible, supporting reproducibility of the dataset and experiments."
      }
    ]
  },
  {
    "id": "HB5q6pC5eb-rubric-10",
    "token_usage": {
      "prompt_tokens": 22294,
      "completion_tokens": 93,
      "total_tokens": 22387
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Checklist section",
        "reasoning": "The paper explicitly states in the abstract and in the Checklist section that their code and data are publicly available at https://github.com/aigc-apps/PertEval. This repository includes the implementation of the PertEval toolkit, which is responsible for generating the perturbed datasets used in the experiments. Therefore, the code related to dataset generation and processing is accessible, enabling reproducibility."
      }
    ]
  },
  {
    "id": "IZtX4RNBeH-rubric-10",
    "token_usage": {
      "prompt_tokens": 12333,
      "completion_tokens": 136,
      "total_tokens": 12469
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 3.2, Checklist item 4(c)",
        "reasoning": "The paper introduces a new benchmark dataset named Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES). It states in the Abstract and Section 3.2 that the dataset and code are publicly available at mbzuai-oryx.github.io/CVRR-Evaluation-Suite/. The checklist section confirms that the authors included the dataset and code in the supplemental materials and provided a public URL. Thus, the code related to data collection, preprocessing, and question-answer generation for CVRR-ES is made publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "JU0QvhhfVp-rubric-10",
    "token_usage": {
      "prompt_tokens": 12902,
      "completion_tokens": 125,
      "total_tokens": 13027
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 3.1",
        "reasoning": "The paper explicitly states in the abstract that MOTIVE resources are available at https://github.com/carpenter-singh-lab/motive, indicating public availability of code for the dataset. Additionally, Section 3.1 references preprocessing pipelines for morphological profiles available at https://github.com/broadinstitute/jump-profiling-recipe/tree/v0.1.0, supporting reproducibility of feature extraction. Therefore, all code related to data collection, preprocessing, and generation of the MOTIVE dataset is made publicly accessible."
      }
    ]
  },
  {
    "id": "KZlJF8kguO-rubric-10",
    "token_usage": {
      "prompt_tokens": 22740,
      "completion_tokens": 113,
      "total_tokens": 22853
    },
    "response": [
      {
        "Has Code": true,
        "reference": "C Data documentation section and checklist item 3(a)",
        "reasoning": "The paper states that all transcripts and annotations will be made publicly available, along with a quickstart IPython notebook. Additionally, checklist item 3(a) confirms that the code, data, and instructions needed to reproduce the main experimental results are provided with the URL provided for the Brain Treebank dataset (https://BrainTreebank.dev/). This indicates that the code related to data collection, preprocessing, and generation necessary for reproducibility is available."
      }
    ]
  },
  {
    "id": "L5aY1mWvXQ-rubric-10",
    "token_usage": {
      "prompt_tokens": 12987,
      "completion_tokens": 102,
      "total_tokens": 13089
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 4",
        "reasoning": "The paper explicitly states in the abstract and Section 4 that the code and datasets for generating the temporally distorted versions of test splits are publicly available at the provided GitHub repository (https://github.com/Aniq55/TLPCF.git). Since the distortion techniques and generation procedures are introduced by the authors and their code is shared, the code for constructing these new distorted datasets is publicly accessible, ensuring reproducibility."
      }
    ]
  },
  {
    "id": "LdRZ9SFBku-rubric-10",
    "token_usage": {
      "prompt_tokens": 24312,
      "completion_tokens": 147,
      "total_tokens": 24459
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section A (Addition Statement for Our New Dataset), specifically Sec. A.1 Dataset Documentation and Intended Use",
        "reasoning": "The paper explicitly states in Appendix A that the dataset is hosted on ModelScope with clear instructions and code snippets to load the data. It provides detailed documentation, download links, and an example code for usage. The authors mention that the data processing pipeline is designed to be modular and easily replaceable, suggesting that the code related to data preprocessing and generation is publicly available. Furthermore, the paper notes users can access the dataset programmatically with the provided Python code. These indicate the code related to data collection, preprocessing, and dataset construction is made publicly accessible for reproducibility."
      }
    ]
  },
  {
    "id": "LdxNWDNvC3-rubric-10",
    "token_usage": {
      "prompt_tokens": 19391,
      "completion_tokens": 118,
      "total_tokens": 19509
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4.3 Baseline Methods and Appendix A AF-200K Dataset",
        "reasoning": "The paper explicitly states that a comprehensive and publicly accessible codebase is released to facilitate future research, including data-driven generative models for airfoil inverse design and data processing. They also mention an open-source codebase of data and benchmark along with the dataset available on their GitHub repository (https://github.com/hitcslj/AFBench). This indicates that the code related to data collection, preprocessing, and generation is made publicly available."
      }
    ]
  },
  {
    "id": "M32Ldpp4Oy-rubric-10",
    "token_usage": {
      "prompt_tokens": 31369,
      "completion_tokens": 123,
      "total_tokens": 31492
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3 and Section 5.1, also noted in the abstract and checklist",
        "reasoning": "The paper explicitly states that all code and data are open-sourced on their website (mentioned in the abstract). In the checklist, under section 3a, the authors confirm that the code, data, and instructions needed to reproduce the main experimental results are fully open sourced on their website. Several sections including Appendix A and the detailed baseline configurations reference the code library. This indicates that the dataset generation, simulation, and rendering code are publicly available to the research community."
      }
    ]
  },
  {
    "id": "MU2s9wwWLo-rubric-10",
    "token_usage": {
      "prompt_tokens": 25540,
      "completion_tokens": 114,
      "total_tokens": 25654
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist item 4(c), Section 4.1, and Appendix C",
        "reasoning": "The paper states in the Checklist and in the main text (Section 4.1) that the code, data, and instructions needed to reproduce the main experimental results and the new dataset (CONCEPTMIX) are provided in the supplemental material. Appendix C details the benchmark prompt generation process, implying availability of the code for dataset construction. Therefore, the authors have made the code related to data generation publicly accessible for reproducibility."
      }
    ]
  },
  {
    "id": "Mbd3QxXjq5-rubric-10",
    "token_usage": {
      "prompt_tokens": 26110,
      "completion_tokens": 143,
      "total_tokens": 26253
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Introduction and Section 2",
        "reasoning": "The paper explicitly states that the dataset generation code and resources are publicly released under a commercially permissive license. In the Introduction, it mentions releasing code, models, and the OpenMathInstruct-1 dataset. The data synthesis and prompting strategies are described in detail in Section 2, and the paper provides URLs and references to the code base (e.g., https://github.com/NVIDIA/NeMo-Skills) and data collection tools (e.g., TensorRT-LLM toolkit). This indicates that all code related to data collection, preprocessing, and solution generation is made available for reproducibility."
      }
    ]
  },
  {
    "id": "OL2JQoO0kq-rubric-10",
    "token_usage": {
      "prompt_tokens": 21177,
      "completion_tokens": 100,
      "total_tokens": 21277
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.1 and Supplementary Material A.1",
        "reasoning": "The paper explicitly states in Section 3.1 and the supplementary material A.1 that the data and code will be available at Quilt-1M's website. It describes the curation pipeline involving multiple models and handcrafted algorithms and mentions providing code to recreate the dataset, indicating that all code related to data collection, preprocessing, and generation is made publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "OTjTKFk7gb-rubric-10",
    "token_usage": {
      "prompt_tokens": 20725,
      "completion_tokens": 118,
      "total_tokens": 20843
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Appendix B.5",
        "reasoning": "The paper states in the Abstract and Appendix B.5 that the code of the environment is implemented in Python and is available at a public GitHub repository (https://github.com/alimama-tech/AuctionNet). Appendix B.5 further clarifies that the AuctionNet Dataset and environment will be open-sourced via GitHub after the NeurIPS 2024 competition ends. This indicates that code relevant to data generation and environment construction is publicly accessible for reproducibility, albeit after competition completion."
      }
    ]
  },
  {
    "id": "PcbSZwVVc5-rubric-10",
    "token_usage": {
      "prompt_tokens": 19669,
      "completion_tokens": 123,
      "total_tokens": 19792
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 4, and Appendix C.1",
        "reasoning": "The paper explicitly states in the Abstract and multiple sections that the DreamCatcher dataset is publicly available along with open-source code for setting up benchmarks and constructing the earable hardware. Specifically, Section 4.2 mentions leveraging open-source code repositories for benchmark models, and Appendix C.1 details implementations referencing publicly available code. The GitHub repository link is provided in the Abstract and throughout the paper, indicating that all code related to data collection, preprocessing, and generation is accessible for reproducibility purposes."
      }
    ]
  },
  {
    "id": "QIJQ1qCGqV-rubric-10",
    "token_usage": {
      "prompt_tokens": 12969,
      "completion_tokens": 85,
      "total_tokens": 13054
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist item 3(a)",
        "reasoning": "The paper's checklist under section 3(a) explicitly states that the authors included the code, data, and instructions needed to reproduce the main experimental results, either in the supplemental material or as a URL. This indicates that the code related to data collection, preprocessing, and generation is publicly available, supporting reproducibility."
      }
    ]
  },
  {
    "id": "Qf8uzIT1OK-rubric-10",
    "token_usage": {
      "prompt_tokens": 36294,
      "completion_tokens": 114,
      "total_tokens": 36408
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Throughout the paper, no section, appendix, or supplementary material describes or provides code related to constructing a new dataset.",
        "reasoning": "The paper discusses ethical considerations and recommendations for responsible data curation in human-centric computer vision datasets, primarily focusing on guidelines, checklist items, and literature review. It does not introduce nor describe any new datasets curated by the authors themselves, nor does it provide associated code for data collection, preprocessing, or generation. Therefore, there is no code publicly available as part of this work."
      }
    ]
  },
  {
    "id": "QpF3DFP3Td-rubric-10",
    "token_usage": {
      "prompt_tokens": 18253,
      "completion_tokens": 116,
      "total_tokens": 18369
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4.1 and supplementary materials",
        "reasoning": "The paper explicitly states in Section 4.1 and in the checklist (Section A.1) that the code, data, and instructions needed to reproduce the main experimental results are included either in the supplementary materials or as a URL. Moreover, the annotation software is open source, and anonymized preprocessing code is available as mentioned in Section A.7.4. This indicates that the code related to data preprocessing and generation is publicly accessible for reproducibility."
      }
    ]
  },
  {
    "id": "R4rNYJ2slJ-rubric-10",
    "token_usage": {
      "prompt_tokens": 15734,
      "completion_tokens": 119,
      "total_tokens": 15853
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 6 (Availability and Maintenance) and A.1.3 Collection Process",
        "reasoning": "The paper explicitly states in Section 6 that the project page (https://opensatmap.github.io) contains the full dataset with annotations and code repository, including code for collecting images from Google Maps and baseline models. In the supplement (A.1.3), it further clarifies that the data collection code will be provided in their GitHub repository. Thus, all code related to data collection and dataset generation is publicly available in an accessible repository."
      }
    ]
  },
  {
    "id": "ScPgzCZ6Lo-rubric-10",
    "token_usage": {
      "prompt_tokens": 34104,
      "completion_tokens": 101,
      "total_tokens": 34205
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 5 Conclusion and Future Works; Appendix C Reproducibility and Limitations",
        "reasoning": "The paper explicitly states that all datasets, algorithm implementations, and experimental settings are publicly available in their open project repository (https://github.com/RingBDStack/GC-Bench). Appendix C details accessibility and licensing, confirming the availability of all codes related to data collection, preprocessing, and generation necessary for reproducing the datasets and experiments discussed in the paper."
      }
    ]
  },
  {
    "id": "USUkwg5pW6-rubric-10",
    "token_usage": {
      "prompt_tokens": 22554,
      "completion_tokens": 172,
      "total_tokens": 22726
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 3.2, Section 4, and B.4 Preprocessing/cleaning/labeling",
        "reasoning": "The paper explicitly states that the algorithm to generate scribble labels is publicly available at https://github.com/wbkit/Scribbles4All, which includes the code for generating the datasets. The scribble generation algorithm is described in detail in Section 3.2 and the parameterization and runtime information are provided in the supplementary material (Section A.2 and A.3). Furthermore, in Appendix B.4, the authors clarify that the scribble labels are generated by the algorithm presented in the paper and that the code is available on GitHub. Hence, all code related to data generation is publicly accessible, allowing reproducibility."
      }
    ]
  },
  {
    "id": "UYgE9IfQIV-rubric-10",
    "token_usage": {
      "prompt_tokens": 28599,
      "completion_tokens": 122,
      "total_tokens": 28721
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4 and Appendix A.1 - A.3",
        "reasoning": "The paper clearly states that the SustainDC environments, which include workload, data center, and battery simulations, are implemented in Python and available at a public GitHub repository (https://github.com/HewlettPackard/dc-rl). Furthermore, the paper provides detailed configuration files and supplementary mathematical and operational models in Appendix A, enabling reproducibility of dataset and simulation generation. The documentation and setup instructions are also linked, ensuring accessibility and reproducibility of the data and environment construction code."
      }
    ]
  },
  {
    "id": "VH1vxapUTs-rubric-10",
    "token_usage": {
      "prompt_tokens": 15749,
      "completion_tokens": 117,
      "total_tokens": 15866
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 7 Availability and Maintenance",
        "reasoning": "The paper explicitly states in Section 7 that the Mesogeos datacube and the derived datasets are made publicly available. Furthermore, the code repository with the code for generating Mesogeos, extracting datasets for the ML tracks, and running models is openly published (https://github.com/Orion-AI-Lab/mesogeos), enabling reproduction of the results and dataset creation pipeline. This confirms that all code related to data collection, preprocessing, and dataset generation is publicly available."
      }
    ]
  },
  {
    "id": "VSJotgbPHF-rubric-10",
    "token_usage": {
      "prompt_tokens": 11354,
      "completion_tokens": 123,
      "total_tokens": 11477
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract; Section 6.1; Footnote 5",
        "reasoning": "The paper explicitly states in the Abstract that they release their code and data under a fully permissive license. Additionally, Section 6.1 mentions that all trained models and related artifacts are released at https://huggingface.co/OpenAssistant, and Footnote 5 confirms that all trained models are publicly available there, strongly implying the availability of associated code. The public hosting on Hugging Face alongside a permissive license supports that the dataset construction and processing code is accessible, enabling reproducibility."
      }
    ]
  },
  {
    "id": "VXohja0vrQ-rubric-10",
    "token_usage": {
      "prompt_tokens": 17383,
      "completion_tokens": 148,
      "total_tokens": 17531
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist item 3(a) and 3(b); Section 2.2; footnote link at Abstract; GitHub repository link provided",
        "reasoning": "The paper states that MEDCALC-BENCH is publicly available at https://github.com/ncbi-nlp/MedCalc-Bench, which indicates that the data as well as the code needed for data processing and evaluation is accessible. Furthermore, in the checklist section, the authors confirm that code, data, and instructions needed to reproduce the main experimental results are described in the supplemental material and are publicly available. These references strongly confirm that all code related to data collection, preprocessing, and generation is publicly available in an accessible repository."
      }
    ]
  },
  {
    "id": "WUWHVN4gxk-rubric-10",
    "token_usage": {
      "prompt_tokens": 17895,
      "completion_tokens": 146,
      "total_tokens": 18041
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3 'Competition Outcomes' and Section 4.1 'Dataset Structure'",
        "reasoning": "The paper explicitly mentions that they have open-sourced the entire capture-the-flag competition codebase used for data collection and generation, available at https://github.com/ethz-spylab/satml-llm-ctf. Additionally, they provide a dataset codebase for data analysis at https://github.com/ethz-spylab/ctf-satml24-data-analysis and the dataset is available on HuggingFace. This indicates that all the code related to dataset construction, including the competition platform and data analysis, is publicly available, enabling reproducibility."
      }
    ]
  },
  {
    "id": "WVQ4Clw1VD-rubric-10",
    "token_usage": {
      "prompt_tokens": 10586,
      "completion_tokens": 119,
      "total_tokens": 10705
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 4 (Checklist)",
        "reasoning": "The paper states in the Abstract that the dataset is publicly available at https://yunfeixie233.github.io/MedTrinity-25M/. Furthermore, in the Checklist (item 3a), the authors confirm that they included the code, data, and instructions needed to reproduce the main experimental results, referring to the project page mentioned in the abstract. This implies that the code for data collection, preprocessing, and generation is publicly shared in an accessible repository linked on the project page."
      }
    ]
  },
  {
    "id": "XBcStBjBIE-rubric-10",
    "token_usage": {
      "prompt_tokens": 28932,
      "completion_tokens": 123,
      "total_tokens": 29055
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract footnote and Appendix Section F",
        "reasoning": "The paper explicitly states in the abstract footnote and Appendix Section F that the source data and code are publicly available on https://pku-yuangroup.github.io/ChronoMagic-Bench. Furthermore, it confirms the inclusion of code and detailed instructions for data collection and preprocessing, enabling reproducibility of the dataset construction process. Therefore, the code related to data collection, preprocessing, and generation for the new datasets introduced (ChronoMagic-Bench and ChronoMagic-Pro) is publicly accessible in an official repository."
      }
    ]
  },
  {
    "id": "ZDvXY56DeP-rubric-10",
    "token_usage": {
      "prompt_tokens": 17975,
      "completion_tokens": 133,
      "total_tokens": 18108
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 2.2 and 3",
        "reasoning": "The paper explicitly states that each experiment in the ORLB dataset includes all necessary information for precise reproduction, including full parameters, frozen versions of dependencies, exact command lines, and random seeds (Section 2.2). Furthermore, the authors mention a utility in the CleanRL project that generates commands for environment setup and experiment replication. The CLI and code examples provided (including in appendices) demonstrate accessible code support for reproducing all experiments and generating plots. Therefore, the code related to data collection, preprocessing, and generation is publicly available and referenced in the paper."
      }
    ]
  },
  {
    "id": "aTXhTD44nF-rubric-10",
    "token_usage": {
      "prompt_tokens": 22595,
      "completion_tokens": 106,
      "total_tokens": 22701
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 3 (USDC Dataset Curation), and Sections 4 (Training Small Language Models)",
        "reasoning": "The paper explicitly states in the abstract and multiple sections that they make the code and dataset publicly available. They describe the dataset collection using Reddit API (praw), the pipeline for annotation generation, and the training of models with full implementation details. The explicit mention of releasing code and dataset indicates code availability for reproducing data collection, preprocessing, and generation steps."
      }
    ]
  },
  {
    "id": "aXeiCbMFFJ-rubric-10",
    "token_usage": {
      "prompt_tokens": 21196,
      "completion_tokens": 169,
      "total_tokens": 21365
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist Section 3(a), Appendix B, and Supplementary Materials; also the repository links in the Appendix A Overview",
        "reasoning": "The paper explicitly states in the Checklist (3a) that the code, data, and instructions needed to reproduce the main experimental results, including dataset construction, are provided. They give a GitHub URL (https://github.com/deepcs233/Visual-CoT) which contains the code, training data, benchmark, and model checkpoints. Appendix A and B refer to training details and prompt design for dataset generation, indicating transparency and availability of the related code. Furthermore, dataset and model assets are shared on HuggingFace hubs with specified licenses. This demonstrates the authors have made all relevant code for dataset collection, preprocessing, and generation publicly accessible, supporting reproducibility."
      }
    ]
  },
  {
    "id": "abXaOcvujs-rubric-10",
    "token_usage": {
      "prompt_tokens": 14005,
      "completion_tokens": 109,
      "total_tokens": 14114
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.4 and footnote 1",
        "reasoning": "The paper states in Section 3.4 that the code is made publicly available on GitHub under a CC-BY license (footnote 1 points to https://github.com/DataManagementLab/wikidbs-public). The repository includes instructions and all necessary files required to reproduce the dataset construction from the Wikidata dump. Therefore, all code related to data collection, preprocessing, and generation is publicly accessible, ensuring reproducibility."
      }
    ]
  },
  {
    "id": "aiGN4UnNM7-rubric-10",
    "token_usage": {
      "prompt_tokens": 15259,
      "completion_tokens": 111,
      "total_tokens": 15370
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 4.1, and Checklist item 3(a)",
        "reasoning": "The paper states in the abstract that the proposed dataset and code are publicly available at provided dataset and code links. Additionally, in the checklist under item 3(a), the authors confirm that they include the code, data, and instructions needed to reproduce the main experimental results. This implies that the code related to data collection, preprocessing, and generation (including neural rendering with NeRF) is accessible, supporting reproducibility."
      }
    ]
  },
  {
    "id": "b6IBmU1uzw-rubric-10",
    "token_usage": {
      "prompt_tokens": 29856,
      "completion_tokens": 144,
      "total_tokens": 30000
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Appendix D.3 and main paper Section 2 'CARES Datasets'",
        "reasoning": "The paper explicitly states in Appendix D.3 that the code for their experiments, which includes data curation procedures, is publicly available at https://github.com/richard-peng-xia/CARES. Moreover, in Section 2, the data curation process and the construction of question-answer pairs from existing medical datasets are described, and the authors mention providing detailed templates and instructions, implying that code for preprocessing and dataset construction is included in the released repository. Therefore, all code related to data collection, preprocessing, and generation is made publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "bAaM8cKoMl-rubric-10",
    "token_usage": {
      "prompt_tokens": 27832,
      "completion_tokens": 151,
      "total_tokens": 27983
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4 (Code and Resources)",
        "reasoning": "The paper states that the authors provide both ready-to-use datasets and scripts to generate them with configurable parameters. It mentions that each dataset has a configuration file specifying available parameters, and that the default configuration file used to generate ready-to-use versions is included. The output of the scripts is the dataset with annotations. Moreover, code and utilities for evaluating DNNs on these datasets using three different methods are provided. The resources are openly available at the provided GitHub link (https://github.com/ValerioB88/mindset-vision) under the MIT license. Thus, the code for data generation and preprocessing is publicly accessible, enabling reproducibility."
      }
    ]
  },
  {
    "id": "cLga8GStdk-rubric-10",
    "token_usage": {
      "prompt_tokens": 14118,
      "completion_tokens": 144,
      "total_tokens": 14262
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.2 and Appendix E.1; also mentioned in Checklist item 3(a)",
        "reasoning": "The paper states in Section 3.2 that python scripts were used for parsing and formatting the puzzles into a standardized JSON format, and that these scripts and validation tools are available in their GitHub repository (https://github.com/am-bean/lingOly). Furthermore, Appendix E.1 and the checklist confirm that all the code, data, and instructions needed to reproduce the main experimental results are publicly released and available at the given GitHub URL. Therefore, all code related to data collection, preprocessing, and generation is publicly accessible for reproducibility."
      }
    ]
  },
  {
    "id": "cR3T1ZYN8I-rubric-10",
    "token_usage": {
      "prompt_tokens": 34498,
      "completion_tokens": 100,
      "total_tokens": 34598
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4.2 Datasets and abstract; 'The benchmarking code will be made publicly available upon publication, including pre-processing steps.'",
        "reasoning": "The paper does not introduce new datasets but uses nine previously published benchmarks. It indicates that the benchmarking code and pre-processing steps will be made publicly available upon publication. This suggests that the dataset preprocessing and generation code used in their benchmark is or will be publicly accessible, supporting reproducibility."
      }
    ]
  },
  {
    "id": "cu8FfaYriU-rubric-10",
    "token_usage": {
      "prompt_tokens": 27472,
      "completion_tokens": 100,
      "total_tokens": 27572
    },
    "response": [
      {
        "Has Code": false,
        "reference": "No explicit section",
        "reasoning": "The paper does not mention the introduction of any new datasets by the authors, nor does it provide or reference any publicly available code repositories for dataset construction, collection, preprocessing, or generation. The focus of the paper is on reporting findings from interviews and a taxonomy of challenges related to fair dataset curation, rather than on providing new datasets or associated code. Therefore, there is no information indicating any code availability for reproducibility."
      }
    ]
  },
  {
    "id": "cy8mq7QYae-rubric-10",
    "token_usage": {
      "prompt_tokens": 80079,
      "completion_tokens": 103,
      "total_tokens": 80182
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4.1 and W.7 Maintenance",
        "reasoning": "The paper mentions inclusion of the codebase and details for reproducing experiments in Section 4.1 and the responses indicate that the codebase and data are released on GitHub and HuggingFace as stated in W.7 maintenance and the dataset structure in section X Misc. This implies that all code related to data collection, preprocessing, and generation is publicly available in accessible repositories, enabling reproducibility."
      }
    ]
  },
  {
    "id": "d1Pup4gkWf-rubric-10",
    "token_usage": {
      "prompt_tokens": 12743,
      "completion_tokens": 102,
      "total_tokens": 12845
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist item 3(a) and Abstract, Section 5 Implementation Details",
        "reasoning": "The paper states in the checklist (item 3a) that the code and environment will be included in the supplement and open-sourced. Additionally, the abstract and implementation details mention providing a unified sports benchmark and baseline implementations, implying code availability. Therefore, the code related to their new simulated sports environments (datasets) is publicly available or will be made available for reproducibility."
      }
    ]
  },
  {
    "id": "dVaWCDMBof-rubric-10",
    "token_usage": {
      "prompt_tokens": 21688,
      "completion_tokens": 139,
      "total_tokens": 21827
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.2 and 3.4; Abstract; Conclusions",
        "reasoning": "The paper explicitly states that the authors publicly release the COMMONPOOL dataset index and the entire code and infrastructure for data collection, preprocessing, filtering, training, and evaluation at www.datacomp.ai. Specifically, Section 3.2 details the pipeline and mentions releasing dataset metadata and tooling as dataset2metadata on GitHub. Additionally, Section 3.4 describes fixed training code being open-sourced. The abstract and conclusion reaffirm code and data release to enable reproducibility and community use. Thus, all code related to dataset construction and processing is made publicly available."
      }
    ]
  },
  {
    "id": "g7OX2sOJtn-rubric-10",
    "token_usage": {
      "prompt_tokens": 45155,
      "completion_tokens": 158,
      "total_tokens": 45313
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4 'LeanDojo: Toolkit and Benchmark'; Section B.3 'Data Hosting, Licensing, and Maintenance'; Section 7 'Conclusion'",
        "reasoning": "The paper introduces the LeanDojo Benchmark as a new dataset constructed by extracting data from Lean's math library using an open-source toolkit named LeanDojo. The authors explicitly state that they have released the data extraction code, models, and datasets under permissive licenses. Section 4 describes the data extraction process, Section B.3 specifies that the data and code are hosted on zenodo.org and GitHub respectively, and Section 7 includes URLs to the data and code repositories. This confirms that the code related to data collection and generation is publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "gg3POFjqq8-rubric-10",
    "token_usage": {
      "prompt_tokens": 10870,
      "completion_tokens": 122,
      "total_tokens": 10992
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist point 4(c) and Section 4 of the paper",
        "reasoning": "The paper explicitly mentions in the Checklist (point 4c) that they have provided new assets in the form of code and data to reproduce their findings. Furthermore, the paper describes in Section 4 the collection of their newly introduced dataset for evaluating out-of-class filtering strategies and details the filtering mechanism used. Their methodology and dataset collection are described thoroughly, and their code and data necessary for reproducing their dataset generation and filtering are indicated as being made publicly available in the supplemental materials."
      }
    ]
  },
  {
    "id": "h8LuywKj6N-rubric-10",
    "token_usage": {
      "prompt_tokens": 43915,
      "completion_tokens": 162,
      "total_tokens": 44077
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3, and Appendix D.1, specifically 'Inference' and 'Finetune' details in Section D.2 and details in Appendix D.",
        "reasoning": "The paper states in the checklist under item 3(a) that the code, data, and instructions needed to reproduce the main experimental results are provided either in the supplemental material or as a URL. Additionally, Appendix D provides detailed experiment setups including hyperparameters and data splits. The text mentions custom annotation software used for data labeling but does not specify public access information there; however, the checklist implies that the code and datasets are included for reproducibility. Thus, code related to data collection, preprocessing, and generation is publicly available to allow reproducibility of the dataset and experiments."
      }
    ]
  },
  {
    "id": "hcOq2buakM-rubric-10",
    "token_usage": {
      "prompt_tokens": 32896,
      "completion_tokens": 119,
      "total_tokens": 33015
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Section 3 and Appendix J.2",
        "reasoning": "The paper assesses 24 existing AI benchmarks but does not introduce any new dataset constructed by the authors. It mentions the use of publicly available official websites, papers, and GitHub repositories published by the benchmark developers for evaluation purposes, but there is no indication that the authors constructed any new dataset or released code for dataset construction. The code to replicate the assessment results will be made available, but this pertains to their evaluation framework, not to data collection or generation for new datasets."
      }
    ]
  },
  {
    "id": "iSwK1YqO7v-rubric-10",
    "token_usage": {
      "prompt_tokens": 97116,
      "completion_tokens": 102,
      "total_tokens": 97218
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Appendix Q.5",
        "reasoning": "The paper explicitly states in Appendix Q.5 that the codebase for dataset annotation and evaluation is publicly available on GitHub at https://github.com/embodied-agent-interface/embodied-agent-interface/. It provides detailed instructions and tools for reproducing the dataset annotations, evaluation scripts, and LLM implementation setups. This ensures that all code related to data collection, preprocessing, and dataset generation is accessible, supporting reproducibility."
      }
    ]
  },
  {
    "id": "iTUlYblV0K-rubric-10",
    "token_usage": {
      "prompt_tokens": 19354,
      "completion_tokens": 120,
      "total_tokens": 19474
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 4",
        "reasoning": "The paper explicitly provides a GitHub repository link in the abstract (https://github.com/henryzhongsc/MQuAKE-Remastered) for assets including the remastered dataset, indicating that the code related to dataset fixing and preprocessing is publicly available. Section 4 further describes the remastering process and the development of an API to dynamically mask conflicting edits, implying that the associated implementation is accessible to users. Therefore, the paper offers all necessary code for reproducing the dataset construction and preprocessing steps."
      }
    ]
  },
  {
    "id": "iwC19lVBoq-rubric-10",
    "token_usage": {
      "prompt_tokens": 12725,
      "completion_tokens": 130,
      "total_tokens": 12855
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Appendix B.3 and Checklist item 3(a)",
        "reasoning": "The paper states in Appendix B.3 that the AVSET-10M dataset can be downloaded from https://avset-10M.github.io, and in the checklist under item 3(a), the authors confirm that they include the code, data, and instructions needed to reproduce the main experimental results, noting that their experiments are based on other open-source works such as ImageBind and ClipSep. This indicates that code related to data collection, preprocessing, and generation is made publicly available in an accessible repository."
      }
    ]
  },
  {
    "id": "j2wasUypqN-rubric-10",
    "token_usage": {
      "prompt_tokens": 18959,
      "completion_tokens": 155,
      "total_tokens": 19114
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract; Section 3.2; Appendix A",
        "reasoning": "The paper explicitly states that MetaBox provides over 300 problem instances, including Synthetic, Noisy-Synthetic, and Protein-Docking problem sets. These testsuites are incorporated as datasets within the MetaBox benchmark platform, which is open-sourced and accessible on GitHub (https://github.com/GMC-DRL/MetaBox). Section 3.2 describes these testsuites in detail, and Appendix A further details their composition and characteristics. The availability of the code for problem instances generation and preparation within the open-source MetaBox repository confirms that the code for constructing these new datasets (testsuites) introduced by the authors is publicly available, ensuring reproducibility."
      }
    ]
  },
  {
    "id": "jSKtxmxc0M-rubric-10",
    "token_usage": {
      "prompt_tokens": 19222,
      "completion_tokens": 103,
      "total_tokens": 19325
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 4.1",
        "reasoning": "The paper explicitly states in the Abstract and Section 4.1 that the data and code are available at https://github.com/showlab/videogui. Additionally, in the checklist under 3.(a), they confirm that they will provide code, data samples and instructions in supplementary material. This indicates that all code related to data collection, preprocessing, and generation is made publicly accessible, supporting dataset reproducibility."
      }
    ]
  },
  {
    "id": "jbrMS0DNaD-rubric-10",
    "token_usage": {
      "prompt_tokens": 14924,
      "completion_tokens": 101,
      "total_tokens": 15025
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section 3.1",
        "reasoning": "The paper states in the abstract and in Section 3.1 that the INQUIRE benchmark, including the iNat24 dataset, pre-computed outputs from state-of-the-art models, and code for evaluation are made available at https://inquire-benchmark.github.io/. This indicates that the code relevant to dataset construction, preprocessing, and evaluation is publicly accessible, supporting reproducibility of the new dataset introduced."
      }
    ]
  },
  {
    "id": "kaHpo8OZw2-rubric-10",
    "token_usage": {
      "prompt_tokens": 103683,
      "completion_tokens": 120,
      "total_tokens": 103803
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Section P.2 Composition/collection process/preprocessing/cleaning/labeling and uses",
        "reasoning": "The paper states in the Abstract that the benchmark is publicly available at https://decodingtrust.github.io/. Furthermore, Section P.2 mentions that the dataset is described in the paper as well as on the website, and Section P.3 specifies that the evaluation dataset is released publicly and hosted on GitHub. These indicate that the code related to data collection, preprocessing, and generation is publicly available in accessible repositories, supporting reproducibility."
      }
    ]
  },
  {
    "id": "l985bXCatk-rubric-10",
    "token_usage": {
      "prompt_tokens": 14046,
      "completion_tokens": 121,
      "total_tokens": 14167
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Appendix A.2",
        "reasoning": "The paper explicitly states in the checklist section 4.(c) that they have included new assets either in the supplemental material or as a URL, providing URLs to the assets in Appendix A.2. Furthermore, point 3.(a) confirms that the code, data, and instructions needed to reproduce the main experimental results are available with a URL specified in Appendix A.2. This indicates that all code related to data collection, preprocessing, and generation is publicly accessible, ensuring reproducibility of the dataset creation process."
      }
    ]
  },
  {
    "id": "lnnNPiZtzR-rubric-10",
    "token_usage": {
      "prompt_tokens": 11153,
      "completion_tokens": 131,
      "total_tokens": 11284
    },
    "response": [
      {
        "Has Code": false,
        "reference": "No specific section",
        "reasoning": "The paper extensively describes the construction, attributes, and features of the FungiTastic dataset, including multiple types of metadata and evaluation protocols, but it does not explicitly mention the availability of the code used for data collection, preprocessing, or generation. There is mention of baseline code and model frameworks being available on GitHub and Kaggle for training and evaluation, but no explicit statement that the code for dataset construction is publicly released or accessible. Therefore, based strictly on the paper's content, the code related to dataset construction is not known to be publicly available."
      }
    ]
  },
  {
    "id": "loJM1acwzf-rubric-10",
    "token_usage": {
      "prompt_tokens": 28385,
      "completion_tokens": 118,
      "total_tokens": 28503
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 4 Experimental Setup and Appendix F Author Statement",
        "reasoning": "The paper states in the Author Statement section (Appendix F) that the dataset and related assets are included and publicly available at https://mayubo2333.github.io/MMLongBench-Doc. Additionally, the Experimental Setup section mentions detailed procedures for dataset construction, annotation, and model evaluation, implying that the code and data for these processes are accessible to ensure reproducibility. Therefore, the code for data collection, preprocessing, and generation is publicly available, enabling reproducibility."
      }
    ]
  },
  {
    "id": "mDRmX8IlBI-rubric-10",
    "token_usage": {
      "prompt_tokens": 15495,
      "completion_tokens": 73,
      "total_tokens": 15568
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist Section 3(a)",
        "reasoning": "According to the Checklist, the authors confirm that the code and data needed to reproduce the main experimental results are included in the supplemental material and also provided via a URL link, indicating that the code and dataset construction scripts are publicly available and accessible for reproducibility."
      }
    ]
  },
  {
    "id": "mEJgnZZyfv-rubric-10",
    "token_usage": {
      "prompt_tokens": 15618,
      "completion_tokens": 147,
      "total_tokens": 15765
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 3 (OpenMixup), Section 3.4 (Experimental Pipeline)",
        "reasoning": "The paper explicitly states that it open-sourced a modular codebase for the mixup methods and benchmarking, which includes data preprocessing modules, mixup augmentations, and dataset handling components (Section 3). The source code is publicly available as mentioned in the abstract and the conclusion. This codebase supports all their benchmarking and broader applications, enabling reproducibility. There is no mention of any new dataset collection or generation code because the datasets used are all existing publicly available datasets, but the preprocessing and pipeline to use these datasets with mixup augmentation methods are included in the codebase."
      }
    ]
  },
  {
    "id": "mlhFJE7PKo-rubric-10",
    "token_usage": {
      "prompt_tokens": 38596,
      "completion_tokens": 139,
      "total_tokens": 38735
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 3 and Section 4",
        "reasoning": "The paper states that all code needed to reproduce the main experimental results, including code for data collection, preprocessing, and generation, is made publicly available. The abstract provides a GitHub link (https://github.com/mahmoodlab/hest) for accessing HEST-1k, HEST-Library, and HEST-Benchmark. Further, Sections 3 and 4 describe the data assembly and the HEST-Library, which includes functions and pipelines for data conversion, alignment, and preprocessing, indicating that these are implemented and released for reproducibility."
      }
    ]
  },
  {
    "id": "moMoWj7jLm-rubric-10",
    "token_usage": {
      "prompt_tokens": 22798,
      "completion_tokens": 116,
      "total_tokens": 22914
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Appendix C.4 and Section 1",
        "reasoning": "The paper explicitly states that the code related to dataset construction, preprocessing, and model training and evaluation is publicly available in the GitHub repository https://github.com/nju-websoft/FormulaReasoning, as mentioned in Section 1 and detailed in Appendix C.4. It further specifies that scripts for preprocessing/cleaning/labeling are included in this repository, and the Machine Learning Reproducibility Checklist confirms availability of the code and detailed instructions necessary for reproduction."
      }
    ]
  },
  {
    "id": "nrEqH502eC-rubric-10",
    "token_usage": {
      "prompt_tokens": 78172,
      "completion_tokens": 110,
      "total_tokens": 78282
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Appendix A.1",
        "reasoning": "The paper states in the Abstract and Appendix A.1 that all data samples, including short answer questions, multiple-choice questions, answers, and the codes used in the work are publicly available at https://github.com/nlee0212/BLEnD. The Appendix details the dataset organization, code locations for collecting answers from LLMs, and evaluation scripts, indicating that the code for dataset construction, preprocessing, and generation is accessible, supporting reproducibility."
      }
    ]
  },
  {
    "id": "p8eUitex7p-rubric-10",
    "token_usage": {
      "prompt_tokens": 18935,
      "completion_tokens": 106,
      "total_tokens": 19041
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section C.3 and Checklist item 3(a)",
        "reasoning": "The paper explicitly states in Checklist item 3(a) and Section C.3 that they provide the code and data needed to reproduce their main experimental results, and they make their ImageNet3D dataset and source code publicly available via URLs to GitHub and Huggingface. This indicates that all code related to data collection, preprocessing, and generation is publicly accessible, ensuring reproducibility of their dataset and experiments."
      }
    ]
  },
  {
    "id": "pUcTrjRLOM-rubric-10",
    "token_usage": {
      "prompt_tokens": 33438,
      "completion_tokens": 107,
      "total_tokens": 33545
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 2 and Section 6; GitHub and Huggingface links in Abstract and Conclusion",
        "reasoning": "The paper explicitly states that the UltraMedical datasets, which include synthetic and manual instructions and preference annotations, are released publicly on GitHub and Huggingface repositories. They provide detailed dataset construction methods in Section 2 and appendices and mention open-sourcing the data. This implies that the related code for data collection, preprocessing, and generation is publicly available to ensure reproducibility."
      }
    ]
  },
  {
    "id": "pYNl76onJL-rubric-10",
    "token_usage": {
      "prompt_tokens": 18695,
      "completion_tokens": 173,
      "total_tokens": 18868
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 3 Curating VidProM, Section 6 Automatic Text-to-video Prompt Completion, and Checklist item 3(a) and 4(c)",
        "reasoning": "The paper states that the project, including the collected dataset VidProM and related code, is publicly available at https://vidprom.github.io under the CC-BY-NC 4.0 License. Section 3 details the data collection and preprocessing steps, and Section 6 provides code references for training the prompt completion model along with links to relevant repositories (e.g., Hugging Face and DeepSpeed). Additionally, in the Checklist, the authors confirm including code and data needed for reproducibility and specify the public availability of code and dataset. Hence, sufficient code for dataset construction and related processes is publicly available, supporting reproducibility."
      }
    ]
  },
  {
    "id": "qmvtDIfbmS-rubric-10",
    "token_usage": {
      "prompt_tokens": 26841,
      "completion_tokens": 125,
      "total_tokens": 26966
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract, Section 1, and supplementary materials",
        "reasoning": "The paper explicitly states in the Abstract and Section 1 that WhodunitBench is open-source and accessible via a GitHub repository (https://github.com/jun0wanan/WhodunitBench-Murder_Mystery_Games). Furthermore, in the checklist and supplementary materials, the authors confirm that code, data, and instructions necessary to reproduce the experimental results are provided. This indicates that all code related to data collection, preprocessing, and generation for the new datasets introduced is publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "r8PnfcWQol-rubric-10",
    "token_usage": {
      "prompt_tokens": 30362,
      "completion_tokens": 116,
      "total_tokens": 30478
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Checklist item 3(a)",
        "reasoning": "The paper explicitly states in the abstract that the code and data are provided at a URL (https://anonymous.4open.science/r/EFOK-CQA/README.md). Further, in the checklist under item 3(a), the authors confirm that the code, data, and instructions needed to reproduce the main experimental results, including data generation, are publicly available via the mentioned URL. This indicates that the code related to dataset construction and generation is accessible for reproducibility."
      }
    ]
  },
  {
    "id": "rdv2Fr6JTC-rubric-10",
    "token_usage": {
      "prompt_tokens": 20586,
      "completion_tokens": 128,
      "total_tokens": 20714
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Section 4 and Appendix H.6",
        "reasoning": "The paper uses existing benchmark datasets (e.g., Cora, Citeseer, Pubmed, Amazon-Photo, Amazon-Computer, Coauthor-Physics) and does not introduce any new datasets. There is no description of any new dataset collection or construction process. Thus, there is no code related to new dataset construction to be shared. Moreover, the authors explicitly state in the checklist (4c) that no new assets, including datasets, are included. Therefore, the question of code availability for new datasets is not applicable."
      }
    ]
  },
  {
    "id": "rovpCs3ZEO-rubric-10",
    "token_usage": {
      "prompt_tokens": 23540,
      "completion_tokens": 144,
      "total_tokens": 23684
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section C Platform Repository and Section E.4 Preprocessing/cleaning/labeling",
        "reasoning": "The paper states in Section C that there is a GitHub repository available at https://github.com/psudslab/FEDMEKI which includes data processing, baselines, environmental setup, platform, and sample execution scripts. Additionally, Section E.4 notes that preprocessing, cleaning, and labeling are done through Python scripts. Furthermore, data preprocessing code is available at https://github.com/psudslab/FEDMEKI/tree/main/data_preprocess. Therefore, the code related to data collection, preprocessing, and generation is publicly available in an accessible repository."
      }
    ]
  },
  {
    "id": "s1K5Z5QPog-rubric-10",
    "token_usage": {
      "prompt_tokens": 12144,
      "completion_tokens": 123,
      "total_tokens": 12267
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.1 (Observations) and Section 6 (Conclusion), Appendix B.4",
        "reasoning": "The paper explicitly states that they provide a one-liner script to process higher resolution inputs (Section 3.1), and mention that the data processing pipeline is open-source to allow users to process inputs at desired resolutions (Section 6, Conclusion). Additionally, the authors invite contributions via their GitHub repository at https://github.com/leap-stc/ChaosBench, indicating that code related to dataset construction and preprocessing is publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "t9aThFL1lE-rubric-10",
    "token_usage": {
      "prompt_tokens": 29168,
      "completion_tokens": 75,
      "total_tokens": 29243
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract and Appendix B",
        "reasoning": "The paper states in the abstract and Appendix B that the dataset, benchmark, and codes are publicly available via the project webpage and GitHub repository. Detailed instructions on the code are provided in Appendix B, including settings for training and implementations, facilitating reproducibility of the dataset construction and related experiments."
      }
    ]
  },
  {
    "id": "tPsw4NeLZx-rubric-10",
    "token_usage": {
      "prompt_tokens": 20545,
      "completion_tokens": 132,
      "total_tokens": 20677
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.2 Data Processing and Augmentation; also mentioned in the Checklist and Abstract",
        "reasoning": "The paper states in the Abstract and the Checklist (item 3a) that all datasets and benchmarks are available at MM-WLAuslan. Specifically, in Section 3.2, the authors describe data processing steps such as background removal using AlphaPose keypoints and provide a reference to a background remover tool. The availability of datasets and benchmarks, as well as the mention of processing pipelines, implies that the relevant code, including for data collection and preprocessing, is made publicly accessible to support reproducibility."
      }
    ]
  },
  {
    "id": "tWvVtOW0qg-rubric-10",
    "token_usage": {
      "prompt_tokens": 14833,
      "completion_tokens": 114,
      "total_tokens": 14947
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Appendix A and C",
        "reasoning": "The paper utilizes multiple existing datasets from prior works (e.g., MNIST, ImageNet, MedMNIST) for benchmarking. The authors do not claim to introduce any new datasets themselves, nor do they mention providing code for new dataset creation or collection. The datasets used are publicly available external assets, and the provided supplementary materials include dataset lists and experimental setup details, but no new dataset construction code. Therefore, there is no code related to constructing new datasets to be made available."
      }
    ]
  },
  {
    "id": "vyraA7xt4c-rubric-10",
    "token_usage": {
      "prompt_tokens": 19650,
      "completion_tokens": 114,
      "total_tokens": 19764
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Abstract footnote and Section 2",
        "reasoning": "The paper explicitly states in the abstract footnote that the code and data are available on GitHub (https://github.com/Elfsong/Mercury) and a public dataset is also hosted on Huggingface (https://huggingface.co/datasets/Elfsong/Mercury). Section 2 further mentions an extensible open-source data collection framework for enriching Mercury, indicating that code related to data collection and dataset construction is publicly accessible for reproducibility."
      }
    ]
  },
  {
    "id": "wOmtZ5FgMH-rubric-10",
    "token_usage": {
      "prompt_tokens": 32190,
      "completion_tokens": 163,
      "total_tokens": 32353
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.2 Data Collection and Construction and Section 4.1 Model and Data Preparation; also Appendix G and H; Code released at https://github.com/jinzhuoran/RWKU",
        "reasoning": "The paper explicitly states that the RWKU benchmark dataset and code are publicly released at https://github.com/jinzhuoran/RWKU and https://huggingface.co/datasets/jinzhuoran/RWKU. The dataset construction process involves GPT-4 generated probes, filtering with open-source models, and manual verification described in Section 3.2 and appendices. Additionally, detailed data preparation prompts and implementation details are provided in the supplement. Hence, all code related to data collection, preprocessing, and generation is publicly available for reproducibility."
      }
    ]
  },
  {
    "id": "wmO7z57wNK-rubric-10",
    "token_usage": {
      "prompt_tokens": 16248,
      "completion_tokens": 120,
      "total_tokens": 16368
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Section 4 and Checklist section",
        "reasoning": "The paper discusses code availability for the benchmark and experiments, mentioning that code will be released after acceptance. However, it explicitly states in the Checklist section under 4.(c) that no new assets are included either in the supplemental material or as a URL, indicating that no new datasets or code for new dataset construction are provided. Furthermore, the paper uses existing public datasets for evaluation and does not introduce or release new datasets. Therefore, there is no code relevant to constructing new datasets available publicly from this paper."
      }
    ]
  },
  {
    "id": "x8RgF2xQTj-rubric-10",
    "token_usage": {
      "prompt_tokens": 39859,
      "completion_tokens": 157,
      "total_tokens": 40016
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Section 13 (New Assets) of the NeurIPS Supplementary Checklist",
        "reasoning": "The authors explicitly state in their responses to the NeurIPS checklist that they do not release new assets; they only record metrics (Section 13). There is no mention in the paper of any new datasets introduced by the authors, nor of any new data collection or generation procedures whose code might be released. The datasets used, such as ImageNet, ImageNet-ReaL, CIFAR-10, and CIFAR-10H, are all pre-existing datasets and credited appropriately. Therefore, there are no new datasets introduced by the authors, and correspondingly, no code related to dataset construction to be made publicly available."
      }
    ]
  },
  {
    "id": "y09S5rdaWY-rubric-10",
    "token_usage": {
      "prompt_tokens": 18997,
      "completion_tokens": 122,
      "total_tokens": 19119
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3 (Bench2Drive), Appendix A (Details of Data Collecting)",
        "reasoning": "The paper explicitly states that all data, codes, and checkpoints are open-sourced on GitHub and Huggingface under the Apache License 2.0 (Section 3). Appendix A provides detailed explanations about the data collection pipeline and annotations, indicating that the data collection code, including the use of the 'Think2Drive' expert model for data generation, is provided. This comprehensive openness supports reproducibility of the dataset."
      }
    ]
  },
  {
    "id": "y10DM6R2r3-rubric-10",
    "token_usage": {
      "prompt_tokens": 26695,
      "completion_tokens": 106,
      "total_tokens": 26801
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Checklist item 3(a), Section 4 Experimental Setup, Appendix A.1 Dataset Construction Details",
        "reasoning": "The paper explicitly states in Checklist 3(a) that code, data, and instructions needed to reproduce the main experimental results are provided via a URL to their GitHub repository (Supplemental Material). Additionally, Appendix A.1 details the dataset construction pipeline, including prompts and processing steps, indicating that code and data generation methods are documented and publicly accessible for full reproducibility."
      }
    ]
  },
  {
    "id": "yS1dUkQFnu-rubric-10",
    "token_usage": {
      "prompt_tokens": 16510,
      "completion_tokens": 133,
      "total_tokens": 16643
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Section 3 (Tasks and Datasets) and Appendix B.1 to B.3",
        "reasoning": "The paper details the use of 30 existing datasets from various domains such as fine-grained visual classification, VTAB, video action recognition, and dense prediction tasks. All these datasets are publicly available and commonly used in the vision community. The paper does not introduce any new dataset nor provides code for data collection, preprocessing, or new dataset generation. It also mentions splitting or usage protocols, but these are based on existing datasets. Therefore, no code related to constructing new datasets is provided or applicable in this paper."
      }
    ]
  },
  {
    "id": "yUEBXN3cvX-rubric-10",
    "token_usage": {
      "prompt_tokens": 19571,
      "completion_tokens": 148,
      "total_tokens": 19719
    },
    "response": [
      {
        "Has Code": false,
        "reference": "Section 3.1 and Appendix B.2",
        "reasoning": "The paper states that the ANDROIDCONTROL dataset is publicly released at https://github.com/google-research/google-research/tree/master/android_control, providing TFRecord data files and detailed data formats. However, there is no indication that the code used for the data collection, preprocessing, or generation is publicly available. The data collection involved a custom setup with physical phones and web apps for annotators, but the paper does not mention releasing the related data collection code or tools. The experimental code is not provided either, only detailed descriptions and prompts are given, implying that code for constructing the dataset is not made available."
      }
    ]
  },
  {
    "id": "z64azPC6Nl-rubric-10",
    "token_usage": {
      "prompt_tokens": 17110,
      "completion_tokens": 106,
      "total_tokens": 17216
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3.6 and the Abstract",
        "reasoning": "The paper explicitly states in Section 3.6 and the abstract that the dataset and the code for processing data and conducting benchmarks are provided and publicly available at https://huggingface.co/datasets/GTSinger/GTSinger and https://github.com/GTSinger/GTSinger. This indicates that all relevant code related to data collection, preprocessing, and generation for the new GTSinger dataset is shared for reproducibility."
      }
    ]
  },
  {
    "id": "zogaeVpbaE-rubric-10",
    "token_usage": {
      "prompt_tokens": 20395,
      "completion_tokens": 91,
      "total_tokens": 20486
    },
    "response": [
      {
        "Has Code": true,
        "reference": "Section 3 (DEVBENCH description)",
        "reasoning": "The paper explicitly states in Section 3 that all code and data related to the DEVBENCH benchmark are available at the provided GitHub URL (github.com/alvinwmtan/dev-bench). This repository contains the implementation for the benchmark tasks, indicating that the code for dataset preprocessing and generation is publicly accessible, enabling reproducibility."
      }
    ]
  }
]