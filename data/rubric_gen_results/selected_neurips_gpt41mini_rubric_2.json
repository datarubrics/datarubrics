[
  {
    "id": "01I55gys19-rubric-2",
    "token_usage": {
      "prompt_tokens": 12977,
      "completion_tokens": 228,
      "total_tokens": 13205
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any detailed instructions about model annotation or labeling guidelines related to data annotation. The focus is on benchmarking vision models and dataset difficulty, not on manual annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present any scoring rubrics or criteria for annotation that would guide labelers in assessing or scoring datasets or labels. The dataset splits and difficulty measures are automatically computed rather than manually annotated."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples illustrating annotation decisions or label guidelines are provided for annotators or labelers. The paper deals with model evaluation and data subset creation but does not include annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces the Few-Class Arena benchmark and new dataset subsets for model evaluation but does not introduce or describe any model annotation guidelines, instructions, rubrics, or examples for manual data labeling. All dataset subsets are created automatically from existing datasets without manual annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "0SMhqvgHST-rubric-2",
    "token_usage": {
      "prompt_tokens": 30316,
      "completion_tokens": 328,
      "total_tokens": 30644
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces the GATE benchmark suite and framework which incorporates a selection of existing benchmark datasets. The paper focuses on model encoder evaluation using these datasets and describes experimental methodologies and recipes for fine-tuning. However, it does not provide specific or detailed instructions for human annotators to label or annotate data within these datasets. Therefore, no model annotation instructions are present or described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not discuss any detailed scoring rubrics or guidelines for annotators regarding how to assign labels or scores during annotation. The evaluation metrics used (e.g., accuracy, AUROC, dice loss) are standard computational metrics computed automatically and do not constitute annotation rubrics. Hence, no annotation rubrics are provided or described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of providing example annotations, exemplary labeling cases, or examples for annotators to follow during data labeling. The datasets used are pre-existing and the paper focuses on benchmark selection and model evaluation rather than annotation guidelines or examples. Therefore, no examples for annotation are included or described."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces new benchmark tiers composed of existing datasets but does not involve new data annotation processes or present any model annotation guidelines, instructions, rubrics, or examples for data labeling. The focus is on evaluation methodology and framework automation rather than human annotation. Consequently, no model-based annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "2HzZIDo48o-rubric-2",
    "token_usage": {
      "prompt_tokens": 20801,
      "completion_tokens": 269,
      "total_tokens": 21070
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide specific instructions for human annotators or model-based annotation guidelines for labeling data; rather, it defines and constructs a benchmark environment for evaluating agent behaviors in meta-referential games. No explicit annotation instructions are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no detailed rubrics or scoring guidelines provided for data labeling or annotation within the proposed benchmarks or datasets. Metrics such as accuracy and compositionality scores are used for evaluation, but these are performance metrics, not annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the paper includes examples of semantic structures, stimuli representations, and agent messages within the benchmark context, these are illustrative examples of the environment and data, rather than of annotation guidelines for producing or labeling data."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The new dataset introduced (Symbolic Behaviour Benchmark based on Meta-Referential Games) is generated algorithmically for meta-learning experiments. No human or model-based annotation guidelines, instructions, rubrics, or examples for data labeling are provided or described in the paper. The dataset is synthetic and used for reinforcement learning experiments rather than requiring annotation."
          }
        }
      }
    ]
  },
  {
    "id": "2myGfVgfva-rubric-2",
    "token_usage": {
      "prompt_tokens": 16669,
      "completion_tokens": 313,
      "total_tokens": 16982
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.4 Video Captioning",
            "reasoning": "The paper describes a detailed annotation process for generating dense and structured captions using GPT-4V, including the extraction of frames arranged in a grid, prompting strategies to produce multi-perspective structured captions (main object, background, camera movements, and style). This indicates detailed instructions were designed and provided to the model for annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any detailed scoring rubrics or grading criteria used during the annotation process by the models or annotators. The captions are generated via large language models with prompts but no rubric for quality control of annotations is presented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the paper provides statistics and some example attributes (e.g., caption length, types of captions), it does not provide explicit examples of annotation guidelines or sample annotated outputs from the guideline instructions as part of a manual or annotation document."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Model annotation guidelines are indeed described and present as detailed instructions for the GPT-4V based caption generation pipeline, so 'N/A' does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "4S8agvKjle-rubric-2",
    "token_usage": {
      "prompt_tokens": 35069,
      "completion_tokens": 530,
      "total_tokens": 35599
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation Verification and Metric Justification; Appendix J; Appendix L (especially L.1 to L.10); Appendix N",
            "reasoning": "The paper details that subgoals for each environment are manually annotated and re-annotated where necessary to ensure quality and uniformity. They describe a rigorous annotation and verification process involving multiple verifications, manual checking, and refinements to ensure annotation consistency (Section 3.2). Appendix J describes multiple rounds of verification and interactive interfaces used for annotation. Appendix L provides task-specific annotation details revealing comprehensive instructions for annotation. Also, prompt details in Appendix N indicate detailed instructions for task understanding and interaction. This evidences that detailed annotation instructions were provided to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation Verification and Metric Justification; Appendix J; Section 2.2 Fine-grained Progress Rate; Appendix L",
            "reasoning": "The progress rate metric is a key novel contribution involving the decomposition of goals into subgoals and assigning progress scores. The paper explains that subgoals are labeled and associated with state observations, and matching scores for these are computed, either continuous or discrete (regex-based 0/1 scores), to measure incremental progress (Section 2.2). In Section 3.2, they emphasize multiple rounds of annotation verification and justification studies correlating human ratings with automatic progress rates, indicating rubric-like metrics guiding annotation. Appendix J describes quality control stages that ensure the rubric adherence. Overall, the labeling follows a clear rubric of subgoal completion and matching to assign progress rates."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2; Appendix K and L; Figure 8 (annotation UI illustration); Appendix N",
            "reasoning": "The paper shows concrete examples of goals, subgoals, progress rates, and trajectories (e.g., Table 2), and provides examples of annotated subgoals for different tasks (Appendix L). Figure 8 depicts the interactive interface used by annotators, illustrating practical examples of annotations and checking. Appendix N provides prompt examples showing how tasks and actions are instructed, thus aiding annotation consistency. The detailed explanations and illustrations serve as clear examples guiding the annotation process."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes multiple annotation guidelines, verification steps, instructions, rubrics, and examples across several sections and appendices, clearly showing the presence of model annotation guidelines. Thus, N/A is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "5WFzk0H27p-rubric-2",
    "token_usage": {
      "prompt_tokens": 16791,
      "completion_tokens": 278,
      "total_tokens": 17069
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 Raw data, Figure 1",
            "reasoning": "Section 2.1 and Figure 1 describe the comparison interface and the criteria used for annotation, specifying how contributors are asked to rate videos comparatively along a slider for main and optional quality criteria. This provides detailed instructions on how contributors should annotate data, including the main criterion and nine secondary quality criteria."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.1 Raw data, description of quality criteria",
            "reasoning": "Nine optional quality criteria (e.g., reliability, clarity, importance) are explicitly defined and provided to contributors as rubric-like criteria for the annotation judgments. Although it is noted that contributors may not read descriptions thoroughly, these criteria act as a rubric guiding the annotation process."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 1, Section 2.1",
            "reasoning": "Figure 1 shows screenshots of the annotation interface, illustrating the slider-based comparison task and criteria presentation, serving as clear examples for contributors on how to label data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides detailed annotation guidelines including instructions, rubric criteria, and example interfaces for contributors, hence it is not true that no model annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "5c1hh8AeHv-rubric-2",
    "token_usage": {
      "prompt_tokens": 101847,
      "completion_tokens": 409,
      "total_tokens": 102256
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix C to G, e.g., Sections C.1, D.1, E.1, F.1, G.1",
            "reasoning": "The paper provides detailed descriptions of the task settings, prompts, and the process for generating and adapting datasets for each evaluation task. These extensive instructions guide annotators and evaluators on how to label data and assess model outputs accurately, ensuring consistency and understanding of the subjective tasks, which effectively serve as detailed instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix C to G, e.g., Sections C.3.1, D.3.1, E.3.1, F.3.1, G.3.1",
            "reasoning": "The paper defines clear scoring rubrics and metrics for various tasks, including accuracy, refusal rates, toxicity scores, GPT-based scoring, keyword matching, and statistical tests. For subjective evaluations, metrics like Refuse-to-Answer (RtA) rates and Stereotype Containing Rate (SCR) are used with detailed computational formulas presented. This systematic and quantitative approach to scoring constitutes detailed rubrics for annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix C to G, numerous figures and examples, e.g., Figures C.1, C.4, C.8, D.1, F.1, G.1",
            "reasoning": "The paper includes multiple illustrative examples and sample prompts in the appendices demonstrating the annotation and evaluation process, as well as showing model outputs with indications of correct and incorrect labels. These clear examples assist annotators in understanding how to label and assess data, improving consistency."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly provides detailed information about the dataset construction, annotation instructions, evaluation metrics, scoring rubrics, and many examples across all curated tasks, indicating the presence of comprehensive annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "67N3FWoDtU-rubric-2",
    "token_usage": {
      "prompt_tokens": 16574,
      "completion_tokens": 254,
      "total_tokens": 16828
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Supplementary Section A.3 Annotation Guidelines",
            "reasoning": "The paper provides a detailed description of annotation guidelines adapted from OCR-D rules in Supplementary Section A.3. This includes instructions about page types, region types, and their subclasses, as well as transcription details, ensuring consistent and systematic data annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe detailed scoring rubrics or quantitative criteria for annotators to follow during annotation. The guidelines focus on classification and transcription rules, but no explicit rubrics or grading scales are provided."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Supplementary Section A.3 and Figures 2 and 4",
            "reasoning": "The paper provides examples of annotated layout regions in Figures 2 and 4, and describes specific annotation decisions such as treatment of drop capitals and advertisements in Supplementary Section A.3, serving as clear examples for annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Given that detailed instructions and examples are provided in the supplementary material and throughout the paper, it is not the case that no model annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "6cCFK69vJI-rubric-2",
    "token_usage": {
      "prompt_tokens": 21330,
      "completion_tokens": 268,
      "total_tokens": 21598
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any detailed instructions for annotators or labelers on how to label or classify the timeseries data or any other data within the BTS dataset. The labeling appears to be done using existing ontologies like the Brick schema, but no human annotation instructions are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention or describe any scoring rubrics or formal evaluation criteria provided for annotators to decide on labels or classifications. The dataset uses standardized labels from the Brick schema; however, no rubrics or scoring systems for annotation consistency are presented."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include any examples or illustrative cases of annotation or labeling guidelines. No sample annotation tasks, annotated examples, or clarification examples are shown to describe how to label the data."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper clearly uses the Brick schema for standardized metadata classification, but it does not describe any human or model annotation guidelines such as instructions, rubrics, or examples for labeling the data. The classification is based on existing ontologies and system data rather than manual annotation processes requiring explicit annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "7TCK0aBL1C-rubric-2",
    "token_usage": {
      "prompt_tokens": 16337,
      "completion_tokens": 449,
      "total_tokens": 16786
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3, 'Infrastructure intent specifications' and Section 2.2 'Dataset characteristics'",
            "reasoning": "The paper describes detailed procedures in constructing the infrastructure intent specifications, which act as annotation guidelines for evaluating generated IaC configurations. These specifications explicitly encode valid resources, optional and required attributes, and dependencies, providing precise instructions for verifying user intent fulfilment. Additionally, the dataset creation involved human experts with deep cloud and IaC knowledge, who followed systematic instructions to ensure correctness and consistency."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.3, 'Infrastructure intent specifications' and Appendix A.4 'IaC-Eval difficulty levels'",
            "reasoning": "The intent specifications act as detailed rubrics, defining what constitutes a correct IaC configuration per problem by explicitly specifying required and optional attributes and valid resources which must or may appear. Furthermore, difficulty levels are calculated automatically by clear criteria (lines of code, resource count, interconnections), serving as a rubric to categorize problem complexity. This formalized framework guides the evaluation of generated code against user intent, offering a reproducible scoring rubric."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2 'Dataset characteristics', Figure 3, and Appendix B prompt templates",
            "reasoning": "The paper provides examples of correct Terraform HCL outputs associated with each problem, as well as example intent specifications in Rego language shown in Figure 3(b). Appendix B includes prompt templates used to guide models for code generation, which serve as examples for data annotation instructions. These examples clarify expected annotation outcomes and evaluation criteria to ensure consistency."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents detailed annotation guidelines, including instructions, rubrics, and examples, for constructing and evaluating the IaC-Eval dataset. Hence, 'No model annotation guidelines' does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "9tVn4f8aJO-rubric-2",
    "token_usage": {
      "prompt_tokens": 41115,
      "completion_tokens": 184,
      "total_tokens": 41299
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide or discuss any detailed instructions given for model annotation guidelines specifically related to data labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No mention of detailed scoring rubrics or guidelines that would instruct annotators on how to consistently rate or score data or model outputs is found in the paper."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include examples demonstrating annotation guidelines or model labeling procedures for data labeling tasks."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper focuses on benchmarking and evaluating existing datasets and models using standardized prompts and automatic metrics like BARTScore but does not provide or discuss any model annotation guidelines, including instructions, rubrics, or examples for labeling or annotating data."
          }
        }
      }
    ]
  },
  {
    "id": "AdpSHMOujG-rubric-2",
    "token_usage": {
      "prompt_tokens": 16820,
      "completion_tokens": 366,
      "total_tokens": 17186
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4, especially 'Image collection and annotation' and Appendix H",
            "reasoning": "Section 4 and Appendix H describe in detail the annotation procedure for the benchmark. Annotators sample multiple points representing the distribution of each material state, group points by exact same material states, and assign relative similarity between groups. Detailed instructions emphasize focusing on regions with clear annotation and sampling of harder, more complex regions, capturing topology and gradual transitions. Annotation is complex and relies on human judgment, with crosschecking by a second annotator to correct errors."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1 and Section 4",
            "reasoning": "Section 4.1 presents the evaluation metric based on triplets measuring relative similarity between points, which implicitly guides annotation to ensure consistency for triplet comparisons. Section 4 elaborates on point grouping to capture partial similarity, and the benchmark is structured to evaluate predictions against ground truth similarity in a quantitative manner. This design serves as a rubric for annotation consistency, capturing partial similarities and graded transitions."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 3 and Appendix H (Benchmark annotation strategy and points selection)",
            "reasoning": "Figure 3 provides visual examples of benchmark images with point-based annotations and grouped points by material state. Appendix H further describes guidelines for point selection with examples illustrating how annotators sample points to capture scattered and soft boundaries, transitional states, and material distribution topology. This offers clear and practical examples to annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides detailed instructions, rubrics, and examples for the annotation process of the introduced MatSeg benchmark dataset; thus, this label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "BZe6dmDk5K-rubric-2",
    "token_usage": {
      "prompt_tokens": 34080,
      "completion_tokens": 329,
      "total_tokens": 34409
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Subjective Action Quality Assessment",
            "reasoning": "The paper details a tutorial and pre-labeling process where participants are instructed on the evaluation dimensions (subject quality, action completeness, and action-scene interaction) before annotation. Participants are guided using multiple expert-reviewed examples during training sessions to ensure consistent understanding, indicating the provision of detailed instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 Subjective Action Quality Assessment",
            "reasoning": "Annotations are collected on a continuous rating scale of [0, 100] with specific evaluation dimensions. The paper mentions thresholds for participant eligibility based on agreement with expert ratings, implying a rubric for judging annotation quality. Furthermore, the careful design of three evaluation dimensions based on causal reasoning forms a structured rubric for the annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3 Subjective Action Quality Assessment and Figure 8 (Appendix)",
            "reasoning": "The paper describes that participants are trained by rating 10 generated-real video pairs with the same caption, and their answers are compared with expert ground-truth ratings as examples. Also, the appendix shows the interface screenshots for rating, which includes examples that illustrate how to rate videos, indicating the presence of clear examples in the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly details the annotation process including instructions, scoring scales, training with exemplar videos, and quality controls, indicating that annotation guidelines are provided and are not absent."
          }
        }
      }
    ]
  },
  {
    "id": "D1MOK2t2t2-rubric-2",
    "token_usage": {
      "prompt_tokens": 11683,
      "completion_tokens": 416,
      "total_tokens": 12099
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Evaluation Dataset and Section 2.2 Evaluation",
            "reasoning": "The Evaluation Dataset includes detailed annotations from human evaluators involving answering a set of questions about pairs of agent videos, including which agent performed better overall, direct questions about concrete achievements, and several comparative questions such as which agent was more human-like. The paper states that all details needed to understand and reproduce these evaluations are provided in Appendix D, and examples of the human evaluation forms are also included in Appendix D. This indicates that detailed instructions are provided for annotators to guide consistent and reproducible human evaluations."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Evaluation Dataset and Section 2.2 Evaluation",
            "reasoning": "The human evaluation process involves a structured set of questions with specific answer options (e.g., Left, Right, Draw, N/A) for multiple factors such as overall task completion, agent human-likeness, and specific direct achievement questions. The presence of defined answer choices and the use of TrueSkill for skill rating indicate a rubric-like framework that guides human annotators on how to score or compare agents, ensuring consistency. Moreover, the paper discusses filtering inappropriate annotations and the use of a leaderboard derived from these evaluations, further reflecting a rubric-guided annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix D",
            "reasoning": "The paper explicitly mentions that examples of the forms that human evaluators see are provided in Appendix D. Given the complexity of the evaluation (videos paired against one another, multiple comparative questions), providing such examples is essential for annotator training and consistency. This confirms that example annotation guidelines or forms are included."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides extensive details about the human evaluation procedure, including instructions, structured questions, and example forms. This demonstrates the presence of comprehensive annotation guidelines, so 'N/A' (no annotation guidelines) does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "DFr5hteojx-rubric-2",
    "token_usage": {
      "prompt_tokens": 92625,
      "completion_tokens": 368,
      "total_tokens": 92993
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2, Figure 22, Appendix V and Q",
            "reasoning": "The paper details that participants were given explicit instructions in the interface on how to conduct the annotation task, including the use of a visual analog scale to rate model responses along various fine-grained attributes. Full text of survey questions and interface instructions are presented in detailed code books (Appendix V), and interface screenshots are provided (Appendix Q), demonstrating the presence of detailed instructions guiding annotators on how to provide feedback."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2, Figure 22, Appendix V",
            "reasoning": "The dataset collects structured ratings on predefined fine-grained behavior attributes (e.g., fluency, factuality, helpfulness, safety) using a visual analog scale from 1 to 100 with labelled anchors. Participants rate both the model responses\u2019 performance and the importance of those attributes influencing their choice, effectively functioning as a rubric to guide annotation consistency and to evaluate multiple dimensions of output quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2, Figure 22, Appendix V and Q",
            "reasoning": "The paper provides screenshots of the annotation interface (Figure 22 and Appendix Q) exemplifying the rating scales and question phrasing. Additionally, the survey codebooks in Appendix V include question texts and example prompts. This indicates that annotators had access to clear examples of the annotation task and the expected input format."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "There are explicit and detailed instructions, rubrics, and interface examples provided to participants to guide the annotation and feedback collection process; therefore, the N/A label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "DJVyRhT8nP-rubric-2",
    "token_usage": {
      "prompt_tokens": 27464,
      "completion_tokens": 317,
      "total_tokens": 27781
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix C.1, Section 2.2",
            "reasoning": "The paper details the process for generating the HA-R2R dataset instructions, including the use of a carefully designed few-shot template prompt with a system prompt and examples to guide GPT-4 in generating contextually appropriate navigation instructions that combine original R2R instructions and human activity descriptions. This constitutes detailed annotation instructions guiding how to create the instruction texts."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any explicit detailed rubrics or scoring criteria for annotation beyond the instruction generation process. While there is some mention of human surveys evaluating human activity descriptions on criteria, no formal annotation rubrics for dataset creation are described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix C.1, Listings 1 and 2",
            "reasoning": "The paper provides clear few-shot examples in the prompt used to generate instructions (Listing 2), illustrating how to incorporate human activity descriptions into navigation instructions. This serves as explicit examples in the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper includes annotation guidelines with instructions and examples for dataset generation and does not lack annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "DjCSjizgsH-rubric-2",
    "token_usage": {
      "prompt_tokens": 16284,
      "completion_tokens": 287,
      "total_tokens": 16571
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 Real-world Data, Data Annotation",
            "reasoning": "The paper states that human annotators labeled the fire regions in the satellite images using specific software (ArcGIS and NV5 GEOSPATIAL), with each mask being a polygon. It also describes that three annotators cross-checked each annotation and refined masks that were disapproved, indicating a detailed process and instructions were provided to annotators to ensure quality and consistency."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any detailed scoring rubrics or criteria used by annotators to evaluate or score their annotations. There is no mention of quantitative or qualitative rubrics guiding the annotation process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of annotations or specific example images with annotations are provided or highlighted in the paper as part of the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper describes a detailed annotation process involving multiple annotators and quality control, indicating that annotation guidelines were indeed provided."
          }
        }
      }
    ]
  },
  {
    "id": "E18kRXTGmV-rubric-2",
    "token_usage": {
      "prompt_tokens": 20580,
      "completion_tokens": 342,
      "total_tokens": 20922
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Annotation Process and Section A Annotation Guidelines",
            "reasoning": "The paper clearly states that concise annotation guidelines in English were developed and provided to all Country-Language subset teams. Section A of the appendix provides detailed annotation guidelines including objectives, image selection criteria, question and answer creation instructions, and category definitions. These instructions guide annotators in selecting culturally relevant images and formulating questions and answers, ensuring cultural appropriateness and quality."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and the appendix do not mention specific scoring rubrics or detailed grading/scoring criteria for annotation quality or label assignment. While there is mention of validation steps for checking the correctness and cultural relevance of questions, no formal rubrics or scoring systems are described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.2 Annotation Process and Appendix A Annotation Guidelines",
            "reasoning": "The paper and appendix provide multiple concrete examples of well-formulated questions and answers across various languages and categories (e.g., examples in Igbo, Indonesian, Malay, Korean, Spanish, and Irish). These examples illustrate expected question formats, answer options, and category assignments, serving as clear references for annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Model annotation guidelines are provided as described with detailed instructions and examples."
          }
        }
      }
    ]
  },
  {
    "id": "EADRzNJFn1-rubric-2",
    "token_usage": {
      "prompt_tokens": 28604,
      "completion_tokens": 236,
      "total_tokens": 28840
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and supplement do not provide any detailed instructions or guidelines for human annotators performing data labeling or annotation for the new datasets. The datasets are constructed from existing data sources or logs, and no manual annotation instructions are mentioned."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any scoring rubrics or systematic annotation protocols to guide labeling decisions. The datasets are prepared from external data and focus on automatic evaluation pipelines rather than manual annotation assessment criteria."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples or illustrative annotation cases are provided as model annotation guidelines. The datasets are derived from factual temporal graphs and interactions with no described annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The datasets introduced are constructed by extracting and processing existing temporal graph data from various sources with automated procedures. No model annotation guidelines, instructions, rubrics, or examples for human labeling are provided in the paper or supplement. The focus is on benchmarking and evaluation pipelines rather than manual annotation protocols."
          }
        }
      }
    ]
  },
  {
    "id": "FXTeJvHE0k-rubric-2",
    "token_usage": {
      "prompt_tokens": 15358,
      "completion_tokens": 271,
      "total_tokens": 15629
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any instructions or protocols provided to human annotators for labeling or data annotation of the NAVSIM dataset. The dataset is derived from an existing dataset (OpenScene) with additional curation and filtering, but no annotation protocol or instructions are mentioned."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no scoring rubrics or detailed annotation guidelines described for humans or models annotating or labeling data in NAVSIM. The work focuses on evaluation metrics computed automatically through simulation-based scoring rather than human annotations with rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide annotation examples or illustrate annotation procedures, as the dataset is constructed via filtering and scenario selection from OpenScene. Annotation is not the focus of the dataset creation or evaluation."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "NAVSIM is a benchmark and simulation framework that uses curated and filtered scenes from an existing dataset (OpenScene). The paper does not present any model annotation guidelines, instructions, rubrics, or examples for manual data annotation. Instead, it defines evaluation metrics computed automatically from the simulated agent trajectories. Therefore, no model annotation guidelines are provided or relevant."
          }
        }
      }
    ]
  },
  {
    "id": "GHlJM45fWY-rubric-2",
    "token_usage": {
      "prompt_tokens": 20879,
      "completion_tokens": 253,
      "total_tokens": 21132
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any explicit detailed instructions targeted for annotators or for the annotation process of the dataset instances. The dataset is built from existing data sources (Presence-Only and Presence-Absence records) and environmental predictors; no instructions for new annotation or labeling are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or detailed evaluation criteria for human annotators or labelers are provided in the paper. Evaluation metrics are described only for model performance on test data, not for labeling consistency or annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide clear annotation examples or exemplar guidelines explaining how data points were labeled or annotated. The dataset is constructed from existing observation data rather than from manual annotation requiring guidelines."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset is assembled from existing presence-only citizen science observations and presence-absence surveys collected by botanists; no new annotation was performed by the authors requiring model annotation guidelines, instructions, rubrics, or examples. Therefore, no model annotation guidelines are provided or necessary."
          }
        }
      }
    ]
  },
  {
    "id": "HB5q6pC5eb-rubric-2",
    "token_usage": {
      "prompt_tokens": 23389,
      "completion_tokens": 339,
      "total_tokens": 23728
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix B.1 and B.2",
            "reasoning": "The paper provides detailed instructions for the knowledge-invariant perturbations, including a formal algorithm for content-level paraphrasing (Algorithm 1 in Appendix B.1), prompt templates for the LLM rewriter (Figure 5), and examples of question format refactoring (Appendix B.2, Table 8). These serve as clear annotation instructions describing how to generate perturbed test samples systematically."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix B.1 (Table 9)",
            "reasoning": "The paper includes detailed rubrics for scoring knowledge invariance of perturbations with a defined 1-5 grading scale and explicit criteria for evaluation (Table 9 in Appendix B.1). This establishes a clear rubric for human and LLM-based annotation of perturbation quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B.1 (Table 7), Appendix B.2 (Table 8)",
            "reasoning": "Clear examples illustrating the perturbation process and outputs are given, such as an example of knowledge-invariant paraphrasing of a test question (Table 7 in Appendix B.1) and examples of format-level perturbations (Table 8 in Appendix B.2). These examples help annotators understand the desired perturbations clearly."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly provides guidelines, rubrics, and examples for model annotation associated with the introduced perturbations, thus the label N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "IZtX4RNBeH-rubric-2",
    "token_usage": {
      "prompt_tokens": 13428,
      "completion_tokens": 329,
      "total_tokens": 13757
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Stage 1: Data collection and Annotation, and Appendix C",
            "reasoning": "The paper indicates that 'annotation instructions' were provided to human annotators to ensure consistent and high-quality captions, with personalized annotation guidelines used for each video category (Section 3.2). This demonstrates detailed instructions were given for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Stage 4: Evaluation Procedure and Appendix E",
            "reasoning": "The evaluation uses an LLM Judge that provides a binary correctness judgment, a score from 1 to 5 representing prediction quality, plus reasoning for the score. This scoring mechanism constitutes detailed rubrics for assessment."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 Stage 2 and Appendix C (Table 4 and Figure 14)",
            "reasoning": "The paper states that example prompts used as instructions to LLMs for QA pair curation are provided in Figure 14 of the Appendix, and also shows example final QA pairs in Table 4 (Appendix C). This demonstrates that clear examples were provided."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Model annotation guidelines with instructions, rubrics, and examples are explicitly described in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "JU0QvhhfVp-rubric-2",
    "token_usage": {
      "prompt_tokens": 13997,
      "completion_tokens": 245,
      "total_tokens": 14242
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the dataset construction, preprocessing steps, and experimental setup but does not provide specific detailed annotation instructions for model annotation or labeling. The annotations come from integrated public databases, and the dataset is constructed rather than manually labeled via annotation protocols."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or inclusion of scoring rubrics or detailed evaluation criteria for annotators or labelers in the dataset creation. The annotations are aggregated from existing databases without additional manual scoring or rubric-based labeling procedures."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide example annotations or examples of how annotations were applied or labeled. Instead, it focuses on data integration and processing from public sources without example-based annotation guidance."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The MOTIVE dataset is an integration of existing publicly available annotations and empirical morphological profiles. The paper does not describe any manual model-based annotation process requiring guidelines, instructions, rubrics, or examples for annotation. Therefore, no model annotation guidelines are provided for this dataset."
          }
        }
      }
    ]
  },
  {
    "id": "KZlJF8kguO-rubric-2",
    "token_usage": {
      "prompt_tokens": 23835,
      "completion_tokens": 284,
      "total_tokens": 24119
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix A.1 and A.2",
            "reasoning": "The paper details manual annotation procedures with instructions for annotators, including how to transcribe and align audio, insert punctuation, segment sentences, and annotate speaker identity with specific conventions. Appendix A.2 describes feature annotation procedures including detailed descriptions of visual, auditory, and language feature extraction and manual correction of POS tags and dependency parses by a trained annotator over a year. This indicates detailed annotation instructions were provided."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and appendices do not mention any scoring rubrics or criteria for evaluation used during annotation for assessing annotation quality or consistency. There is no description of quantitative rubrics or guidelines defining annotation categories beyond procedural instructions."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide explicit examples of annotations or annotated data samples as part of the annotation guidelines. Although figures show example electrodes and neural responses, they do not serve as annotation examples. No concrete examples illustrating annotation decisions or corrected parses are presented."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Model annotation guidelines are clearly described, including detailed instructions for manual and automated annotation corrections, feature extraction, and speaker identification. Thus, 'N/A' does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "L5aY1mWvXQ-rubric-2",
    "token_usage": {
      "prompt_tokens": 14082,
      "completion_tokens": 313,
      "total_tokens": 14395
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces temporal distortion techniques to modify test splits of existing datasets for model evaluation. However, it does not provide any specific instructions or detailed guidelines for human annotators or manual data labeling. The datasets used are existing benchmarks without newly created annotated data requiring human annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No detailed scoring rubrics or criteria for human annotations are provided since the evaluation relies on automated metrics (AU-ROC, AP, ATD, and ACD) applied on existing datasets and their temporally distorted versions. The paper focuses on model evaluation methodology rather than on manual annotation or label scoring."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no illustrative examples or samples specifically related to annotation guidelines or manual labeling procedures described in the paper. The inclusion of sample annotation examples is not relevant to the datasets or experiments presented."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce new datasets with manual or model-based annotation procedures requiring guidelines. Instead, it applies temporal distortion transformations on existing public datasets to generate modified test sets. Therefore, no new annotation guidelines are provided or needed."
          }
        }
      }
    ]
  },
  {
    "id": "LdRZ9SFBku-rubric-2",
    "token_usage": {
      "prompt_tokens": 25407,
      "completion_tokens": 315,
      "total_tokens": 25722
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Collection, Figure 4 and related description",
            "reasoning": "The paper describes a manual process for collecting and annotating the dataset from public international news, including reading each news item manually to collect clean and consistent information such as title, content, time, images, image description, event description, hierarchical event names, and event attributes (location, date, etc). This detailed explanation implies that there are instructions guiding annotators on what to collect and how to structure the annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no explicit mention or description of detailed scoring rubrics or systematic criteria for annotation quality, nor for how to score or rate labels. The event annotations are hierarchical and manually done, but no rubric for scoring or grading annotations is provided."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Collection, Figure 4 and related description",
            "reasoning": "The paper provides an example explaining the hierarchical event annotation distinguished into Event-11 and Event-9185 with example categories such as 'Sports' and specific real events like '2019 Daytona 500'. The event annotation example including detailed textual and visual content demonstrates clear examples of how annotations appear and are structured."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper does provide guidance and examples for annotation as described above, so it is not the case that no model annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "LdxNWDNvC3-rubric-2",
    "token_usage": {
      "prompt_tokens": 20486,
      "completion_tokens": 252,
      "total_tokens": 20738
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the creation of a large-scale dataset (AF-200K) with extensive geometric and aerodynamic annotations generated via computational simulations and physical parametric models, rather than annotations made by human annotators requiring model annotation guidelines. There is no mention of detailed instructions provided for annotators labeling data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or structured criteria for human annotation are described in the paper or appendix. The dataset annotations are generated algorithmically with computed parameters and computational fluid dynamics simulations, not manually assigned via subjective rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and appendices do not provide illustrative examples of annotation guidelines being applied, since annotations are computationally generated physical and aerodynamic parameters, not involving subjective or manual annotation tasks needing examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset annotations are computed labels from parametric models and CFD simulations, rather than human-annotated subjective labels. Consequently, there are no model annotation guidelines, instructions, rubrics, or examples provided or required for labeling the data."
          }
        }
      }
    ]
  },
  {
    "id": "M32Ldpp4Oy-rubric-2",
    "token_usage": {
      "prompt_tokens": 32464,
      "completion_tokens": 350,
      "total_tokens": 32814
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 5.1 and 5.2, Appendix B",
            "reasoning": "The paper describes detailed configurations for the two new tasks (Safe Path Following and Visual Action Prediction) including clear predicate definitions, rule clauses, reward functions, and training/testing splits (Section 5.1, 5.2; Appendix B). These constitute detailed instructions for annotating the data and understanding the reasoning tasks, especially since the datasets are synthetic and generated by rules."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5.1 under Metrics, Section 5.2 under Metrics",
            "reasoning": "The paper defines specific quantitative metrics for evaluation that relate to label quality and rule adherence, including Trajectory Success Rate (TSR), Decision Success Rate (DSR), Score for SPF task, and recall, average accuracy, weighted accuracy for VAP task. These metrics serve as rubrics for assessing the correctness of the annotations and model predictions."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 1, Figure 2, Appendix F, and visualizations in Figures C and D",
            "reasoning": "The paper provides clear examples and visualizations of agent entities, predicate groundings, and annotated rules, including qualitative examples of episodes with illustrations comparing model predictions and ground truth. Appendix F and Figures offer concrete examples of annotation and reasoning outcomes, supplementing the guidelines with sample annotated data."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly provides model annotation guidelines via instructions, rubrics, and examples for the new datasets it introduces. Therefore, the 'N/A' label is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "MU2s9wwWLo-rubric-2",
    "token_usage": {
      "prompt_tokens": 26635,
      "completion_tokens": 294,
      "total_tokens": 26929
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix A.1",
            "reasoning": "The paper provides detailed human evaluation instructions in Appendix A.1, including step-by-step procedures for evaluators to assess image-prompt alignment and answer individual yes/no questions related to visual concepts. It also clarifies judgment criteria for subjective concepts like size and style, ensuring annotators understand the evaluation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix A.1",
            "reasoning": "The instructions in Appendix A.1 serve as a rubric by clearly defining how annotations should be made (e.g., binary yes/no answers for each specific question, evaluation of overall alignment). The paper also discusses consistency analyses and criteria for judgment, indicating the presence of structured scoring rubrics to aid annotators."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A.1 and Appendix A.5",
            "reasoning": "The paper includes examples of prompts with associated human evaluation instructions (Appendix A.1) and presents qualitative analyses showing example disagreements between human and GPT-4o grading (Appendix A.5), demonstrating clear examples given to annotators to illustrate the annotation task and expected judgments."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Contrary to having no guidelines, the paper provides detailed annotation instructions, rubrics, and examples for human evaluation, particularly in Appendix A."
          }
        }
      }
    ]
  },
  {
    "id": "Mbd3QxXjq5-rubric-2",
    "token_usage": {
      "prompt_tokens": 27205,
      "completion_tokens": 321,
      "total_tokens": 27526
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.1 and Appendix B.4 (Table 13)",
            "reasoning": "The paper describes the few-shot prompting instructions used to generate solutions, including specifying to put the answer inside special boxed delimiters (e.g., '\\boxed{}'). Section 2.1 details the prompting setup with instructions (denoted as \\mathcal{I}) guiding the model on the task, and Appendix B.4 provides the exact instructions used for few-shot data generation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe or provide any formal scoring rubrics or detailed annotation guidelines for rating or scoring the generated data. The annotation is automatic model generation with correctness measured by matching final answers, not by human annotation according to rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B.2 and Figures 5, 7, 9-13",
            "reasoning": "The paper provides multiple clear example solutions illustrating the model annotation format, including code-interpreter style solutions with code blocks and text, examples of masked solutions, and example prompts with annotated questions and answers (e.g., Figure 5, 7, 9, and Appendix B.2). These examples clarify how data is generated and formatted."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "There are model annotation guidelines in the form of instructions for prompting and example solutions provided, so it is not the case that no guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "OL2JQoO0kq-rubric-2",
    "token_usage": {
      "prompt_tokens": 22272,
      "completion_tokens": 341,
      "total_tokens": 22613
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 and Appendix A.1",
            "reasoning": "The paper describes a detailed data curation pipeline using large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition to extract and align image-text pairs from YouTube videos of histopathology. Section 3.1 elaborates on the multi-step method and includes instructions for filtering videos, correcting ASR text using LLMs with specific prompts, and extracting medical and ROI text. Appendix A.1 provides additional elaboration on the pipeline steps and algorithms, indicating that detailed instructions exist for automated annotation processes."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention the use of any scoring rubrics or criteria with defined scoring schemes for annotation. The annotation is performed via automated pipelines leveraging models and algorithms without a rubric-based human scoring system."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 3 and Section 3.1",
            "reasoning": "The paper provides illustrative examples of ASR text correction with LLM prompts in Figure 3, showing before-and-after correction of medical terms and extraction of medical and ROI text. Additional prompt examples for sub-pathology classification and text extraction are included in the supplement. These examples clarify the annotation and correction process, serving as clear examples within the model annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset was annotated using automated methods supported by LLMs and domain-specific ontologies, with instructions and illustrative examples provided. Hence, annotation guidelines are present and N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "OTjTKFk7gb-rubric-2",
    "token_usage": {
      "prompt_tokens": 21820,
      "completion_tokens": 260,
      "total_tokens": 22080
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention providing any detailed instructions for annotation or labeling of the dataset. The dataset is generated automatically from the simulation environment and baseline agents' interactions without any human annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication that the dataset creation involved any scoring rubrics or structured evaluation criteria for subjective or ambiguous annotations, as the data is generated from an environment simulation and agent bidding logs."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any labeled examples or illustrative samples aimed at guiding annotation or labeling. Examples in the paper show dataset format and sample auctions for explanatory purposes but are not annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset is automatically generated from the simulated auction environment and agent interactions without any human annotation process requiring guidelines. Therefore, no model annotation guidelines are necessary or provided."
          }
        }
      }
    ]
  },
  {
    "id": "PcbSZwVVc5-rubric-2",
    "token_usage": {
      "prompt_tokens": 20764,
      "completion_tokens": 394,
      "total_tokens": 21158
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Statistics; Appendix A.4 Manual Annotation; Figure 5 and Table 8",
            "reasoning": "The paper describes a hierarchical inspection process where annotators reviewed earbud data to identify and label eight distinct sleep events using specialized software. Annotators examined dual-channel audio and IMU data along with the sleep partner's audio to determine event categories and sources. Each label was checked by at least three annotators for consensus. The protocol includes detailed guidance on how to interpret multimodal data to attribute events correctly, indicating detailed instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Statistics; Appendix A.4 Manual Annotation",
            "reasoning": "The annotation process entails assigning a category to each selected event interval from eight defined sleep event types, which serves as a rubric for categorization. The defined events have clear label definitions and summary statistics in Table 2. Additionally, the consensus among multiple annotators and use of voting for conflicts indicates structured rubrics to ensure consistency and reproducibility of labels."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Statistics; Figure 2 (not included in the excerpt but referenced); Appendix A.4 Manual Annotation; Figure 6",
            "reasoning": "The paper provides examples of each event type (mentioned in Section 3.2 and Figure 2) to assist annotators. The detailed descriptions of how annotators use audio spectrograms and IMU data to distinguish similar events (e.g., swallowing vs. body movement) serve as practical examples in the annotation guidelines, making examples clearly present."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes a comprehensive annotation process with detailed instructions, rubrics, and examples provided to annotators, so this label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "QIJQ1qCGqV-rubric-2",
    "token_usage": {
      "prompt_tokens": 14064,
      "completion_tokens": 267,
      "total_tokens": 14331
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Data Annotation Pipeline",
            "reasoning": "The paper details that 15 human annotators, including three motion experts and 12 novices, were provided with detailed instructions, exemplars, and feedback. Annotators received carefully crafted instructions describing how to annotate both high-level and low-level descriptions of motion and mobility aid use, ensuring clear and concise guidance for the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any specific scoring rubrics or quantitative criteria used by annotators during labeling. Annotation quality control is described via feedback and expert involvement but no formal rubrics are described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.3 Data Annotation Pipeline, Figure 2, and accompanying text",
            "reasoning": "The paper provides clear examples of both high-level and low-level annotations in text, illustrating the kind of descriptions annotators were expected to produce. Additionally, expert demonstrative samples were provided to novice annotators as references to aid the annotation process."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset includes a thorough annotation process with detailed instructions and examples, so it cannot be considered as lacking annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "Qf8uzIT1OK-rubric-2",
    "token_usage": {
      "prompt_tokens": 37389,
      "completion_tokens": 253,
      "total_tokens": 37642
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper focuses on ethical considerations and guidelines for data curation in human-centric computer vision (HCCV) but does not introduce or describe any new dataset with associated detailed model annotation instructions. Instead, it provides a checklist of ethical considerations to guide dataset curation processes rather than explicit annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Although the paper emphasizes fairness, consent, privacy, and diversity considerations, it does not present or include detailed scoring rubrics or evaluation criteria for annotations in any new dataset. The focus remains on ethical recommendations rather than concrete annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide clear examples of annotation guidelines or examples related to labeling data. It discusses ethical frameworks and considerations at a conceptual level without concrete annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets with model-based annotation guidelines. It primarily discusses ethical considerations, recommendations, and a checklist for dataset curation but does not provide specific annotation guidelines, instructions, rubrics, or examples for new datasets."
          }
        }
      }
    ]
  },
  {
    "id": "QpF3DFP3Td-rubric-2",
    "token_usage": {
      "prompt_tokens": 19348,
      "completion_tokens": 340,
      "total_tokens": 19688
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 'Annotation' and Section 3.2 'Annotation'",
            "reasoning": "The paper details that annotations were made by expert archaeologists who individually traced and field-verified archaeological features using manual delineation in GIS software (QGIS). There is a described 12-class granular classification system that was revised into five classes to reduce ambiguity and class imbalance. This indicates the presence of detailed instructions guiding the annotation process to ensure semantic consistency and expert validation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide detailed, formal scoring rubrics or criteria quantifying annotation quality or scoring. While annotation classes and ambiguities are discussed, no explicit rubric or systematic protocol for scoring or judging annotations is described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 3 and Section 3.1 'Annotation'",
            "reasoning": "The paper includes clear examples of annotations with illustrations of the five main classes (temple, mound, hydrology, void, background) shown with images, hillshaded elevation maps, and annotated polygons. These examples clarify the nature of annotation categories and their appearance, serving as reference points for annotation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Annotation guidelines exist as evidenced by manual expert delineation, classification scheme, and example figures."
          }
        }
      }
    ]
  },
  {
    "id": "R4rNYJ2slJ-rubric-2",
    "token_usage": {
      "prompt_tokens": 16829,
      "completion_tokens": 356,
      "total_tokens": 17185
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Appendix A.2 Dataset Annotation Documentation",
            "reasoning": "The paper describes detailed instructions for fine-grained instance-level annotation in Section 3.2, including line categories, attributes, instance definition rules, occlusion handling, and special cases like overpasses. Appendix A.2 further elaborates these annotation rules in depth, indicating the presence of comprehensive annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Annotation and Appendix A.2 Dataset Annotation Documentation",
            "reasoning": "The annotation guidelines include clear rubrics as they define specific categories (e.g., curb, lane line, virtual line) and eight distinct line attributes (color, line type, number of lines, function, bidirection, boundary, occlusion, clearness) with detailed descriptions and criteria. Also, instance definitions based on attribute changes and forks are explicitly provided, constituting a rubric for consistent labeling."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures 3, 4, 10 and Appendix A.2 Dataset Annotation Documentation",
            "reasoning": "The paper provides multiple illustrative examples and figures demonstrating annotation cases such as line categories (Fig. 3a), line attributes (Fig. 3b), instance definition in fork/merge situations (Fig.4), occlusion and overpass cases (Fig.10), and image-level tags (Appendix A.2), serving as clear examples to guide annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides extensive detailed annotation guidelines, including instructions, rubrics, and examples, so the label N/A is not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "ScPgzCZ6Lo-rubric-2",
    "token_usage": {
      "prompt_tokens": 35199,
      "completion_tokens": 219,
      "total_tokens": 35418
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the use of public graph datasets and evaluates graph condensation methods on these datasets, but does not discuss any manual data labeling or human annotation process requiring detailed annotation instructions for model annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper of scoring rubrics or formal criteria provided for annotators to label any data, as the datasets are pre-existing and the labels are already provided or inherent to the datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper includes experimental results and visualizations for analysis but does not provide examples related to annotation guidelines or labeling instructions, as no manual or subjective annotation task is introduced."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces a benchmark integrating existing graph datasets for evaluation, not creating new annotated datasets or involving a subjective annotation process requiring model annotation guidelines. Therefore, no model-based annotation guidelines are provided or necessary."
          }
        }
      }
    ]
  },
  {
    "id": "USUkwg5pW6-rubric-2",
    "token_usage": {
      "prompt_tokens": 23649,
      "completion_tokens": 287,
      "total_tokens": 23936
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes an automatic scribble generation algorithm to create synthetic scribble labels from dense segmentation masks, but it does not describe any detailed model annotation instructions for human annotators on how to label data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper of detailed scoring rubrics or criteria used to guide or evaluate the human annotation process. The scribbles are generated automatically rather than via a manual annotation process guided by rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the paper provides some example images showing the generated scribbles (e.g., Fig. 2, Fig. 3) these are examples of generated scribble labels, not examples included in annotation guidelines to instruct human annotators."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The new datasets introduced (s4Pascal, s4KITTI360, s4Cityscapes, s4ADE20K) are generated by an automatic scribble generation algorithm rather than manual annotation. Therefore, there are no model annotation guidelines such as instructions, rubrics, or example annotations provided for human labeling. This is explicitly stated as the scribble generation algorithm is automatic and designed to produce synthetic scribble labels from existing dense segmentation labels."
          }
        }
      }
    ]
  },
  {
    "id": "UYgE9IfQIV-rubric-2",
    "token_usage": {
      "prompt_tokens": 29694,
      "completion_tokens": 279,
      "total_tokens": 29973
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces the SustainDC Python environment for benchmarking multi-agent reinforcement learning algorithms in data center control, focusing on simulating and optimizing energy and carbon emissions in data center operations. It details the model components, environment configurations, and agents' action and state spaces, but does not provide instructions or guidelines for human annotators or data labelers since the dataset or environment is not based on human annotation but rather simulation and RL environment interactions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of detailed rubrics or scoring guidelines for annotators or labelers in the dataset or environment. SustainDC is a simulation benchmark for RL algorithms, not a labeled dataset requiring rubric-defined human annotations."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper includes examples of workloads, actions, and state definitions for agents within the simulation but does not provide examples intended as annotation guidance for human labelers, as no human annotation or labeling is involved."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The SustainDC benchmark is a simulation environment for multi-agent reinforcement learning with no human-annotated datasets or labeling involved. Therefore, no model annotation guidelines (instructions, rubrics, examples) are provided or applicable."
          }
        }
      }
    ]
  },
  {
    "id": "VH1vxapUTs-rubric-2",
    "token_usage": {
      "prompt_tokens": 16844,
      "completion_tokens": 292,
      "total_tokens": 17136
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the dataset creation and ML task formulations but does not provide detailed instructions on annotation procedures for data labeling. The data sources are from existing curated datasets and processed into a datacube, with no mention of human annotators or specific detailed instructions for labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or guidelines for subjective annotation are presented. The dataset variables are derived from measured or computed data sources, not annotated with subjective labels that require rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include examples of annotations or labeling instructions, since the dataset is constructed from multiple established data sources integrated into a datacube rather than requiring guided manual annotations."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The Mesogeos dataset is built by collating and harmonizing pre-existing data sources (meteorology, vegetation, burned area maps, etc.) rather than through human annotation or labeling procedures requiring model annotation guidelines. Therefore, no model annotation guidelines are provided or necessary."
          }
        }
      }
    ]
  },
  {
    "id": "VSJotgbPHF-rubric-2",
    "token_usage": {
      "prompt_tokens": 12449,
      "completion_tokens": 358,
      "total_tokens": 12807
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Contributor Guidelines and Section 3.1 Single-Step Collection",
            "reasoning": "The paper details that contributors were issued clear and detailed guidelines to achieve quality and consistency across tasks such as labeling and ranking messages. Section 3.3 explicitly states that guidelines clarify meanings, scales, and criteria for assigning labels and rankings, including politeness, helpfulness, safety awareness, and instruct prompts to be diverse and challenging, evidencing detailed instructions given to annotators."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 Single-Step Collection and Section 3.3 Contributor Guidelines",
            "reasoning": "The labeling tasks involve categorizing messages along multiple dimensions (spam detection, guideline adherence, and quality) with quality rated on a five-point Likert scale on traits such as quality, creativity, humorousness, politeness, and harmlessness. This use of scales and defined dimensions constitutes detailed rubrics for annotation, as elaborated in Section 3.1 and supported by annotations guideline descriptions in Section 3.3."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A and Figure 4",
            "reasoning": "The paper mentions that a full copy of the contributor guidelines is in Appendix A and references Figure 4 for labels associated to guideline adherence, indicating that examples or illustrative labels are provided to guide annotators, which shows that clear examples accompany the annotation instructions and rubrics."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly provides extensive annotation guidelines with instructions, rubrics, and examples for the crowd-sourced labeling tasks. Therefore, the label N/A for no annotation guidelines does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "VXohja0vrQ-rubric-2",
    "token_usage": {
      "prompt_tokens": 18478,
      "completion_tokens": 421,
      "total_tokens": 18899
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Dataset Instance Collection and Section 2.1 Calculation Task Curation",
            "reasoning": "The paper describes a detailed three-step pipeline for data collection and preparation, including note collection, attribute extraction using GPT-4, followed by manual verification and correction by a medically trained individual. It also describes the manual synthesis of patient notes for calculators lacking eligible notes. These steps imply detailed instructions were followed by annotators or data curators, especially the manual verification by a medically trained individual for correctness, indicating the presence of detailed model annotation instructions to ensure quality."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2 Dataset Instance Collection and Section 4.1 Error Analysis",
            "reasoning": "The paper discusses the verification process for ground truth attribute extraction and explanation generation, done by individuals with medical background, suggesting quality control criteria. Additionally, in Section 4.1, the paper defines clear error types (Type A-D) with examples for evaluating model predictions. While these rubrics are oriented toward error analysis rather than annotation labeling, the systematic categorization and verification imply rubric-like guidelines were used to maintain annotation consistency."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 1 and Table 3 in Sections 1 Introduction and 4.1 Error Analysis",
            "reasoning": "Figure 1 provides explicit example instances of the dataset including patient note, question, explanation, and final answer, demonstrating the format and detailed calculation steps. Table 3 in Section 4.1 shows specific examples of different error types, indicating that examples were used or generated during annotation or evaluation. These examples serve as clear guidance or exemplars for annotation or error classification, indicating the presence of examples in annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper describes the detailed process of dataset construction, manual verification by medically trained individuals, quality control via error type distinctions, and provides examples, confirming that there are annotation guidelines present rather than none."
          }
        }
      }
    ]
  },
  {
    "id": "WUWHVN4gxk-rubric-2",
    "token_usage": {
      "prompt_tokens": 18990,
      "completion_tokens": 230,
      "total_tokens": 19220
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and supplementary materials describe the dataset and competition setup in detail, but do not contain explicit instructions provided to annotators or users for labeling the dataset. The dataset was collected from competition interactions rather than manual annotation guided by instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No description in the paper indicates the use of scoring rubrics or detailed grading criteria for labeling the dataset data. Scoring relates to competition evaluation but not to annotation guideline rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the paper shows example dataset entries and format examples in Appendix G, these are not model annotation guideline examples but rather example data points. There is no indication of annotation guideline examples or demonstrations for labelers."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset consists of competition-generated data with no indication of model annotation guideline instructions, rubrics, or examples for labeling. Data was collected from automated and human participant interactions, not manual annotation with guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "WVQ4Clw1VD-rubric-2",
    "token_usage": {
      "prompt_tokens": 11681,
      "completion_tokens": 267,
      "total_tokens": 11948
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2.2 Generation of Multigranular Text Description",
            "reasoning": "The paper describes a detailed, structured prompt template that guides the MLLMs to generate multigranular textual annotations, including a three-level hierarchical framework with specific questions for global, local, and local-global interactions. This indicates detailed instructions for annotation generation by the model."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any scoring rubrics or explicit quantitative criteria used for model annotations or judgments for consistency or reproducibility. No rubrics for annotation quality scoring are provided."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures 3, 4, 5, 6, 7, and 8",
            "reasoning": "The paper provides multiple qualitative example comparisons illustrating generated textual descriptions with and without coarse captions, ROIs, medical knowledge, and comparison to GPT-4V outputs. These examples demonstrate the outputs and implicitly show the annotation format and richness."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper presents extensive details on the automatic annotation process, including instructions, prompting strategies, and examples; thus, model annotation guidelines are clearly present."
          }
        }
      }
    ]
  },
  {
    "id": "XBcStBjBIE-rubric-2",
    "token_usage": {
      "prompt_tokens": 30027,
      "completion_tokens": 365,
      "total_tokens": 30392
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix B.1 and D.6",
            "reasoning": "The paper provides detailed instructions for model annotation, especially for the GPT4o-MTScore evaluation, where a 5-point scoring guideline is created to ensure consistent evaluations by the model and human annotators. Additionally, in human evaluation (Appendix D.6), specific scoring criteria and guidelines are given to participants to minimize bias."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix B.1 Table 6 and Appendix D.6",
            "reasoning": "The authors define a clear 5-point rating rubric for GPT4o-MTScore, with detailed descriptions for each score to guide the annotation and evaluation process. Similarly, human evaluators use a detailed rubric with explicit rating scales for assessing visual quality, text relevance, metamorphic amplitude, and temporal coherence."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B.4 Figure 8 and Appendix D.6",
            "reasoning": "The paper provides visual references and examples illustrating varying scoring levels for MTScore and CHScore (Appendix B.4), aiding understanding of annotation guidelines. Also, sample prompts and evaluation examples are given to annotators in the human evaluation section to ensure clear understanding."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes model annotation instructions, rubrics, and offers examples for the new datasets and evaluations; hence, it cannot claim absence of annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "ZDvXY56DeP-rubric-2",
    "token_usage": {
      "prompt_tokens": 19070,
      "completion_tokens": 200,
      "total_tokens": 19270
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any model annotation guidelines involving detailed instructions for data labeling since the dataset consists of tracked reinforcement learning experiment runs and metrics rather than human-labeled subjective data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The dataset is composed of logged experimental runs and quantitative metrics; no mention is made of any scoring rubrics used for subjective annotation tasks."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of annotation guidelines since the data is generated from experiments and automated metric collection, not subjective human annotation requiring examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "Since ORLB's dataset contains recorded runs of reinforcement learning experiments and associated metrics without involving subjective annotation or human labeling tasks, there are no model annotation guidelines, instructions, rubrics, or examples relevant to this dataset."
          }
        }
      }
    ]
  },
  {
    "id": "aTXhTD44nF-rubric-2",
    "token_usage": {
      "prompt_tokens": 23690,
      "completion_tokens": 283,
      "total_tokens": 23973
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Obtaining LLM Annotations and Appendix B System Prompt",
            "reasoning": "The paper provides detailed annotation guidelines in the system prompt supplied to LLMs, which include definitions of stance and dogmatism, descriptions of labels, input/output JSON formats, and instructions for labeling stance and dogmatism with reasoning, ensuring clear, structured instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix B System Prompt",
            "reasoning": "The system prompt clearly defines the specific categories for stance (five-point scale) and dogmatism (four-point scale), describing each label category in detail, effectively serving as a rubric to guide annotation decisions by LLMs."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix E Samples of JSON Outputs from LLMs (E.1 to E.6)",
            "reasoning": "The paper includes multiple sample JSON annotation outputs from GPT-4 and Mistral Large under zero-shot, one-shot, and few-shot settings, showing actual labeled examples that serve as concrete illustrations of guidelines in practice."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly documents detailed model annotation guidelines with instructions, rubrics, and examples for the data labeling process, thus N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "aXeiCbMFFJ-rubric-2",
    "token_usage": {
      "prompt_tokens": 22291,
      "completion_tokens": 360,
      "total_tokens": 22651
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Generation, and Appendix E",
            "reasoning": "The paper details the data generation process involving both linguistic annotators (GPT-4) and visual annotators (PaddleOCR). It describes step-by-step instructions for annotating intermediate chain-of-thought bounding boxes to highlight key image regions essential for answering the question. The paper also refers to prompts used for question and reasoning step generation in Appendix E, indicating detailed instructions guiding the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any detailed scoring rubrics or quantitative guidelines used during annotation to ensure consistency or assess annotation quality. The annotations rely on GPT-4 generation and OCR detection, but no formal rubric or scoring system for annotation is described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3 and Table 1, Figure 1, Appendix E",
            "reasoning": "The paper provides clear examples of annotated data, including detailed reasoning steps with corresponding bounding boxes (e.g., Table 1). Figures 1, 8, and 9 show example images with annotated questions, answers, and bounding boxes. Appendix E shows prompt designs that include examples used to generate annotations, serving as guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Since the paper provides explicit annotation instructions and examples for the new Visual CoT dataset, the label N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "abXaOcvujs-rubric-2",
    "token_usage": {
      "prompt_tokens": 15100,
      "completion_tokens": 328,
      "total_tokens": 15428
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3, Figures 4 and 8",
            "reasoning": "The paper describes detailed prompts given to a large language model (GPT-4) to paraphrase table and column names to improve name diversity and realism. These prompts function as explicit instructions for how to rename tables and columns, specifying the expected output format (JSON object with improved names). The authors provide exact prompt text examples (Figures 4 and 8) that serve as detailed procedural instructions for this annotation step."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any scoring rubrics, evaluation criteria, or systematic scoring guidelines for the model-based annotation process. The approach relies on prompts to LLMs without formal rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures 4 and 8",
            "reasoning": "The paper provides concrete prompt examples that include example table rows embedded within the prompt (Figures 4 and 8). These serve as clear examples illustrating the input data context for annotation (paraphrasing), which help guide the model in generating appropriate paraphrased names."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Model annotation guidelines are provided via detailed LLM prompting for renaming tables and columns."
          }
        }
      }
    ]
  },
  {
    "id": "aiGN4UnNM7-rubric-2",
    "token_usage": {
      "prompt_tokens": 16354,
      "completion_tokens": 357,
      "total_tokens": 16711
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4.2 Annotation",
            "reasoning": "The paper provides detailed instructions on how the TTC ground truth is generated using 2D and 3D detections and LiDAR data. They describe the process of projecting 3D boxes to 2D detection boxes, the selection of the closest corner for depth, velocity estimation by fitting past depth frames using RANSAC, and the calculation of TTC. They also discuss handling dynamics with different velocity fitting window lengths and manual checking by an annotation team. This constitutes a comprehensive set of instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the presence of any detailed rubrics or scoring guidelines for annotation consistency or quality evaluation. The annotation process relies on automated calculation plus manual checking, with no rubric-based scoring or qualitative guidelines mentioned."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 4.2 Annotation, Figure 2",
            "reasoning": "The paper includes a figure (Fig. 2) illustrating the annotation process which serves as a clear example of how annotations are generated. In addition, in Section 4.2, the step-by-step description and formulae serve as concrete examples guiding the annotation process."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Since detailed instructions and examples are clearly provided for model annotation guidelines, it is not appropriate to consider the guidelines as absent."
          }
        }
      }
    ]
  },
  {
    "id": "b6IBmU1uzw-rubric-2",
    "token_usage": {
      "prompt_tokens": 30951,
      "completion_tokens": 328,
      "total_tokens": 31279
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2, 'Construction of QA Pairs' and Appendix C",
            "reasoning": "The paper details the process for constructing question-answer pairs, including using GPT-4 for generating diverse question templates (10-30 per question type) for closed-ended questions, and transforming medical reports into open-ended QA pairs by GPT-4, thus providing clear and detailed instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2, 'Types of Questions and Metrics','Post-processing' and Appendix D.1",
            "reasoning": "The authors specify metrics such as accuracy for closed-ended questions and a scoring rubric using GPT-4 to rate helpfulness, relevance, accuracy, and detail with an overall score from 1 to 10 for open-ended questions. Also, they perform post-processing with GPT-4 self-checks followed by manual correction, indicating presence of detailed rubrics guiding annotation quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix C and Tables 9-12",
            "reasoning": "The paper presents numerous example question templates for closed-ended questions in Appendix C and Tables 9-11, as well as the prompt templates for GPT-4 to generate open-ended QA pairs (Table 12). This inclusion of concrete examples in guidelines supports annotators and helps ensure consistency and reproducibility."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly describes detailed annotation guidelines including instructions, rubrics, and examples, so the label of no guidelines does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "bAaM8cKoMl-rubric-2",
    "token_usage": {
      "prompt_tokens": 28927,
      "completion_tokens": 355,
      "total_tokens": 29282
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4 Code and Resources, and Appendix A General Dataset Info and Appendix C Detailed Datasets Information",
            "reasoning": "The paper describes detailed instructions for generating the datasets using configuration files in TOML format, which specify all parameters for each dataset. It also provides documented code and utilities to evaluate DNNs, with configurable options and examples. Each dataset is accompanied by descriptive information about its purpose, parameters, and suggested testing methods, reflecting detailed instructions for annotation and usage."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3 Testing Methods and Appendix C Detailed Datasets Information (e.g., Sections C.1 to C.3)",
            "reasoning": "The paper outlines explicit testing methods, including similarity judgment analysis, decoder methods, and out-of-distribution classification, each with criteria to qualitatively or quantitatively measure model alignment with human perception. These testing procedures effectively act as rubrics by defining how to evaluate model responses to different stimuli, ensuring consistent and reproducible scoring aligned with psychological phenomena."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3 Testing Methods (Figure 2) and Appendix C Detailed Datasets Information",
            "reasoning": "The paper provides concrete examples illustrating how to use the datasets and testing methods, such as evaluating the Ebbinghaus Illusion and Texturized Unfamiliar dataset with ResNet-152. Figures and detailed dataset conditions serve as examples demonstrating the implementation of guidelines and interpretation of results."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly provides detailed instructions, testing rubrics, and examples for the new datasets introduced, so no lack of model annotation guidelines is present."
          }
        }
      }
    ]
  },
  {
    "id": "cLga8GStdk-rubric-2",
    "token_usage": {
      "prompt_tokens": 15213,
      "completion_tokens": 393,
      "total_tokens": 15606
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1, Section 4.1",
            "reasoning": "The paper describes how the UK Linguistics Olympiad (UKLO) puzzles are designed so that all necessary information is provided within the problem context, enabling test-takers to deduce linguistic patterns without prior knowledge. The dataset preserves original instructions and context, providing detailed task instructions inherent in each puzzle's preamble, context, and questions (Section 3.1). The evaluation metric described in Section 4.1 also indicates careful consideration of the nature of the tasks, implying task-specific guidelines were established for annotation and scoring."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4.1",
            "reasoning": "Section 4.1 details an automated scoring method based on exact match to the full answer, with strict criteria to define correctness, reflecting a rubric for annotation. The paper mentions human expert scoring by UKLO members with the possibility of partial credit based on correctness of phrase parts or rules, showing that detailed rubrics exist for manual annotation in the original UKLO context."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.4, Figure 2",
            "reasoning": "An example puzzle excerpt is provided in Section 3.4 and illustrated in Figure 2, showing the puzzle format, sections (preamble, context, questions), and the expected answer with explanation of reasoning steps. This serves as clear examples in the annotation guidelines demonstrating how to interpret and solve puzzles, which guides annotators or evaluators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The benchmark is derived from UKLO puzzles, which are carefully constructed with detailed instructions, rubrics, and examples as part of their official design and scoring. The paper discusses these aspects explicitly, so it is not the case that no model annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "cR3T1ZYN8I-rubric-2",
    "token_usage": {
      "prompt_tokens": 35593,
      "completion_tokens": 345,
      "total_tokens": 35938
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix E and Figure 6",
            "reasoning": "The paper includes detailed instructions for human annotators during a human evaluation of model predictions as described in Appendix E, including briefing documents and annotation forms shown in Figure 6. Annotators are given clear guidance on how to rate model-generated responses with a detailed scale (1 to 5). This shows a clear set of instructions provided to human annotators involved in evaluation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix E, Figure 6, and Section 5.3 Qualitative Evaluation",
            "reasoning": "The paper defines a rubric for human evaluators to rate predicted responses according to a scale from 1 (completely wrong) to 5 (completely right) as specified in Appendix E, Fig. 6, and discussed in the qualitative evaluation (Section 5.3). This rubric is explicitly provided to annotators and used to assess semantic similarity between generated model responses and ground truth. Hence, it includes detailed rubrics for annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix E, Figure 7",
            "reasoning": "Examples of annotation cases with ground truth labels and model-generated responses are provided in Appendix E and illustrated in Figure 7. These examples demonstrate how annotators should review responses and assign scores, serving as clear examples to guide annotation. Thus, model annotation guidelines include clear examples."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides thorough instructions, rubrics, and examples for the annotation process during human evaluation, so it does not lack model annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "cu8FfaYriU-rubric-2",
    "token_usage": {
      "prompt_tokens": 28567,
      "completion_tokens": 260,
      "total_tokens": 28827
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce any new datasets, nor does it present detailed model annotation guidelines or instructions for data labeling associated with any new dataset. The paper focuses on interviewing dataset curators about challenges and recommendations for fair dataset curation, without providing concrete annotation instructions for a specific new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any scoring rubrics or detailed criteria used for annotating data in a new dataset created by the authors. The focus is on challenges and considerations in fair dataset curation, rather than on providing annotation rubrics for a newly introduced dataset."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No clear examples of annotation or labeling guidelines are provided for any new dataset, since the paper does not present or release any new datasets or their associated annotation protocols."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets or model annotation guidelines. Its primary contribution is a taxonomy of challenges in curating fair datasets based on interviews and an associated set of recommendations, without presenting detailed annotation instructions, rubrics, or examples for any new datasets."
          }
        }
      }
    ]
  },
  {
    "id": "cy8mq7QYae-rubric-2",
    "token_usage": {
      "prompt_tokens": 81174,
      "completion_tokens": 353,
      "total_tokens": 81527
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 and Appendix O and P",
            "reasoning": "The paper describes detailed instructions for question construction and response generation for both descriptive and reasoning questions in Section 3.2. They specify different categories of descriptive questions with explicit instructions (Appendix O), and for reasoning questions they provide answer-type-based instructions (Appendix P). These instructions guide annotators on how to formulate questions and instruct model response generation, demonstrating detailed model annotation guidelines for data labeling."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 and Appendices O.2 and P.2",
            "reasoning": "Section 3.3 explains the evaluation and grading metrics for model answers including the use of GPT-4o as an automated judge with clearly defined binary scoring rubrics. Detailed grading instructions with rule-based criteria are provided in Appendices O.2 (descriptive questions) and P.2 (reasoning questions). This constitutes detailed rubrics to ensure consistent scoring during annotation and evaluation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendices S, T, U, and V",
            "reasoning": "The paper provides multiple concrete examples of modified questions (Appendix S), modified charts (Appendix T), common failure cases for descriptive questions (Appendix U) and reasoning questions (Appendix V). These examples illustrate the annotation process and clarify expected answers and scoring, serving as practical exemplars for annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly provides multiple detailed annotation guidelines including instructions, rubrics, and examples, so it is inappropriate to claim no model annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "d1Pup4gkWf-rubric-2",
    "token_usage": {
      "prompt_tokens": 13838,
      "completion_tokens": 229,
      "total_tokens": 14067
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any instructions for model annotators or labelers for data labeling. The dataset aspects focus on simulated environments and demonstrations from videos but do not describe annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no scoring rubrics or guidelines described for annotation or labeling of data. The paper discusses reward functions and training rewards, but these are part of environment design and training, not annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any examples of annotation or scoring guidelines. The main content focuses on simulated sports environments and training details, without reference to data labeling examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper introduces new simulated sports environments for humanoid agents; however, it does not provide any model-based annotation guidelines for data labeling, including instructions, rubrics, or examples. The human demonstration data is acquired by automated methods from videos, not manual annotation requiring guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "dVaWCDMBof-rubric-2",
    "token_usage": {
      "prompt_tokens": 22783,
      "completion_tokens": 291,
      "total_tokens": 23074
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the construction of the COMMONPOOL dataset and its filtering, but does not provide any detailed instructions intended for data annotators or labelers on how to annotate or label data for training. The dataset is collected and filtered mostly via automated methods rather than manual annotation, and no instructions for model-based annotation are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of detailed rubrics or scoring criteria used during the annotation or filtering process for the datasets. The paper focuses on filtering based on scores such as CLIP similarity, language detection, and image features, not on rubric-based annotation by humans or models."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not contain example annotation guidelines, examples of labeled data with instructions, or exemplars illustrating annotation decisions. The data is from web scraping and filtering, not from a manual annotation process requiring guidance with examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The new datasets introduced (COMMONPOOL and DATACOMP-1B) are constructed via large-scale web data collection and automated filtering methods. The paper does not provide or mention any model annotation guidelines, instructions, rubrics, or examples for data labeling. Therefore, no model-based annotation guidelines are provided for these datasets."
          }
        }
      }
    ]
  },
  {
    "id": "g7OX2sOJtn-rubric-2",
    "token_usage": {
      "prompt_tokens": 46250,
      "completion_tokens": 219,
      "total_tokens": 46469
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any detailed instructions or guidelines specifically for annotators or model annotation processes related to the labeling or curation of the dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper of detailed rubrics or scoring criteria for annotations. The annotation appears to be automatic extraction from Lean proofs without explicit subjective labeling or scoring requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No explicit examples of annotation guidelines or annotation examples are presented in the paper or appendices. The dataset is derived from formal proof extraction, not manual annotations with examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "LeanDojo Benchmark data and premises are extracted programmatically by instrumenting the Lean proof assistant and do not rely on human annotation or subjective labeling necessitating model annotation guidelines, rubrics, instructions, or examples. Thus, no model annotation guidelines are provided or relevant."
          }
        }
      }
    ]
  },
  {
    "id": "gg3POFjqq8-rubric-2",
    "token_usage": {
      "prompt_tokens": 11965,
      "completion_tokens": 362,
      "total_tokens": 12327
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 4, 'Dataset for Evaluating OOC Filtering Strategies' and Appendix A",
            "reasoning": "The paper describes a detailed manual labeling procedure for their new dataset to identify out-of-class (OOC) samples, involving two stages of filtering and labeling by two human annotators with multi-level labels ('class', 'partial class properties', 'not class'). They provide explicit instructions regarding which images are considered 'OOC' and 'in-class'. This reflects the presence of detailed instructions for model annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 4, 'Dataset for Evaluating OOC Filtering Strategies' and Appendix A",
            "reasoning": "The annotation labels include multiple categories such as 'class', 'partial class properties', and 'not class', and an image is defined as OOC if at least one annotator labels it as such. This indicates the presence of a rubric or a scoring scheme guiding annotators on how to categorize images, which helps ensure consistency in labeling."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 2 and Section 4, Appendix A",
            "reasoning": "The paper provides qualitative examples of images used in the dataset, including examples of OOC samples (Figure 2) and partial classes, helping annotators understand sample distinctions. Additionally, the Appendix A is mentioned to contain more details on the labeling strategy and dataset statistics, which likely includes concrete examples for guidance."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly details an annotation procedure with instructions, rubrics, and examples for the newly introduced dataset for evaluating OOC filtering strategies, thus it is not applicable that no annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "h8LuywKj6N-rubric-2",
    "token_usage": {
      "prompt_tokens": 45010,
      "completion_tokens": 322,
      "total_tokens": 45332
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.3 and Appendix B.3",
            "reasoning": "The paper describes a detailed annotation process involving human annotators and large language model collaboration. In subsection 2.3 and Appendix B.3, the authors provide instructions for human annotators on video recording, segmenting, keyframe selection, detailed captioning, question-answer generation, and verification steps. They include guidelines for generating types of questions, ensuring question answerability, and refining annotations with large language models. This reflects detailed instructions provided to annotators for labeling the dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe the use of detailed scoring rubrics or explicit quantitative guidelines for annotation quality or labeling decisions. Although evaluation metrics are discussed for model benchmarking, no annotation rubrics for data labeling tasks are presented."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 2.3, Table 3, Appendix B.3",
            "reasoning": "The paper includes clear examples of generated question-answer pairs and annotation types in Table 3, illustrating different question categories and their example questions and answers. Additionally, Appendix B.3 shows screenshots of the annotation interface and prompts used for human-LLM collaboration on annotations, providing concrete examples of the annotation procedure and instruction."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The authors provide extensive descriptions of the annotation guidelines, instructions, and examples, so the dataset cannot be regarded as having no model annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "hcOq2buakM-rubric-2",
    "token_usage": {
      "prompt_tokens": 33991,
      "completion_tokens": 227,
      "total_tokens": 34218
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce new datasets with model annotation guidelines that include detailed instructions for data labeling. The assessments focus on existing AI benchmarks and their qualities rather than describing new datasets or their annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper evaluates benchmarks using an assessment framework but does not introduce new datasets with model annotation guidelines that include detailed scoring rubrics. The emphasis is on benchmarking criteria and assessment, not on dataset annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No clear examples of model annotation guidelines or examples for data labeling are provided for new datasets in the paper. The datasets evaluated are existing AI benchmarks, and the paper analyzes their quality rather than providing new annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce new datasets with model annotation guidelines. It assesses existing benchmarks and proposes an assessment framework. Therefore, no model annotation guidelines are provided for new datasets introduced by the authors."
          }
        }
      }
    ]
  },
  {
    "id": "iSwK1YqO7v-rubric-2",
    "token_usage": {
      "prompt_tokens": 98211,
      "completion_tokens": 399,
      "total_tokens": 98610
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix M - Annotation Details; Appendix I - Prompt and Analysis",
            "reasoning": "The paper thoroughly describes the annotation process and provides detailed prompting instructions to guide the annotation of goal specifications, action trajectories, subgoal decompositions, and transition models. The prompts, presented in Appendix I, include precise instructions about the expected output formats, predicate definitions, action parameter requirements, and rules to ensure consistency and logical correctness. Additionally, the annotation procedures in Appendix M detail steps taken to manually annotate and verify data, ensuring high-quality label generation, indicating detailed instructions were provided to annotators or during annotation scripting."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendix D - Fine-Grained Metrics and Automatic Error Detection",
            "reasoning": "The paper introduces fine-grained evaluation metrics and automatic error detection methods that break down errors in generated labels into several types (e.g., hallucination, missing step, additional step, wrong order, affordance errors) and apply set-based F1 scoring for goal matching. The detailed description in Appendix D outlines these rubrics and their application for scoring LLM outputs against ground truth annotation, indicating that detailed rubrics were established for annotation and evaluation to maintain consistency and reproducibility."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix I - Prompt and Analysis; Appendix E - Full Results with 18 models",
            "reasoning": "The paper provides concrete prompt templates with example inputs and outputs for each ability module in Appendix I. Moreover, in Appendix E, qualitative error case studies and example outputs are shared, illustrating the annotation and evaluation process. These examples clarify the expectations and serve as practical guidelines supporting consistent annotation, hence model annotation guidelines include clear examples."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper extensively documents annotation guidelines, detailed prompting, rubrics, and provides examples, thus it does not lack model annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "iTUlYblV0K-rubric-2",
    "token_usage": {
      "prompt_tokens": 20449,
      "completion_tokens": 288,
      "total_tokens": 20737
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide explicit detailed instructions aimed at human annotators or models for labeling data in the introduced remastered dataset. The data is generated via a process involving knowledge graphs and LLM-generated questions, but specific annotation instructions are not described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of scoring rubrics or detailed criteria for evaluation or annotation of the data samples provided to annotators or for model-based labeling. The evaluation is conducted quantitatively using question answering accuracy rather than rubric-based annotation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1 and 3.2, and Sections 4.1 and Appendices (e.g., Appendix B.1)",
            "reasoning": "The paper includes illustrative examples demonstrating issues found in the dataset such as contamination examples (case_id examples in Sections 3.1 and 3.2) and how questions are generated. These examples clarify the annotation and generation process and the errors being remedied, indicating that examples are part of the annotation/guideline understanding."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Since the paper provides examples and discussion about question generation and error correction in dataset construction, it does not qualify as having no annotation guidelines at all."
          }
        }
      }
    ]
  },
  {
    "id": "iwC19lVBoq-rubric-2",
    "token_usage": {
      "prompt_tokens": 13820,
      "completion_tokens": 320,
      "total_tokens": 14140
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the data collection and filtering stages in detail but does not provide explicit detailed annotation instructions for human annotators or models used during data labeling. The initial labels in AVSET-700K are derived from AudioSet which includes manual audio category labels, but the paper itself does not present annotation guidelines written by the authors for labeling this new dataset."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any scoring rubrics or evaluation criteria used for annotation. The dataset labeling process relies on filtering through model-based similarity measures rather than manual scoring or rubrics, and no rubric details are provided by the authors."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Though the paper shows audio-visual consistency sample illustrations (Figure 5) and statistical analyses of the dataset, it does not provide specific annotated examples or instructions on how annotation decisions were made in a step-by-step manner as part of model annotation guidelines."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The AVSET-10M dataset is constructed by filtering existing datasets using model-based similarity measures and audio classifiers, but the authors do not provide explicit model annotation guidelines, instructions, rubrics, or examples describing how labeling or annotation decisions were made for dataset creation. Annotation labels for AVSET-700K are inherited from AudioSet but no new annotation guidelines were introduced. Hence, no model annotation guidelines are provided specifically for the new dataset."
          }
        }
      }
    ]
  },
  {
    "id": "j2wasUypqN-rubric-2",
    "token_usage": {
      "prompt_tokens": 20054,
      "completion_tokens": 276,
      "total_tokens": 20330
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces new benchmark testsuites (Synthetic, Noisy-Synthetic, Protein-Docking) mainly consisting of problem instances for black-box optimization tasks, which are standard mathematical or scientific problems rather than subjective annotation tasks. There is no indication of any model-based annotation guidelines or instructions intended for labeling data in subjective tasks."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or specific evaluative criteria for human annotation or subjective labeling are discussed in relation to the new datasets. The evaluation metrics presented (AEI, MGD, MTE) are performance metrics for optimization algorithms, not annotation rubrics for data labeling."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The new datasets do not involve annotation tasks that would require example annotations. The paper provides examples and detailed descriptions of the optimization problems and baseline methods but does not provide annotation examples since no subjective labeling is involved."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The new datasets introduced are benchmark problem instances for black-box optimization and do not involve any model-based annotation or subjective labeling that require annotation guidelines. Therefore, there are no model annotation guidelines such as instructions, rubrics, or examples provided or relevant for these datasets."
          }
        }
      }
    ]
  },
  {
    "id": "jSKtxmxc0M-rubric-2",
    "token_usage": {
      "prompt_tokens": 20317,
      "completion_tokens": 370,
      "total_tokens": 20687
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Data Construction, Appendix A.1 Data Collection Settings, Appendices D and E for prompt templates and annotation instructions",
            "reasoning": "The paper details extensive instructions for annotators, including manual reproduction of instructional videos, breaking down operations into subtasks, and labeling active elements for each action. Annotation tools highlight keyframes with action regions, and annotators provide textual element names, action narrations, and quadruple details for drag actions. The Appendix includes detailed prompt templates for various planning and action tasks, demonstrating comprehensive instructions guiding consistent annotation and evaluation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Evaluation and Metrics, Appendix A.3 Evaluation Settings, Table 28 Evaluation Prompt Template",
            "reasoning": "The paper describes well-defined quantitative metrics and scoring rubrics for each hierarchical level: planning scored on a 0\u20135 scale by a GPT-4-Turbo critic, and atomic actions evaluated by distance-based and recall metrics. Detailed evaluation criteria and correctness scores are provided to measure accuracy and provide feedback on model outputs, enabling consistent and reproducible annotation and model evaluation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix D Dataset Examples and Tables 10 through 17, Figure 6 Qualitative Results",
            "reasoning": "The paper provides numerous concrete examples of full tasks, high-level and mid-level plans, and atomic actions with corresponding visual queries and textual descriptions. Qualitative results and detailed sample annotations illustrate the annotation format and guidelines, helping annotators and evaluators understand expected outputs."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides comprehensive model annotation guidelines including instructions, rubrics, and examples, so 'N/A' does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "jbrMS0DNaD-rubric-2",
    "token_usage": {
      "prompt_tokens": 16019,
      "completion_tokens": 283,
      "total_tokens": 16302
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Query and Image Collection Process",
            "reasoning": "The paper states that annotators were instructed to label all candidate images as either relevant or not relevant to a query, marking as not relevant if there was reasonable doubt. This indicates that detailed instructions were provided to annotators to ensure comprehensive and consistent labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention the presence of any detailed scoring rubrics or multi-level rating scales for annotations. The labeling was binary (relevant or not relevant) without describing any rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no indication in the paper that clear examples were included in the annotation guidelines. While some example queries and images are shown for illustration of the benchmark, the paper does not describe providing annotation examples to annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly describes a careful annotation process with instructions to annotators, so model annotation guidelines do exist for this dataset."
          }
        }
      }
    ]
  },
  {
    "id": "kaHpo8OZw2-rubric-2",
    "token_usage": {
      "prompt_tokens": 104778,
      "completion_tokens": 391,
      "total_tokens": 105169
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendices C, D, E, F, G, H, I, J",
            "reasoning": "The paper provides detailed descriptions on the design of system prompts, user prompts, and evaluation protocols for various datasets and tasks, including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, adversarial demonstrations, privacy, ethics, and fairness. These descriptions include instructions on how to formulate prompts, what scenarios to test, and how to measure model outputs, indicating detailed guidance on labeling and evaluation processes."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Appendices D.1, E.1, H.2, I.1, J.1",
            "reasoning": "The paper includes quantitative metrics and evaluation rules for annotations such as agreementIndex for stereotypes, toxicity probability, expected maximum toxicity, accuracy, refusal rate, fair metrics like demographic parity difference and equalized odds difference, and false positive rate for ethics. These metrics serve as rubrics for scoring and interpreting annotations consistently."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures 14, 15, 17, 19, 22, 26, 28, 30, 35, 36, 37, Appendix D.3, Appendix E.1, Appendix H.2, Appendix I.2, Appendix J.2",
            "reasoning": "The paper presents numerous example prompts, system prompts, user inputs and corresponding annotated outputs or model responses to illustrate annotation scenarios across toxicity, stereotype bias, adversarial attacks, privacy, ethics, and fairness evaluations, demonstrating clarity and concreteness in the annotation guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly provides comprehensive annotation guidelines with instructions, rubrics, and examples across new datasets for trustworthiness evaluation, so lack of guidelines does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "l985bXCatk-rubric-2",
    "token_usage": {
      "prompt_tokens": 15141,
      "completion_tokens": 321,
      "total_tokens": 15462
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes the dataset construction process which includes extraction of product identifiers, synthetic label generation, active learning for classifier training, dataset splitting, cleaning, and some manual verification. However, it does not provide explicit instructions or procedural guidelines directed at human annotators or models for annotating the data. The labeling relies primarily on automated processes and heuristics rather than manual annotation with detailed instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention in the paper of any scoring rubric, criteria, or quantitative scales used for annotation. The dataset labels (complexity, category) are generated mainly by classifiers trained via active learning and not through manual labeling requiring rubrics. Thus, no annotation rubrics are provided or described."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper includes sample images from the dataset (Figure 3) to illustrate the data distribution, but these are examples of data instances rather than examples in annotation guidelines. No examples of annotation instructions, annotation outputs, or labeling decisions are given as part of an annotation guideline for consistent labeling by annotators."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset is constructed mostly using automated extraction, classifier-based labeling and synthetic caption generation without involving manual annotation that required model-based annotation guidelines. The paper does not provide explicit annotation instructions, rubrics or example clarifications for an annotation process. Therefore, no model annotation guidelines are provided for this dataset."
          }
        }
      }
    ]
  },
  {
    "id": "lnnNPiZtzR-rubric-2",
    "token_usage": {
      "prompt_tokens": 12248,
      "completion_tokens": 254,
      "total_tokens": 12502
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not contain any section or mention of instructions provided to annotators or labelers regarding how to annotate or label the fungi observations. The dataset is based on expert-labeled fungal records, but no explicit annotation instructions are described."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or description of detailed rubrics or scoring criteria used during the annotation process in the paper. Although expert curation is referenced, no rubrics for labeling consistency or quality assessment are provided."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper provides multiple visual examples of dataset content (images, segmentations, satellite data), but it does not present any explicit examples or walkthroughs of annotation or labeling guidelines used during the data labeling process."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper states that the fungal records were labeled and curated by experts, but it does not provide any model annotation guidelines, instructions, rubrics, or examples related to the data labeling. No explicit guidelines for annotators are described in the paper, thus no model annotation guidelines are considered present."
          }
        }
      }
    ]
  },
  {
    "id": "loJM1acwzf-rubric-2",
    "token_usage": {
      "prompt_tokens": 29480,
      "completion_tokens": 293,
      "total_tokens": 29773
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 and Appendix A.4, A.5",
            "reasoning": "The paper describes a detailed annotation pipeline with expert-level annotators who receive training and iterative feedback. Annotators are given clear standards for question quality, distribution requirements for question types and evidence sources, and instructions on modifying or adding questions to ensure quality and balance."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 and Appendix A.4, A.5",
            "reasoning": "The paper outlines systematic classification of problems in original annotations (six detailed problem types) and specifies criteria for retaining, revising, or removing questions. Additionally, quantitative minimum requirements for question types per document type are specified, constituting detailed rubrics guiding annotation consistency and quality."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A.4 (Examples Figures 15-20) and Appendix A.7 (Figures 21 and 22)",
            "reasoning": "The appendix provides explicit examples illustrating various annotation problems and their revisions, as well as screenshots of annotation interfaces used for editing and adding questions. These serve as clear examples guiding annotators during the annotation process."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Model annotation guidelines are explicitly provided and extensively detailed in the paper and appendices through instructions, rubrics, and examples."
          }
        }
      }
    ]
  },
  {
    "id": "mDRmX8IlBI-rubric-2",
    "token_usage": {
      "prompt_tokens": 16590,
      "completion_tokens": 243,
      "total_tokens": 16833
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Manual Data Collection",
            "reasoning": "The authors describe a two-stage manual data collection process where they examine disciplines and identify subdisciplines and then craft questions focused on seven aspects of multimodal video understanding. This indicates detailed instructions guided the question annotation stage to ensure coverage of multi-faceted reasoning aspects."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any specific scoring rubrics or detailed rubric guidelines provided to annotators for labeling or grading the questions. The evaluation is automated or via GPT-4 judging but no rubric-based human annotation guidelines are described."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1 (with examples referred to in Appendix F)",
            "reasoning": "The paper states that examples, such as the temporal understanding question type, can be found in Section F in the Appendix, indicating that annotation guidelines include clear examples to guide question creation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Model annotation guidelines are provided, with instructions and examples, so no case for N/A."
          }
        }
      }
    ]
  },
  {
    "id": "mEJgnZZyfv-rubric-2",
    "token_usage": {
      "prompt_tokens": 16713,
      "completion_tokens": 261,
      "total_tokens": 16974
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any detailed instructions for model annotation or data labeling. The focus is on mixup augmentation methods and benchmarking rather than annotation procedures."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no description or inclusion of scoring rubrics or evaluation criteria for subjective annotations or labeling tasks in the context of the datasets. The paper primarily benchmarks visual classification tasks based on existing and standard classification datasets."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present explicit examples of annotation guidelines or annotated samples. While examples of activation mapping and visualizations are provided for analysis, these are model outputs rather than annotation guidance."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "No new datasets introduced by the authors include model-based annotation guidelines. The datasets used are standard open-source classification datasets without new annotation procedures described. Therefore, model annotation guidelines are not applicable."
          }
        }
      }
    ]
  },
  {
    "id": "mlhFJE7PKo-rubric-2",
    "token_usage": {
      "prompt_tokens": 39691,
      "completion_tokens": 322,
      "total_tokens": 40013
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any description or mention of detailed model annotation instructions for data labeling of the HEST-1k dataset or the HEST-Benchmark tasks. The focus is on data assembly, preprocessing, and benchmarking downstream models, without manual annotation processes."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no evidence in the paper of detailed scoring rubrics being provided for subjective annotations or labels. The primary labels are gene expression quantitative values generated by experimental assays with computational normalization rather than human subjective annotation requiring rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present any example annotation cases or exemplars illustrating how to perform annotations or guidelines for model-based labeling. The data labeling is mainly experimental molecular data, not requiring model annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The datasets introduced (HEST-1k and HEST-Benchmark) are assembled from public and internal data consisting of paired spatial transcriptomics and histology images. There is no mentioning or provision of model-based annotation guidelines, instructions, scoring rubrics, or examples for human annotators or models, as the data labels are derived from experimental molecular profiling."
          }
        }
      }
    ]
  },
  {
    "id": "moMoWj7jLm-rubric-2",
    "token_usage": {
      "prompt_tokens": 23893,
      "completion_tokens": 299,
      "total_tokens": 24192
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3, Appendix C.1.1",
            "reasoning": "Section 3 describes the detailed annotation process, including formula normalization involving manual and LLM-assisted steps, with explicit instructions for annotators and prompts used for generating and correcting normalized formula explanations. Appendix C.1.1 provides the actual prompts used to guide annotators and LLMs in formula normalization and error correction, constituting detailed instructions for annotation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and appendix do not describe any explicit scoring rubrics or quantitative guidelines for annotations that would serve as rubrics to consistently guide annotation decisions beyond the procedural instructions and error correction prompts."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3, Table 2, Figures 1, 2, Appendix C.1.1 and C.1.2",
            "reasoning": "The paper provides multiple concrete examples of annotated questions with normalized formulas and parameter tables (e.g., Figure 1 and Table 2), along with error case examples (Figure 2) and detailed example prompts in Appendix C.1.1, demonstrating clear examples to guide annotators and ensure consistency."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly documents the annotation guidelines, instructions, and provides examples for the new FormulaReasoning dataset, thus guidelines are present."
          }
        }
      }
    ]
  },
  {
    "id": "nrEqH502eC-rubric-2",
    "token_usage": {
      "prompt_tokens": 79267,
      "completion_tokens": 384,
      "total_tokens": 79651
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3 Construction of BLEND, subsections Answer Annotation and B.5 Answer Annotation Guidelines (Figure 11)",
            "reasoning": "The paper explicitly describes detailed answer annotation guidelines provided to annotators including clear instructions on how to answer cultural questions, answer formats, handling multiple answers, and handling unknown answers. These instructions are shown in Section 3 and Appendix B.5 (Figure 11). Annotators were told to provide short/concrete answers, use their own knowledge, and avoid AI or search engines."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section D.3 Human Evaluation and Appendix D.3.1 Human Evaluation Schema",
            "reasoning": "The human evaluation uses a detailed rubric including categories like Applicability (with sublabels: Applicable, Conditionally Applicable, Incorrect), Unnatural Language, Stereotypical, Partially Correct, Refusal, Nonsensical, Different Country's View. These scoring rubrics help ensure consistency and are described in detail in Section D.3 and Appendix D.3.1."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A.3 Annotation Examples (Figures 5-10) and Appendix B.4 Question Construction Guidelines (with examples) and B.5 Answer Annotation Guidelines (Figure 11)",
            "reasoning": "The paper provides numerous examples of annotation responses and question templates in Appendix A.3, including answers in multiple languages and their translations. The question construction guidelines and answer annotation guidelines contain explicit examples of acceptable and unacceptable answers (e.g., Figure 11). These serve as clear examples to guide annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly provides extensive annotation guidelines including instructions, rubrics, and examples, so stating no guidelines are provided is invalid."
          }
        }
      }
    ]
  },
  {
    "id": "p8eUitex7p-rubric-2",
    "token_usage": {
      "prompt_tokens": 20030,
      "completion_tokens": 412,
      "total_tokens": 20442
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction; Section A.2 Annotator Guidelines",
            "reasoning": "The paper explicitly states that 30 annotators were recruited and underwent an onboarding stage consisting of training sessions presenting detailed instructions and proper ways to handle boundary cases (Section 3.1). Further, Section A.2 describes that detailed guidelines and tutorials were provided to annotators, covering parameter introductions, use of the annotation web app, and handling boundary cases, indicating comprehensive annotation instructions."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction; Section A.2 Annotator Guidelines",
            "reasoning": "The paper describes specific annotation categories and labels with clear definitions that serve as rubrics, such as scene density being a binary label, and visual quality labeled categorically with four discrete options (good, partially visible, barely visible, not visible). The onboarding and guidelines likely include these rubrics to ensure consistency across annotators."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section 3.1 Dataset Construction; Appendix Section A.2",
            "reasoning": "Section 3.1 mentions the existence of detailed instructions for annotators, which typically include examples. Section A.2 states that tutorials and guidelines are provided, which by standard practice would include concrete examples to help annotators understand the annotation process and to handle boundary cases properly. Also, screenshots of the annotation tool (Figure 5) and visual examples (Figure 6) support example-based guidance."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides evidence of dedicated annotation guidelines, instructions, rubrics, and examples for ensuring quality data annotation, excluding the possibility of no guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "pUcTrjRLOM-rubric-2",
    "token_usage": {
      "prompt_tokens": 34533,
      "completion_tokens": 337,
      "total_tokens": 34870
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2.2 Data Annotation and Appendix C.4 Details of Human Annotation",
            "reasoning": "The paper describes detailed annotation instructions for preference annotation, wherein GPT-4 annotates preferences ranking various model completions and human experts correct and validate these preferences using a custom WebUI. The criteria for human annotation include honesty, helpfulness, harmlessness, and length bias, emphasizing comprehensive instructions for consistent labeling."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 2.2.2 Preference Annotation and Appendix C.4 Details of Human Annotation",
            "reasoning": "The paper provides a detailed 5-point scale rubric for evaluating model completions, scoring them from 1 (inadequate) to 5 (excellent) based on scientific accuracy, completeness, clarity, and appropriateness in the biomedical context. These rubrics guide both GPT-4 automated preference annotation and human expert corrections."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix E UltraMedical Examples and Section C.4 Details of Human Annotation",
            "reasoning": "The appendix provides concrete examples of annotated questions and corresponding GPT-4 and GPT-3.5 scores, illustrating how instructions and annotations are applied. Additionally, Appendix C.4 shows screenshots of the human annotation WebUI, demonstrating examples of annotation tasks, thereby supporting example-based guidelines."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Since comprehensive annotation guidelines including instructions, rubrics, and examples are explicitly described and exemplified in the paper and appendices, the label of no model annotation guidelines does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "pYNl76onJL-rubric-2",
    "token_usage": {
      "prompt_tokens": 19790,
      "completion_tokens": 246,
      "total_tokens": 20036
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any instructions given to annotators or labelers for the dataset curation. The dataset is automatically collected from real user prompts and generated videos without human annotation guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of any rubric or scoring system used for annotating or labeling the data in the dataset. The information related to NSFW content is generated by an automated model (Detoxify), not human annotators following rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of annotation guideline usage or annotated examples to guide human labelers. The dataset is collected from existing user input and generated outputs, not annotated under model guidelines."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset VidProM is collected automatically from real user prompts on a Discord server and generated videos from diffusion models. No human annotation or labeling guided by model annotation guidelines, instructions, rubrics, or examples is involved or described. Annotation is not a process applied in the dataset construction."
          }
        }
      }
    ]
  },
  {
    "id": "qmvtDIfbmS-rubric-2",
    "token_usage": {
      "prompt_tokens": 27936,
      "completion_tokens": 453,
      "total_tokens": 28389
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2.3 and Section A.4.1",
            "reasoning": "The paper describes a detailed annotation process for cognitive evaluation questions (Section 3.2.3), including construction of reasoning chains and multi-choice questions. The methodology specifies how annotators use a truth manual and expert knowledge to form reasoning chains and generate questions with correct answers and distractors. Additionally, structured prompts that guide evaluations (Section A.4.1) provide clear instructions for annotators and evaluators on how to assess agent responses and assign scores."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 5.1 and Section A.4.1",
            "reasoning": "The paper provides explicit scoring formulas for evaluation metrics (Section 5.1) such as accuracy-based scores for multiple-choice perception categories and role-play interaction metrics with clearly defined scales (e.g., role-play index rated out of ten points). The evaluation prompts in Appendix A.4.1 further specify rating criteria and scoring scales for open-ended answers using GPT-4 and for analyzing dialogue content, implying rubric-based annotation guidelines for consistent scoring."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figures 3, 9, 10, and Appendix A.4.1",
            "reasoning": "The paper presents multiple examples of annotated data and reasoning chains (Figure 3, Figures 9 and 10), which illustrate how questions are structured and how reasoning steps are linked. In the appendix (A.4.1), the paper provides sample prompts and detailed templates used for guiding both the data annotation process and the evaluation of model outputs, serving as clear examples to annotators."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper clearly details the process of data annotation with instructions, rubrics, and examples for the newly introduced Chain of Evaluation dataset, thus model annotation guidelines are indeed provided."
          }
        }
      }
    ]
  },
  {
    "id": "r8PnfcWQol-rubric-2",
    "token_usage": {
      "prompt_tokens": 31457,
      "completion_tokens": 210,
      "total_tokens": 31667
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper describes a framework and dataset generation process for complex query answering on knowledge graphs but does not provide explicit instructions for human annotators or data labelers regarding annotation procedures or guidelines."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No detailed scoring rubrics or criteria for human annotation are provided. Evaluation metrics are designed for model performance assessment rather than annotation consistency or reproducibility guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "While the paper provides examples of query types and discusses query graph representations, it does not include explicit examples illustrating annotation guidelines or instructions for annotators."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The dataset EFO_k-CQA is generated through a computational framework involving enumeration, grounding, and CSP solving rather than manual annotation. Consequently, there are no model annotation guidelines such as instructions, rubrics, or examples described in the paper."
          }
        }
      }
    ]
  },
  {
    "id": "rdv2Fr6JTC-rubric-2",
    "token_usage": {
      "prompt_tokens": 21681,
      "completion_tokens": 259,
      "total_tokens": 21940
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any model annotation guidelines involving human annotators or model-based annotation instructions for labeling datasets. The focus is on proposing a methodology for graph data valuation using PC-Winter value, and experiments are conducted on pre-existing benchmark datasets."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or description of scoring rubrics or structured criteria for annotating or labeling data elements in the paper. The datasets used are standard benchmark graph datasets, and no annotations or scoring systems for human or model annotation are detailed."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No examples of annotation guidelines or illustrative annotation cases are provided in the paper or appendix. The experiments use existing datasets and no new annotated datasets or annotation processes with examples are introduced."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets nor provide any model annotation guidelines, instructions, rubrics, or examples for data labeling. All datasets used are pre-existing benchmark datasets, and the focus is on valuing graph data elements with the proposed PC-Winter method rather than on data annotation or labeling procedures."
          }
        }
      }
    ]
  },
  {
    "id": "rovpCs3ZEO-rubric-2",
    "token_usage": {
      "prompt_tokens": 24635,
      "completion_tokens": 347,
      "total_tokens": 24982
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section E.2 Composition and subsections F-M Task Descriptions and Access/Preprocessing details",
            "reasoning": "The paper provides detailed descriptions for each dataset task, including task definitions, data sources, and preprocessing procedures (e.g., Sections E.2, F, G, H, I, J, K, L, M). These sections include instructions regarding data acquisition, task formulation, and pre-processing steps, which constitute detailed instructions supporting the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not present any explicit detailed scoring rubrics, guidelines, or grading schemes for annotations or labeling. While the tasks have labels, no information about annotation rubrics or scales is included."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Sections F.4, G.4, H.4, I.4, J.3, K.4, L.4, M.4",
            "reasoning": "For many tasks, the paper provides sample data visualizations and example inputs and outputs (e.g., sample images, ECG signals, question-answer pairs), illustrating what annotated data instances look like (e.g., Figures in Sections F.4, G.4, H.4, I.4, J.3, K.4, L.4, M.4). This amounts to clear examples aiding annotation understanding."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides explicit and detailed information about dataset creation, task descriptions, and sample data for all datasets; thus, it cannot be considered as having no annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "s1K5Z5QPog-rubric-2",
    "token_usage": {
      "prompt_tokens": 13239,
      "completion_tokens": 264,
      "total_tokens": 13503
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any detailed instructions specifically aimed at model annotation guidelines for labeling data. The focus is on data collection, benchmarking, and metrics rather than guidelines for annotators."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any rubrics or scoring systems designed for human annotators to label the data. Metrics described are for model evaluation, not for annotation."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No illustrative examples or annotation walkthroughs are provided in the paper for the purpose of guiding annotation."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper describes a benchmark dataset comprised of reanalysis and simulation data with physics-based and probabilistic metrics for evaluation of forecasting models, but provides no model annotation guidelines, instructions, rubrics, or examples for data labeling. The dataset is mostly reanalysis and forecast data rather than human-labeled annotations."
          }
        }
      }
    ]
  },
  {
    "id": "t9aThFL1lE-rubric-2",
    "token_usage": {
      "prompt_tokens": 30263,
      "completion_tokens": 392,
      "total_tokens": 30655
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix A, Section A.3 Collection and Labeling Process",
            "reasoning": "The paper describes that each image in the UNLEARNCANVAS dataset is annotated with both its style and object classes, and the images are labeled with a prompt 'An image of object in style.' This indicates that the dataset construction involved clear instructions on how seed images are stylized and labeled, providing detailed instructions for the annotation process."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3, 'Evaluation pipeline via UNLEARNCANVAS' (Phase IV: MU performance assessment)",
            "reasoning": "The dataset enables evaluation through quantitative metrics such as Unlearning Accuracy (UA), In-domain Retain Accuracy (IRA), and Cross-domain Retain Accuracy (CRA), with precise definitions of these metrics and how they are computed via classification accuracy. This reflects the presence of clear scoring rubrics defining how annotations (style/object labels) are to be used for quantitative evaluation."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix E, Sections E.1 and E.2",
            "reasoning": "The paper provides extensive visual examples illustrating the styles and object classes in UNLEARNCANVAS (Fig. A9, A10), as well as visualizations of unlearning performance with annotated examples of generated images in different styles and objects. These are clear examples demonstrating the annotation guidelines and usage."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides detailed descriptions of the annotation process, evaluation metrics (rubrics), and visual examples, indicating that model annotation guidelines are present."
          }
        }
      }
    ]
  },
  {
    "id": "tPsw4NeLZx-rubric-2",
    "token_usage": {
      "prompt_tokens": 21640,
      "completion_tokens": 233,
      "total_tokens": 21873
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Recording Setup and Workflow",
            "reasoning": "The paper describes a process where each sign video is supervised and checked by at least one Auslan expert to ensure the precision of the sign language expression, indicating detailed instructions and quality control during recording."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any evidence or mention of detailed scoring rubrics or explicit annotation rubrics for the labeling process."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no examples or exemplars of annotated signs or annotation guidelines provided in the paper or appendix."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides evidence of annotation instructions via expert supervision, hence it is not the case that no guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "tWvVtOW0qg-rubric-2",
    "token_usage": {
      "prompt_tokens": 15928,
      "completion_tokens": 254,
      "total_tokens": 16182
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any detailed instructions regarding model annotation guidelines for data labeling. The focus is on federated data measurements and benchmarking various measures, without describing annotation procedures or instructions."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention of detailed rubrics or scoring systems to guide subjective annotations. The paper evaluates computational measures without employing or discussing explicit annotation rubrics."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not include examples of annotation guidelines or demonstrate example annotations related to model-based data labeling. The examples in the paper are related to dataset images for experiments, not annotation guidelines."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "No model annotation guidelines are described or provided for the new datasets introduced or used. The paper's focus is on computational data measurements and benchmarks, not on human annotation processes."
          }
        }
      }
    ]
  },
  {
    "id": "vyraA7xt4c-rubric-2",
    "token_usage": {
      "prompt_tokens": 20745,
      "completion_tokens": 260,
      "total_tokens": 21005
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 2, 'Mercury Datasets' and Appendix A.3",
            "reasoning": "The paper describes a detailed data schema for each task that includes a task description with instructions, constraints, examples, and a test case generator. There are explicit task filters and difficulty stratifications, indicating detailed annotation instructions for task inclusion and evaluation."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any formal or detailed rubrics for model annotation or scoring other than the automatic evaluation metrics (Pass and Beyond). The evaluation metrics are computational and do not serve as annotation rubrics guiding model annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 7 and Appendix A.7",
            "reasoning": "The paper provides clear example tasks with problem descriptions, example input/output, and multiple sample solutions illustrating variance in efficiency and correctness. These function as clear examples in the annotation guidelines for understanding and evaluation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "As model annotation guidelines exist in the form of detailed task descriptions, test case generation processes, difficulty level stratifications, and examples, it is incorrect to state that no guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "wOmtZ5FgMH-rubric-2",
    "token_usage": {
      "prompt_tokens": 33285,
      "completion_tokens": 336,
      "total_tokens": 33621
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Data Collection and Construction, Section E.1 Prompt Templates",
            "reasoning": "The paper details the process of probe construction using GPT-4 with carefully designed prompt templates for generating different types of questions (cloze-format, QA style, adversarial attack probes, and neighbor probes). These prompt templates serve as detailed instructions guiding the generation of annotations in a structured and unambiguous way, ensuring clarity and consistency in annotation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.3 Evaluation Framework, Section F.1 Evaluation Prompts",
            "reasoning": "The evaluation metrics and methods for assessing responses are clearly specified, including ROUGE-L recall, membership inference attack scores, and accuracy or F1 scores for various tasks. Additionally, the adversarial attack types and evaluation prompts in Appendix F.1 define a detailed rubric for scoring model responses, critical for consistent evaluation of subjective and complex annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Section E.2 Data Examples, Section J Case Study",
            "reasoning": "The paper provides explicit annotated examples of different probe types (cloze, QA, adversarial), as shown in Appendix E.2 and the case studies in Section J. These examples demonstrate the expected format and style of annotation, illustrating the guidelines and serving as reference points for data labeling and evaluation."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper provides detailed descriptions of the annotation guidelines including instructions, rubrics, and examples for dataset construction and evaluation, thus N/A does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "wmO7z57wNK-rubric-2",
    "token_usage": {
      "prompt_tokens": 17343,
      "completion_tokens": 200,
      "total_tokens": 17543
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not introduce new datasets created by the authors, nor does it provide any detailed instructions for model-based annotation guidelines for labeling. The datasets used are existing ones from prior work."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No detailed scoring rubrics are described for annotation of any new datasets, as the paper focuses on benchmarking model compression algorithms rather than creating or annotating new data."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There are no examples provided related to annotation guidelines because the paper does not introduce any new datasets requiring annotation."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets or provide model-based annotation guidelines for data labeling. All datasets referenced are publicly available existing datasets; thus, no annotation guidelines are provided or needed in the context of this benchmark."
          }
        }
      }
    ]
  },
  {
    "id": "x8RgF2xQTj-rubric-2",
    "token_usage": {
      "prompt_tokens": 40954,
      "completion_tokens": 249,
      "total_tokens": 41203
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper is a benchmarking study that reimplements and evaluates existing uncertainty quantification methods on established datasets (ImageNet-1k and CIFAR-10), using existing ground truths for aleatoric uncertainty (human annotator label distributions). It does not introduce any new datasets, nor does it provide instructions for annotation or labeling new data."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No new datasets are introduced by the authors, and thus no rubric or scoring guidelines for annotation are described anywhere in the paper or appendix."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "Since no new datasets or annotation tasks are introduced, there are no examples of annotation guidelines or labeling provided."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper does not introduce any new datasets requiring annotations; it uses existing datasets (ImageNet-1k, ImageNet-ReaL, CIFAR-10, CIFAR-10H) with existing labels. Consequently, there are no model annotation guidelines, instructions, rubrics, or examples provided by the authors for data labeling."
          }
        }
      }
    ]
  },
  {
    "id": "y09S5rdaWY-rubric-2",
    "token_usage": {
      "prompt_tokens": 20092,
      "completion_tokens": 291,
      "total_tokens": 20383
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Appendix A (Details of Data Collecting) and Section 3.2 (Expert Dataset)",
            "reasoning": "The paper describes the data collection process in detail, including automatic expert model data collection using Think2Drive and manual filtering to ensure quality (Appendix A). The annotations cover multiple aspects such as 3D bounding boxes, semantic and instance segmentation, depth, HD-Map elements, as well as RL value estimations, demonstrating comprehensive instructions and guidelines for annotation creation and handling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not describe any scoring rubrics or detailed annotation rules for subjective or ambiguous labels during data annotation. The dataset annotations are generated via the expert model and CARLA APIs, rather than human annotators applying rubrics."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Figure 3 and Appendix G (Description of Scenarios)",
            "reasoning": "The paper provides examples of sensor settings and annotation types in Figure 3, and explicitly details the 44 scenario types in Appendix G. This serves as concrete examples that guide the annotation content and scenario definitions, supporting clarity and consistency in the dataset."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset is accompanied by thorough instructions and examples, so it is not that no annotation guidelines are provided."
          }
        }
      }
    ]
  },
  {
    "id": "y10DM6R2r3-rubric-2",
    "token_usage": {
      "prompt_tokens": 27790,
      "completion_tokens": 434,
      "total_tokens": 28224
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Construction Pipeline and Appendix A.1 Dataset Construction Details",
            "reasoning": "The paper describes in detail the dataset construction pipeline including multiple steps such as initial filtering based on model answers, question collection from various sources, option augmentation from 4 to 10 options using GPT-4-Turbo, and two rounds of expert review to verify question correctness and distractor validity. The process includes detailed instructions for transforming problems into multiple-choice questions and guidelines for generating plausible distractors, as specified by the prompts presented in Table 6 in Appendix A.1. This indicates the presence of detailed instructions in the annotation guidelines to ensure high-quality question and option creation."
          },
          "Has Rubrics": {
            "is_applicable": true,
            "reference": "Section 3.2 Dataset Construction Pipeline and Table 1: Distribution of Issues Identified during the Expert Review Process",
            "reasoning": "The expert review process operates on defined criteria that include classification of errors into categories such as Incorrect Answers, False Negative Options, and Bad Questions, with actions to remove problematic questions or options accordingly. This structured evaluation and error categorization functions as a rubric that guides annotators in deciding the acceptability of questions and options, ensuring consistency and reproducibility in annotations."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix A.2 5-shot CoT Prompt example and Table 6 Prompt Instructions for Dataset Construction Pipeline",
            "reasoning": "The paper provides clear examples of prompts used with language models for question transformation and augmentation, including JSON-formatted responses expected from GPT-4-Turbo during option creation, as well as detailed 5-shot chain-of-thought examples in physics demonstrating both the question format, reasoning steps, and final answer extraction. These provide concrete examples that serve as references in the annotation guidelines, facilitating understanding and standardization among annotators and model-based procedures."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The paper explicitly details an elaborate process involving instructions, rubrics, and examples for dataset construction and annotation. Hence, it does not lack model annotation guidelines."
          }
        }
      }
    ]
  },
  {
    "id": "yS1dUkQFnu-rubric-2",
    "token_usage": {
      "prompt_tokens": 17605,
      "completion_tokens": 182,
      "total_tokens": 17787
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper provides comprehensive details about datasets and evaluation, but does not include any description of model annotation guidelines or detailed instructions for data labeling."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No scoring rubrics or criteria for subjective annotation tasks are described in the paper in relation to data annotation or labeling guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide any illustrative examples regarding annotation processes or annotation guidelines for the datasets."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "The paper focuses on benchmarking PETL algorithms on existing datasets and does not present any new annotation guidelines, instructions, rubrics, or examples for data labeling. All datasets appear to be pre-existing benchmarks without newly introduced annotation processes described."
          }
        }
      }
    ]
  },
  {
    "id": "yUEBXN3cvX-rubric-2",
    "token_usage": {
      "prompt_tokens": 20666,
      "completion_tokens": 400,
      "total_tokens": 21066
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.1 Data collection",
            "reasoning": "The paper describes detailed instructions given to annotators during data collection. Annotators were provided with generic feature descriptions for Android apps, and were instructed to create task demonstrations by controlling an Android device through a web app interface. They were asked to produce high-level task descriptions that are clear and unambiguous, include the target app name if needed, and to type short natural language descriptions for each action (low-level instructions). Annotators also underwent a training phase and received feedback to ensure consistent data annotation. This detailed set of instructions indicates the presence of model annotation guidelines including instructions for producing both high-level and low-level task annotations."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not mention any explicit detailed rubrics or scoring guidelines for annotators to assess or score annotations. While the paper describes that annotators tag demonstrations as successful, infeasible, or failed, there is no indication of detailed rubrics defining annotation quality criteria or scoring measures."
          },
          "Has Examples": {
            "is_applicable": true,
            "reference": "Appendix B.1 Data collection and Figure 6 Examples of episodes",
            "reasoning": "The paper includes example episodes of the dataset in Figure 6 and describes the data format with sample fields. The appendix details example step instructions and UI interactions, illustrating what the data looks like and how instructions correspond to actions. This inclusion serves as examples for annotators to understand the annotation format and expected quality."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "The dataset clearly involves human annotation with provided instructions and examples, hence this label does not apply."
          }
        }
      }
    ]
  },
  {
    "id": "z64azPC6Nl-rubric-2",
    "token_usage": {
      "prompt_tokens": 18205,
      "completion_tokens": 241,
      "total_tokens": 18446
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": true,
            "reference": "Section 3.3 Annotation and Appendix A.2 Details of Annotation",
            "reasoning": "The paper describes a detailed annotation process involving initial forced alignment with Montreal Forced Aligner and subsequent manual corrections by annotators with musical backgrounds using Praat. Annotators are instructed to correct phoneme boundaries, word and phoneme errors, and mark unvoiced regions. They further perform phoneme-level annotations of six singing techniques and label global style attributes. Annotation guidelines include instructions to consider auditory perception and use specific phoneme standards per language."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper and appendices describe the annotation process but do not include any detailed scoring rubrics or formal criteria for annotation scoring or quality assessment in the guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "No explicit examples of annotation from the guidelines or clear examples illustrating annotation decisions are provided in the main text or appendices."
          },
          "N/A": {
            "is_applicable": false,
            "reasoning": "Detailed annotation instructions are clearly provided in the paper and appendices, indicating guidelines exist."
          }
        }
      }
    ]
  },
  {
    "id": "zogaeVpbaE-rubric-2",
    "token_usage": {
      "prompt_tokens": 21490,
      "completion_tokens": 243,
      "total_tokens": 21733
    },
    "response": [
      {
        "model_annot_guidelines": {
          "Has Instructions": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper introduces DEVBENCH as a new benchmark composed of multiple existing developmental tasks with their own datasets; it does not describe new model annotation guidelines or instructions provided to human annotators for data labeling within the paper itself."
          },
          "Has Rubrics": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "There is no mention or description in the paper of scoring rubrics or detailed guidelines that would apply to annotation or labeling of the datasets from the perspective of the model-based annotation guidelines."
          },
          "Has Examples": {
            "is_applicable": false,
            "reference": "",
            "reasoning": "The paper does not provide examples of annotation guidelines or annotated examples that would serve as model annotation instructions; instead, it uses existing human datasets with their own collection methods, without detailing model annotation examples."
          },
          "N/A": {
            "is_applicable": true,
            "reasoning": "DEVBENCH aggregates existing developmental behavioral datasets and evaluates models against human data without introducing or describing specific model annotation guidelines for data labeling. The paper focuses on evaluation metrics and comparison of model output to human behavioral responses, rather than describing annotation procedures or guidelines for creating new labeled data."
          }
        }
      }
    ]
  }
]